Issue_id,Type,Priority,Component,Title,Description,Status,Resolution,Duplicated_issue,Assignee,Reporter,Created_time,Updated_time,Resolved_time,Fix_version,Affects_version,Labels,Blocked_issue,Related_issue,Link
HADOOP-1,Task,Major,,initial import of code from Nutch,The initial code for Hadoop will be copied from Nutch.,Closed,Fixed,,Doug Cutting,Doug Cutting,Wed; 1 Feb 2006 02:54:35 +0000,Thu; 17 Sep 2015 06:00:01 +0000,Sat; 4 Feb 2006 05:57:14 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-1
HADOOP-2,Bug,Major,,Reused Keys and Values fail with a Combiner,If the map function reuses the key or value by destructively modifying it after the output.collect(key;value) call and your application uses a combiner; the data is corrupted by having lots of instances with the last key or value.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Sat; 4 Feb 2006 05:54:30 +0000,Tue; 11 Aug 2015 07:18:50 +0000,Fri; 31 Mar 2006 05:11:16 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-2
HADOOP-3,Bug,Minor,,Output directories are not cleaned up before the reduces run,The output directory for the reduces is not cleaned up and therefore if you can see left overs from previous runs; if they had more reduces. For example; if you run the application once with reduces=10 and then rerun with reduces=8; your output directory will have frag00000 to frag00009 with the first 8 fragments from the second run and the last 2 fragments from the first run.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Sat; 4 Feb 2006 05:58:10 +0000,Tue; 5 Dec 2017 04:19:27 +0000,Thu; 23 Mar 2006 04:14:18 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-3
HADOOP-4,Improvement,Major,,tool to mount dfs on linux,"This is a FUSE module for Hadoop's HDFS.It allows one to mount HDFS as a Unix filesystem and optionally exportthat mount point to other machines.rmdir; mv; mkdir; rm are all supported. just not cp; touch; ...; but actual writes require: https://issues.apache.org/jira/browse/HADOOP-3485For the most up-to-date documentation; see: http://wiki.apache.org/hadoop/MountableHDFSBUILDING:Requirements:   1. a Linux kernel  2.6.9 or a kernel module from FUSE - i.e.; you   compile it yourself and then modprobe it. Better off with the   former option if possible.  (Note for now if you use the kernel   with fuse included; it doesn't allow you to export this through NFS   so be warned. See the FUSE email list for more about this.)   2. FUSE should be installed in /usr/local or FUSE_HOME ant   environment variableTo build:   1. in HADOOP_HOME: ant compile-contrib -Dcompile.c++=1 -Dfusedfs=1 -Dlibhdfs=1NOTE: for amd64 architecture; libhdfs will not compile unless you editthe Makefile in src/c++/libhdfs/Makefile and set OS_ARCH=amd64(probably the same for others too).--------------------------------------------------------------------------------CONFIGURING:Look at all the paths in fuse_dfs_wrapper.sh and either correct themor set them in your environment before running. (note for automountand mount as root; you probably cannnot control the environment; sobest to set them in the wrapper)INSTALLING:1. mkdir /mnt/dfs (or wherever you want to mount it)2. fuse_dfs_wrapper.sh dfs://hadoop_server1.foo.com:9000 /mnt/dfs -d; and from another terminal; try ls /mnt/dfsIf 2 works; try again dropping the debug mode; i.e.; -d(note - common problems are that you don't have libhdfs.so orlibjvm.so or libfuse.so on your LD_LIBRARY_PATH; and your CLASSPATHdoes not contain hadoop and other required jars.)--------------------------------------------------------------------------------DEPLOYING:in a root shell do the following:1. add the following to /etc/fstab -  fuse_dfs#dfs://hadoop_server.foo.com:9000 /mnt/dfs fuse  allow_other;rw 0 02. mount /mnt/dfs Expect problems with not finding fuse_dfs. You will   need to probably add this to /sbin and then problems finding the   above 3 libraries. Add these using ldconfig.--------------------------------------------------------------------------------EXPORTING:Add the following to /etc/exports:  /mnt/hdfs *.foo.com(no_root_squash;rw;fsid=1;sync)NOTE - you cannot export this with a FUSE module built into the kernel	e.g.; kernel 2.6.17. For info on this; refer to the FUSE wiki.--------------------------------------------------------------------------------ADVANCED:you may want to ensure certain directories cannot be deleted from theshell until the FS has permissions. You can set this in the build.xmlfile in src/contrib/fuse-dfs/build.xml",Closed,Fixed,HADOOP-17,Pete Wyckoff,John Xing,Sun; 5 Feb 2006 03:55:29 +0000,Thu; 19 Jul 2012 11:12:58 +0000,Thu; 29 May 2008 20:18:37 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-4
HADOOP-5,Bug,Minor,,need commons-logging-api jar file,The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.,Closed,Fixed,,Unassigned,Owen O'Malley,Tue; 7 Feb 2006 03:04:15 +0000,Thu; 3 Aug 2006 17:46:26 +0000,Tue; 7 Feb 2006 04:28:13 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5
HADOOP-6,Bug,Minor,,missing build directory in classpath,When running a developer build; the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory.,Closed,Fixed,,Unassigned,Owen O'Malley,Tue; 7 Feb 2006 03:19:03 +0000,Thu; 3 Aug 2006 17:46:26 +0000,Tue; 7 Feb 2006 03:34:35 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-6
HADOOP-7,Bug,Major,,MapReduce has a series of problems concerning task-allocation to worker nodes,"The MapReduce JobTracker is not great at allocating tasks to TaskTracker worker nodes.Here are the problems:1) There is no speculative execution of tasks2) Reduce tasks must wait until all map tasks are completed before doing any work3) TaskTrackers don't distinguish between Map and Reduce jobs.  Also; the number oftasks at a single node is limited to some constant.  That means you can get weird deadlockproblems upon machine failure.  The reduces take up all the available execution slots; but theydon't do productive work; because they're waiting for a map task to complete.  Of course; thatmap task won't even be started until the reduce tasks finish; so you can see the problem...4) The JobTracker is so complicated that it's hard to fix any of these.The right solution is a rewrite of the JobTracker to be a lot more flexible in task handling.It has to be a lot simpler.  One way to make it simpler is to add an abstraction I'll call""TaskInProgress"".  Jobs are broken into chunks called TasksInProgress.  All the TaskInProgressobjects must be complete; somehow; before the Job is complete.A single TaskInProgress can be executed by one or more Tasks.  TaskTrackers are assigned Tasks.If a Task fails; we report it back to the JobTracker; where the TaskInProgress lives.  The TIP can thendecide whether to launch additional  Tasks or not.Speculative execution is handled within the TIP.  It simply launches multiple Tasks in parallel.  TheTaskTrackers have no idea that these Tasks are actually doing the same chunk of work.  The TIPis complete when any one of its Tasks are complete.",Closed,Fixed,,Unassigned,Mike Cafarella,Sat; 21 Jan 2006 05:40:53 +0000,Mon; 18 May 2015 04:11:26 +0000,Tue; 7 Feb 2006 03:38:43 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-7
HADOOP-8,Bug,Major,,NDFS DataNode advertises localhost as it's address,,Closed,Won't Fix,,Unassigned,Peter Sandstr√∂m,Sun; 24 Jul 2005 23:46:18 +0000,Mon; 18 May 2015 04:15:07 +0000,Fri; 24 Feb 2006 08:13:30 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-8
HADOOP-9,Bug,Minor,,mapred.local.dir  temp dir. space allocation limited by smallest area,"When mapred.local.dir is used to specify multiple  temp dir. areas; space allocation limited by smallest area because the temp dir. selection algorithm is ""round robin starting from a randomish point"".   When round robin is used with approximately constant sized chunks; the smallest area runs out of space first; and this is a fatal error. Workaround: only list local fs dirs in mapred.local.dir with similarly-sized available areas.I wrote a patch to JobConf (currenly being tested) which uses df to check available space (once a minute or less often) and then uses an efficient roulette selection to do allocation weighted by magnitude of available space.",Closed,Fixed,,Ari Rabkin,Paul Baclace,Tue; 17 Jan 2006 10:09:39 +0000,Sat; 6 Nov 2010 05:27:57 +0000,Tue; 12 Aug 2008 21:21:12 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-9
HADOOP-10,Bug,Trivial,,ndfs.replication is not documented within the nutch-default.xml configuration file.,ndfs.replication is not documented within the nutch-default.xml configuration file.,Closed,Fixed,,Unassigned,Rod Taylor,Fri; 14 Oct 2005 06:02:45 +0000,Wed; 8 Jul 2009 16:41:47 +0000,Tue; 7 Feb 2006 04:15:25 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10
HADOOP-12,Bug,Minor,,InputFormat used in job must be in JobTracker classpath (not loaded from job JAR),"During development; I've been creating/tweaking custom InputFormat implementations. However; when you try to run a job against a running cluster; you get:  Exception in thread ""main"" java.io.IOException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: my.custom.InputFormat          at org.apache.nutch.ipc.Client.call(Client.java:294)          at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)          at $Proxy0.submitJob(Unknown Source)          at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)          at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)          at com.parc.uir.wikipedia.WikipediaJob.main(WikipediaJob.java:85)This error goes away if I restart the TaskTrackers/JobTracker with a classpath which includes the needed code. Other classes (Mapper; Reducer) appear to be available out of the jar file specified in the JobConf; but not the InputFormat. Obviously; it's less than idea to have to restart the JobTracker whenever there's a change to a job-specific class.",Closed,Fixed,,Unassigned,Bryan Pendleton,Tue; 31 Jan 2006 07:00:17 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Fri; 10 Feb 2006 08:01:55 +0000,,,,,HADOOP-16,https://issues.apache.org/jira/browse/HADOOP-12
HADOOP-12,Bug,Minor,,InputFormat used in job must be in JobTracker classpath (not loaded from job JAR),"During development; I've been creating/tweaking custom InputFormat implementations. However; when you try to run a job against a running cluster; you get:  Exception in thread ""main"" java.io.IOException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: my.custom.InputFormat          at org.apache.nutch.ipc.Client.call(Client.java:294)          at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)          at $Proxy0.submitJob(Unknown Source)          at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)          at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)          at com.parc.uir.wikipedia.WikipediaJob.main(WikipediaJob.java:85)This error goes away if I restart the TaskTrackers/JobTracker with a classpath which includes the needed code. Other classes (Mapper; Reducer) appear to be available out of the jar file specified in the JobConf; but not the InputFormat. Obviously; it's less than idea to have to restart the JobTracker whenever there's a change to a job-specific class.",Closed,Fixed,,Unassigned,Bryan Pendleton,Tue; 31 Jan 2006 07:00:17 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Fri; 10 Feb 2006 08:01:55 +0000,,,,,HADOOP-16,https://issues.apache.org/jira/browse/HADOOP-12
HADOOP-16,Bug,Major,,RPC call times out while indexing map task is computing splits,"We've been using Nutch 0.8 (MapReduce) to perform some internet crawling. Things seemed to be going well until...060129 222409 Lost tracker 'tracker_56288'060129 222409 Task 'task_m_10gs5f' has been lost.060129 222409 Task 'task_m_10qhzr' has been lost.   ........   ........060129 222409 Task 'task_r_zggbwu' has been lost.060129 222409 Task 'task_r_zh8dao' has been lost.060129 222455 Server handler 8 on 8010 caught: java.net.SocketException: Socket closedjava.net.SocketException: Socket closed        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:99)        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)        at java.io.DataOutputStream.flush(DataOutputStream.java:106)        at org.apache.nutch.ipc.Server$Handler.run(Server.java:216)060129 222455 Adding task 'task_m_cia5po' to set for tracker 'tracker_56288'060129 223711 Adding task 'task_m_ffv59i' to set for tracker 'tracker_25647'I'm hoping that someone could explain why task_m_cia5po got added to tracker_56288 after this tracker was lost.The Crawl .main process died with the following output:060129 221129 Indexer: adding segment: /user/crawler/crawl-20060129091444/segments/20060129200246Exception in thread ""main"" java.io.IOException: timed out waiting for response    at org.apache.nutch.ipc.Client.call(Client.java:296)    at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)    at $Proxy1.submitJob(Unknown Source)    at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)    at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)    at org.apache.nutch.indexer.Indexer.index(Indexer.java:263)    at org.apache.nutch.crawl.Crawl.main(Crawl.java:127)However; it definitely seems as if the JobTracker is still waiting for the job to finish (no failed jobs).Doug Cutting's response:The bug here is that the RPC call times out while the map task is computing splits.  The fix is that the job tracker should not compute splits until after it has returned from the submitJob RPC.  Please submit a bug in Jira to help remind us to fix this.",Closed,Fixed,,Mike Cafarella,Chris Schneider,Wed; 1 Feb 2006 08:25:38 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Fri; 3 Mar 2006 08:09:22 +0000,,0.1.0,,,HADOOP-12,https://issues.apache.org/jira/browse/HADOOP-16
HADOOP-16,Bug,Major,,RPC call times out while indexing map task is computing splits,"We've been using Nutch 0.8 (MapReduce) to perform some internet crawling. Things seemed to be going well until...060129 222409 Lost tracker 'tracker_56288'060129 222409 Task 'task_m_10gs5f' has been lost.060129 222409 Task 'task_m_10qhzr' has been lost.   ........   ........060129 222409 Task 'task_r_zggbwu' has been lost.060129 222409 Task 'task_r_zh8dao' has been lost.060129 222455 Server handler 8 on 8010 caught: java.net.SocketException: Socket closedjava.net.SocketException: Socket closed        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:99)        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)        at java.io.DataOutputStream.flush(DataOutputStream.java:106)        at org.apache.nutch.ipc.Server$Handler.run(Server.java:216)060129 222455 Adding task 'task_m_cia5po' to set for tracker 'tracker_56288'060129 223711 Adding task 'task_m_ffv59i' to set for tracker 'tracker_25647'I'm hoping that someone could explain why task_m_cia5po got added to tracker_56288 after this tracker was lost.The Crawl .main process died with the following output:060129 221129 Indexer: adding segment: /user/crawler/crawl-20060129091444/segments/20060129200246Exception in thread ""main"" java.io.IOException: timed out waiting for response    at org.apache.nutch.ipc.Client.call(Client.java:296)    at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)    at $Proxy1.submitJob(Unknown Source)    at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)    at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)    at org.apache.nutch.indexer.Indexer.index(Indexer.java:263)    at org.apache.nutch.crawl.Crawl.main(Crawl.java:127)However; it definitely seems as if the JobTracker is still waiting for the job to finish (no failed jobs).Doug Cutting's response:The bug here is that the RPC call times out while the map task is computing splits.  The fix is that the job tracker should not compute splits until after it has returned from the submitJob RPC.  Please submit a bug in Jira to help remind us to fix this.",Closed,Fixed,,Mike Cafarella,Chris Schneider,Wed; 1 Feb 2006 08:25:38 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Fri; 3 Mar 2006 08:09:22 +0000,,0.1.0,,,HADOOP-12,https://issues.apache.org/jira/browse/HADOOP-16
HADOOP-16,Bug,Major,,RPC call times out while indexing map task is computing splits,"We've been using Nutch 0.8 (MapReduce) to perform some internet crawling. Things seemed to be going well until...060129 222409 Lost tracker 'tracker_56288'060129 222409 Task 'task_m_10gs5f' has been lost.060129 222409 Task 'task_m_10qhzr' has been lost.   ........   ........060129 222409 Task 'task_r_zggbwu' has been lost.060129 222409 Task 'task_r_zh8dao' has been lost.060129 222455 Server handler 8 on 8010 caught: java.net.SocketException: Socket closedjava.net.SocketException: Socket closed        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:99)        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)        at java.io.DataOutputStream.flush(DataOutputStream.java:106)        at org.apache.nutch.ipc.Server$Handler.run(Server.java:216)060129 222455 Adding task 'task_m_cia5po' to set for tracker 'tracker_56288'060129 223711 Adding task 'task_m_ffv59i' to set for tracker 'tracker_25647'I'm hoping that someone could explain why task_m_cia5po got added to tracker_56288 after this tracker was lost.The Crawl .main process died with the following output:060129 221129 Indexer: adding segment: /user/crawler/crawl-20060129091444/segments/20060129200246Exception in thread ""main"" java.io.IOException: timed out waiting for response    at org.apache.nutch.ipc.Client.call(Client.java:296)    at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)    at $Proxy1.submitJob(Unknown Source)    at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)    at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)    at org.apache.nutch.indexer.Indexer.index(Indexer.java:263)    at org.apache.nutch.crawl.Crawl.main(Crawl.java:127)However; it definitely seems as if the JobTracker is still waiting for the job to finish (no failed jobs).Doug Cutting's response:The bug here is that the RPC call times out while the map task is computing splits.  The fix is that the job tracker should not compute splits until after it has returned from the submitJob RPC.  Please submit a bug in Jira to help remind us to fix this.",Closed,Fixed,,Mike Cafarella,Chris Schneider,Wed; 1 Feb 2006 08:25:38 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Fri; 3 Mar 2006 08:09:22 +0000,,0.1.0,,,HADOOP-12,https://issues.apache.org/jira/browse/HADOOP-16
HADOOP-17,New Feature,Major,fs,tool to mount ndfs on linux,tool to mount ndfs on linux. It depends on fuse and fuse-j.,Closed,Duplicate,HADOOP-4,Unassigned,John Xing,Fri; 3 Feb 2006 15:59:31 +0000,Thu; 3 Aug 2006 17:46:25 +0000,Tue; 7 Feb 2006 04:18:42 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-17
HADOOP-18,Bug,Critical,,Crash with multiple temp directories,A brief read of the code indicated it may be possible to use multiple local directories using something like the below:  property    namemapred.local.dir/name    value/local;/local1;/local2/value    descriptionThe local directory where MapReduce stores intermediate    data files.    /description  /propertyThis failed with the below exception during either the generate or update phase (not entirely sure which).java.lang.ArrayIndexOutOfBoundsException        at java.util.zip.CRC32.update(CRC32.java:51)        at org.apache.nutch.fs.NFSDataInputStream$Checker.read(NFSDataInputStream.java:92)        at org.apache.nutch.fs.NFSDataInputStream$PositionCache.read(NFSDataInputStream.java:156)        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)        at java.io.BufferedInputStream.read(BufferedInputStream.java:313)        at java.io.DataInputStream.readFully(DataInputStream.java:176)        at org.apache.nutch.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:55)        at org.apache.nutch.io.DataOutputBuffer.write(DataOutputBuffer.java:89)        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:378)        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:301)        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:323)        at org.apache.nutch.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:60)        at org.apache.nutch.segment.SegmentReader$InputFormat$1.next(SegmentReader.java:80)        at org.apache.nutch.mapred.MapTask$2.next(MapTask.java:106)        at org.apache.nutch.mapred.MapRunner.run(MapRunner.java:48)        at org.apache.nutch.mapred.MapTask.run(MapTask.java:116)        at org.apache.nutch.mapred.TaskTracker$Child.main(TaskTracker.java:604),Closed,Invalid,,Unassigned,Rod Taylor,Wed; 11 Jan 2006 02:45:05 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Tue; 7 Mar 2006 06:30:32 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-18
HADOOP-19,Bug,Critical,,Datanode corruption,Our admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.This happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.The ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content; where they had about 150GB not all that much earlier.I think the solution is improved interlocking within the data directory itself (file locked with flock or something similar).,Closed,Fixed,HADOOP-56,Doug Cutting,Rod Taylor,Fri; 7 Oct 2005 11:46:42 +0000,Wed; 8 Jul 2009 16:41:48 +0000,Wed; 29 Mar 2006 03:16:00 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-19
HADOOP-20,Improvement,Major,,Mapper; Reducer need an occasion to cleanup after the last record is processed.,"Mapper; Reducer need an occasion to do some cleanup after the last record is processed.Proposal (patch attached)in interface Mapper: add method void finished();in interface Reducer: add method void finished();finished() methods are called from MapTask; CombiningCollector; ReduceTask.------------Known limitation: Fetcher (a multithreaded MapRunnable) does not call finished().This is not currently a problem bec. fetcher Map/Reduce modules do not do anything in finished().The right way to add finished() support to Fetcher would be to wait for all threads to finish; then do:     if (collector instanceof CombiningCollector) ((CombiningCollector)collector).finished();------------patch begins: (svn trunk)Index: src/test/org/apache/nutch/mapred/MapredLoadTest.java=================================================================== src/test/org/apache/nutch/mapred/MapredLoadTest.java	(revision 374781)+++ src/test/org/apache/nutch/mapred/MapredLoadTest.java	(working copy)@@ -69;6 +69;8 @@                 out.collect(new IntWritable(Math.abs(r.nextInt())); new IntWritable(randomVal));             }         }+        public void finished() {+        }     }     static class RandomGenReducer implements Reducer {         public void configure(JobConf job) {@@ -81;6 +83;8 @@                 out.collect(new UTF8("""" + val); new UTF8(""""));             }         }+        public void finished() {+        }     }     static class RandomCheckMapper implements Mapper {         public void configure(JobConf job) {@@ -92;6 +96;8 @@              out.collect(new IntWritable(Integer.parseInt(str.toString().trim())); new IntWritable(1));         }+        public void finished() {+        }     }     static class RandomCheckReducer implements Reducer {         public void configure(JobConf job) {@@ -106;6 +112;8 @@             }             out.collect(new IntWritable(keyint); new IntWritable(count));         }+        public void finished() {+        }     }     int range;Index: src/test/org/apache/nutch/fs/TestNutchFileSystem.java=================================================================== src/test/org/apache/nutch/fs/TestNutchFileSystem.java	(revision 374783)+++ src/test/org/apache/nutch/fs/TestNutchFileSystem.java	(working copy)@@ -155;6 +155;8 @@       reporter.setStatus(""wrote "" + name);     }+    +    public void finished() {}   }   public static void writeTest(NutchFileSystem fs; boolean fastCheck)@@ -247;6 +249;9 @@       reporter.setStatus(""read "" + name);     }+    +    public void finished() {}+       }   public static void readTest(NutchFileSystem fs; boolean fastCheck)@@ -339;6 +344;9 @@         in.close();       }     }+    +    public void finished() {}+       }   public static void seekTest(NutchFileSystem fs; boolean fastCheck)Index: src/java/org/apache/nutch/indexer/DeleteDuplicates.java=================================================================== src/java/org/apache/nutch/indexer/DeleteDuplicates.java	(revision 374776)+++ src/java/org/apache/nutch/indexer/DeleteDuplicates.java	(working copy)@@ -225;6 +225;7 @@         }       }     }+    public void finished() {}   }   private NutchFileSystem fs;@@ -265;6 +266;8 @@       reader.close();     }   }+  +  public void finished() {}   /** Write nothing. */   public RecordWriter getRecordWriter(final NutchFileSystem fs;Index: src/java/org/apache/nutch/indexer/Indexer.java=================================================================== src/java/org/apache/nutch/indexer/Indexer.java	(revision 374778)+++ src/java/org/apache/nutch/indexer/Indexer.java	(working copy)@@ -227;6 +227;8 @@     output.collect(key; new ObjectWritable(doc));   }+  +  public void finished() {}   public void index(File indexDir; File crawlDb; File linkDb; File[] segments)     throws IOException {Index: src/java/org/apache/nutch/segment/SegmentReader.java===================================================================--- src/java/org/apache/nutch/segment/SegmentReader.java	(revision 374778)+++ src/java/org/apache/nutch/segment/SegmentReader.java	(working copy)@@ -143;7 +143;9 @@     }     output.collect(key; new ObjectWritable(dump.toString()));   }-+  +  public void finished() {}+     public void reader(File segment) throws IOException {     LOG.info(""Reader: segment: "" + segment);Index: src/java/org/apache/nutch/mapred/Mapper.java=================================================================== src/java/org/apache/nutch/mapred/Mapper.java	(revision 374737)+++ src/java/org/apache/nutch/mapred/Mapper.java	(working copy)@@ -39;4 +39;9 @@   void map(WritableComparable key; Writable value;            OutputCollector output; Reporter reporter)     throws IOException;++  /** Called after the last {@link #map} call on this Mapper object.+      Typical implementations do nothing.+  */+  void finished(); }Index: src/java/org/apache/nutch/mapred/lib/RegexMapper.java=================================================================== src/java/org/apache/nutch/mapred/lib/RegexMapper.java	(revision 374737)+++ src/java/org/apache/nutch/mapred/lib/RegexMapper.java	(working copy)@@ -53;4 +53;5 @@       output.collect(new UTF8(matcher.group(group)); new LongWritable(1));     }   }+  public void finished() {} }Index: src/java/org/apache/nutch/mapred/lib/InverseMapper.java=================================================================== src/java/org/apache/nutch/mapred/lib/InverseMapper.java	(revision 374737)+++ src/java/org/apache/nutch/mapred/lib/InverseMapper.java	(working copy)@@ -38;4 +38;6 @@     throws IOException {     output.collect((WritableComparable)value; key);   }++  public void finished() {} }Index: src/java/org/apache/nutch/mapred/lib/IdentityReducer.java=================================================================== src/java/org/apache/nutch/mapred/lib/IdentityReducer.java	(revision 374737)+++ src/java/org/apache/nutch/mapred/lib/IdentityReducer.java	(working copy)@@ -42;4 +42;5 @@     }   }+  public void finished() {} }Index: src/java/org/apache/nutch/mapred/lib/IdentityMapper.java=================================================================== src/java/org/apache/nutch/mapred/lib/IdentityMapper.java	(revision 374737)+++ src/java/org/apache/nutch/mapred/lib/IdentityMapper.java	(working copy)@@ -39;4 +39;5 @@     output.collect(key; val);   }+  public void finished() {} }Index: src/java/org/apache/nutch/mapred/lib/LongSumReducer.java=================================================================== src/java/org/apache/nutch/mapred/lib/LongSumReducer.java	(revision 374737)+++ src/java/org/apache/nutch/mapred/lib/LongSumReducer.java	(working copy)@@ -47;4 +47;6 @@     // output sum     output.collect(key; new LongWritable(sum));   }++  public void finished() {} }Index: src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java=================================================================== src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java	(revision 374737)+++ src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java	(working copy)@@ -50;4 +50;6 @@       output.collect(new UTF8(st.nextToken()); new LongWritable(1));     }   }++  public void finished() {} }Index: src/java/org/apache/nutch/mapred/ReduceTask.java=================================================================== src/java/org/apache/nutch/mapred/ReduceTask.java	(revision 374781)+++ src/java/org/apache/nutch/mapred/ReduceTask.java	(working copy)@@ -275;6 +275;7 @@       }     } finally {+      reducer.finished();       in.close();       lfs.delete(new File(sortedFile));           // remove sorted       out.close(reporter);Index: src/java/org/apache/nutch/mapred/MapTask.java=================================================================== src/java/org/apache/nutch/mapred/MapTask.java	(revision 374737)+++ src/java/org/apache/nutch/mapred/MapTask.java	(working copy)@@ -50;7 +50;7 @@   public void write(DataOutput out) throws IOException {     super.write(out);     split.write(out);-    +   }   public void readFields(DataInput in) throws IOException {     super.readFields(in);@@ -126;6 +126;10 @@         }       } finally {+        if (combining) {+          ((CombiningCollector)collector).finished();+        }+         in.close();                               // close input       }     } finally {@@ -147;5 +151;5 @@   public NutchConf getConf() {     return this.nutchConf;   }	+ }Index: src/java/org/apache/nutch/mapred/MapRunner.java===================================================================								src/java/org/apache/nutch/mapred/MapRunner.java	(revision 374737)+++ src/java/org/apache/nutch/mapred/MapRunner.java	(working copy)@@ -38;18 +38;22 @@   public void run(RecordReader input; OutputCollector output;                   Reporter reporter)     throws IOException {							while (true) {	// allocate new key  value instances	WritableComparable key =	(WritableComparable)job.newInstance(inputKeyClass);	Writable value = (Writable)job.newInstance(inputValueClass);+    try   finally {+        mapper.finished();     }   }Index: src/java/org/apache/nutch/mapred/CombiningCollector.java=================================================================== src/java/org/apache/nutch/mapred/CombiningCollector.java	(revision 374780)+++ src/java/org/apache/nutch/mapred/CombiningCollector.java	(working copy)@@ -78;4 +78;9 @@     count = 0;   }+  public synchronized void finished()+  {+    combiner.finished();+  }+ }Index: src/java/org/apache/nutch/mapred/Reducer.java=================================================================== src/java/org/apache/nutch/mapred/Reducer.java	(revision 374737)+++ src/java/org/apache/nutch/mapred/Reducer.java	(working copy)@@ -38;4 +38;10 @@   void reduce(WritableComparable key; Iterator values;               OutputCollector output; Reporter reporter)     throws IOException;++  /** Called after the last {@link #reduce} call on this Reducer object.+      Typical implementations do nothing.+  */+  void finished();+ }Index: src/java/org/apache/nutch/crawl/CrawlDbReader.java=================================================================== src/java/org/apache/nutch/crawl/CrawlDbReader.java	(revision 374737)+++ src/java/org/apache/nutch/crawl/CrawlDbReader.java	(working copy)@@ -50;9 +50;9 @@ /**	Read utility for the CrawlDB.	*+ *	@author Andrzej Bialecki	*+ *  */ public class CrawlDbReader { @@ -68;6 +68;7 @@       output.collect(new UTF8(""retry""); new LongWritable(cd.getRetriesSinceFetch()));       output.collect(new UTF8(""score""); new LongWritable((long) (cd.getScore() * 1000.0)));     }+    public void finished() {}   }   public static class CrawlDbStatReducer implements Reducer {@@ -121;6 +122;7 @@         output.collect(new UTF8(""avg score""); new LongWritable(total / cnt));       }     }+    public void finished() {}   }   public static class CrawlDbDumpReducer implements Reducer {@@ -133;8 +135;11 @@     public void configure(JobConf job) {     }++    public void finished() {+    }   }	+   public void processStatJob(String crawlDb; NutchConf config) throws IOException {     LOG.info(""CrawlDb statistics start: "" + crawlDb);     File tmpFolder = new File(crawlDb; ""stat_tmp"" + System.currentTimeMillis());@@ -219;7 +224;7 @@       System.out.println(""not found"");     }   }	+   public void processDumpJob(String crawlDb; String output; NutchConf config) throws IOException {      LOG.info(""CrawlDb dump: starting"");@@ -270;4 +275;5 @@     }     return;   }+ }Index: src/java/org/apache/nutch/crawl/LinkDb.java===================================================================								src/java/org/apache/nutch/crawl/LinkDb.java	(revision 374779)+++ src/java/org/apache/nutch/crawl/LinkDb.java	(working copy)@@ -118;7 +118;8 @@     output.collect(key; result);   }						-+  public void finished() {}+	   public void invert(File linkDb; File segmentsDir) throws IOException {     LOG.info(""LinkDb: starting"");     LOG.info(""LinkDb: linkdb: "" + linkDb);Index: src/java/org/apache/nutch/crawl/Injector.java===================================================================--- src/java/org/apache/nutch/crawl/Injector.java	(revision 374779)+++ src/java/org/apache/nutch/crawl/Injector.java	(working copy)@@ -65;6 +65;8 @@                                              interval));       }     }+    +    public void finished() {}   }   /** Combine multiple new entries for a url. */@@ -76;6 +78;7 @@       throws IOException {       output.collect(key; (Writable)values.next()); // just collect first value     }+    public void finished() {}   }   /** Construct an Injector. */Index: src/java/org/apache/nutch/crawl/Generator.java=================================================================== src/java/org/apache/nutch/crawl/Generator.java	(revision 374779)+++ src/java/org/apache/nutch/crawl/Generator.java	(working copy)@@ -63;6 +63;8 @@       output.collect(crawlDatum; key);          // invert for sort by score     }+    public void finished() {}+         /** Partition by host (value). */     public int getPartition(WritableComparable key; Writable value;                             int numReduceTasks) {Index: src/java/org/apache/nutch/crawl/CrawlDbReducer.java===================================================================--- src/java/org/apache/nutch/crawl/CrawlDbReducer.java	(revision 374781)+++ src/java/org/apache/nutch/crawl/CrawlDbReducer.java	(working copy)@@ -115;4 +115;5 @@     }   }+  public void finished() {} }Index: src/java/org/apache/nutch/parse/ParseSegment.java=================================================================== src/java/org/apache/nutch/parse/ParseSegment.java	(revision 374776)+++ src/java/org/apache/nutch/parse/ParseSegment.java	(working copy)@@ -78;6 +78;8 @@     throws IOException {     output.collect(key; (Writable)values.next()); // collect first value   }+  +  public void finished() {}   public void parse(File segment) throws IOException {     LOG.info(""Parse: starting"");",Closed,Fixed,,Unassigned,Michel Tourn,Sat; 4 Feb 2006 09:06:20 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Fri; 10 Feb 2006 02:22:13 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-20
HADOOP-21,Bug,Minor,,the webapps need to be updated for the move from nutch,The webapp files need to be updated so that the reference the new packages. I'll include a patch to fix it.,Closed,Fixed,,Unassigned,Owen O'Malley,Tue; 7 Feb 2006 04:12:05 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Tue; 7 Feb 2006 04:40:17 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-21
HADOOP-22,Bug,Trivial,,remove unused imports,Following patch will remove unused imports from java source files,Closed,Fixed,,Unassigned,Sami Siren,Tue; 7 Feb 2006 04:33:17 +0000,Thu; 3 Aug 2006 17:46:26 +0000,Tue; 7 Feb 2006 04:50:07 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-22
HADOOP-23,Bug,Minor,,single node cluster gets one reducer,Running on a single node cluster (it runs a job tracker and a single task tracker); even though my application asks for 7 reduces; it only gets one. I haven't tracked down what is happening yet.,Closed,Won't Fix,,Owen O'Malley,Owen O'Malley,Tue; 7 Feb 2006 04:45:26 +0000,Sat; 22 Jun 2013 00:35:03 +0000,Tue; 14 Mar 2006 13:23:40 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-23
HADOOP-24,Improvement,Major,conf,make Configuration an interface,The Configuration class should become an interface; e.g.:public interface Configuration {  String get(String nam);  String set(String name; String value);  int getInt(String name);  void setInt(String name; int value);  float getFloat(String name);  void setFloat(String name; float value);  //... other utility methods based on get(String) and set(String;String) ...}An abstract class named ConfigurationBase should be implemented as follows:public abstract class ConfigurationBase implements Configuration {  abstract public String get(String nam);  abstract public String set(String name; String value);  public  int getInt(String name) { ... implementation in terms of get(String) ... }  public void setInt(String name; int value) {... implementation in terms of set(String; String) ...}  public float getFloat(String name)  { ... implementation in terms of get(String) ... }  public void setFloat(String name; float value)  {... implementation in terms of set(String; String) ...}  //... other utility methods based on get(String) and set(String;String) ...}A concrete; default implementation will be provided as follows:public class ConfigurationImpl implements Writable extends ConfigurationBase {  private Properties properties;  // implement abstract methods from ConfigurationBase  public String get(String name) { ... implemented in terms of props ...}  public String set(String name; String value) { .. implemented in terms of props ... }  // Writable methods  public write(DataOutputStream out);  public readFields(DataInputStream in);  // permit chaining of configurations  public Configuration getDefaults();  public void setDefaults(Configuration defaults);}Only code which creates configurations should need to be updated; so this shouldn't be a huge change.,Closed,Won't Fix,,Doug Cutting,Doug Cutting,Tue; 7 Feb 2006 05:09:24 +0000,Thu; 23 Apr 2009 19:24:57 +0000,Thu; 15 Jan 2009 17:27:13 +0000,,0.7.2,,,,https://issues.apache.org/jira/browse/HADOOP-24
HADOOP-25,Improvement,Minor,,a new map/reduce example and moving the examples from src/java to src/examples,The new example is the word count example from Google's paper. I moved the examples into a separate jar file to demonstrate how to run stand-alone application code.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 7 Feb 2006 08:15:24 +0000,Tue; 24 Jul 2012 19:08:08 +0000,Thu; 9 Feb 2006 04:23:26 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-25
HADOOP-26,Bug,Major,,DFS node choice doesn't take available space into account effectively,We used to have node-allocation be sensitive to available disk space.  It turned out thatthis was a little accident-prone and hard to debug on then-available machines; so I removed it.  DFS is mature enough now that this needs to come back and work properly.  A node with lessthan X bytes should never accept a new block for allocation.,Closed,Fixed,,Mike Cafarella,Mike Cafarella,Wed; 8 Feb 2006 03:05:14 +0000,Wed; 8 Jul 2009 16:41:48 +0000,Sun; 5 Mar 2006 10:03:58 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-26
HADOOP-27,Bug,Major,,MapRed tries to allocate tasks to nodes that have no available disk space,What it says above.  MapRed TaskTrackers should not offer task service if the local diskspace is too constrained.,Closed,Fixed,HADOOP-336,Unassigned,Mike Cafarella,Wed; 8 Feb 2006 03:07:03 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Wed; 28 Jun 2006 22:27:40 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-27
HADOOP-28,Bug,Major,,webapps broken,Changing the classes to private broke the webapps. The required public classes are:org.apache.hadoop.mapred.JobInProgressorg.apache.hadoop.mapred.JobProfileorg.apache.hadoop.mapred.JobStatusorg.apache.hadoop.mapred.TaskTrackerStatusTo fix; we need one of:  1. The classes need to be made public again  2. The functionality needs to be made available through the classes that are public  3. The webapps need to move into the mapred package.,Closed,Fixed,,Unassigned,Owen O'Malley,Thu; 9 Feb 2006 02:41:46 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Thu; 9 Feb 2006 05:39:50 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-28
HADOOP-29,Improvement,Minor,,JobConf newInstance() method imposes a default constructor,The Nutch parse command fails; because the ParseSegment class has no default constructor.However; ParseSegment extends Configured; so it can be directly instanciated with a Configuration parameter.,Closed,Won't Fix,,Owen O'Malley,Jerome Charron,Thu; 9 Feb 2006 22:28:40 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Thu; 15 Jan 2009 17:28:44 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-29
HADOOP-30,Improvement,Major,,DFS shell: support for ls -r and cat,patch attached,Closed,Fixed,,Unassigned,Michel Tourn,Fri; 10 Feb 2006 08:14:53 +0000,Thu; 3 Aug 2006 17:46:27 +0000,Sat; 11 Feb 2006 02:37:16 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-30
HADOOP-31,New Feature,Minor,,Stipulate main class in a job jar when using 'hadoop jar JARNAME',One use case I forsee is building one jar but using this one jar running multiple jobs: E.g. A single nutch job jar would now be used to do indexing job; later same jar is used to do dedup; etc. Currently; the recently added hadoop 'jar' option just takes the jar name then looks in the jar MANIFEST.MF for the Main-Class; failing if not present. This is grand but for the scenario above; it means I have to create a jar per job I want to run  each with a different MANIFEST.MF#Main-Class entry. Can we pass the Main-Class on the hadoop command-line as an (optional) argument to 'hadoop jar JAR_NAME'? (I can make a patch if wanted).,Closed,Won't Fix,,Unassigned,stack,Fri; 10 Feb 2006 09:16:53 +0000,Fri; 15 Dec 2006 23:02:08 +0000,Sat; 11 Feb 2006 01:31:30 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-31
HADOOP-32,Bug,Major,,Creating job with InputDir set to non-existant directory locks up jobtracker,"This; in the very least; affects anything using the default listFiles() from InputFormatBase. If no files are enumerated; an exception is thrown... but the JobTracker keeps attempting to run listFiles() for this job. Trying to stop the job with hadoop job -kill job_name just results in timeouts; and further started jobs also don't progress. This happens every single time with; say; ""wordcount""; and a non-existent input path.",Closed,Duplicate,NULL,Owen O'Malley,Bryan Pendleton,Fri; 10 Feb 2006 16:07:02 +0000,Wed; 8 Jul 2009 16:51:39 +0000,Thu; 27 Jul 2006 03:13:00 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-32
HADOOP-33,Improvement,Minor,fs,DF enhancement: performance and win XP support,1. DF is called twice for each heartbeat; which happens each 3 seconds.There is a simple fix for that in the attached patch.2. cygwin is required to run df program in windows environment.There is a class org.apache.commons.io.FileSystemUtils; which can return disk free spacefor different OSs; but it does not have means to get disk capacity.In general in windows there is no efficient and uniform way to calculate disk capacityusing a shell command.The choices are 'chkdsk' and 'defrag -a'; but both of them are too slow to be calledevery 3 seconds.WinXP and 2003 server have a new tool called fsutil; which provides all necessary info.I implemented a call to fsutil in case df fails; and the OS is right.Other win versions should still run cygwin.I tested this fetaure for linux; winXP and cygwin.See attached patch.,Closed,Fixed,NUTCH-187,Konstantin Shvachko,Konstantin Shvachko,Sat; 11 Feb 2006 07:43:36 +0000,Wed; 8 Jul 2009 16:41:48 +0000,Thu; 30 Mar 2006 07:39:48 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-33
HADOOP-34,Bug,Trivial,,Build Paths Relative to PWD in build.xml,In the build.xml file; many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.xml. Thus; whenever trying to compile from a directory other than the hadoop root; errors such as this appear:BUILD FAILED/home/jeremy/cvs/hadoop/build.xml:109: org.apache.jasper.JasperException: The -uriroot option must specify a pre-existing directoryI have scripts / vim parameters that connect to other machines for compiling using ssh; and am not necessarily always in the root whenever I compile.I am attaching a patch which sets all paths relative to ${basedir}; and removes the override of ${basedir} to the PWD. Please let me know if there are reasons why the build environment must be relative to the working directory.,Closed,Fixed,,Unassigned,Jeremy Bensley,Tue; 14 Feb 2006 02:00:26 +0000,Thu; 3 Aug 2006 17:46:27 +0000,Tue; 14 Feb 2006 04:03:32 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-34
HADOOP-35,Bug,Major,,Files missing chunks can cause mapred runs to get stuck,"I've now several times run into a problem where a large run gets stalled as a result of a missing data block. The latest was a stall in the Summer - ie; the data might've all been there; but it was impossible to proceed because the CRC file was missing a block. It would be nice to:1) Have a ""health check"" running on a map reduce. If any data isn't available; emmit periodic warnings; and maybe have a timeout for if the data never comes back. Such warnings should specify which file(s) are affected by the missing blocks.2) Have a utility; possible part of the existing dfs utility; which can check for dfs files with unlocatable blocks. Possibly; even show a 'health' of a file - ie; what percentage of its blocks are currently at the desired replication level. Currently; there's no way that I know of to find out if a file in DFS is going to be unreadable.",Closed,Duplicate,HADOOP-83,Unassigned,Bryan Pendleton,Tue; 14 Feb 2006 03:30:18 +0000,Wed; 8 Jul 2009 16:41:46 +0000,Sat; 25 Mar 2006 05:33:24 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-35
HADOOP-36,Improvement,Major,conf,Adding some uniformity/convenience to environment management,"Currently; ""slaves"" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slavesPerhaps split slaves; having a different set for ""datanodes"" vs. ""tasktracker"" nodes. ie; conf/hadoop-slaves-tasktracker; conf/hadoop-slaves-datanodes; or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset; and thus implicitly includes datanodes; but this might be a bad assumption.Also; make sure all scripts source something like conf/hadoop-env.sh. Thus; the user can edit hadoop-env.sh to specify JAVA_HOME; or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH; to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable.These changes would probably be useful to the nutch project; too.",Closed,Fixed,,Unassigned,Bryan Pendleton,Tue; 14 Feb 2006 05:03:38 +0000,Thu; 3 Aug 2006 17:46:27 +0000,Thu; 16 Feb 2006 07:03:53 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-36
HADOOP-37,New Feature,Major,,A way to determine the size and overall activity of the cluster,There is currently no way for an application to determine the size or activity of the cluster.,Closed,Fixed,,Unassigned,Owen O'Malley,Tue; 14 Feb 2006 17:11:58 +0000,Wed; 8 Jul 2009 16:51:38 +0000,Wed; 15 Feb 2006 03:39:03 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-37
HADOOP-38,Improvement,Major,,default splitter should incorporate fs block size,"By default; the file splitting code should operate as follows.  inputs are file*; numMapTasks; minSplitSize; fsBlockSize  output is file;start;length*  totalSize = sum of all file sizes;  desiredSplitSize = totalSize / numMapTasks;  if (desiredSplitSize  fsBlockSize)             /* new */    desiredSplitSize = fsBlockSize;  if (desiredSplitSize  minSplitSize)    desiredSplitSize = minSplitSize;  chop input files into desiredSplitSize chunks  return themIn other words; the numMapTasks is a desired minimum.  We'll try to chop input into at least numMapTasks chunks; each ideally a single fs block.If there's not enough input data to create numMapTasks tasks; each with an entire block; then we'll permit tasks whose input is smaller than a filesystem block; down to a minimum split size.This handles cases where:	each input record takes a lot of time to process.  In this case we want to make sure we use all of the cluster.  Thus it is important to permit splits smaller than the fs block size.	input i/o dominates.  In this case we want to permit the placement of tasks on hosts where their data is local.  This is only possible if splits are fs block size or smaller.Are there other common cases that this algorithm does not handle well?The part marked 'new' above is not currently implemented; but I'd like to add it.Does this sound reasonble?",Closed,Fixed,,Unassigned,Doug Cutting,Wed; 15 Feb 2006 04:25:04 +0000,Wed; 8 Jul 2009 16:51:38 +0000,Thu; 16 Feb 2006 03:53:49 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-38
HADOOP-39,Bug,Major,,Create a job-configurable best effort for job execution,I propose having a job option that when a tip fails 4 times; stops trying to run that tip; but does not kill the job.,Closed,Duplicate,HADOOP-1144,Arun C Murthy,Owen O'Malley,Thu; 16 Feb 2006 10:01:08 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Tue; 8 May 2007 04:21:50 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-39
HADOOP-40,Bug,Minor,fs,bufferSize argument is ignored in FileSystem.create(File; boolean; int),org.apache.hadoop.fs.FileSystem.create(File f; boolean overwrite; int bufferSize)ignores the input parameter bufferSize.It passes further down the internal configuration; which includes the buffer size; but not the parameter value.This works fine within the file system; since everything that calls create extracts buffer size from the same config. MapReduce although is probably affected by that; see org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size; String outName; boolean done)The attached patch would fix it.,Closed,Fixed,,Unassigned,Konstantin Shvachko,Sat; 18 Feb 2006 07:43:11 +0000,Thu; 3 Aug 2006 17:46:28 +0000,Sat; 25 Feb 2006 06:38:22 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-40
HADOOP-41,Improvement,Minor,conf,JAVA_OPTS for the TaskRunner Child,Currently; its possible to set the java heap size the TaskRunner child runs in; but thats all thats configurable about the child process.  Hereabouts; we've found it useful being able to specify other options for the child JVM; especially when debugging and monitoring long-lived processes.  Examples of why its useful being able to set options are the child include:+ Being able to set '-server' option or '-c64'.+ Passing logging.properties to configure child logging.+ Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process.  Allows connecting with jconsole to watch long-lived children; their heap and thread usage; and when seemingly hung; take thread dumps.,Closed,Fixed,,Unassigned,stack,Sat; 18 Feb 2006 09:47:33 +0000,Thu; 3 Aug 2006 17:46:28 +0000,Fri; 24 Feb 2006 08:36:39 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-41
HADOOP-42,Bug,Major,fs,PositionCache decrements its position for reads at the end of file,Seeint org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b; int off; int len) if in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented; while it should be retained.The attached patch would fix it.,Closed,Fixed,,Unassigned,Konstantin Shvachko,Sat; 18 Feb 2006 09:58:44 +0000,Thu; 3 Aug 2006 17:46:28 +0000,Thu; 23 Feb 2006 14:17:08 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-42
HADOOP-43,Improvement,Major,,JobTracker dumps TaskTrackers if it takes too long to service an RPC call,If the JobTracker takes too long to service any RPC request; it is unable to receive emitHeartBeat calls from TaskTrackers. The monitoring thread thus dumps the TaskTracker; losing any incomplete work and forcing a slow reconnect process to begin before it starts assigning work again.,Closed,Duplicate,MAPREDUCE-42,Owen O'Malley,Bryan Pendleton,Tue; 21 Feb 2006 14:51:49 +0000,Wed; 8 Jul 2009 16:51:41 +0000,Wed; 31 May 2006 04:03:56 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-43
HADOOP-44,New Feature,Major,ipc,RPC exceptions should include remote stack trace,Remote exceptions currently only report the exception string.  Instead they should report the entire remote stack trace; as a string; to facilitate debugging.,Closed,Fixed,,Doug Cutting,Doug Cutting,Wed; 22 Feb 2006 07:06:46 +0000,Thu; 3 Aug 2006 17:46:28 +0000,Wed; 22 Mar 2006 06:51:15 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-44
HADOOP-45,Improvement,Major,,JobTracker should log task errors,Task errors are currently propagated to the JobTracker and viewable in the web interface but they are not logged.  These should be logged as well.,Closed,Fixed,,Doug Cutting,Doug Cutting,Wed; 22 Feb 2006 07:08:46 +0000,Wed; 8 Jul 2009 16:51:39 +0000,Wed; 22 Mar 2006 07:20:11 +0000,,0.1.0,,,HADOOP-53,https://issues.apache.org/jira/browse/HADOOP-45
HADOOP-46,New Feature,Major,,user-specified job names,It should be possible to supply a name when a job is submitted.  This can then be used in the web ui to describe the job.  Perhaps this should just be a job property (mapred.job.name).,Closed,Fixed,,Owen O'Malley,Doug Cutting,Wed; 22 Feb 2006 07:10:56 +0000,Wed; 8 Jul 2009 16:51:39 +0000,Fri; 24 Mar 2006 06:53:25 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-46
HADOOP-47,New Feature,Major,,include records/second and bytes/second in  task reports,TaskReport should include fields for total records processed; total bytes processed and total seconds of task execution.  These should also be reported in the web ui; as bytes/second and records/second.  The job display could sum these to report total bytes/second and records/second for map and reduce.  Better yet would be a graph displaying the total rates over the course of the job...,Closed,Duplicate,HADOOP-237,Unassigned,Doug Cutting,Wed; 22 Feb 2006 07:17:42 +0000,Wed; 8 Jul 2009 16:51:39 +0000,Wed; 19 Jul 2006 09:00:32 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-47
HADOOP-48,New Feature,Major,,add user data to task reports,The Reporter interface should permit tasks to set user-data for a task; either as a String or; better-yet; as a Writable.  This should be returned with each TaskReport.  This would facilitate programmatic instrumentation of tasks.,Closed,Won't Fix,,Unassigned,Doug Cutting,Wed; 22 Feb 2006 07:22:23 +0000,Wed; 8 Jul 2009 16:51:39 +0000,Tue; 13 Mar 2007 03:53:38 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-48
HADOOP-49,Improvement,Major,,JobClient cannot use a non-default server (unlike DFSShell),"JobClient cannot use a non-default Job Tracker server:It will use the Job Tracker specified in conf/hadoop-default.xml or conf/hadoop-site.xmlFor users with multiple Hadoop systems; it is useful to be able to specify the Job Tracker.Other hadoop command-line tools like DFSShell already have:bin/hadoop dfsUsage: java DFSShell -local | -dfs namenode:port&#93;  ...Similarly I propose to add a -jt parameter:bin/hadoop jobJobClient -submit job | -status id | -kill id -jt jobtracker:port|config&#93;Where: -jt jobtracker:port is similar to -dfs namenode:portAnd:  jt config will load as a final resource: hadoopconfig.xmlThe latter syntax is discoverable by users because on failure the tool will say:bin/hadoop job -kill m7n6pi -jt unknownException in thread ""main"" java.lang.RuntimeException: hadoop-unknown.xml not found on CLASSPATHOr in case of success:bin/hadoop job -kill job_m7n6pi -jt myconfig060221 221911 parsing file:/trunk/conf/hadoop-default.xml060221 221911 parsing file:/trunk/conf/hadoop-myconfig.xml060221 221911 parsing file:/trunk/conf/hadoop-site.xml060221 221911 Client connection to 66.196.91.10:7020: startingAnd with a machine:port spec:bin/hadoop job -kill job_m7n6pi -jt machine:8020060221 222109 parsing file:/trunk/conf/hadoop-default.xml060221 222109 parsing file:/trunk/conf/hadoop-site.xml060221 222109 Client connection to 66.196.91.10:8020: startingPatch attached.",Closed,Fixed,,Michel Tourn,Michel Tourn,Wed; 22 Feb 2006 07:25:42 +0000,Wed; 8 Jul 2009 16:51:39 +0000,Thu; 23 Feb 2006 07:54:34 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-49
HADOOP-50,Bug,Major,,dfs datanode should store blocks in multiple directories,The datanode currently stores all file blocks in a single directory.  With 32MB blocks and terabyte filesystems; this will create too many files in a single directory for many filesystems.  Thus blocks should be stored in multiple directories; perhaps even a shallow hierarchy.,Closed,Fixed,,Milind Bhandarkar,Doug Cutting,Wed; 22 Feb 2006 07:25:57 +0000,Wed; 8 Jul 2009 16:41:47 +0000,Thu; 7 Sep 2006 08:03:46 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-50
HADOOP-51,New Feature,Major,,per-file replication counts,It should be possible to specify different replication counts for different files.  Perhaps an option when creating a new file should be the desired replication count.  MapReduce should take advantage of this feature so that job.xml and job.jar files; which are frequently accessed by lots of machines; are more highly replicated than large data files.,Closed,Fixed,,Konstantin Shvachko,Doug Cutting,Wed; 22 Feb 2006 07:29:49 +0000,Wed; 8 Jul 2009 16:41:47 +0000,Tue; 11 Apr 2006 03:47:26 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-51
HADOOP-52,Bug,Major,,mapred input and output dirs must be absolute,DFS converts relative pathnames to be under /user/$USER.  But MapReduce jobs may be submitted by a different user than is running the jobtracker and tasktracker.  Thus relative paths must be resolved before a job is submitted; so that only absolute paths are seen on the job tracker and tasktracker.  I think the simplest way to fix this is to make JobConf.setInputDir(); setOutputDir(); etc. resolve relative pathnames.,Closed,Fixed,HADOOP-448,Owen O'Malley,Doug Cutting,Wed; 22 Feb 2006 07:38:02 +0000,Wed; 8 Jul 2009 16:51:41 +0000,Fri; 24 Mar 2006 02:40:19 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-52
HADOOP-53,New Feature,Major,,MapReduce log files should be storable in dfs.,It should be possible to cause a job's log output to be stored in dfs.  The jobtracker's log output and (optionally) all tasktracker log output related to a job should be storable in a job-specified dfs directory.,Closed,Duplicate,HADOOP-2206,Enis Soztutar,Doug Cutting,Thu; 23 Feb 2006 05:31:41 +0000,Wed; 8 Jul 2009 16:51:38 +0000,Tue; 20 Nov 2007 08:57:43 +0000,,0.16.0,,,HADOOP-45,https://issues.apache.org/jira/browse/HADOOP-53
HADOOP-54,Improvement,Major,io,SequenceFile should compress blocks; not individual entries,SequenceFile will optionally compress individual values.  But both compression and performance would be much better if sequences of keys and values are compressed together.  Sync marks should only be placed between blocks.  This will require some changes to MapFile too; so that all file positions stored there are the positions of blocks; not entries within blocks.  Probably this can be accomplished by adding a getBlockStartPosition() method to SequenceFile.Writer.,Closed,Fixed,,Arun C Murthy,Doug Cutting,Thu; 23 Feb 2006 05:39:30 +0000,Thu; 2 May 2013 02:28:58 +0000,Mon; 28 Aug 2006 19:23:19 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-54
HADOOP-55,Bug,Minor,,map-reduce job overhead is too high,map-reduce job overhead is too high. Running a null map-reduce job; with no input; no intermediate data and no output; on a several-hundred node cluster; takes several minutes; rather than several seconds.Enhanced distribution; launching and monitoring mechanisms should reduce the overhead to levels that match human online patience for short jobs.,Closed,Duplicate,NULL,Sameer Paranjpye,Yoram Arnon,Thu; 23 Feb 2006 08:14:20 +0000,Wed; 8 Jul 2009 16:51:38 +0000,Thu; 27 Sep 2007 21:43:01 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-55
HADOOP-56,Bug,Critical,,hadoop nameserver does not recognise ndfs nameserver image,hadoop nameserver does not recognise ndfs imageThus; upgrading from ndfs to hadoop dfs results in total data loss.The upgrade should be seemless; with the new server recognising all previous version that are not end-of-life'd.,Closed,Duplicate,HADOOP-19,Sameer Paranjpye,Yoram Arnon,Thu; 23 Feb 2006 11:13:31 +0000,Wed; 8 Jul 2009 16:41:47 +0000,Sat; 25 Mar 2006 05:39:08 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-56
HADOOP-57,Bug,Minor,,hadoop dfs -ls / does not show root of file system,hadoop dfs -ls / does not show root of file system - it shows the user's home directory.It's thus impossible to learn the contents of the root file system via the shell.,Closed,Fixed,,Unassigned,Yoram Arnon,Thu; 23 Feb 2006 11:17:35 +0000,Wed; 8 Jul 2009 16:41:47 +0000,Wed; 8 Mar 2006 05:22:43 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-57
HADOOP-58,Bug,Minor,,Hadoop requires configuration of hadoop-site.xml or won't run,"On a new install; I would expect '${HADOOP_HOME}/bin/start-all.sh"" to bring up a basic instance; one that is using local filesystem (Or if not; then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost.  Currently this is not the case.  Hadoop complains 'java.lang.RuntimeException: Not a host:port pair: local'.  It doesn't like the 'local' default value for mapred.job.tracker and fs.default.name properties.Revision: 379930",Closed,Won't Fix,,Unassigned,stack,Fri; 24 Feb 2006 03:43:08 +0000,Thu; 3 Aug 2006 17:46:29 +0000,Fri; 24 Feb 2006 05:15:10 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-58
HADOOP-59,Improvement,Minor,conf,support generic command-line options,Hadoop commands should all support some common options.  For example; it should be possible to specify the namenode; datanode; and; for that matter; any config option; in a generic way.This could be implemented with code like:public interface Tool extends Configurable {  void run(String[] args) throws Exception;}public class ToolBase implements Tool extends Configured {  public final void main(String[] args) throws Exception {    Configuration conf = new Configuration();    ... parse config options from args into conf ...    this.configure(conf);    this.run();  }}public MyTool extends ExcecutableBase {  public static void main(String[] args) throws Exception {    new MyTool().main(args);  }}The general command line syntax could be:bin/hadoop generalOptions command commandOptionsWhere generalOptions are things that ToolBase handles; and only the commandOptions are passed to Tool.run().  The most important generalOption would be '-D'; which would define name/value pairs that are set in the configuration.  This alone would permit folks to set the namenode; datanode; etc.,Closed,Fixed,,Hairong Kuang,Doug Cutting,Fri; 24 Feb 2006 07:30:38 +0000,Wed; 3 Jan 2007 00:36:32 +0000,Tue; 27 Jun 2006 04:11:09 +0000,,0.2.0,,,HADOOP-220,https://issues.apache.org/jira/browse/HADOOP-59
HADOOP-60,Improvement,Minor,,Specification of alternate conf. directory,Currently; hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf.  Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct.  Benefits include: Binary can be made read-only; or binary is blanket-updateable with configuration undisturbed.,Closed,Fixed,HADOOP-62,Unassigned,stack,Tue; 28 Feb 2006 01:56:17 +0000,Thu; 3 Aug 2006 17:46:29 +0000,Fri; 3 Mar 2006 03:52:36 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-60
HADOOP-61,Improvement,Minor,scripts,Bashless Hadoop Start Script,This is for the people who want to try to get hadoop running without cygwin.  The code needs some work and could be improved.Attachment to follow.,Resolved,Won't Fix,,Unassigned,Jeff Ritchie,Wed; 1 Mar 2006 03:26:20 +0000,Thu; 11 Aug 2011 18:53:56 +0000,Thu; 11 Aug 2011 18:53:56 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-61
HADOOP-62,Bug,Minor,conf,can't get environment variables from HADOOP_CONF_DIR,The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.,Closed,Duplicate,HADOOP-60,Owen O'Malley,Owen O'Malley,Thu; 2 Mar 2006 08:49:53 +0000,Thu; 3 Aug 2006 17:46:29 +0000,Thu; 2 Mar 2006 09:43:24 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-62
HADOOP-63,Bug,Minor,,problem with webapp when start a jobtracker,there exist two issues with starting up webapp1. webapp is not be able to be loaded from a jar file.2. web.xml can not be parsed properly using java 1.4,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Fri; 3 Mar 2006 10:38:35 +0000,Wed; 8 Jul 2009 16:51:40 +0000,Wed; 31 May 2006 07:16:45 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-63
HADOOP-64,Improvement,Minor,,DataNode should be capable of managing multiple volumes,The dfs Datanode can only store data on a single filesystem volume. When a node runs its disks JBOD this means running a Datanode per disk on the machine. While the scheme works reasonably well on small clusters; on larger installations (several 100 nodes) it implies a very large number of Datanodes with associated management overhead in the Namenode.The Datanod should be enhanced to be able to handle multiple volumes on a single machine.,Closed,Fixed,HADOOP-74;HADOOP-257,Milind Bhandarkar,Sameer Paranjpye,Tue; 7 Mar 2006 07:05:03 +0000,Wed; 8 Jul 2009 16:41:49 +0000,Tue; 5 Sep 2006 22:14:28 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-64
HADOOP-65,New Feature,Minor,io;ipc,add a record I/O framework to hadoop,Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs; vectors; maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier.,Closed,Fixed,,Unassigned,Sameer Paranjpye,Tue; 7 Mar 2006 07:12:40 +0000,Thu; 3 Aug 2006 17:46:29 +0000,Thu; 4 May 2006 09:04:14 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-65
HADOOP-66,Bug,Major,,dfs client writes all data for a chunk to /tmp,The dfs client writes all the data for the current chunk to a file in /tmp; when the chunk is complete it is shipped out to the Datanodes. This can cause /tmp to fill up fast when a lot of files are being written. A potentially better scheme is to buffer the written data in RAM (application code can set the buffer size) and flush it to the Datanodes when the buffer fills up.,Closed,Fixed,,Doug Cutting,Sameer Paranjpye,Tue; 7 Mar 2006 07:55:56 +0000,Wed; 8 Jul 2009 16:41:48 +0000,Thu; 9 Mar 2006 08:18:06 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-66
HADOOP-67,Improvement,Trivial,,Added statistic/reporting info to DFS,The DatanodInfo; DFSFileInfo; and DFSClient were recently changed to package level protection; this hampers the ability to get some useful reporting data that can be used for DataNode and DFS health/performance.,Closed,Fixed,,Doug Cutting,Barry Kaplan,Wed; 8 Mar 2006 06:18:53 +0000,Wed; 8 Jul 2009 16:41:49 +0000,Thu; 30 Mar 2006 07:21:16 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-67
HADOOP-68,Bug,Major,,"Cannot abandon block during write to <file> and ""Cannot obtain additional block for file <file>"" errors during dfs write test",In the namenode's log file; when trying to run my writer benchmark; I get a bunch of messages like:060307 112402 Server handler 2 on 9020 call error: java.io.IOException: Cannot abandon block during write to /user/oom/random/.part001389.crcjava.io.IOException: Cannot abandon block during write to /user/oom/random/.part001389.crc        at org.apache.hadoop.dfs.NameNode.abandonBlock(NameNode.java:188)        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        at java.lang.reflect.Method.invoke(Method.java:585)        at org.apache.hadoop.ipc.RPC$1.call(RPC.java:208)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:200)and 060307 112402 Server handler 1 on 9020 call error: java.io.IOException: Cannot abandon block during write to /user/oom/random/part001695java.io.IOException: Cannot abandon block during write to /user/oom/random/part001695        at org.apache.hadoop.dfs.NameNode.abandonBlock(NameNode.java:188)        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        at java.lang.reflect.Method.invoke(Method.java:585)        at org.apache.hadoop.ipc.RPC$1.call(RPC.java:208)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:200)and060307 112402 Server handler 2 on 9020 call error: java.io.IOException: Cannot obtain additional block for file /user/oom/random/part001274java.io.IOException: Cannot obtain additional block for file /user/oom/random/part001274        at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:160)        at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        at java.lang.reflect.Method.invoke(Method.java:585)        at org.apache.hadoop.ipc.RPC$1.call(RPC.java:208)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:200),Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 8 Mar 2006 08:11:00 +0000,Wed; 8 Jul 2009 16:41:49 +0000,Sat; 22 Apr 2006 00:23:47 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-68
HADOOP-69,Bug,Major,,Unchecked lookup value causes NPE in FSNamesystemgetDatanodeHints,,Closed,Fixed,,Unassigned,Bryan Pendleton,Wed; 8 Mar 2006 08:51:29 +0000,Wed; 8 Jul 2009 16:41:49 +0000,Fri; 21 Apr 2006 01:17:28 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-69
HADOOP-70,Bug,Major,,the two file system tests TestDFS and TestFileSystem take too long,"Running ""ant test"" takes hours and uses the conf directory; which forces it to run on the real cluster.I propose that we split rename the test classes fromsrc/test/org/apache/hadoop/dfs/TestDFS.java to LongTestDFS.javasrc/test/org/apache/hadoop/fs/TestFileSystem.java to LongTestFileSystem.javaand then we set up a new ant target ""long-test"" that runs all tests that match ""*/LongTest.java"".",Closed,Fixed,,Owen O'Malley,Owen O'Malley,Thu; 9 Mar 2006 07:30:48 +0000,Wed; 8 Jul 2009 16:41:49 +0000,Tue; 14 Mar 2006 04:30:43 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-70
HADOOP-71,Bug,Major,,The SequenceFileRecordReader uses the default FileSystem rather than the supplied one,"The mapred.TestSequenceFileInputFormat test was failing when run with a conf directory that pointed to a dfs cluster. The reason was that SequenceFileRecordReader was using the default FileSystem from the config; while the test program was assuming the ""local"" file system was being used.",Closed,Fixed,,Owen O'Malley,Owen O'Malley,Thu; 9 Mar 2006 08:18:06 +0000,Wed; 8 Jul 2009 16:51:41 +0000,Fri; 10 Aug 2007 20:26:25 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-71
HADOOP-72,Test,Major,fs,hadoop doesn't take advatage of distributed compiting in TestDFSIO,"TestDFSIO runs N map jobs; each either writing to or reading from a separate file of the same size; and collects statistical information on its performance. The reducer further calculates the overall statistics for all maps. It outputs the following data:	read or write test	date and time the test finished	number of files	total number of bytes processed	overall throughput in mb/sec	average IO rate in mb/sec per file_Results_I run 7 iterations of the test one after another on a cluster of ~200 nodes. The file size is the same in all cases 320Mb. The number of files tried is 1;2;4;8;16;32;64.The log file with statistics is attached.It looks like we don't have any distributed computing here at all.The total execution time increases proportionally to the total size of data both for writes and reads.Another thing is that the io ratio for read is higher than the write rate just gradually.For comparison I attach time measuring for the same ios performed on the same cluster but sequentially in a simple loop.This is the summary:Files	map/red time	sequential time 1		49			  34  2		86			  69 4		158			131 8		299			26616		569			53232		113164		2218This doesn't look good; unless there is something wrong with my test (attached) or the cluster settings.",Closed,Won't Fix,,Unassigned,Konstantin Shvachko,Fri; 10 Mar 2006 04:39:51 +0000,Wed; 8 Jul 2009 16:51:41 +0000,Sat; 22 Apr 2006 00:27:50 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-72
HADOOP-73,Bug,Trivial,,bin/hadoop dfs -rm works only for absolute paths,every dfs command like -du works with relative paths but remove only works only for absolute paths.,Closed,Cannot Reproduce,,Sameer Paranjpye,Stefan Groschupf,Fri; 10 Mar 2006 07:47:30 +0000,Wed; 8 Jul 2009 16:41:48 +0000,Sat; 25 Mar 2006 05:35:21 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-73
HADOOP-74,Improvement,Major,,hash blocks into dfs.data.dirs,When dfs.data.dir has multiple values; we currently start a DataNode for each (all in the same JVM).  Instead we should run a single DataNode that stores block files into the different directories.  This will reduce the number of connections to the namenode.  We cannot hash because different devices might be different amounts full.  So the datanode will need to keep a table mapping from block id to file location; and add new blocks to less full devices.,Closed,Duplicate,HADOOP-64,Konstantin Shvachko,Doug Cutting,Sat; 11 Mar 2006 06:33:10 +0000,Wed; 8 Jul 2009 16:41:49 +0000,Sat; 25 Mar 2006 06:18:20 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-74
HADOOP-75,Improvement,Minor,,dfs should check full file availability only at close,Currently it appears that dfs checks that the namenode knows about all blocks in a file as each block is written.  It would be more efficient to only check that all blocks are stored somewhere when the file is closed.,Closed,Fixed,,Milind Bhandarkar,Doug Cutting,Sat; 11 Mar 2006 06:35:35 +0000,Wed; 8 Jul 2009 16:41:50 +0000,Wed; 31 May 2006 06:01:46 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-75
HADOOP-76,Improvement,Minor,,Implement speculative re-execution of reduces,As a first step; reduce task outputs should go to temporary files which are renamed when the task completes.,Closed,Fixed,HADOOP-253,Sanjay Dahiya,Doug Cutting,Sat; 11 Mar 2006 06:57:00 +0000,Thu; 2 May 2013 02:29:02 +0000,Tue; 21 Nov 2006 20:40:44 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-76
HADOOP-77,Bug,Critical,,hang / crash when input folder does not exists.,"The jobtracker hangs and jobtracker info server throws Internal Server Error when until task initialization a exception will be thrown. Future jobs will no processed and also the job info server does not show any information since it throws a a http 500.This is a show blocker especially when hadoop is shell script driven. 060312 235707 TaskInProgress tip_6jd6g8 has failed 4 times.060312 235707 Aborting job job_hsg7y8060312 235708 Server connection on port 50020 from  XX.100.XXX.2: exiting060312 235709 Server connection on port 50020 from  XX.100.XXX.2: starting060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml060312 235710 parsing file:/home/myuser/nutch/conf/hadoop-site.xml060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml060312 235710 parsing file:/home/myuser/nutch/conf/hadoop-site.xml060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml060312 235710 parsing /u1/hadoop-data/tmp/hadoop/mapred/local/jobTracker/job_2p6ywq.xml060312 235710 parsing file:/home/myuser/nutch/conf/hadoop-site.xml060312 235711 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml060312 235711 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml060312 235711 parsing /u1/hadoop-data/tmp/hadoop/mapred/local/jobTracker/job_2p6ywq.xml060312 235711 parsing file:/home/myuser/nutch/conf/hadoop-site.xml060312 235712 job init failedjava.io.IOException: Not a file: /user/myuser/segments/20060312214035/crawl_fetch/part-00001/data        at org.apache.hadoop.mapred.InputFormatBase.getSplits(InputFormatBase.java:99)        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:127)        at org.apache.hadoop.mapred.JobTracker$JobInitThread.run(JobTracker.java:208)        at java.lang.Thread.run(Thread.java:595)Exception in thread ""Thread-20"" java.lang.NullPointerException        at org.apache.hadoop.mapred.JobInProgress.kill(JobInProgress.java:437)        at org.apache.hadoop.mapred.JobTracker$JobInitThread.run(JobTracker.java:212)        at java.lang.Thread.run(Thread.java:595)060312 235713 Server connection on port 50020 from  XX.100.XXX.2: exiting...60312 235715 parsing file:/home/myuser/nutch/conf/hadoop-site.xml060312 235755 /jobtracker.jsp: java.lang.NullPointerException        at org.apache.hadoop.mapred.JobInProgress.finishedMaps(JobInProgress.java:205)        at org.apache.hadoop.mapred.jobtracker_jsp.generateJobTable(jobtracker_jsp.java:67)        at org.apache.hadoop.mapred.jobtracker_jsp._jspService(jobtracker_jsp.java:130)        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)        at org.mortbay.http.HttpServer.service(HttpServer.java:954)        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)060313 014526 /jobtracker.jsp:",Closed,Fixed,,Unassigned,Stefan Groschupf,Mon; 13 Mar 2006 10:05:40 +0000,Wed; 8 Jul 2009 16:51:41 +0000,Tue; 14 Mar 2006 04:39:29 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-77
HADOOP-78,Bug,Major,ipc,rpc commands not buffered,Calls using Hadoop's RPC framework get sent across the network byte by byte.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 14 Mar 2006 06:48:27 +0000,Thu; 3 Aug 2006 17:46:31 +0000,Tue; 14 Mar 2006 07:17:51 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-78
HADOOP-79,Improvement,Major,,listFiles optimization,In FSDirectory.getListing() looking at linelistingi = new DFSFileInfo(curName; cur.computeFileLength(); cur.computeContentsLength(); isDir(curName));1. computeContentsLength() is actually calling computeFileLength(); so this is called twice;meaning that file length is calculated twice.2. isDir() is looking for the INode (starting from the rootDir) that has actually been obtainedjust two lines above; note that the tree is locked by that time.I propose a simple optimization for this; see attachment.3. A related question: Why DFSFileInfo needs 2 separate fields len for file length andcontentsLen for directory contents size? It looks like these fields are mutually exclusive;and we can use just one; interpreting it one way or another with respect to the value of isDir.,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Tue; 14 Mar 2006 07:53:30 +0000,Wed; 8 Jul 2009 16:41:49 +0000,Wed; 15 Mar 2006 04:57:23 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-79
HADOOP-80,New Feature,Major,io,binary key,I needed a binary key type; so I extended BytesWritable to be comparable also.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 14 Mar 2006 15:18:51 +0000,Thu; 3 Aug 2006 17:46:31 +0000,Wed; 22 Mar 2006 02:47:17 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-80
HADOOP-81,Bug,Major,,speculative execution is only controllable from the default config,The application's JobConf is not consulted when checking whether speculative execution should be used.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 15 Mar 2006 15:26:04 +0000,Wed; 8 Jul 2009 16:51:41 +0000,Thu; 16 Mar 2006 08:02:15 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-81
HADOOP-82,Bug,Minor,,JobTracker loses it: NoSuchElementException,On a number of occasions; JobTracker goes into a loop that it never recovers from.  Over and over it prints the below to the jobtracker log.060304 124522 Server handler 5 on 8010 call error: java.io.IOException: java.util.NoSuchElementExceptionjava.io.IOException: java.util.NoSuchElementException   at java.util.TreeMap.key(TreeMap.java:433)   at java.util.TreeMap.firstKey(TreeMap.java:287)   at java.util.TreeSet.first(TreeSet.java:407)   at org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(TaskInProgress.java:428Timed out.org.apache.hadoop.fs.ChecksumException: Checksum error:/2/hadoop/nara/data/tmp/task_r_m80hob/all.1 at 1554810368 atorg.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122)at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98)at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158)at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) atjava.io.BufferedInputStream.read(BufferedInputStream.java:235) atorg.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:210)at java.io.DataInputStream.readInt(DataInputStream.java:353) atorg.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:367) atorg.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:557)at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:523)at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:511)at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:254) atorg.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:666)I added debug to TIP#getTaskToRun so i could tell which TIP had empted its allotment of tasks.  Below is extract from jobtracker log that shows sequence of events for TIP tip_fizr7m that lead up to JT losing it:060314 203637 Adding task 'task_m_4d6ht0' to tip tip_fizr7m; for tracker 'tracker_41791' on ia109314.archive.org 060314 204758 Task 'task_m_4d6ht0' has been lost.060314 204811 Adding task 'task_m_fb0wf0' to tip tip_fizr7m; for tracker 'tracker_70065' on ia109314.archive.org060314 210118 Task 'task_m_fb0wf0' has been lost.060314 210119 Adding task 'task_m_irar47' to tip tip_fizr7m; for tracker 'tracker_82285' on ia109324.archive.org060314 211541 Taskid 'task_m_irar47' has finished successfully.060314 211541 Task 'task_m_irar47' has completed.060314 211543 Adding task 'task_m_qo1g69' to tip tip_fizr7m; for tracker 'tracker_97839' on ia109306.archive.org060314 213004 Taskid 'task_m_qo1g69' has finished successfully.060314 213004 Task 'task_m_qo1g69' has completed.060314 213005 Adding task 'task_m_t0lnzk' to tip tip_fizr7m; for tracker 'tracker_57273' on ia109314.archive.org060314 214118 Task 'task_m_t0lnzk' has been lost.So; we lose two; complete two; then lose a third.TIP should have been done on first completion.TIP accounting is off.,Closed,Fixed,,Unassigned,stack,Thu; 16 Mar 2006 01:15:11 +0000,Thu; 3 Aug 2006 17:46:31 +0000,Thu; 16 Mar 2006 08:05:06 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-82
HADOOP-83,Bug,Major,,infinite retries accessing a missing block,A file in the DFS got corrupted - the reason for that is unknown; but might be justified.when accessing the file; I get an infinite stream of error messages from the client - attached below.The client aparently increments an error counter; but never checks it.Correct behaviour is for the client to retry a few times; then abort.060315 105436 No node available for block blk_2690692619196463439060315 105436 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105446 No node available for block blk_2690692619196463439060315 105446 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105456 No node available for block blk_2690692619196463439060315 105456 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105506 No node available for block blk_2690692619196463439060315 105506 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105516 No node available for block blk_2690692619196463439060315 105516 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105526 No node available for block blk_2690692619196463439060315 105526 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105536 No node available for block blk_2690692619196463439060315 105536 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105546 No node available for block blk_2690692619196463439060315 105546 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105556 No node available for block blk_2690692619196463439060315 105556 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105606 No node available for block blk_2690692619196463439060315 105606 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105616 No node available for block blk_2690692619196463439060315 105616 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105626 No node available for block blk_2690692619196463439060315 105626 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105636 No node available for block blk_2690692619196463439060315 105636 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105646 No node available for block blk_2690692619196463439060315 105646 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105657 No node available for block blk_2690692619196463439060315 105657 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105707 No node available for block blk_2690692619196463439060315 105707 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105717 No node available for block blk_2690692619196463439060315 105717 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105727 No node available for block blk_2690692619196463439060315 105727 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105737 No node available for block blk_2690692619196463439060315 105737 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105747 No node available for block blk_2690692619196463439060315 105747 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105757 No node available for block blk_2690692619196463439060315 105757 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105807 No node available for block blk_2690692619196463439060315 105807 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105817 No node available for block blk_2690692619196463439,Closed,Fixed,HADOOP-35,Konstantin Shvachko,Yoram Arnon,Thu; 16 Mar 2006 02:59:24 +0000,Wed; 8 Jul 2009 16:41:50 +0000,Fri; 24 Mar 2006 02:13:29 +0000,,0.1.0,,,HADOOP-1911,https://issues.apache.org/jira/browse/HADOOP-83
HADOOP-84,Bug,Minor,,client should report file name in which IO exception occurs,A file in the DFS got corrupted somehow.The client gets an exception accessing a block in the file:060315 105907 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block060315 105917 No node available for block blk_2690692619196463439It could (and should) report the file in which the error occurred; together with the block information.,Closed,Fixed,,Konstantin Shvachko,Yoram Arnon,Thu; 16 Mar 2006 03:01:31 +0000,Wed; 8 Jul 2009 16:41:49 +0000,Fri; 31 Mar 2006 03:46:48 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-84
HADOOP-85,Bug,Major,,a single client stuck in a loop blocks all clients on same machine,I was running a set of clients; running cp from one dfs to another dfs using fuse; one file per cp process.One client got stuck in a loop because of a bad block in one of the files (separate bug filed).The other clients; running in parallel in the background; all stopped making progress on their respective files.,Closed,Duplicate,HADOOP-83,Sameer Paranjpye,Yoram Arnon,Thu; 16 Mar 2006 05:44:09 +0000,Wed; 8 Jul 2009 16:41:49 +0000,Tue; 2 May 2006 02:50:09 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-85
HADOOP-86,Bug,Major,,If corrupted map outputs; reducers get stuck fetching forever,In our rack; there is a machine that reliably corrupts map output parts.  When reducers try to pickup the map output; Server#Handler checks the checksum; notices corruption; moves the bad map output part aside and throws a ChecksumException.  Undeterred; the reducer comes back again minutes later only this time it gets a FileNotFoundException out of Server#Handler (Because the part was moved aside).  And so it goes till the cows come home.Doug applied a patch that in map output  file; when it notices a fatal exception; it logs a severe error on the TaskTracker#LOG. Then in TT; if a severe logging has occurred; TT does a soft restart (TT stays up but closes down all services and then goes through init again).  This patch was committed (after I suggested it was working); only; later; I noticed the severe log flag is not cleared across TT restart so TT goes into a cycle of continuous restarts.  A further patch that clears the severe flag was posted to the list.  This improves things but has issues too in that on revival; the TT continues to be plagued by reducers looking for parts no longer available for a period of ten minutes or so until the JobTracker gets around to updating them about change in where to go get map outputs.  During this period; the TT gets restarted 5-10 times  but eventually comes back on line (There may have been too much damage done during this period of flux making it so the job will fail).This issue covers implementing a better solution.  Suggestions include having the TT stay down a period to avoid the incoming reducers or somehow examining the incoming reducer request; checking its list of tasks to see if it knows anything of the reducers' request and rejecting it with a non-severe error if not a map of the currently running TT.  A little birdie (named DC) suggests a better soln. is probably an addition to intertrackerprotocol so either the TT or the reducer updates JT when corrupted map output.,Closed,Fixed,,Doug Cutting,stack,Fri; 17 Mar 2006 01:58:09 +0000,Thu; 3 Aug 2006 17:46:32 +0000,Tue; 21 Mar 2006 03:08:51 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-86
HADOOP-87,Improvement,Major,io,SequenceFile performance degrades substantially compression is on and large values are encountered,The code snippet in quesiton is:     if (deflateValues) {        deflateIn.reset();        val.write(deflateIn);        deflater.reset();        deflater.setInput(deflateIn.getData(); 0; deflateIn.getLength());        deflater.finish();        while (!deflater.finished()) {          int count = deflater.deflate(deflateOut);          buffer.write(deflateOut; 0; count);        }      } else {A couple of issues with this code:1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer'; this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If; for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the entire 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem.2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second; the value is copied over the JNI boundary in every iteration of the while loop. Third; the compressed data is copied piecemeal into 'deflateOut'. Finally; it is appended to 'buffer'.Proposed fix:1. Don't let big buffers persist. Allow 'deflateIn' to grow to a persistent maximum reasonable size; say 64KB. If a larger value is encountered; grow the buffer in order to process the value; then shrink it back to the maximum size. To do this; we add a 'reset' method which takes a buffer size.2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer; of course; needs to shrink as well.,Closed,Fixed,,Doug Cutting,Sameer Paranjpye,Fri; 17 Mar 2006 06:49:48 +0000,Thu; 3 Aug 2006 17:46:32 +0000,Wed; 22 Mar 2006 05:42:02 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-87
HADOOP-88,Wish,Minor,conf,Configuration: separate client config from server config (and from other-server config),"servers = JobTracker; NameNode; TaskTracker; DataNodeclients =  runs JobClient (to submit MapReduce jobs); or runs DFSShell (to browse )Server machines are administered together.So it is OK to have all server config together (esp file paths and network ports).This is stored in hadoop-default.xml or hadoop-mycluster.xmlClient machines:there may be as many client machines as there are MapRed developers.the temp space for DFS needs to be writable by the active user.So it should be possible to select the client temp space directory for the machine and for the user.(The global /tmp is not an option as discussed elsewhere: partition may be full)Current situation: Both the server and the clients have a copy of the server config: hadoop-default.xmlBut the XML property  ""dfs.data.dir"" is being used as a LOCAL directory path on both the server machines (Data nodes) and the client machines.Effect:Exception in thread ""main"" java.io.IOException: No valid local directories in property: dfs.data.dir at org.apache.hadoop.conf.Configuration.getFile(Configuration.java:286) at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.newBackupFile(DFSClient.java:560) ... at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:267)Current Workaround:On the client use hadoop-site.xml to override dfs.data.dirOne proposed solution:For the purpose of JobClient operations; use a different property in place of dfs.data.dir.(Ex: dfs.client.data.dir) On the client; set this property in hadoop-site.xml so that it will override hadoop-default.xml Another proposed solution:Handle the fact that the world is made of a federation of independant Hadoop systems.They can talk to each other (as peers) but they are administered separately.Each Hadoop system should have its own separate XML config file.Clients should be able to specify the Hadoop system they want to talk to.An advantage is that clients can then easily sync their local copy of a given Hadoop system config: just pull its config fileIn this view of the world; a Job client is also a kind of independant (serverless) Hadoop systemIn this case the client config file may have its own dfs.data.dir; which is separate from the dfs.data.dir in the server config file.",Closed,Duplicate,NULL,Arun C Murthy,Michel Tourn,Fri; 17 Mar 2006 09:15:54 +0000,Wed; 3 Oct 2007 18:06:12 +0000,Thu; 27 Sep 2007 22:02:32 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-88
HADOOP-89,Bug,Critical,,files are not visible until they are closed,the current behaviour; whereby a file is not visible until it is closed has several flaws;including:1. no practical way to know if a file/job is progressing2. no way to implement files that never close; such as log files3. failure to close a file results in loss of the fileThe part of the file that's written should be visible.,Closed,Fixed,,dhruba borthakur,Yoram Arnon,Fri; 17 Mar 2006 10:16:35 +0000,Wed; 8 Jul 2009 16:41:50 +0000,Wed; 19 Sep 2007 22:14:20 +0000,,0.1.0,,,HADOOP-1708;HADOOP-1700,https://issues.apache.org/jira/browse/HADOOP-89
HADOOP-90,Bug,Major,,DFS is succeptible to data loss in case of name node failure,"Currently; DFS name node stores its log and state in local files.This has the disadvantage that a hardware failure of the name node causes a total data loss. Several approaches may be used to address this flaw:1. replicate the name server state files using copy or rsync once in a while; either manually or using a cron job.2. set up secondary name servers and a protocol whereby the primary updates the secondaries. In case of failure; a secondary can take over.3. store the state files as distributed; replicated files in the DFS itself. The difficulty is that it becomes a bootstrap problem; where the name node needs some information; typically stored in its state files; in order to read those same state files.solution 1 is fine for non critical systems; but for systems that need to guarantee no data loss it's insufficient.Solutions 2 and 3 both seem valid; 3 seems more elegant in that it doesn't require an extra protocol; it leverages the DFS and allows any level of replication for robustness. Below is a proposition for  solution 3.1.	The name node; when it starts up; needs some basic information. That information is not large and can easily be stored in a single block of DFS. We hard code the block location; using block id 0. Block 0 will contain the list of blocks that contain the name node metadata - not the metadata itself (file names; servers; blocks etc); just the list of blocks that contain it. With a block identified by 8 bytes; and 32 MB blocks; we can fit 256K block id's in block 0. 256K blocks of 32MB each can hold 8TB of metadata; which can map a large enough file system; so a single block of block_ids is sufficient.2.	The name node writes his state basically the same way as now: log file plus occasional full state. DFS needs to change to commit changes to open files while allowing continued writing to them; or else the log file wouldn't be valid on name server failure; before the file is closed. 3.	The name node will use double buffering for its state; using blocks 0 and 1. Starting with block 0; it writes its state; then a log of changes. When it's time to write a new state it writes it to node 1. The state includes a generation number; a single byte starting at 0; to enable the name server to identify the valid state. A CRC is written at the end of the block to mark its validity and completeness. The log file is identified by the same generation number as the state it relates to. 4.	The log file will be limited to a single block as well. When that block fills up a new state is written. 32MB of transaction logs should suffice. If not; we could set aside a set of blocks; and set aside a few locations in the super-block (block 0/1) to store that set of block ids.5.	The super-block; the log and the metadata blocks may be exposed as read only files in reserved files in the DFS: /.metadata/* or something.6.	When a name nodes starts; it waits for data nodes to connect to it to report their blocks. It waits until it gets a report about blocks 0 and 1; from which it can continue to read its entire state. After that it continues normally.",Closed,Fixed,,Konstantin Shvachko,Yoram Arnon,Fri; 17 Mar 2006 10:49:53 +0000,Wed; 8 Jul 2009 16:41:49 +0000,Tue; 31 Oct 2006 22:14:05 +0000,,0.7.2,,,,https://issues.apache.org/jira/browse/HADOOP-90
MAPREDUCE-443,New Feature,Minor,,snapshot a map-reduce to DFS ... and restore,The idea is to be able to issue a command to the job tracker thatwill halt a map-reduce and archive it to a directory in such a waythat it can later be restarted.We could also set a mode that would cause this to happen to a jobwhen it fails.  This would allow one to debug and restart a failingjob reasonably; which might be important; for long running jobs.  Ithas certainly been important in similar systems I've seen before.  One could restart with a new jar or work bench a single failing map or reduce.,Resolved,Duplicate,MAPREDUCE-457,Owen O'Malley,eric baldeschwieler,Fri; 17 Mar 2006 12:26:20 +0000,Fri; 18 Jul 2014 17:37:10 +0000,Fri; 18 Jul 2014 17:36:26 +0000,,,,,HADOOP-313;MAPREDUCE-460;MAPREDUCE-452,https://issues.apache.org/jira/browse/MAPREDUCE-443
HADOOP-92,Bug,Minor,,Error Reporting/logging in MapReduce,Currently Mapreduce does not tell you which machine failed to execute the task. Also; it would be nice to have features wherein there is a log report with each job; saying the number of tasks it ran (reporting which one failed and on which machine; listing any error information it can)  with  the start/end/execute time of each task.,Closed,Fixed,,Mahadev konar,Mahadev konar,Fri; 17 Mar 2006 12:48:15 +0000,Wed; 8 Jul 2009 16:51:41 +0000,Thu; 13 Apr 2006 07:04:45 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-92
HADOOP-93,Bug,Major,,allow minimum split size configurable,The current default split size is the size of a block (32M) and a SequenceFile sets it to be SequenceFile.SYNC_INTERVAL(2K). We currently have a Map/Reduce application working on crawled docuements. Its input data consists of 356 sequence files; each of which is of a size around 30G. A jobtracker takes forever to launch the job because it needs to generate 356*30G/2K map tasks!The proposed solution is to let the minimum split size configurable so that the programmer can control the number of tasks to generate.,Closed,Fixed,,Doug Cutting,Hairong Kuang,Sat; 18 Mar 2006 03:59:07 +0000,Wed; 8 Jul 2009 16:51:41 +0000,Wed; 22 Mar 2006 01:38:19 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-93
HADOOP-94,Bug,Major,,disallow more than one datanode running on one computing sharing the same data directory,Currently dfs disallows more one datanode to run on the same computer if they are started up using the same hadoop conf dir. However; this does not prevent more than one data node gets started; each using a different conf dir (strickly speaking; a different pid file). If every machine has two such datanodes running; namenode will be busy on deleting and replicating blocks or eventually lead to block loss.Suggested solution: put pid file in  the data directory and disallow configuration.,Closed,Duplicate,HADOOP-124,Unassigned,Hairong Kuang,Sat; 18 Mar 2006 09:24:31 +0000,Wed; 8 Jul 2009 16:41:50 +0000,Wed; 3 May 2006 06:07:57 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-94
HADOOP-95,Improvement,Major,,dfs validation,Dfs needs a validation operation similiar to fsck; so that we get to know the files that are corrupted and which data blocks are missing.Dfs namenode also needs to log more specific information such as which block is replication or is deleted. So when something goes wrong; we have a clue what has happened.,Closed,Duplicate,NULL,Unassigned,Hairong Kuang,Sat; 18 Mar 2006 09:31:35 +0000,Wed; 8 Jul 2009 16:41:49 +0000,Wed; 18 Oct 2006 18:31:16 +0000,,0.1.0,,,HADOOP-500,https://issues.apache.org/jira/browse/HADOOP-95
HADOOP-96,Improvement,Critical,,name server should log decisions that affect data: block creation; removal; replication,"currently; there's no way to analyze and debug DFS errors where blocks disapear.name server should log its decisions that affect data; including block creation; removal; replication:	block b created; assigned to datanodes A; B; ...	datanode A dead; block b underreplicated(1); replicating to datanode C	datanode B dead; block b underreplicated(2); replicating to datanode D	datanode A alive; block b overreplicated; removing from datanode D	block removed from datanodes C; D; ...that will enable me to track down; two weeks later; a block that's missing from a file; and to debug the name server.extra credit:	rotate log file; as it might grow large	make this behaviour optional/configurable",Closed,Fixed,,Hairong Kuang,Yoram Arnon,Wed; 22 Mar 2006 01:09:50 +0000,Wed; 8 Jul 2009 16:41:50 +0000,Sat; 6 May 2006 00:08:43 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-96
HADOOP-97,Bug,Major,,DFSShell.cat returns NullPointerException if file does not exist,DFSShell.cat crashes with a NullPointerException if file to be displayed does not exist.The bug is fixed in the attached patch; and the general exception handling in main() is added.,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Wed; 22 Mar 2006 04:27:07 +0000,Wed; 8 Jul 2009 16:41:50 +0000,Wed; 22 Mar 2006 06:02:57 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-97
HADOOP-98,Bug,Major,,The JobTracker's count of the number of running maps and reduces is wrong,"When a heatbeat comes in from a task tracker; the job tracker just adds the number of currently running maps and reduces. The jobs from the previous heartbear are never subtracted. This causes the scheduling to misjudge the ""loading"" levels of the task trackers.",Closed,Fixed,,Owen O'Malley,Owen O'Malley,Thu; 23 Mar 2006 16:13:39 +0000,Wed; 8 Jul 2009 16:51:42 +0000,Fri; 24 Mar 2006 04:08:21 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-98
HADOOP-99,Bug,Major,,task trackers can only be assigned one task every heartbeat,Task trackers only call pollForNewTask once per a heartbeat (10 seconds) rather than when a task finishes. Especially for quick maps this means that the cluster is under utilized; because each map finishes in a cycle and the task tracker never gets a second; third or fourth task to run.,Closed,Fixed,HADOOP-305,Owen O'Malley,Owen O'Malley,Thu; 23 Mar 2006 16:30:16 +0000,Wed; 8 Jul 2009 16:51:42 +0000,Thu; 29 Jun 2006 00:43:04 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-99
HADOOP-100,Bug,Major,,Inconsistent locking of the JobTracker.taskTrackers field,The JobTracker is using an inconsistant lock for protecting taskTrackers; which is the list of current task trackers. Some of the routines lock the JobTracker and others lock the taskTrackers field.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 24 Mar 2006 01:08:27 +0000,Wed; 8 Jul 2009 16:51:41 +0000,Fri; 31 Mar 2006 09:00:50 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-100
HADOOP-101,New Feature,Major,,DFSck - fsck-like utility for checking DFS volumes,This is a utility to check health status of a DFS volume; and collect some additional statistics.,Closed,Fixed,,Andrzej Bialecki,Andrzej Bialecki,Fri; 24 Mar 2006 02:50:45 +0000,Tue; 13 Oct 2009 21:25:35 +0000,Thu; 30 Mar 2006 04:25:41 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-101
HADOOP-102,Bug,Major,,Two identical consecutive loops in FSNamesystem.chooseTarget(),I think this was meant to be the way I corrected.Otherwise there is no difference in two loops except for doublebracketing in the if statement.,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Fri; 24 Mar 2006 10:03:45 +0000,Wed; 8 Jul 2009 16:41:51 +0000,Fri; 31 Mar 2006 09:38:50 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-102
HADOOP-103,Improvement,Minor,,introduce a common parent class for Mapper and Reducer,I'd like to a base class that implements the default (empty) bodies for configure and close so that Mapper and Reducer classes that derive from it; which is optional; don't have to implement those methods.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Sat; 25 Mar 2006 06:07:04 +0000,Wed; 8 Jul 2009 16:51:39 +0000,Fri; 31 Mar 2006 07:17:42 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-103
HADOOP-104,Bug,Major,,Reflexive access to non-public class with public ctor requires setAccessible (with some JVMs),"Multiple times I have hit this problem which prevents the NameNode from starting.The only fix I had so far was to loose all my DFS data...Exception in thread ""main"" java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)        at org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:81)        at org.apache.hadoop.dfs.FSDirectory.loadFSEdits(FSDirectory.java:374)        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:347)        at org.apache.hadoop.dfs.FSDirectory.init(FSDirectory.java:258)        at org.apache.hadoop.dfs.FSNamesystem.init(FSNamesystem.java:151)        at org.apache.hadoop.dfs.NameNode.init(NameNode.java:82)According to this; http://forum.java.sun.com/thread.jspa?threadID=704100messageID=4082902this is a known issue when usingreflexive access to a non-public class with public ctor (class org.apache.hadoop.dfs.Block is such a class)This problem may not occur with all JVM releases. (I build on 1.5.0-b64 and run on 1.5.0_05-b05)This problem only occured for me when I upgrade code or change XML configuration AND have existing files in the DFS.This problem does not occur when I just stop / restart the NameServer.In any case; the attached patch fixes it by calling setAccessiblebefore constructing the instance with reflection.",Closed,Duplicate,HADOOP-367,Hairong Kuang,Michel Tourn,Sat; 25 Mar 2006 07:07:06 +0000,Wed; 13 Sep 2006 03:53:24 +0000,Wed; 13 Sep 2006 03:53:14 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-104
HADOOP-105,Improvement,Major,,DFS commands either do not support some popular unix commands or have inconsistent behaviors,Several issues.1. DFS commandline does not support rmdir2. When executing a command like dfs -rmdir PATH; it goes through silently; without warning that -rmdir is not supported3. DFS -rm PATH works even though PATH is a non-empty directory; which is inconsistent with unix convention.,Closed,Duplicate,HADOOP-431,Sameer Paranjpye,Runping Qi,Sat; 25 Mar 2006 09:50:17 +0000,Wed; 8 Jul 2009 16:41:51 +0000,Thu; 31 Aug 2006 18:38:10 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-105
HADOOP-106,Wish,Major,,Data blocks should be record-oriented.,"If data blocks were starting and ending on data record boundaries; and not in random places within a file; it would give some important advantages:	it would be possible to avoid ""fishing"" for the beginning of first record in a split (see SequenceFile.Reader.sync()).	it would make recovering from DFS errors much more successful and easier - in most cases missing blocks could be just skipped and the remaining parts combined together.",Closed,Won't Fix,,Sameer Paranjpye,Andrzej Bialecki,Sun; 26 Mar 2006 04:21:10 +0000,Sun; 19 Jun 2011 19:48:13 +0000,Tue; 23 Oct 2007 22:13:47 +0000,,0.2.0,,,HADOOP-7404,https://issues.apache.org/jira/browse/HADOOP-106
HADOOP-107,Bug,Major,,"Namenode errors ""Failed to complete filename.crc  because dir.getFile()==null and null""",We're getting lot of these errors and here is what I see in namenode log: 060327 002016 Removing lease Lease.  Holder: DFSClient_1897466025; heldlocks: 0; pendingcreates: 0; leases remaining: 1060327 002523 Block report from member2.local:50010: 91895 blocks.060327 003238 Block report from member1.local:50010: 91895 blocks.060327 005830 Failed to complete /feedback/.feedback_10.1.10.102-33877.log.crc  because dir.getFile()==null and null060327 005830 Server handler 1 on 50000 call error: java.io.IOException: Could not complete write to file /feedback/.feedback_10.1.10.102-33877.log.crc by DFSClient_1897466025java.io.IOException: Could not complete write to file /feedback/.feedback_10.1.10.102-33877.log.crc by DFSClient_1897466025        at org.apache.hadoop.dfs.NameNode.complete(NameNode.java:205)        at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        at java.lang.reflect.Method.invoke(Method.java:585)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:237)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:216)I can't be 100% sure; but it looks like these errors happen with checksum files for very small data files.,Closed,Fixed,,Doug Cutting,Igor Bolotin,Tue; 28 Mar 2006 02:04:46 +0000,Wed; 8 Jul 2009 16:41:51 +0000,Fri; 31 Mar 2006 07:38:30 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-107
HADOOP-108,Bug,Major,,EOFException in DataNode$DataXceiver.run,This morning - after upgrade of the system - something got wrong and we started to get lot of exceptions.Situation didn't change after removing everything and creating new file system. Multiple exceptions on all data nodes:060328 145922 108 DataXCeiverjava.io.EOFException        at java.io.DataInputStream.readFully(DataInputStream.java:178)        at java.io.DataInputStream.readLong(DataInputStream.java:380)        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:448)        at java.lang.Thread.run(Thread.java:595)No errors on the name node.DFS clients report following:060328 150923 task_r_2twzsl  Error while writing.060328 150923 task_r_2twzsl java.net.SocketTimeoutException: Read timed out060328 150923 task_r_2twzsl     at java.net.SocketInputStream.socketRead0(Native Method)060328 150923 task_r_2twzsl     at java.net.SocketInputStream.read(SocketInputStream.java:129)060328 150923 task_r_2twzsl     at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)060328 150923 task_r_2twzsl     at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)060328 150923 task_r_2twzsl     at java.io.BufferedInputStream.read(BufferedInputStream.java:313)060328 150923 task_r_2twzsl     at java.io.DataInputStream.readFully(DataInputStream.java:176)060328 150923 task_r_2twzsl     at java.io.DataInputStream.readLong(DataInputStream.java:380)060328 150923 task_r_2twzsl     at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.internalClose(DFSClient.java:776)060328 150923 task_r_2twzsl     at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:751)060328 150923 task_r_2twzsl     at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:814)060328 150923 task_r_2twzsl     at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:202),Closed,Duplicate,NULL,Unassigned,Igor Bolotin,Wed; 29 Mar 2006 08:15:19 +0000,Wed; 8 Jul 2009 16:41:51 +0000,Sat; 27 May 2006 01:37:26 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-108
HADOOP-109,Bug,Major,,Blocks are not replicated when...,When the block is under-replicated the namenode places it intoFSNamesystem.neededReplications list.When a datanode D1 sends getBlockwork() request to the namenode; the namenodeselects another node D2 (which it thinks is up and running) where the new replica of theunder-replicated block will be stored.Then namenode removes the block from the neededReplications list and places it tothe pendingReplications list; and then asks D1 to replicate the block to D2.If D2 is in fact down; then replication will fail and will never be retried later; becausethe block is not in the neededReplications list; but rather in the pendingReplications list;which namenode never checks.,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Wed; 29 Mar 2006 12:43:07 +0000,Wed; 8 Jul 2009 16:41:51 +0000,Wed; 28 Feb 2007 20:18:50 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-109
HADOOP-110,Bug,Major,,new key and value instances are allocated before each map,Each time map is called with a new key and value rather than reusing the old ones.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Thu; 30 Mar 2006 02:11:17 +0000,Wed; 8 Jul 2009 16:51:42 +0000,Fri; 31 Mar 2006 05:14:42 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-110
HADOOP-111,Bug,Major,,JobClient.runJob() should return exit status for a job.,"JobClient.runJob() doesn't return any values. Any information about the exit status of a job is discarded (or appears only in logs).It should be possible to return the exit status; so that JobClient users can determine whether a job was successfully completed or not. This is also important when using cmd-line tools in shell scripts - currently they don't return any exit codes; because it's not possible to determine the outcome of a job submitted through JobClient. As a consequence; it's difficult to automate repetitive jobs using shells scripts.It would be also nice to have an exit message in case of errors; for human consumption.I propose to implement one of the following:	change the return type of this method from void to int	or; better yet; to put the exit code and optional exit messages inside the JobConf instance under pre-defined keys.",Closed,Duplicate,NULL,Owen O'Malley,Andrzej Bialecki,Thu; 30 Mar 2006 19:49:38 +0000,Wed; 8 Jul 2009 16:51:42 +0000,Mon; 21 May 2007 17:27:43 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-111
HADOOP-112,Bug,Minor,,copyFromLocal should exclude .crc files,"Doug Cutting says: ""The problem is that when copyFromLocal enumerates local files it should exclude .crc files; but it does not. This is the listFiles() call on DistributedFileSystem:160.  It should filter this; excluding files that are FileSystem.isChecksumFile().BTW; as a workaround; it is safe to first remove all of the .crc files; but your files will no longer be checksummed as they are read.  On systems without ECC memory file corruption is not uncommon; but I have seen very little on clusters that have ECC.""Original observations:Hello Team;I created a backup of my DFS database:	bin/hadoop dfs -copyToLocal /user/root/crawl /mylocaldirI now want to restore from the backup using:	bin/hadoop dfs -copyFromLocal /mylocaldir/crawl /user/rootHowever I'm getting the following error:copyFromLocal: Target /user/root/crawl/crawldb/current/part-00000/.data.crcalready existsI get this message with every permutation of the command that I've tried; andeven after totally deleting all content in the DFS directories.I'd be grateful for any pointers.Many thanks;",Closed,Fixed,,Doug Cutting,Monu Ogbe,Thu; 30 Mar 2006 23:25:35 +0000,Wed; 8 Jul 2009 16:41:50 +0000,Sat; 8 Apr 2006 08:03:46 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-112
HADOOP-113,New Feature,Major,,Allow multiple Output Dirs to be specified for a job,Allow a single job to create multiple outputs. 2 additional simple functions onlyThis allows for more complex branching of the process to occur either with multiple steps of the same type or allow different actions to take place on each output directory depending on the required actions.For my specific use; it allows me to run multiple Generate Outputs instead of a single Generate Output as submitted in NUTCH-171(http://issues.apache.org/jira/browse/NUTCH-171),Closed,Won't Fix,,Unassigned,Rod Taylor,Fri; 31 Mar 2006 01:34:35 +0000,Wed; 8 Jul 2009 16:51:42 +0000,Thu; 24 May 2007 06:20:36 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-113
HADOOP-114,Improvement,Trivial,,Non-informative error message,060330 105006 mapred.child.heap.size is deprecated. Use mapred.child.heap.size instead. Meantime; interpolated child.heap.size into child.java.opt: -Xmx200mThe instructions inform you to use the deprecated option.,Closed,Fixed,,Doug Cutting,Rod Taylor,Fri; 31 Mar 2006 00:52:51 +0000,Wed; 8 Jul 2009 16:51:42 +0000,Wed; 19 Apr 2006 02:50:41 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-114
HADOOP-115,New Feature,Major,,permit reduce input types to differ from reduce output types,When map tasks write intermediate data out; they always use SequencialFile RecordWriter with key/value classes from the job object.When the reducers write the final results out; its output format is obtained from the job object. By default; it is TextOutputFormat; and no conflicts.However; if one wants to use SequencialFileFormat for the final results; then the key/value classes are also obtained from the job object; the same as the map tasks' output. Now we have a problem. It is impossible for the map outputs and reducer outputs use different key/value classes; if one wants the reducers generate outputs in SequentialFileFormat.A simple fix would be to add another two attributes to JobConf class: mapOutputLeyClass and mapOutputValueClass. That allows the user to have different key/value classes for the intermediate and final outputs.,Closed,Fixed,,Runping Qi,Runping Qi,Sat; 1 Apr 2006 00:30:32 +0000,Wed; 8 Jul 2009 16:51:42 +0000,Wed; 17 May 2006 06:38:26 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-115
HADOOP-116,Improvement,Major,,cleaning up /tmp/hadoop/mapred/system,Clean up directoriessubmit_ which contain job.xml and job.jar files as and when the job is finished,Closed,Fixed,,Doug Cutting,raghavendra prabhu,Tue; 4 Apr 2006 12:13:58 +0000,Wed; 8 Jul 2009 16:51:42 +0000,Sat; 8 Apr 2006 02:24:38 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-116
HADOOP-117,Bug,Blocker,,mapred temporary files not deleted,I found out that JobConf.javaCreated interchanged names with parent being file and file being parent directoryAs a result files were not getting deleted,Closed,Fixed,,Doug Cutting,raghavendra prabhu,Tue; 4 Apr 2006 12:46:44 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Wed; 5 Apr 2006 06:05:24 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-117
HADOOP-118,Bug,Critical,,Namenode does not always clean up pendingCreates,In some failure modes; the pending creates list is not cleaned up and prevents that file from ever being created.When I try to create the file after the first job was killed (hours previously); I get:060404 084619 Cannot start file because pendingCreates is non-null. src=/user/oom/rand/part000118060404 084619 Server handler 0 on 8020 call error: java.io.IOException: Cannot create file /user/oom/rand/part000118 on client DFSClient_-1656137458java.io.IOException: Cannot create file /user/oom/rand/part000118 on client DFSClient_-1656137458        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:147)        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        at java.lang.reflect.Method.invoke(Method.java:585)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:237)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:216),Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 5 Apr 2006 00:30:38 +0000,Wed; 8 Jul 2009 16:41:50 +0000,Wed; 19 Apr 2006 01:58:32 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-118
HADOOP-119,Bug,Major,,ReduceTask.configure() is called twice,ReduceTask.configure() is called twice for each created reduce task First call is done indirect from org.apache.hadoop.mapred.JobConf.newInstance()called in ReduceTask.run(). Second call was in ReduceTask.run() just after creating a new instance. I suggest to remove the second call. For all new instances created in case of map task there are no directly xxx.configure();,Closed,Cannot Reproduce,,Unassigned,Darek Zbik,Wed; 5 Apr 2006 04:27:00 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Wed; 9 Apr 2008 10:27:54 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-119
HADOOP-120,Bug,Major,io,Reading an ArrayWriter does not work because valueClass does not get initialized,If you have a Reducer whose value type is an ArrayWriter it gets enstreamed alright but at reconstruction type when ArrayWriter::readFields(DataInput in) runs on a DataInput that has a nonempty ArrayWriter ; newInstance fails trying to instantiate the null class.,Closed,Fixed,,Cameron Pope,Dick King,Wed; 5 Apr 2006 07:54:37 +0000,Mon; 5 Nov 2007 18:11:57 +0000,Tue; 18 Sep 2007 18:26:19 +0000,,0.15.0,,,,https://issues.apache.org/jira/browse/HADOOP-120
HDFS-110,Bug,Major,,Failed to execute fsck with -move option,"I received the following error when running fsck with -move option. The dfs was started by a user while the fsck was ran by a different user that does not have the write access to the hadoop dfs data directory.	moving to /lost+found: /data.txtjava.io.FileNotFoundException: hadoop-dfs-data-dir/tmp/client-8234960199756230677 (Permission denied)at java.io.FileOutputStream.open(Native Method)at java.io.FileOutputStream.init(FileOutputStream.java:179)at java.io.FileOutputStream.init(FileOutputStream.java:131)at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.init(DFSClient.java:546)at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:99)at org.apache.hadoop.dfs.DFSck.lostFoundMove(DFSck.java:222)at org.apache.hadoop.dfs.DFSck.check(DFSck.java:178)at org.apache.hadoop.dfs.DFSck.check(DFSck.java:124)at org.apache.hadoop.dfs.DFSck.fsck(DFSck.java:112)at org.apache.hadoop.dfs.DFSck.main(DFSck.java:433)Failed to move /data.txt to /lost+found: hadoop-dfs-data-dir/tmp/client-8234960199756230677 (Permission denied)",Resolved,Won't Fix,,Hairong Kuang,Hairong Kuang,Thu; 6 Apr 2006 08:00:00 +0000,Wed; 5 Jan 2011 00:05:45 +0000,Wed; 5 Jan 2011 00:05:45 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-110
HADOOP-123,New Feature,Major,,mini map/reduce cluster for junit tests,I would like a single process map-reduce pseduo-distributed cluster that can be used for unit tests. We should add a unit test with a word count map/reduce.,Closed,Fixed,,Milind Bhandarkar,Owen O'Malley,Fri; 7 Apr 2006 02:34:07 +0000,Wed; 8 Jul 2009 16:51:42 +0000,Tue; 20 Jun 2006 04:48:27 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-123
HADOOP-124,Bug,Critical,,don't permit two datanodes to run from same dfs.data.dir,"DFS files are still rotting.I suspect that there's a problem with block accounting/detecting identical hosts in the namenode. I have 30 physical nodes; with various numbers of local disks; meaning that my current 'bin/hadoop dfs -report"" shows 80 nodes after a full restart. However; when I discovered the  problem (which resulted in losing about 500gb worth of temporary data because of missing blocks in some of the larger chunks) -report showed 96 nodes. I suspect somehow there were extra datanodes running against the same paths; and that the namenode was counting those as replicated instances; which then showed up over-replicated; and one of them was told to delete its local block; leading to the block actually getting lost.I will debug it more the next time the situation arises. This is at least the 5th time I've had a large amount of file data ""rot"" in DFS since January.",Closed,Fixed,HADOOP-94,Konstantin Shvachko,Bryan Pendleton,Sat; 8 Apr 2006 01:45:55 +0000,Wed; 8 Jul 2009 16:41:51 +0000,Thu; 1 Jun 2006 01:53:00 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-124
HADOOP-125,Bug,Minor,fs,LocalFileSystem.makeAbsolute bug on Windows,LocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop).Problem:  if a pathname such as /tmp/hadoop... is given in a config file; when the jobconf file is created; it is put into the relative directory named: currentdir/tmp/hadoop...; but when hadoop tries to open the file; it looks in c:/tmp/hadoop...; and the job fails.Cause: while Unix has two kinds of filespecs (relative and absolute); WIndows actually has three:(1) relative to current directory (subdir/file)(2) relative to current disk (/dir/subdir/file)(3) absolute (c:/dir/subdir/file)So when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...); the makeAbsolute() method will not work correctly. Basically; File.isAbsolute() will return false for cases (1) and (2) above; but true for case (3); which is not expected by the code below. The solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem:    private File makeAbsolute(File f) {      if (f.isAbsolute()) {        return f;      } else {        return new File(workingDir; f.toString());      }    }Im happy to explain if this explanation is confusing...,Closed,Fixed,,Doug Cutting,p sutter,Sat; 8 Apr 2006 06:43:59 +0000,Thu; 3 Aug 2006 17:46:35 +0000,Sat; 8 Apr 2006 07:27:21 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-125
HADOOP-126,Bug,Major,,hadoop dfs -cp does not copy crc files,"DFSShell.copy() uses FileUtil.copyContents() which works with ""Raw"" files; and does not copy crc files.Tryhadoop dfs -cp a bThen eitherhadoop dfs -cat bhadoop dfs -copyToLocal b cThe last two complain about crc filesIn fact DFSShell.copy() should use the same FileSystem methods as copyToLocal and copyFromLocal do.",Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Sat; 8 Apr 2006 10:47:11 +0000,Wed; 8 Jul 2009 16:41:51 +0000,Tue; 11 Apr 2006 03:13:57 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-126
HADOOP-127,Bug,Major,conf,Unclear precedence of config files and property definitions,"The order in which configuration resources are read is not sufficiently documented; and also there are no mechanisms preventing harmful re-definition of certain properties; if they are put in wrong config files.From reading the code in Hadoop Configuration.java; JobConf.java and Nutch NutchConfiguration.java I think this is what's happening.There are two groups of resources: default resources; loaded first; and final resources; loaded at the end. All properties (re)-defined in files loaded later will override any previous definitions:	default resources: loaded in the order as they are added. The following files are added here; in order:    1. hadoop-default.xml (Configuration)    2. nutch-default.xml  (NutchConfiguration)    3. mapred-default.xml (JobConf)    4. job_xx_xxx.xml       (JobConf; in JobConf(File config))	final resource: which always come after default resources; i.e. if any value is defined here it will always override those set in default resources (NOTE: including per job settings!!!). The following files are added here; in reversed order:    2. hadoop-site.xml (Configuration)    1. nutch-site.xml    (NutchConfiguration)(i.e. hadoop-site.xml will take precedence over anything else defined in any other config file).I would appreciate checking that this is indeed the case; and suggestions how to ensure that you cannot so easily shoot yourself in the foot if you define wrong properties in hadoop-site or nutch-site ...",Closed,Duplicate,NULL,Doug Cutting,Andrzej Bialecki,Mon; 10 Apr 2006 17:47:40 +0000,Wed; 3 Oct 2007 18:06:13 +0000,Thu; 27 Sep 2007 22:01:29 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-127
HADOOP-128,Bug,Major,,Failure to replicate dfs block kills client,When the datanode gets an exception; which is logged as:060407 155835 13 DataXCeiverjava.io.EOFException        at java.io.DataInputStream.readFully(DataInputStream.java:178)        at java.io.DataInputStream.readLong(DataInputStream.java:380)        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:462)        at java.lang.Thread.run(Thread.java:595)It closes the user's connection to the data node; which causes the client to get an IOException from:        at java.io.DataInputStream.readFully(DataInputStream.java:178)        at java.io.DataInputStream.readLong(DataInputStream.java:380)        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.internalClose(DFSClient.java:883),Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 11 Apr 2006 00:19:13 +0000,Wed; 8 Jul 2009 16:41:51 +0000,Fri; 14 Apr 2006 00:44:25 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-128
HADOOP-129,Improvement,Major,fs,FileSystem should not name files with java.io.File,"In Hadoop's FileSystem API; files are currently named using java.io.File.  This is confusing; as many methods on that class are inappropriate to call on Hadoop paths.  For example; calling isDirectory(); exists(); etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file.  Using java.io.File also makes correct operation on Windows difficult; since java.io.File operates differently on Windows in order to accomodate Windows path names.  For example; new File(""/foo"") is not absolute on Windows; and prints its path as ""foo""; which causes confusion.To fix this we could replace the uses of java.io.File in the FileSystem API with String; a new FileName class; or perhaps java.net.URI.  The advantage of URI is that it can also naturally include the namenode host and port.  The disadvantage is that URI does not support tree operations like getParent().This change will cause a lot of incompatibility.  Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it.",Closed,Fixed,,Doug Cutting,Doug Cutting,Tue; 11 Apr 2006 03:05:09 +0000,Thu; 3 Aug 2006 17:46:35 +0000,Wed; 19 Apr 2006 00:06:17 +0000,,0.1.0;0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-129
HADOOP-130,Improvement,Minor,,"Should be able to specify ""wide"" or ""full"" replication","Should be able to specify that a file be ""fully"" or ""widely"" replicated; rather than an explicit replication count. This would be useful for job configuration and jar files; and probably other files whose use is wide enough to necessitate reducing latency to access them.The current implementation will also complain if you specify replication that is wider than the system's maximum replication value; and has no facility to enable ""full"" replication should the number of datanodes exceed the current maximum settable value of 32k.",Closed,Duplicate,HADOOP-171,Konstantin Shvachko,Bryan Pendleton,Tue; 11 Apr 2006 04:47:43 +0000,Wed; 8 Jul 2009 16:41:52 +0000,Thu; 11 Oct 2007 19:17:24 +0000,,0.2.0,,,HADOOP-170,https://issues.apache.org/jira/browse/HADOOP-130
HADOOP-131,Improvement,Minor,,Separate start/stop-dfs.sh and start/stop-mapred.sh scripts,Hadoop needs start-dfs/mapred.sh scripts; and stop-dfs/mapred.sh scripts; to independently start mapred; or independently start dfs. This way; users can use a single subsystem of the full Hadoop component library.,Closed,Fixed,,Doug Cutting,Chris A. Mattmann,Tue; 11 Apr 2006 09:48:05 +0000,Wed; 8 Jul 2009 16:41:51 +0000,Wed; 12 Apr 2006 00:51:30 +0000,,0.1.0;0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-131
HADOOP-132,New Feature,Major,,An API for reporting performance metrics,"I'd like to propose adding an API for reporting performance metrics.  I will post some javadoc as soon as I figure out how to do so.  The idea is for the API to be sufficiently abstract that various different implementations can be plugged in.  In particular; there would be one that just writes the metric data to a file; and another that sends metrics to Ganglia.  It would also be possible to plug in an implementation that can support high-frequency (say; per-second) sending of fairly large amounts of data (up to hundreds of metrics) across the network.I'd be very interested in people's thoughts about what the requirements should be for such an API.	David Bowen",Closed,Fixed,,Unassigned,David Bowen,Fri; 14 Apr 2006 03:12:36 +0000,Thu; 3 Aug 2006 17:46:35 +0000,Tue; 25 Apr 2006 09:25:36 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-132
HADOOP-133,Bug,Major,,the TaskTracker.Child.ping thread calls exit,The TaskTracker.Child.startPinging thread calls exit if the TaskTracker doesn't respond. Calling exit in a mutli-threaded program is really problematic. In particular; it prevents cleanup/finally clauses from running. We need to move to a model where it uses Thread.interrupt(); which means we need to check the interrupt flag in place in the map loop and reduce loop and stop masking the InterruptExceptions.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 14 Apr 2006 04:09:39 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Wed; 19 Apr 2006 05:03:50 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-133
HADOOP-134,Bug,Major,,JobTracker trapped in a loop if it fails to localize a task,"The symptoms:    When I ran  jobs on a big cluster; I noticed that some jobs got stucked. Some map tasks never got started. When I look at the log of the task tracker responsible for the tasks; I saw the following exceptions:060413 160702 Lost connection to JobTracker kry1040/72.30.116.100:50020.  Retrying...java.io.IOException: No valid local directories in property: mapred.local.dir        at org.apache.hadoop.conf.Configuration.getFile(Configuration.java:282)        at org.apache.hadoop.mapred.JobConf.getLocalFile(JobConf.java:127)        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:391)        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.init(TaskTracker.java:383)        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:270)        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:336)        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:756)The reason for the exception is that the directory hadoop/mapred/local has ""wrong"" owner; thus the task tracker cannot access to it.This caused the task tracker stucked into the following loops:            while (running) {                boolean staleState = false;                try {                    // This while-loop attempts reconnects if we get network errors                    while (running &amp; ! staleState) {                        try {                            if (offerService() == STALE_STATE) {                                staleState = true;                            }                        } catch (Exception ex) {                            LOG.log(Level.INFO; ""Lost connection to JobTracker &quot; + jobTrackAddr + &#93;.  Retrying...""; ex);                            try {                                Thread.sleep(5000);                            } catch (InterruptedException ie) {                            }                        }                    }                } finally {                    close();                }                LOG.info(""Reinitializing local state"");                initialize();            }Issue 1:    Method offerService() must catch and handle the exceptions that may be thrown from new TaskInProgress() call; and report back to the job tracker if it cannot run the task. This way; the task can be assigned to other task tracker.Issue 2:    The taskTracker should check whether it can access to the local dir at the initialization time; before taking any tasks.Runping",Closed,Fixed,,Owen O'Malley,Runping Qi,Fri; 14 Apr 2006 06:58:16 +0000,Wed; 8 Jul 2009 16:51:42 +0000,Wed; 19 Apr 2006 04:49:28 +0000,,0.1.0,,,HADOOP-137,https://issues.apache.org/jira/browse/HADOOP-134
HADOOP-135,Bug,Major,,Potential deadlock in JobTracker.,org.apache.hadoop.mapred.JobTracker.RetireJobs.run()locks resources in this order                synchronized (jobs) {                    synchronized (jobInitQueue) {                        synchronized (jobsByArrival) {org.apache.hadoop.mapred.JobTracker.submitJob()locks resources in a different order        synchronized (jobs) {            synchronized (jobsByArrival) {                synchronized (jobInitQueue) {This potentially can lead to a deadlock.Unless there is common lock on top of it in which case thesethree locks are redundant.,Closed,Fixed,,Owen O'Malley,Konstantin Shvachko,Fri; 14 Apr 2006 07:28:03 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Tue; 27 Jun 2006 00:35:44 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-135
HADOOP-136,Bug,Minor,io,Overlong UTF8's not handled well,"When we feed an overlong string to the UTF8 constructor; two suboptimal things happen.First; we truncate to 0xffff/3 characters on the assumption that every character takes three bytes in UTF8.  This can truncate strings that don't need it; and it can be overoptimistic since there are characters that render as four bytes in UTF8.Second; the code doesn't actually handle four-byte characters.Third; there's a behavioral discontinuity.  If the string is ""discovered"" to be overlong by the arbitrary limit described above; we truncate with a log message; otherwise we signal a RuntimeException.  One feels that both forms of truncation should be treated alike.  However; this issue is concealed by the second issue; the exception will never be thrown because UTF8.utf8Length can't return more than three times the length of its input.I would recommend changing UTF8.utf8Length to let its caller know how many characters of the input string will actually fit if there's an overflow perhaps by returning the negative of that number and doing the truncation accurately as needed.-dk",Closed,Duplicate,HADOOP-302,Hairong Kuang,Dick King,Sat; 15 Apr 2006 03:11:52 +0000,Wed; 30 Aug 2006 23:33:51 +0000,Wed; 30 Aug 2006 23:33:12 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-136
HADOOP-137,Bug,Critical,,Different TaskTrackers may get the same task tracker id; thus cause many problems.,"In the TaskTracker#Initialize method; the following line assigns task tracker name (id):this.taskTrackerName = ""tracker_"" + (Math.abs(r.nextInt()) % 100000);For a fair size cluster; it is possible that different task trackers to get the same id; causing name conflict.I encountered this problem with a cluster of 274 nodes. Once such conflict happens; a lot of strange things may happen.For example; a reducer task tried to copy from a machine (task tracker) a map output file that was actually produced on another machine.",Closed,Fixed,,Owen O'Malley,Runping Qi,Sat; 15 Apr 2006 07:10:22 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Tue; 2 May 2006 03:05:36 +0000,,0.2.0,,,HADOOP-134,https://issues.apache.org/jira/browse/HADOOP-137
HADOOP-138,Improvement,Trivial,,stop all tasks,When a tasktracker runs X tasks it require X heartbeats to stop all jobs.Stop all tasks with one heartbeat improve the availability and free resources faster.,Closed,Fixed,,Doug Cutting,Stefan Groschupf,Sat; 15 Apr 2006 08:59:28 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Thu; 20 Apr 2006 00:16:02 +0000,,0.1.0;0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-138
HADOOP-139,Bug,Major,fs,Deadlock in LocalFileSystem lock/release,"LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: 1. First thread locks the file and starts some long-running process.2. Second thread tries to lock the file and it blocks inside channel lock method. It  keeps LocalFileSystem instance ""locked"" as well. 3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is ""locked"" by second thread - both threads are waiting to each other.",Closed,Fixed,,Doug Cutting,Igor Bolotin,Sat; 15 Apr 2006 10:39:58 +0000,Thu; 3 Aug 2006 17:46:36 +0000,Wed; 19 Apr 2006 02:36:59 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-139
HADOOP-140,Wish,Minor,,General documentation,Getting a grasp of how Hadoop works is a little hard; because one first has to get a grip on the whole MapReduce thing and then figure out how it is carried out in Hadoop. Judging from the mailing list a little more general documentation would help a lot.,Closed,Fixed,,Unassigned,Teppo Kurki,Sat; 15 Apr 2006 13:58:58 +0000,Fri; 15 Dec 2006 23:02:11 +0000,Wed; 19 Apr 2006 22:36:38 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-140
HADOOP-141,Bug,Major,,Disk thrashing / task timeouts during map output copy phase,MapOutputProtocol connections cause timeouts because of system thrashing and transferring the same file over and over again; ultimately leading to making no forward progress(medium sized job; 500GB input file; map output about as large as the input; 10 node cluster).There are several bugs behind this; but the following two changes improved matters considerably.(1) The buffersize in MapOutputFile is currently hardcoded to 8192 bytes (for both reads and writes). By changing this buffer size to 256KB; the number of disk seeks are reduced and the problem went away. Ideally there would be a buffer size parameter for this that is separate from the DFS io buffer size.(2)I also added the following code to the socket configuration in both Server.java and Client.java. No linger is a minor good idea in an enivronment with some packet loss (and you will have that when all the nodes get busy at once); but 256KB buffers is probably excessive; especially on a LAN; but it takes me two hours to test changes so I havent experimented.socket.setSendBufferSize(256*1024);socket.setReceiveBufferSize(256*1024);socket.setSoLinger(false; 0);socket.setKeepAlive(true);,Closed,Fixed,,Owen O'Malley,p sutter,Tue; 18 Apr 2006 05:21:17 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Sat; 14 Oct 2006 03:30:33 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-141
HADOOP-142,Improvement,Major,,failed tasks should be rescheduled on different hosts after other jobs,Currently when tasks fail; they are usually rerun immediately on the same host. This causes problems in a couple of ways.   1.The task is more likely to fail on the same host.   2.If there is cleanup code (such as clearing pendingCreates) it does not always run immediately; leading to cascading failures.For a first pass; I propose that when a task fails; we start the scan for new tasks to launch at the following task of the same type (within that job). So if maps99 fails; when we are looking to assign new map tasks from this job; we scan like maps100...mapsN; maps0..;maps99.A more involved change would avoid running tasks on nodes where it has failed before. This is a little tricky; because you don't want to prevent re-excution of tasks on 1 node clusters and the job tracker needs to schedule one task tracker at a time.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 18 Apr 2006 05:25:02 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Wed; 19 Apr 2006 05:07:36 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-142
HADOOP-143,Bug,Major,,exception call stacks are word wrapped in webapp,The exception call stacks in the webapp word wrap; which makes them much harder to read. It is particularly unfortunate in the remote exceptions; which use a blank line to separate the local from the remote call stacks.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 18 Apr 2006 22:22:03 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Thu; 20 Apr 2006 00:18:30 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-143
HADOOP-144,Improvement,Major,,the dfs client id isn't relatable to the map/reduce task ids,From the dfs logs you can't tell which map/reduce tasks where involved; which makes debugging harder.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 18 Apr 2006 22:49:25 +0000,Wed; 8 Jul 2009 16:41:51 +0000,Wed; 19 Apr 2006 00:21:57 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-144
HADOOP-145,Bug,Major,io,io.skip.checksum.errors property clashes with LocalFileSystem#reportChecksumFailure,Below is from email to the dev list on Tue; 11 Apr 2006 14:46:09 -0700.Checksum errors seem to be a fact of life given the hardware we use.  They'll often cause my jobs to fail so I have been trying to figure how to just skip the bad records and files.  At the end is a note where Stefan pointed me at 'io.skip.checksum.errors'.   This property; when set; triggers special handling of checksum errors inside SequenceFile$Reader: If a checksum; try to skip to next record.  Only; this behavior can conflict with another checksum handler that moves aside the problematic file whenever a checksum error is found.  Below is from a recent log.060411 202203 task_r_22esh3  Moving bad file /2/hadoop/tmp/task_r_22esh3/task_m_e3chga.out to /2/bad_files/task_m_e3chga.out.1707416716060411 202203 task_r_22esh3  Bad checksum at 3578152. Skipping entries.060411 202203 task_r_22esh3  Error running child060411 202203 task_r_22esh3 java.nio.channels.ClosedChannelException060411 202203 task_r_22esh3     at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:89)060411 202203 task_r_22esh3     at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:276)060411 202203 task_r_22esh3     at org.apache.hadoop.fs.LocalFileSystem$LocalFSFileInputStream.seek(LocalFileSystem.java:79)060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream$Checker.seek(FSDataInputStream.java:67)060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream$PositionCache.seek(FSDataInputStream.java:164)060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream$Buffer.seek(FSDataInputStream.java:193)060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:243)060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.seek(SequenceFile.java:420)060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.sync(SequenceFile.java:431)060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:412)060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:389)060411 202203 task_r_22esh3     at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:209)060411 202203 task_r_22esh3     at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:709)(Ignore line numbers.  My code is a little different from main because I've other debugging code inside in SequenceFile.  Otherwise I'm running w/ head of hadoop).The SequenceFile$Reader#handleChecksumException is trying to skip to next record but the file has been closed by the move-aside.On the list there is some discussion on merit of moving aside file when bad checksum found.  I've trying to test what happens if we leave the file in place but haven't had a checksum error in a while.  Opening this issue so place to fill in experience as we go.,Resolved,Fixed,,Owen O'Malley,stack,Tue; 18 Apr 2006 23:16:49 +0000,Fri; 18 Jul 2014 04:51:14 +0000,Fri; 18 Jul 2014 04:51:14 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-145
HADOOP-146,Bug,Major,,potential conflict in block id's; leading to data corruption,currently; block id's are generated randomly; and are not tested for collisions with existing id's.while ids are 64 bits; given enough time and a large enough FS; collisions are expected.when a collision occurs; a random subset of blocks with that id will be removed as extra replicas; and the contents of that portion of the containing file are one random version of the block.to solve this one could check for id collision when creating a new block; getting a new id in case of conflict. This approach requires the name node to keep track of all existing block id's (rather than just the ones who have reported in); and to identify old versions of a block id as in valid (in case a data node dies; a file is deleted; then a block id is reused for a new file).Alternatively; one could simply use sequential block id's. Here the downsides are: 1. migration from an existing file system is hard; requiring compaction of the entire FS2. once you cycle through 64 bits of id's (quite a few years at full blast); you're in trouble again (or run occasional/background compaction)3. you must never lose the high watermark block id.synchronized Block allocateBlock(UTF8 src) {        Block b = new Block();        FileUnderConstruction v = (FileUnderConstruction) pendingCreates.get(src);        v.add(b);        pendingCreateBlocks.add(b);        return b;    }static Random r = new Random();    /**     */    public Block() {        this.blkid = r.nextLong();        this.len = 0;    },Closed,Fixed,HADOOP-158,Konstantin Shvachko,Yoram Arnon,Wed; 19 Apr 2006 05:07:03 +0000,Wed; 8 Jul 2009 16:41:52 +0000,Sat; 13 May 2006 04:46:41 +0000,,0.1.0;0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-146
HADOOP-147,Bug,Major,,MapTask removed mapout files before the reduce tasks copy them,"I was running a job on a cluster of 138 nodes. The job had 1050 map tasks and 128 reduce tasks. It stucked at the reduce stage.All the reduce tasks were trying to copy file from a map task with the following status show on the web interface:reduce  copy  task_m_ehz5q1@node1262.foo.com:60040However; the log on the machine node1262 (where the map task task_m_ehz5q1 ran) showed that the map task finished even before the reduce tasks copied the map output files:060417 103554 Server connection on port 60050 from 72.30.117.220: starting060417 103554 task_m_ehz5q1  Client connection to 0.0.0.0:60050: starting060417 103554 task_m_ehz5q1 1.0% /user/runping/runping/proj/part-00039:0+71060417 103554 Task task_m_ehz5q1 is done.060417 103554 parsing file:/local/hadoop/conf2/hadoop-default.xml......................060417 103613 parsing file:/local/hadoop/conf2/hadoop-site.xml060417 103623 task_m_ehz5q1 done; removing files.060417 103633 parsing file:/local/hadoop/conf2/hadoop-default.xml060417 103633 parsing file:/local/hadoop/conf2/mapred-default.xml060417 103633 parsing file:/local/hadoop/conf2/hadoop-site.xml...........................................060417 190241 SEVERE Can't open map output:/local/hadoop/mapred/local/task_m_ehz5q1/part-32.outjava.io.FileNotFoundException: /local/hadoop/mapred/local/task_m_ehz5q1/part-32.out        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:115)        at org.apache.hadoop.fs.FSDataInputStream$Checker.init(FSDataInputStream.java:46)        at org.apache.hadoop.fs.FSDataInputStream.init(FSDataInputStream.java:228)        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:154)        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:117)        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:231)060417 190241 Unknown child with bad map output: task_m_ehz5q1. Ignored.060417 190241 Server handler 2 on 60040 caught: java.io.FileNotFoundException: /local/hadoop/mapred/local/task_m_ehz5q1/part-32.outjava.io.FileNotFoundException: /local/hadoop/mapred/local/task_m_ehz5q1/part-32.out        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:115)        at org.apache.hadoop.fs.FSDataInputStream$Checker.init(FSDataInputStream.java:46)        at org.apache.hadoop.fs.FSDataInputStream.init(FSDataInputStream.java:228)        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:154)        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:117)        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:231)060417 190241 parsing file:/local/hadoop/conf2/hadoop-default.xml060417 190241 parsing file:/local/hadoop/conf2/mapred-default.xmlAnd the above exceptions repeated for many (not sure whether it is tru for all the reduce task) other reduce tasks.Another strange thing noticed from the logs.On another machine's log; I saw:060417 190528 parsing file:/local/hadoop/conf2/hadoop-site.xml060417 190528 task_r_24d8k4 copy failed: task_m_ehz5q1 from node1262.foo.com/72.30.117.220:60040java.io.IOException: timed out waiting for response        at org.apache.hadoop.ipc.Client.call(Client.java:305)        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:141)        at org.apache.hadoop.mapred.$Proxy2.getFile(Unknown Source)        at org.apache.hadoop.mapred.ReduceTaskRunner.prepare(ReduceTaskRunner.java:110)        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:66)060417 190528 task_r_24d8k4 0.11523809% reduce  copy  task_m_ehz5q1@node1262.foo.com:60040060417 190528 task_r_24d8k4 Copying task_m_epatk8 output from node1387.foo.com.which is expected. However; before this line; I saw another copy activity in the log:060417 103608 parsing file:/local/hadoop/conf2/hadoop-site.xml060417 103608 task_r_a4yl3t Copying task_m_ehz5q1 output from node1262.foo.com.060417 103608 parsing file:/local/hadoop/conf2/hadoop-default.xmlAnd the task task_r_a4yl3t does not belong to the concerned job; according to the Web interface. That is strange.And I checked a few other machines where some reduce tasks ran; and I saw the same thing.I suspect there was a conflict in job ID. If two jobs had the same ID; when one closes; it may also mark the other as ""closed"" too; thus trggering map tasksto clean up prematurely.A simple way to avoid potential jobid conflict is to use sequential numbers.",Closed,Duplicate,NULL,Unassigned,Runping Qi,Wed; 19 Apr 2006 07:53:06 +0000,Wed; 8 Jul 2009 16:51:39 +0000,Wed; 31 May 2006 01:48:40 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-147
HADOOP-148,New Feature,Major,,add a failure count to task trackers,Adds a count of failures that have occurred on each TaskTracker in the TaskTrackerStatus. The webapp displays the list of failures for each TaskTracker. In addition; the TaskTracker that has the most failures is listed.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 19 Apr 2006 12:04:58 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Wed; 19 Apr 2006 23:08:04 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-148
HADOOP-149,Bug,Minor,,TaskTracker#unJar trashes file modes,Last Changed Rev: 395069The unJar'ing of the job 'jar'; trashes any file modes I've lovingly set at zip time.  This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves.I ain't sure how to address the issue though. Nought about unix file modes in JarEntry nor ZipEntry.  I tried the ant 1.6.5 task unjar and unzip tasks thinking they'd respect file modes but they do same as TaskTracker#unJar (Commons zip makes mention of unix file modes but I haven't tried it).  Perhaps support for jobs as tar(.gz) bundles?  (But again; couldn't use ant to untar. It does same as unzip/unjar trashing file permissions).,Closed,Won't Fix,,Unassigned,stack,Thu; 20 Apr 2006 00:59:20 +0000,Thu; 3 Aug 2006 17:46:36 +0000,Thu; 20 Apr 2006 02:32:10 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-149
HADOOP-150,Improvement,Major,,tip and task names should reflect the job name,The tip and task names should be related to the job id. I'd propose:job name: job_random 32bit/base36tip: tip_job idmrfragment #task: task_tip id_attempt #so examples would be:job_abc123tip_abc123_m_00034task_abc123_m_00034_1,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Thu; 20 Apr 2006 01:34:39 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Tue; 25 Apr 2006 01:00:29 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-150
HADOOP-151,Bug,Major,ipc,RPC code has socket leak?,"In RPC.java; the field named CLIENT should be neither static; nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler();and (just further down); (b) a local variable in the RPC.call() method below.  The comment above the declaration was a bit of giveaway:    //TODO mb@media-style.com: static client or non-static client?  private static Client CLIENT;	  private static class Invoker implements InvocationHandler {    private InetSocketAddress address;    public Invoker(InetSocketAddress address; Configuration conf) {      this.address = address;      CLIENT = (Client) conf.getObject(Client.class.getName());      if(CLIENT == null) {          CLIENT = new Client(ObjectWritable.class; conf);          conf.setObject(Client.class.getName(); CLIENT);      }    }    public Object invoke(Object proxy; Method method; Object[] args)      throws Throwable {      ObjectWritable value = (ObjectWritable)        CLIENT.call(new Invocation(method; args); address);      return value.get();    }  }  /** Construct a client-side proxy object that implements the named protocol;	talking to a server at the named address. */  public static Object getProxy(Class protocol; InetSocketAddress addr; Configuration conf)    /** Expert: Make multiple; parallel calls to a set of servers. */  public static Object[] call(Method method; Object[][] params;                              InetSocketAddress[] addrs; Configuration conf)    throws IOException {    Invocation[] invocations = new Invocationparams.length;    for (int i = 0; i  params.length; i++)      invocationsi = new Invocation(method; paramsi);    CLIENT = (Client) conf.getObject(Client.class.getName());    if(CLIENT == null) {        CLIENT = new Client(ObjectWritable.class; conf);        conf.setObject(Client.class.getName(); CLIENT);    }    Writable[] wrappedValues = CLIENT.call(invocations; addrs);    if (method.getReturnType() == Void.TYPE) {      return null;    }    Object[] values =      (Object[])Array.newInstance(method.getReturnType();wrappedValues.length);    for (int i = 0; i  values.length; i++)      if (wrappedValuesi != null)        valuesi = ((ObjectWritable)wrappedValuesi).get();    return values;  }.",Closed,Fixed,,Doug Cutting,p sutter,Thu; 20 Apr 2006 05:18:25 +0000,Thu; 3 Aug 2006 17:46:36 +0000,Thu; 20 Apr 2006 06:57:14 +0000,,0.1.0;0.1.1;0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-151
HADOOP-152,Bug,Minor,,Speculative tasks not being scheduled,"The criteria for starting up a speculative task includes a check that the ""average progress""-""progress""  the speculative gap; currently 0.2.I don't know if this is the right metric; but it doesn't seem to be correctly calculated. I've regularly seen the ""average progress"" with values of less than 0.01; while the ""progress"" value is showing in the range .90-1.0; and yet; still no speculative tasks are started up. This has caused at least one long-running task to run about 10% longer while overloaded hosts catch up.",Closed,Duplicate,NULL,Unassigned,Bryan Pendleton,Fri; 21 Apr 2006 00:32:31 +0000,Wed; 8 Jul 2009 16:51:43 +0000,Thu; 24 May 2007 06:08:13 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-152
HADOOP-153,New Feature,Major,,skip records that fail Task,MapReduce should skip records that throw exceptions.If the exception is thrown under RecordReader.next() then RecordReader implementations should automatically skip to the start of a subsequent record.Exceptions in map and reduce implementations can simply be logged; unless they happen under RecordWriter.write().  Cancelling partial output could be hard.  So such output errors will still result in task failure.This behaviour should be optional; but enabled by default.  A count of errors per task and job should be maintained and displayed in the web ui.  Perhaps if some percentage of records (50%?) result in exceptions then the task should fail.  This would stop jobs early that are misconfigured or have buggy code.Thoughts?,Closed,Fixed,,Sharad Agarwal,Doug Cutting,Fri; 21 Apr 2006 00:40:52 +0000,Thu; 2 May 2013 02:29:16 +0000,Mon; 11 Aug 2008 12:04:34 +0000,,0.2.0,,,HADOOP-3954,https://issues.apache.org/jira/browse/HADOOP-153
HADOOP-154,Bug,Trivial,,fsck fails when there is no file in dfs,"First type in command     hadoop namenode -formatThen      hadoop fsck /Then is the following exception:Exception in thread ""main"" java.lang.ArithmeticException: / by zero        at org.apache.hadoop.dfs.DFSck$Result.toString(DFSck.java:563)        at java.lang.String.valueOf(String.java:2577)        at java.io.PrintStream.print(PrintStream.java:616)        at java.io.PrintStream.println(PrintStream.java:753)        at org.apache.hadoop.dfs.DFSck.main(DFSck.java:435)Possible Solution: Check whether totalBlocks equal to 0 in line 563 of DFSck.java",Closed,Fixed,,Unassigned,Lei Chen,Fri; 21 Apr 2006 03:11:25 +0000,Wed; 8 Jul 2009 16:41:52 +0000,Tue; 25 Apr 2006 05:46:00 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-154
HADOOP-155,Improvement,Major,conf,Add a conf dir parameter to the scripts,"We'd like a conf_dir parameter on the startup scripts (ie. ""-c confdif""). In particular; it would be nice if it propagated down from hadoop-daemons.sh to slaves.sh to hadoop-daemon.sh using the command line rather than using the ssh -o SendEnv=HADOOP_CONF_DIR; which is not supported in many environments.",Closed,Duplicate,HADOOP-260,Milind Bhandarkar,Owen O'Malley,Fri; 21 Apr 2006 03:19:14 +0000,Fri; 10 Nov 2006 21:29:39 +0000,Fri; 10 Nov 2006 20:34:12 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-155
HADOOP-156,Bug,Major,,Reducer  threw IOEOFException,A job was running with all the map tasks completed.The reducers were appending the intermediate files into the large intermediate file.java.io.EOFException was thrown when the record reader tried to read the version numberduring initialization. Here is the stack trace:java.io.EOFException     at java.io.DataInputStream.readFully(DataInputStream.java:178)     at java.io.DataInputStream.readFully(DataInputStream.java:152)     at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:251)     at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:236)     at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:226)     at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:205)     at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:709) Appearantly; the intermediate file was empty. I suspect that one map taskgenerated empty intermidiate files for all the reducers; since all the reducersfailed at the same place; and failed at the same place during retries.Unfortunately; we cannot know which map task generated the empty files;since the exception does not offer any clue.One simple enhancement is that the record reader should catch IOException and re-throw it with additional information; such as the file name.,Closed,Cannot Reproduce,,Owen O'Malley,Runping Qi,Fri; 21 Apr 2006 04:10:48 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Thu; 15 Jan 2009 17:30:10 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-156
HADOOP-157,Bug,Major,,job fails because pendingCreates is not cleaned up after a task fails,When a task fails under map/reduce; if the client doesn't abandon the files in progress (usually because it was killed); the lease on the name node lasts 1 minute. During that minute; I see 3 backup copies of the task fail because pendingCreates is non-null.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 21 Apr 2006 23:03:19 +0000,Wed; 8 Jul 2009 16:41:52 +0000,Tue; 25 Apr 2006 00:02:39 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-157
HADOOP-158,Bug,Major,,dfs should allocate a random blockid range to a file; then assign ids sequentially to blocks in the file,A random number generator is used to allocate block ids in dfs.  Sometimes a block id is allocated that is already used in the filesystem; which causes filesystem corruption.A short-term fix for this is to simply check when allocating block ids whether any file is already using the newly allocated id; and; if it is; generate another one.  There can still be collisions in some rare conditions; but these are harder to fix and will wait; since this simple fix will handle the vast majority of collisions.,Closed,Duplicate,HADOOP-146;HADOOP-1497,Konstantin Shvachko,Doug Cutting,Sat; 22 Apr 2006 00:06:18 +0000,Wed; 8 Jul 2009 16:41:52 +0000,Thu; 11 Oct 2007 19:19:39 +0000,,0.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-158
HADOOP-159,Bug,Blocker,,name node at 100% cpu; making redundant replications,some hours after adding some new nodes to the cluster; the name node went into a state where it's consuming 100% cpu.The log file keeps logging messages of the forms060421 155049 Obsoleting block blk_8093115169359854355060421 155049 Pending transfer (block blk_-6965677235456960523) from node1383:50010 to 2 destinations060421 155049 Block report from node1283:50010: 2140 blocks.060421 155049 Redundant addStoredBlock request received for block blk_-6836937139917042917 on node node1143:50010many DFS operations time out; making useful work impossible.restarting dfs solved the problem for a while; but it came back within an hour.,Closed,Duplicate,NULL,Sameer Paranjpye,Yoram Arnon,Sat; 22 Apr 2006 05:56:24 +0000,Wed; 8 Jul 2009 16:41:52 +0000,Wed; 31 May 2006 02:04:37 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-159
HADOOP-160,Bug,Major,,sleeping with locks held,I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular); and this may be contributing to that.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Sun; 23 Apr 2006 07:01:20 +0000,Thu; 3 Aug 2006 17:46:37 +0000,Wed; 26 Apr 2006 02:26:24 +0000,,0.1.1,,,HADOOP-165,https://issues.apache.org/jira/browse/HADOOP-160
HADOOP-161,Bug,Major,,dfs blocks define equal; but not hashcode,Findbugs reports that dfs.Block defines equals but not hashcode; which is problematic if it is ever put into a hash table.,Closed,Fixed,,Milind Bhandarkar,Owen O'Malley,Sun; 23 Apr 2006 07:04:00 +0000,Wed; 8 Jul 2009 16:41:52 +0000,Wed; 17 May 2006 06:01:52 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-161
HADOOP-162,Bug,Major,,concurrent modification exception in FSNamesystem.Lease.releaseLocks,FSNameSystem.Lease.releaseLocks iterates through the creates set; calling InternalReleaseCreate on each element; which changes the creates set in the Lease. This causes a ConcurrentModificationException if you have more than two files that are owned by the lease that times out.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 25 Apr 2006 02:54:19 +0000,Wed; 8 Jul 2009 16:41:52 +0000,Tue; 25 Apr 2006 06:23:16 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-162
HADOOP-163,Bug,Major,,If a DFS datanode cannot write onto its file system. it should tell the name node not to assign new blocks to it.,I observed that sometime; if a file of a data node is not mounted properly; it may not be writable. In this case; any data writes will fail. The name node should stop assigning new blocks to that data node. The webpage should show that node is in an abnormal state.,Closed,Fixed,,Hairong Kuang,Runping Qi,Tue; 25 Apr 2006 04:52:02 +0000,Wed; 8 Jul 2009 16:41:53 +0000,Sat; 27 May 2006 05:42:26 +0000,,0.2.0,,,HADOOP-1200,https://issues.apache.org/jira/browse/HADOOP-163
MAPREDUCE-133,Bug,Major,,Getting errors in reading the output files of a map/reduce job immediately after the job is complete,"I have an app that fire up map/reduce jobs sequentially. The output of one job if the input of the next.I observe that many map tasks failed due to file read errors:java.rmi.RemoteException: java.io.IOException: Cannot open filename /user/runping/runping/docs_store/stage_2/base_docs/part-00186 at org.apache.hadoop.dfs.NameNode.open(NameNode.java:130) at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:237) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:216) at org.apache.hadoop.ipc.Client.call(Client.java:303) at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:141) at org.apache.hadoop.dfs.$Proxy1.open(Unknown Source) at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:315) at org.apache.hadoop.dfs.DFSClient$DFSInputStream.(DFSClient.java:302) at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:95) at org.apache.hadoop.dfs.DistributedFileSystem.openRaw(DistributedFileSystem.java:78) at org.apache.hadoop.fs.FSDataInputStream$Checker.(FSDataInputStream.java:46) at org.apache.hadoop.fs.FSDataInputStream.(FSDataInputStream.java:220) at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:146) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:234) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:226) at org.apache.hadoop.mapred.SequenceFileRecordReader.(SequenceFileRecordReader.java:36) at org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:53) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:105) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:709) Those tasks succeeded in the second or third try.After interting 10 seconds sleep between consecutive jobs; the problem disappear.Here is my code to detect whether a job is completed:      try {        running = jc.submitJob(job);        String jobId = running.getJobID();        System.out.println(""start job:\t"" + jobId);        while (!running.isComplete()) {          try {            Thread.sleep(1000);          } catch (InterruptedException e) {}          running = jc.getJob(jobId);        }        sucess = running.isSuccessful();      } finally {        if (!sucess &amp; (running != null)) {          running.killJob();        }        jc.close();      }",Resolved,Cannot Reproduce,,Owen O'Malley,Runping Qi,Tue; 25 Apr 2006 05:01:13 +0000,Sun; 17 Jul 2011 20:50:14 +0000,Sun; 17 Jul 2011 20:50:14 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-133
HADOOP-165,Bug,Major,,long response times from task trackers under load,we are seeing very slow response times from the task tracker. I put in some instrumentation to measure how long each call took for the RPC.Sever code to run the method (so it does not include serialization/deserialization time). The top of the list (in ms) looks like:7581 progress node11927022 ping node11925393 ping node11624854 progress node11624749 progress node11944709 ping node11943813 ping node11003486 ping node11903266 progress node11903187 progress node12653078 ping node12032972 progress node12032947 progress node12402889 progress node11002875 ping node11162843 ping node11892772 ping node11832737 ping node11102727 progress node11832710 ping node11232563 ping node13042527 progress node11442479 ping node11372476 progress node13042430 ping node12402416 ping node11442377 progress node11762339 ping node11092321 progress node11142311 ping node11572185 ping node12652185 ping node11092172 ping node11142145 progress node11092127 ping node11762083 progress node11892076 ping node12292073 progress node11882072 progress node11232048 ping node11612003 progress node11101989 ping node11801963 ping node1114,Closed,Duplicate,NULL,Unassigned,Owen O'Malley,Tue; 25 Apr 2006 23:34:02 +0000,Wed; 8 Jul 2009 16:51:44 +0000,Thu; 24 May 2007 06:12:03 +0000,,0.2.0,,,HADOOP-160,https://issues.apache.org/jira/browse/HADOOP-165
HADOOP-166,Improvement,Minor,ipc,IPC is unable to invoke methods that use interfaces as parameter,Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.,Closed,Fixed,,Doug Cutting,Stefan Groschupf,Wed; 26 Apr 2006 06:12:25 +0000,Thu; 3 Aug 2006 17:46:37 +0000,Thu; 27 Apr 2006 03:26:32 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-166
HADOOP-167,Improvement,Major,conf,reducing the number of Configuration & JobConf objects created,Currently; Configuration and JobConf objects are created many times during executing a job. In particular; the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 26 Apr 2006 10:52:22 +0000,Thu; 3 Aug 2006 17:46:37 +0000,Sat; 29 Apr 2006 01:41:56 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-167
HADOOP-168,Bug,Minor,,"JobSubmissionProtocol and InterTrackerProtocol don't include ""throws IOException"" on all methods",Timeouts for RPC's are thrown as IOExceptions so unless the method is declared as throwing IOException in the Procotol interface; the Java library wraps the exception in an UndeclaredThrowableException.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 26 Apr 2006 12:13:37 +0000,Wed; 8 Jul 2009 16:51:44 +0000,Thu; 27 Apr 2006 04:44:50 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-168
HADOOP-169,Bug,Critical,,a single failure from locateMapOutputs kills the entire job,Any communication failure in locateMapOutputs kills both the reduce task and the entire job.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 26 Apr 2006 12:24:43 +0000,Wed; 8 Jul 2009 16:51:44 +0000,Thu; 27 Apr 2006 05:27:33 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-169
HADOOP-170,Improvement,Major,fs,setReplication and related bug fixes,Having variable replication (HADOOP-51) it is natural to be able tochange replication for existing files. This patch introduces the functionality.Here is a detailed list of issues addressed by the patch.1) setReplication() and getReplication() methods are implemented.2) DFSShell prints file replication for any listed file.3) Bug fix. FSDirectory.delete() logs delete operation even if it is not successful.4) Bug fix. This is a distributed bug.Suppose that file replication is 3; and a client reduces it to 1.Two data nodes will be chosen to remove their copies; and will do that.After a while they will report to the name node that the copies have been actually deleted.Until they report the name node assumes the copies still exist.Now the client decides to increase replication back to 3 BEFORE the data nodesreported the copies are deleted. Then the name node can choose one of the data nodes;which it thinks have a block copy; to replicate the block to new data nodes.This setting is quite unusual but possible even without variable replications.5) Logging for name and data nodes is improved in several cases.E.g. data nodes never logged that they deleted a block.,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Thu; 27 Apr 2006 04:25:48 +0000,Wed; 8 Jul 2009 16:41:48 +0000,Thu; 27 Apr 2006 06:16:58 +0000,,0.1.1,,,HADOOP-171;HADOOP-130,https://issues.apache.org/jira/browse/HADOOP-170
HADOOP-171,New Feature,Major,,need standard API to set dfs replication = high,There should be a standard way to indicate that files should be highly replicated; appropriate for files that all nodes will read.  This should be settable both on file creation and for already-existing files.  Perhaps specifying a particular replication value; like Short.MAX_VALUE; or zero; can be used to signal this.  The level should not be constant; but should be relative to the cluster size and network topography.  If more nodes are added or if nodes are deleted; the actual replication count should increase or decrease.Initially; all that is needed is an API to specify this.  It could initially be implemented with a constant (e.g.; 10) or with something related to the number of datanodes (sqrt?); and needn't auto-adjust as the cluster size changes  That is only  the long-term goal.When JobClient copies job files (job.xml  job.jar) into the job's filesystem; it should specify this replication level.,Closed,Duplicate,HADOOP-130,Konstantin Shvachko,Doug Cutting,Thu; 27 Apr 2006 06:24:07 +0000,Wed; 8 Jul 2009 16:41:47 +0000,Thu; 27 Apr 2006 07:27:18 +0000,,0.2.0,,,HADOOP-170,https://issues.apache.org/jira/browse/HADOOP-171
HADOOP-172,Bug,Blocker,ipc,rpc doesn't handle returning null for a String[],The job tracker gets errors in returning the result from pollForTaskWithClosedJob060427 100434 Served: pollForTaskWithClosedJob 0declaredClass = [Ljava.lang.String;instance class = org.apache.hadoop.io.NullWritable        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95)        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230),Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 28 Apr 2006 00:15:15 +0000,Thu; 3 Aug 2006 17:46:38 +0000,Fri; 28 Apr 2006 03:58:22 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-172
HADOOP-173,Improvement,Major,,optimize allocation of tasks w/ local data,When a job first starts; all task trackers ask the job tracker for jobs at once.  With lots of task trackers; the job tracker gets very slow.  The first type of task that the job tracker attempts to find is one with some of its input data stored on the same node as the task tracker.  This case currently loops through tasks blindly; which; on average; requires numHosts/(replication*2) iterations to find a match (I think).  This could be optimized by adding a table mapping from host to task.,Closed,Fixed,,Doug Cutting,Doug Cutting,Fri; 28 Apr 2006 03:58:09 +0000,Wed; 8 Jul 2009 16:51:45 +0000,Sat; 29 Apr 2006 00:06:06 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-173
HADOOP-174,Bug,Major,,jobclient kills job for one timeout,The launching application (via JobClient) checks the status once a second. If any timeouts or errors occur; the user's job is killed.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Sat; 29 Apr 2006 04:01:14 +0000,Wed; 8 Jul 2009 16:51:45 +0000,Sat; 29 Apr 2006 05:15:03 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-174
HADOOP-175,Improvement,Minor,io,Utilities for reading SequenceFile and MapFile,Most data in Hadoop is stored in SequenceFile-s and MapFile-s. Sometimes there is a need to examine such files; but no specialized utilities exist ro read them.These two classes provide a functionality to examine individual records in such files; and also to dump the content of such files to a plain text output.,Closed,Duplicate,NULL,Owen O'Malley,Andrzej Bialecki,Sat; 29 Apr 2006 04:57:24 +0000,Thu; 23 Apr 2009 19:24:57 +0000,Fri; 16 Jan 2009 05:53:06 +0000,,,,,HADOOP-2501,https://issues.apache.org/jira/browse/HADOOP-175
HADOOP-176,Bug,Major,io,comparators of integral writable types are not transitive for inequalities,Consider the following code from IntWritable.java :    public int compare(byte[] b1; int s1; int l1;                       byte[] b2; int s2; int l2) {      int thisValue = readInt(b1; s1);      int thatValue = readInt(b2; s2);      return thisValue - thatValue;    }If a Java Runtime subtracts 20 from -(2^31 - 10) it gets a huge positive number; not the negative value that the comparator should return.LongWritable does this right; of course.That last line should be       return (thisValuethatValue ? -1 : (thisValue==thatValue ? 0 : 1));-dk,Closed,Fixed,,Unassigned,Dick King,Sat; 29 Apr 2006 06:25:32 +0000,Fri; 8 Sep 2006 21:19:43 +0000,Thu; 17 Aug 2006 20:42:56 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-176
HADOOP-177,Improvement,Minor,,improvement to browse through the map/reduce tasks,The Jobtracker webbapp currently shows all the maps and reduce tasks on a single page. This sometimes causes the browser to crash with 1000's of maps/recudes running.,Closed,Fixed,,Mahadev konar,Mahadev konar,Mon; 1 May 2006 05:16:03 +0000,Wed; 8 Jul 2009 16:51:45 +0000,Tue; 2 May 2006 00:50:26 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-177
HADOOP-178,Improvement,Major,,piggyback block work requests to heartbeats and move block replication/deletion startup delay from datanodes to namenode,"Currently each datanode sends at least two messages to namenode within a heartbeat interval. One is a heartbeat message and another is block work request. By piggybacking the block work request to a heartbeat can greatly cut the number of messages between a datanode and the namenode.Secondly each datanode waits for a configurable ""StartupPeriod"" before it sends a block work request in order to avoid uneccessary block replication at startup time. But if the namenode starts much later than datanodes; this scheme does not work. Furthermore; the namenode has more information to decide when to send block work to datanodes. For example; all datanodes send block reports etc. It is more resonable to move the startup delay from datanodes to the namenode",Closed,Fixed,,Hairong Kuang,Hairong Kuang,Tue; 2 May 2006 01:09:16 +0000,Wed; 8 Jul 2009 16:41:53 +0000,Tue; 2 May 2006 02:45:39 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-178
HADOOP-179,Bug,Major,,task tracker ghosts remain after 10 minutes,I had a bunch of TaskTrackers time out because they were in the middle of job cleanup and the JobTracker restarted them by responding to emitHeartbeat with UNKNOWN_TASKTRACKER. Afterwards; I ended up with both the new and the restarted TaskTrackers on the list:node1100_1234; 0-10 seconds since heartbeatnode1100_4321;  10;000 seconds since heartbeat,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 2 May 2006 01:40:36 +0000,Wed; 8 Jul 2009 16:51:45 +0000,Tue; 16 May 2006 10:59:23 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-179
HADOOP-180,Bug,Major,,task tracker times out cleaning big job,After completing a big job (63;920 maps; 1880 reduces; 188 nodes); lots of the TaskTrackers timed out because the task cleanup is handled by the same thread as the heartbeats.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 2 May 2006 01:42:42 +0000,Wed; 8 Jul 2009 16:51:45 +0000,Tue; 16 May 2006 02:07:24 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-180
MAPREDUCE-42,Bug,Major,,task trackers should not restart for having a late heartbeat,TaskTrackers should not close and restart themselves for having a late heartbeat. The JobTracker should just accept their current status.,Resolved,Invalid,HADOOP-43,Devaraj Das,Owen O'Malley,Tue; 2 May 2006 01:45:01 +0000,Sat; 31 Dec 2011 09:21:16 +0000,Sat; 31 Dec 2011 09:21:15 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-42
HADOOP-182,Bug,Major,,lost task trackers should not update status of completed jobs,When a Task Tracker is lost (by not sending a heartbeat for 10 minutes); the JobTracker marks the tasks that were active on that node as failed. There are two issues:   1. No task from a completed or failed job should be modified.   2. No reduces should be marked as failed; since their output is in dfs and therefore not only on the dead node.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 2 May 2006 01:48:33 +0000,Wed; 8 Jul 2009 16:51:44 +0000,Sat; 6 May 2006 01:09:35 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-182
HADOOP-183,Bug,Minor,,adjust file replication factor when loading image and edits according to replication.min and replication.max,Currently in dfs; when namenode starts; a file's replication factor is loaded either from image or edits. The replication factor may be smaller than replication.min or greater than replication.max.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Tue; 2 May 2006 02:11:55 +0000,Wed; 8 Jul 2009 16:41:52 +0000,Tue; 2 May 2006 03:01:24 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-183
HADOOP-184,Test,Minor,,hadoop nightly build and regression test on a cluster,create a jar file for the tests and have  filesystem and mapreduce tests on the cluster,Closed,Fixed,,Mahadev konar,Mahadev konar,Tue; 2 May 2006 04:38:42 +0000,Thu; 3 Aug 2006 17:46:39 +0000,Wed; 3 May 2006 06:12:23 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-184
HADOOP-185,Bug,Critical,,tasks are lost during pollForNewTask,"There is the potential for ""losing"" tasks that are assigned by the JobTracker to a TaskTracker; but that fail during returning the result (usually due to a RPC timeout). In this case; the Job becomes ""wedged"" in that the tasks will never run and never time out.",Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 2 May 2006 10:57:38 +0000,Wed; 8 Jul 2009 16:51:44 +0000,Wed; 3 May 2006 05:05:16 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-185
HADOOP-186,Bug,Major,,communication problems in the task tracker cause long latency,The Task Tracker's offerService loop has no protection from exceptions; so that any communication problems with the Job Tracker; such as RPC timeouts; cause the TaskTracker to sleep 5 seconds and start again at the top of the loop.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 2 May 2006 11:21:01 +0000,Wed; 8 Jul 2009 16:51:44 +0000,Wed; 3 May 2006 01:15:57 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-186
HADOOP-187,Test,Major,,simple distributed dfs random data writer & sort example applications,These are the examples/benchmark programs that I've been using to test Hadoop map/reduce with. The first is a program that runs 10 maps/node and each map writes 1 gig of random data to a dfs file as a SequenceFile of BytesWritable/BytesWritable.The second uses the identity map and reduce to sort the data and write it out to dfs.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 2 May 2006 12:30:38 +0000,Wed; 8 Jul 2009 16:51:44 +0000,Wed; 3 May 2006 01:49:00 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-187
HADOOP-188,Bug,Major,,more unprotected RPC calls in JobClient.runJob allow loss of job due to timeout,I fixed one of the RPC calls in JobClient.runJob; but I missed a couple of others.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 2 May 2006 23:16:03 +0000,Wed; 8 Jul 2009 16:51:45 +0000,Wed; 3 May 2006 03:08:21 +0000,,0.1.1,,,,https://issues.apache.org/jira/browse/HADOOP-188
HADOOP-189,New Feature,Major,,Add job jar lib; classes; etc. to CLASSPATH when in standalone mode,Currently; in standalone mode;  hadoop is unable to launch other than the most basic of job jars where 'basic' is a job jar with nought but class files at top level of the jar with Main-Class pointing at entry point.  If the job jar has dependencies on jars under the job jar lib or there are job jar plugins in the classes dir; etc.;  these dependencies are not loaded and the job fails launch.,Closed,Fixed,,Doug Cutting,stack,Wed; 3 May 2006 00:44:04 +0000,Wed; 8 Jul 2009 16:51:45 +0000,Thu; 4 May 2006 07:13:48 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-189
HADOOP-190,Bug,Major,,Job fails though task succeeded if we fail to exit,This is an odd case.  Main cause will be programmer error but I suppose it could happen during normal processing. Whichever; would be grand if hadoop was better able to deal.My map task completed 'successfully' but because I had started threads inside in my task that were not set to be of daemon type that under certain circumstances were left running;  my child stuck around after reporting 'done'  the JVM wouldn't go down while non-daemon threads still running.  After ten minutes; TT steps in;  kills the child and does cleanup of the successful output.  Because JT has been told the task completed successfully; reducers keep showing up looking for the output now removed  until the job fails.Below is illustration of the problem using log output:....060501 090401 task_0001_m_000798_0 0.99491096% adding http://www.score.umd.edu/aum.jpg 24891 image/jpeg060501 090401 task_0001_m_000798_0 1.0% adding http://www.score.umd.edu/album.jp24891 image/jpeg060501 090401 Task task_0001_m_000798_0 is done....060501 091410 task_0001_m_000798_0: Task failed to report status for 608 secondsKilling.....060501 091410 Calling cleanup because was killed or FAILED task_0001_m_000798_0060501 091410 task_0001_m_000798_0 done; removing files.Then; subsequently....060501 091422 SEVERE Can't open map output:/1/hadoop/tmp/task_0001_m_000798_0/pa-12.outjava.io.FileNotFoundException: LocalFS...and on and on.,Closed,Fixed,,Unassigned,stack,Wed; 3 May 2006 04:28:56 +0000,Thu; 3 Aug 2006 17:46:39 +0000,Wed; 3 May 2006 05:54:01 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-190
HADOOP-191,New Feature,Major,,add hadoopStreaming to src/contrib,This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree.hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks.The unit test TestStreaming runs the Unix tools tr (as Map) and uniq (as Reduce)TO test the patch: Merge the patch. The only existing file that is modified is trunk/build.xmltrunkant deploy-contribtrunkbin/hadoopStreaming : should show usage messagetrunkant test-contrib    : should run one test successfullyTO add src/contrib/someOtherProject:edit src/contrib/build.xml,Closed,Fixed,,Doug Cutting,Michel Tourn,Wed; 3 May 2006 05:10:00 +0000,Thu; 3 Aug 2006 17:46:40 +0000,Wed; 24 May 2006 02:40:26 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-191
HADOOP-192,Bug,Blocker,,Trivial JRE 1.5 versus 1.4 bug,Long.valueOf(long) is in 1.5 but not in 1.4.x.  Use new Long(long) instead.,Closed,Fixed,,Unassigned,David Bowen,Thu; 4 May 2006 00:23:46 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Thu; 4 May 2006 03:51:06 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-192
HADOOP-193,Test,Major,fs,DFS i/o benchmark.,"DFS i/o benchmark is a map-reduce based test that measures performance of the cluster for reads and writes.This is an evolved version of HADOOP-72; and HADOOP-95 test.This test writes into or reads from a specified number of files.File size is specified as a parameter to the test.Each file is processed in a separate map task.The unique reducer then collects stats.Finally; the following information is displayed	read or write test	date and time the test finished	number of files processed	total number of bytes processed	throughput in mb/sec (total number of bytes / sum of processing times)	average i/o rate in mb/sec per file	standard i/o rate deviationI  included the test into the AllTestDriver.",Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Thu; 4 May 2006 08:57:17 +0000,Thu; 10 Aug 2017 18:20:29 +0000,Thu; 4 May 2006 11:39:48 +0000,,,,,HDFS-708,https://issues.apache.org/jira/browse/HADOOP-193
HADOOP-194,Test,Major,,Distributed checkup of the file system consistency.,This is a map-reduce based test that checks consistency of the file systemby  reading all blocks of all files; and detecting which of them are missing or corrupted.See HADOOP-95 and HADOOP-101 for related discussions.This could be an alternative to the sequential checkup in dfsck.It would be nice to integrate distributed checkup with dfsck; but I don't yet see how.This test reuses classes defined in HADOOP-193.,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Thu; 4 May 2006 09:11:48 +0000,Thu; 3 Aug 2006 17:46:40 +0000,Thu; 4 May 2006 11:40:04 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-194
HADOOP-195,Improvement,Major,,improve performance of map output transfers,The data transfer of the map output should be transfered via http instead rpc; because rpc is very slow for this application and the timeout behavior is suboptimal. (server sends data and client ignores it because it took more than 10 seconds to be received.),Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 5 May 2006 00:50:50 +0000,Wed; 8 Jul 2009 16:51:45 +0000,Thu; 25 May 2006 05:37:55 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-195
HADOOP-196,Bug,Major,conf,Fix buggy uselessness of Configuration( Configuration other) constructor,The constructor public Configuration(Configuration other),Closed,Fixed,,Unassigned,alan wootton,Fri; 5 May 2006 04:49:48 +0000,Fri; 8 Sep 2006 21:19:43 +0000,Thu; 31 Aug 2006 20:58:38 +0000,,0.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-196
MAPREDUCE-473,Improvement,Minor,,Hook InetAddress.getLocalHost().getHostName() to support cluster simulatation.,"I have been running a simulation for weeks now (and also a 30 machine crawl). To make it work I need to sometimes let DataNodes; and TaskTrackers think they have a different machine-name than the one in InetAddress.getLocalHost()The patch is:1) replace InetAddress.getLocalHost().getHostName() with xxxx	1.a)xxxx could be ""conf.get(""inetaddress.localhost.name"";InetAddress.getLocalHost().getHostName())""	1.b)or; xxxx could be a static call; I chose the latter: ""InetAddressWrapper.getLocalHostName(conf)""2) InetAddressWrapper.getLocalHostName(conf) checks the config for a hostname; and then calls InetAddress.getLocalHost().getHostName()There's 3 places where it happens:DataNodeDFSClientTaskTrackerI did not patch the two tests that call InetAddress because they are not really using hostname.",Open,Unresolved,,Owen O'Malley,alan wootton,Fri; 5 May 2006 05:39:09 +0000,Sat; 20 Jun 2009 07:57:25 +0000,,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-473
HADOOP-198,Improvement,Minor,,adding owen's examples to exampledriver,owen's sorter and randomwriter are not added to the examples.jar file,Closed,Fixed,,Mahadev konar,Mahadev konar,Fri; 5 May 2006 07:33:24 +0000,Thu; 3 Aug 2006 17:46:40 +0000,Fri; 5 May 2006 23:29:51 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-198
HADOOP-199,Bug,Minor,,reduce copy progress not updating,I'm running with the svn head from Friday and my patch for hadoop-180 and I'm not getting progress updates until reduces finish. I'm worried that it may be related to my changes for hadoop-182. I'll track it down; but I wanted to let everyone know.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Sun; 7 May 2006 10:26:54 +0000,Wed; 8 Jul 2009 16:51:38 +0000,Tue; 9 May 2006 01:13:51 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-199
HADOOP-200,Bug,Major,,The map task names are sent to the reduces,As each reduce is created; it is given the entire set of potential map names. For my large sort jobs with 64k maps; this means that each reduce task is given a two dimensional array that is 5 tasks/map * 64k maps = 320k strings. Since the reduce task is passed from the job tracker to the task tracker and down to the task runner; passing the entire list is very expensive. I suspect that this is the cause of the slow downs that I see in the task trackers heart beats when the reduce tasks are being launched.I propose that the ReduceTask be changed to just get the count of maps; with ids from 0 .. maps -1.  public ReduceTask(String jobFile; String taskId; int maps; int partition);Then we need to change the protocol for finding map outputs:  MapOutputLocation[] locateMapOutputs(String jobId; int[] mapIds; int partition);,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Sun; 7 May 2006 10:54:51 +0000,Wed; 8 Jul 2009 16:51:38 +0000,Tue; 16 May 2006 02:18:17 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-200
HADOOP-201,Bug,Minor,,hadoop dfs -report throws exception,"Running hadoop dfs -report throws the lovely exception below.Changing org.apache.hadoop.dfs.DatanodeInfo back to being a public class solves the problem.~/hadoop$ bin/hadoop dfs -report060508 104801 parsing file:/home/hadoop/hadoop/conf/hadoop-default.xml060508 104801 parsing file:/home/hadoop/hadoop/conf/hadoop-site.xml060508 104801 No FS indicated; using default:xxx:9000060508 104801 Client connection to 10.0.0.12:9000: startingTotal raw bytes: 2763338170368 (2573.55 Gb)Used raw bytes: 1548564473694 (1442.21 Gb)% used: 56.03%Total effective bytes: 145953744375 (135.93 Gb)Effective replication multiplier: 10.609967427182013-------------------------------------------------060508 104801 Client connection to 10.0.0.12:9000 caught: java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.DatanodeInfo with modifiers ""public""java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.DatanodeInfo with modifiers ""public""        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:226)        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:163)        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:211)        at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:60)        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:170)Caused by: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.DatanodeInfo with modifiers ""public""        at sun.reflect.Reflection.ensureMemberAccess(Reflection.java:65)        at java.lang.Class.newInstance0(Class.java:344)        at java.lang.Class.newInstance(Class.java:303)        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:45)        ... 5 more060508 104801 Client connection to 10.0.0.12:9000: closing",Closed,Fixed,,Doug Cutting,Johan Oskarsson,Mon; 8 May 2006 18:21:43 +0000,Wed; 8 Jul 2009 16:41:53 +0000,Tue; 9 May 2006 01:23:46 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-201
HADOOP-202,Improvement,Trivial,,sort should use a smaller number of reduces,We should see better performance with fewer reduces. I'll change the default number of reduces to be equal to the capacity of the cluster.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 9 May 2006 05:27:55 +0000,Wed; 8 Jul 2009 16:51:45 +0000,Tue; 16 May 2006 02:21:54 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-202
HADOOP-203,Improvement,Minor,fs,remove deprecated java.io.File methods,Now that the 0.2 release is out; we should remove the deprecated FileSystem methods that use java.io.File; since using org.apache.hadoop.fs.Path is less error-prone.,Closed,Duplicate,NULL,Doug Cutting,Doug Cutting,Tue; 9 May 2006 05:38:30 +0000,Fri; 15 Dec 2006 23:02:15 +0000,Wed; 1 Nov 2006 21:57:38 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-203
HADOOP-204,Improvement,Major,metrics,Need to tweak a few things in the metrics package to support the Simon plugin,(1) added an extra metrics.jar target to the build.xml so that I can build a stand-alone library containing only themetrics package and its subpackages.(2) added serialversionUIDs to a bunch of classes to make Eclipse happy(3) made AbstractMetricsContext.createRecord final; and added a protected newRecord that subclasses can useto customize record creation without breaking the parent class.(4) minor fix to how errors in callbacks are handled(5) constructor in MetricsRecordImpl made protected rather than package private so that it can be subclassed(6) extended Util.parse(String serverSpecs; int defaultPort) to handle the case of a null serverSpecs by defaulting to localhost,Closed,Fixed,,David Bowen,David Bowen,Wed; 10 May 2006 03:02:56 +0000,Sat; 14 Oct 2006 02:07:37 +0000,Sat; 13 May 2006 04:15:04 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-204
HADOOP-205,Bug,Major,,the job tracker does not schedule enough map on the cluster,The job tracker during my big sort runs only has 80-120 maps running at a time with 2 tasks/node and 179 nodes and a large queue of map tasks. It seems to be caused by the load balancing using the current load rather than the required load to run all of the queued tasks.,Closed,Fixed,,Mahadev konar,Owen O'Malley,Wed; 10 May 2006 03:56:48 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Wed; 24 May 2006 05:56:42 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-205
MAPREDUCE-434,Improvement,Minor,,LocalJobRunner limited to single reducer,when mapred.job.tracker is set to 'local'; my setNumReduceTasks call is ignored; and the number of reduce tasks is set at 1.This prevents me from locally debugging my partition function; which tries to partition based on the number of reduce tasks.,Closed,Fixed,,Aaron Kimball,Yoram Arnon,Wed; 10 May 2006 06:11:57 +0000,Mon; 24 Feb 2014 20:57:42 +0000,Tue; 6 Aug 2013 06:41:58 +0000,,,,,MAPREDUCE-4337,https://issues.apache.org/jira/browse/MAPREDUCE-434
HADOOP-207,Bug,Critical,,Patch to HADOOP-96 uses long deprecated call,System.getenv() was deprecated in Java 1.4. My mixed Java 1.4/Java 1.5 cluster won't start up with this code in place. Should probably change the scripts to pass the necessary environment variables in using -D or explicit arguments.,Closed,Fixed,,Hairong Kuang,Bryan Pendleton,Wed; 10 May 2006 06:52:58 +0000,Fri; 15 Dec 2006 23:02:15 +0000,Sat; 13 May 2006 03:18:03 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-207
HADOOP-208,Improvement,Major,,add failure page to webapp,I'd like a webapp page that just lists the failures in a job so that I can find them more easily. I also want to put back the job detail page.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Thu; 11 May 2006 05:31:52 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Sat; 13 May 2006 04:16:03 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-208
HADOOP-209,New Feature,Major,fs,Add a program to recursively copy directories across file systems,"A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI; such as:hadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir""cp"" command would invoke a map-reduce program to copy files recursively.I willl attach a patch as soon as svn is up and running.",Closed,Fixed,,Unassigned,Milind Bhandarkar,Thu; 11 May 2006 06:05:18 +0000,Thu; 3 Aug 2006 17:46:41 +0000,Sat; 13 May 2006 04:35:21 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-209
HADOOP-210,Bug,Major,,Namenode not able to accept connections,"I am running owen's random writer on a 627 node cluster (writing 10GB/node).  After running for a while (map 12% reduce 1%) I get the following error on the Namenode:Exception in thread ""Server listener on port 60000"" java.lang.OutOfMemoryError: unable to create new native thread        at java.lang.Thread.start0(Native Method)        at java.lang.Thread.start(Thread.java:574)        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:105)After this; the namenode does not seem to be accepting connections from any of the clients. All the DFSClient calls get timeout. Here is a trace for one of them:java.net.SocketTimeoutException: timed out waiting for rpc response	at org.apache.hadoop.ipc.Client.call(Client.java:305)	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:149)	at org.apache.hadoop.dfs.$Proxy1.open(Unknown Source)	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:419)	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.(DFSClient.java:406)	at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:171)	at org.apache.hadoop.dfs.DistributedFileSystem.openRaw(DistributedFileSystem.java:78)	at org.apache.hadoop.fs.FSDataInputStream$Checker.(FSDataInputStream.java:46)	at org.apache.hadoop.fs.FSDataInputStream.(FSDataInputStream.java:228)	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:43)	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:105)	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:785).The namenode then has around 1% CPU utilization at this time (after the outofmemory exception has been thrown). I have profiled the NameNode and it seems to be using around a maixmum heap size of 57MB (which is not much). So; heap size does not seem to be a problem. It might be happening due to lack of Stack space? Any pointers?",Closed,Fixed,,Devaraj Das,Mahadev konar,Thu; 11 May 2006 07:09:22 +0000,Wed; 8 Jul 2009 16:41:53 +0000,Thu; 22 Jun 2006 01:15:29 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-210
HADOOP-211,Improvement,Minor,,logging improvements for Hadoop,"Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 broad changes to the way logging is currently done; these being:	The use of a uniform logging format by all Hadoop subsystems	The use of Apache commons logging as a facade above an underlying logging framework	The use of Log4J as the underlying logging framework instead of java.util.loggingThis is largely polishing work; but it seems like it would make log analysis and debuggingeasier in the short term. In the long term; it would future proof logging to the extent ofallowing the logging framework used to change while requiring minimal code change. The propos changes are motivated by the following requirements which we think Hadoops logging should meet:	Hadoops logs should be amenable to analysis by tools like grep; sed; awk etc.	Log entries should be clearly annotated with a timestamp and a logging level	Log entries should be traceable to the subsystem from which they originated	The logging implementation should allow log entries to be annotated with source codelocation information like classname; methodname; file and line number; without requiringcode changes	It should be possible to change the logging implementation used without having to changethousands of lines of code	The mapping of loggers to destinations (files; directories; servers etc.) should bespecified and modifiable via configurationUniform logging format:All Hadoop logs should have the following structure.Header\nLogEntry\n &lt;Exception\n...where the header line specifies the format of each log entry. The header line has the format:'# Fieldname Fieldname...\n'. The default format of each log entry is: '# Timestamp Level LoggerName Message'; where:	Timestamp is a date and time in the format MM/DD/YYYY:HH:MM:SS	Level is the logging level (FATAL; WARN; DEBUG; TRACE; etc.)	LoggerName is the short name of the logging subsystem from which the message originated e.g.fs.FSNamesystem; dfs.Datanode etc.	Message is the log message producedWhy Apache commons logging and Log4J?Apache commons logging is a facade meant to be used as a wrapper around an underlying loggingimplementation. Bridges from Apache commons logging to popular logging implementations (Java logging; Log4J; Avalon etc.) are implemented and available as part of the commons loggingdistribution. Implementing a bridge to an unsupported implementation is fairly striaghtforwardand involves the implementation of subclasses of the commons logging LogFactory and Logger classes. Using Apache commons logging and making all logging calls through it enables us tomove to a different logging implementation by simply changing configuration in the best case.Even otherwise; it incurs minimal code churn overhead.Log4J offers a few benefits over java.util.logging that make it a more desirable choice for thelogging back end.	Configuration Flexibility: The mapping of loggers to destinations (files; sockets etc.)can be completely specified in configuration. It is possible to do this with Java logging aswell; however; configuration is a lot more restrictive. For instance; with Java logging all log files must have names derived from the same pattern. For the namenode; log files could be named with the pattern ""%h/namenode%u.log"" which would put log files in the user.homedirectory with names like namenode0.log etc. With Log4J it would be possible to configurethe namenode to emit log files with different names; say heartbeats.log; namespace.log;clients.log etc. Configuration variables in Log4J can also have the values of system properties embedded in them.	Takes wrappers into account: Log4J takes into account the possibility that an applicationmay be invoking it via a wrapper; such as Apache commons logging. This is important becauselogging event objects must be able to infer the context of the logging call such as classname;methodname etc. Inferring context is a relatively expensive operation that involves creatingan exception and examining the stack trace to find the frame just before the first frame of the logging framework. It is therefore done lazily only when this information actually needs to be logged. Log4J can be instructed to look for the frame corresponding to the wrapperclass; Java logging cannot. In the case of Java logging this means that a) the bridge from Apache commons logging is responsible for inferring the calling context and setting it in the logging event and b) this inference has to be done on every logging call regardless of whetheror not it is needed.	More handy features: Log4J has some handy features that Java logging doesn't. A coupleof examples of these:a) Date based rolling of log files b) Format control through configuration. Log4J has a PatternLayout class that can be configured to generate logs with a user specified pattern. The logging format describedabove can be described as ""%d{MM/dd/yyyy:HH:mm:SS} %c{2} %p %m"". The format specifiersindicate that each log line should have the date and time followed by the logger name followedby the logging level or priority followed by the application generated message.",Closed,Fixed,,Sameer Paranjpye,Sameer Paranjpye,Fri; 12 May 2006 06:16:09 +0000,Thu; 3 Aug 2006 17:46:41 +0000,Sat; 3 Jun 2006 02:20:13 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-211
HADOOP-212,Improvement,Critical,,allow changes to dfs block size,Trying to change the DFS block size; led the realization that the 32;000;000 was hard coded into the source code. I propose:  1. Change the default block size to 64 * 1024 * 1024.  2. Add the config variable dfs.block.size that sets the default block size.  3. Add a parameter to the FileSystem; DFSClient; and ClientProtocol create method that let's the user control the block size.  4. Rename the FileSystem.getBlockSize to getDefaultBlockSize.  5. Add a new method to FileSytem.getBlockSize that takes a pathname.  6. Use long for the block size in the API; which is what was used before. However; the implementation will not work if block size is set bigger than 2**31.  7. Have the InputFormatBase use the blocksize of each file to determine the split size.Thoughts?,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 12 May 2006 22:45:28 +0000,Wed; 8 Jul 2009 16:41:53 +0000,Tue; 16 May 2006 04:26:14 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-212
HADOOP-213,Improvement,Minor,,add/change TestDFSIO command line arguments,"TestDFSIO would benefit from the follwing changes on the command line:1. allow a ""-dfs dfs"" option; to specify which dfs to test. Just like the hadoop dfs command.2. allow a ""-jt job tracker"" option; to specify a job tracker. Just like the hadoop job command.3. allign the file size units with the dfs block size. Using MB (2^20) when the dfs uses 10^6 skews the results a bit.4. add a ""-replication replication"" option; to specify files' replication factor.",Resolved,Fixed,,Konstantin Shvachko,Yoram Arnon,Sat; 13 May 2006 02:05:54 +0000,Wed; 8 Jul 2009 16:41:54 +0000,Wed; 10 Dec 2008 17:36:26 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-213
HDFS-112,Bug,Major,,ClusterTestDFS fails,"The dfs unit tests; from the ant target 'cluster' have been failing. (ClusterTestDFSNamespaceLogging; ClusterTestDFS). I don't know if anyone but me cares about these tests; but I do. I would like to write better tests for dns. I think we all need that.They have been partially broken since  ""test.dfs.same.host.targets.allowed"" went away and replication ceased for these tests. They got really broken when NameNode stopped automatically formatting itself .Since they seem to be ignored; I took the liberty of changing how they work.The main thing is; you must put this into your hosts file:127.0.0.1       localhost0127.0.0.1       localhost1127.0.0.1       localhost2127.0.0.1       localhost3127.0.0.1       localhost4127.0.0.1       localhost5127.0.0.1       localhost6127.0.0.1       localhost7127.0.0.1       localhost8127.0.0.1       localhost9127.0.0.1       localhost10127.0.0.1       localhost11127.0.0.1       localhost12127.0.0.1       localhost13127.0.0.1       localhost14127.0.0.1       localhost15This way you can start DataNodes; and TaskTrackers (up to 16 of them) with unique hostnames.Also; I changed all the places that used to call InetAddress.getLocalHost().getHostName() to get it from a new method in Configuration (this issue is the same as http://issues.apache.org/jira/browse/HADOOP-197 ).",Resolved,Not A Problem,,Sameer Paranjpye,alan wootton,Sat; 13 May 2006 02:49:30 +0000,Sun; 17 Jul 2011 17:37:59 +0000,Sun; 17 Jul 2011 17:37:59 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-112
HADOOP-215,New Feature,Minor,,New method for obtaining report of NameNode and JobTracker internals,"Many weeks ago we (at shopping.com research) decided we wanted to be able to get reports from the internals of JobTracker and NameNode. The hadoop web server provides some of this; but we wanted a more structured output; and easier extensibility.So; we decided to use xml; and I wrote it.There is a very thin interface to ClientProtocol; and JobSubmissionProtocol like this:public XmlReporter getXmlReport(String classname; String question);The implementation (in JobTracker and NameNode ) looks like this:public XmlReporter getXmlReport(String classname; String question)    {		XmlReporter reporter = XmlReporter.getInstance( classname; this;  question);		reporter.report();		return reporter;    }The idea being that you pass in some xml (question); an XmlReporter (classname) is instanciated and passed back. An XmlReporter object consists of nothing more that two org.w3c.dom.Document objects (one in the question; the other is the answer). The Writable interface; for the RPC; simply serializes the dom tree to a string; and then parses it back to a dom tree.Anyway; here it is. I would like for it to either make it into the code; or for me to find anoher way.",Closed,Won't Fix,,Unassigned,alan wootton,Sat; 13 May 2006 03:48:55 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Tue; 20 Jun 2006 04:22:31 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-215
HADOOP-216,Improvement,Trivial,,Task Detail web page missing progress,You can see progress on the job view web page; but not the individual task page.,Closed,Fixed,,Doug Cutting,Bryan Pendleton,Sat; 13 May 2006 03:58:34 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Thu; 18 May 2006 06:47:37 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-216
HADOOP-217,Bug,Major,,IllegalAcessException when creating a Block object via WritableFactories,"When I ran the dfs namenode; I received an error message listed below. Changing Block class to be public will be able to fix the problem.java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:226)        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:163)        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:211)        at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:88)        at org.apache.hadoop.ipc.Server$Connection.run(Server.java:154)Caused by: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""        at sun.reflect.Reflection.ensureMemberAccess(Reflection.java:65)        at java.lang.Class.newInstance0(Class.java:344)        at java.lang.Class.newInstance(Class.java:303)        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:45)        ... 5 more",Closed,Fixed,,Hairong Kuang,Hairong Kuang,Sat; 13 May 2006 07:56:39 +0000,Wed; 8 Jul 2009 16:41:53 +0000,Sat; 3 Jun 2006 02:59:19 +0000,,0.3.0,,,HADOOP-264,https://issues.apache.org/jira/browse/HADOOP-217
HADOOP-218,Improvement,Minor,,Inefficient calls to get configuration values in TaskInprogress,Each time a pollforNewTask in called on the JobTracker; the taskinprogress makes a call to hasSpeculativeTask() which current does a conf.getSpeculativeExecution() each time its called. The fix would be to store it in the TaskInProgress as soon as it is created and make only a single call to conf.getSpeculativeExecution().,Closed,Fixed,,Mahadev konar,Mahadev konar,Mon; 15 May 2006 05:07:07 +0000,Wed; 8 Jul 2009 16:51:47 +0000,Tue; 16 May 2006 04:50:08 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-218
HADOOP-219,Bug,Trivial,io,SequenceFile#handleChecksumException NPE,"The SequenceFile#handleChecksumException assumes the conf data member has been set.  It will not be set if we use the 'Reader(FileSystem fs; Path file; int bufferSize; long start; long length)' constructor.  The latter is used by ReduceTask Sorter:java.lang.NullPointerException	at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407)	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400)	at org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837)	at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881)	at org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766)	at org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702)	at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528)	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253)	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787)",Closed,Fixed,,Doug Cutting,stack,Tue; 16 May 2006 01:20:43 +0000,Thu; 3 Aug 2006 17:46:42 +0000,Tue; 16 May 2006 03:14:22 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-219
HADOOP-220,Sub-task,Major,fs,Add -dfs and -jt command-line parameters to specify namenode and jobtracker.,Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.,Closed,Fixed,,Milind Bhandarkar,Milind Bhandarkar,Tue; 16 May 2006 05:18:08 +0000,Thu; 3 Aug 2006 17:46:42 +0000,Sat; 20 May 2006 02:52:16 +0000,,,,,HADOOP-59,https://issues.apache.org/jira/browse/HADOOP-220
HADOOP-221,Improvement,Major,,make the number of map output configurable,The number of map output server threads in each task tracker is currently set to the number of task slots. Since the optimum setting depends on the network; it would be nice to have more threads serving map output files than reduces.,Closed,Duplicate,NULL,Owen O'Malley,Owen O'Malley,Tue; 16 May 2006 10:32:21 +0000,Wed; 8 Jul 2009 16:51:49 +0000,Wed; 31 May 2006 10:53:41 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-221
HADOOP-222,New Feature,Trivial,,Set replication from dfsshell,Added ability to set replication for a directory/file from the command line.Not heavily tested...,Closed,Fixed,,Johan Oskarsson,Johan Oskarsson,Tue; 16 May 2006 18:16:12 +0000,Wed; 8 Jul 2009 16:41:55 +0000,Wed; 31 May 2006 05:41:55 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-222
MAPREDUCE-460,New Feature,Major,mrv2,Should be able to re-run jobs; collecting only missing output,"For jobs with no side effects (roughly == jobs with speculative execution enabled); if partial output has been generated; it should be possible to re-run the job; and fill in the missing pieces. I have now run the same job twice; once finishing 42 of 44 reduce tasks; another time finishing only 17. Each time; many nodes have failed; causing many many tasks to fail ( in one case; 5k failures from 15k map tasks; 23 failures from 44 reduces); but some valid output was generated. Since the output is only dependent on the input; and both jobs used the same input; I will now be able to combine these two failed task outputs to get a completed job's output. This should be something that can be more automatic.In particular; it should be possible to resubmit a job; with a list of partitions that should be ignored. A special Combiner; or pre-Combiner; would throw out any map output for partitions that have already been successfully completed; thus reducing the amount of data that needs to be reduced to complete the job. It would; of course; be nice to support ""filling in"" existing outputs; rather than having to do a move operation on completed outputs.",Reopened,Unresolved,,Unassigned,Bryan Pendleton,Wed; 17 May 2006 01:10:21 +0000,Fri; 18 Jul 2014 04:56:12 +0000,,,,,,MAPREDUCE-443,https://issues.apache.org/jira/browse/MAPREDUCE-460
HDFS-315,Improvement,Major,,Allow simplified versioning for namenode and datanode metadata.,"Currently namenode has two types of metadata: The FSImage; and FSEdits. FSImage contains information abut Inodes; and FSEdits contains a list of operations that were not saved to FSImage. Datanode currently does not have any metadata; but would have it some day. The file formats used for storing these metadata will evolve over time. It is important for the file-system to be backward compatible. That is; the metadata readers need to be able to identify which version of the file-format we are using; and need to be able to read information therein. As we add information to these metadata; the complexity of the reader increases dramatically.I propose a versioning scheme with a major and minor version number; where a different reader class is associated with a major number; and that class interprets the minor number internally. The readers essentially form a chain starting with the latest version. Each version-reader looks at the file and if it does not recognize the version number; passes it to the version reader next to it by calling the parse method; returnng the results of the parse method up the chain (In case of the namenode; the parse result is an array of Inodes.This scheme has an advantage that every time a new major version is added; the new reader only needs to know about the reader for its immediately previous version; and every reader needs to know only about which major version numbers it can read.The writer is not so versioned; because metadata is always written in the most current version format.One more change that is needed for simplified versioning is that the ""struct-surping"" of dfs.Block needs to be removed. Block's contents will change in later versions; and older versions should still be able to readFields properly. This is more general than Block of course; and in general only basic datatypes should be used as Writables in DFS metadata.For edits; the reader should return opcode; ArrayWritable pairs' array. This will also remove the limitation of two operands for very opcodes; and will be more extensible.Even with this new versioning scheme; the last Reader in the reader-chain would recognize current format; thus maintaining full backward compatibility.",Resolved,Fixed,,Sameer Paranjpye,Milind Bhandarkar,Wed; 17 May 2006 04:55:48 +0000,Thu; 17 Jul 2014 15:53:54 +0000,Thu; 17 Jul 2014 15:53:54 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-315
HADOOP-225,Bug,Minor,,tasks are left over when a job fails,when jobs are stopped or otherwise fail; tasks are often left around.the job tracker shows that there are map or reduce (mostly reduce) tasks running; when no job is running.these accumulate over time.eventually there are so many of those; that the job tracker can't launch new tasks; requiring a restart of the MR cluster.,Closed,Cannot Reproduce,HADOOP-244,Owen O'Malley,Yoram Arnon,Thu; 18 May 2006 00:51:27 +0000,Wed; 8 Jul 2009 16:51:48 +0000,Thu; 24 May 2007 06:13:01 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-225
HADOOP-226,Bug,Major,,DFSShell problems. Incorrect block replication detection in fsck.,1. We need to adjust Dfsck to the new per-file replication feature.fsck checks block replication based on the configured global replication parameter.Which is now just the default. The actual file replication is returned in DFSFileInfo.So at least the reporting is screwed by that; although I didn't check what will happen withother options -move and -delete.2. fsck throws NullPointerException if you typebin/hadoop fsck -files /docinstead ofbin/hadoop fsck /doc -files3. Unfortunately; there are several commands that throw different kinds of Exceptionsrather than at least printing the usage info; when some of its arguments are missing ormisplaced. ArrayIndexOutOfBoundsException is one them. Trybin/hadoop dfs -mvbin/hadoop dfs -cpbin/hadoop dfs -rm4. In general the shell is growing and getting more sophisticated.Should we work out a general convention on how the parameters should be structured; named;short/long version of the keywords; help; etc.,Closed,Fixed,,Unassigned,Konstantin Shvachko,Thu; 18 May 2006 05:10:51 +0000,Wed; 8 Jul 2009 16:41:56 +0000,Fri; 4 Aug 2006 19:36:48 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-226
HADOOP-227,Bug,Major,,Namespace check pointing is not performed until the namenode restarts.,In current implementation when the name node starts; it reads its image file; thenthe edits file; and then saves the updated image back into the image file.The image file is never updated after that.In order to provide the system reliability reliability the namespace information shouldbe check pointed periodically; and the edits file should be kept relatively small.,Closed,Fixed,HADOOP-334,dhruba borthakur,Konstantin Shvachko,Thu; 18 May 2006 06:39:19 +0000,Sun; 23 Jun 2013 13:55:23 +0000,Fri; 26 Jan 2007 23:46:42 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-227
HADOOP-228,Bug,Minor,fs,hadoop cp should have a -config option,hadoop cp should have a -config option to enable overriding of default parameters.it  would perhaps be good to rename the command as well to something like dcp or distcp; since it's not a simple command; but rather an entire map-recude job,Closed,Fixed,,Milind Bhandarkar,Yoram Arnon,Thu; 18 May 2006 07:12:00 +0000,Thu; 3 Aug 2006 17:46:42 +0000,Sat; 20 May 2006 02:52:47 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-228
HADOOP-229,Bug,Minor,fs,hadoop cp should generate a better number of map tasks,hadoop cp currently assigns 10 files to copy per map task.in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster); this results in long execution times.better would be to assign files per task such that the entire cluster is utilized: one file per map; with a cap of 10000 maps total; so as not to over burden the job tracker.,Closed,Fixed,,Milind Bhandarkar,Yoram Arnon,Thu; 18 May 2006 07:22:37 +0000,Thu; 3 Aug 2006 17:46:42 +0000,Sat; 20 May 2006 02:53:02 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-229
HADOOP-230,Improvement,Minor,,improve syntax of the hadoop dfs command,the hadoop dfs syntax (hadoop dfs -option value -cmd arg) is clunky: options must appear before the command; the command looks like just another option.I propose a more standard syntax:1. the command (ls; mv; du etc.) always comes first2. no '-' for the command3. options may appear anywhere; including between the command and its argumentsallowed syntax would be:hadoop dfs ls -dfs dfs /pathhadoop dfs ls /path -dfs dfsother commands may benefit from a similar syntax change:hadoop job -status job_0002 -jt jtetc.,Closed,Won't Fix,,Mahadev konar,Yoram Arnon,Thu; 18 May 2006 22:56:40 +0000,Wed; 8 Jul 2009 16:42:04 +0000,Thu; 31 Jan 2008 23:54:35 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-230
HADOOP-231,Improvement,Major,,DFSShell Improvement wish list,This is to discuss general DFSShell improvements; standardization; conventions; etc.Here are links to assorted ideas on this issue from previous issues.We need to generalize them at some point.HADOOP-59HADOOP-226HADOOP-222#action_12412326HADOOP-230,Closed,Invalid,,Mahadev konar,Konstantin Shvachko,Fri; 19 May 2006 02:42:31 +0000,Wed; 8 Jul 2009 16:42:13 +0000,Thu; 31 Jan 2008 23:55:28 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-231
HADOOP-232,Bug,Minor,,jar files sent to task tracker should override existing jar,jar files sent to task tracker are appended to list; rather than prepended to it.this results in the original jar getting executed; although a new one was sent  - not the intent.,Closed,Won't Fix,,Milind Bhandarkar,Yoram Arnon,Fri; 19 May 2006 05:17:33 +0000,Wed; 8 Jul 2009 16:51:48 +0000,Fri; 12 Jan 2007 05:35:31 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-232
HADOOP-233,New Feature,Major,,add a http status server for the task trackers,I'd like to have:  1. a jetty server on each task tracker to serve up logs and status  2. the jetty server on the job tracker serve up logs,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 19 May 2006 07:04:40 +0000,Wed; 8 Jul 2009 16:51:47 +0000,Sat; 20 May 2006 06:46:05 +0000,,0.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-233
HADOOP-234,New Feature,Major,,Hadoop Pipes for writing map/reduce jobs in C++ and python,MapReduce C++ supportRequirements 1. Allow users to write Map; Reduce; RecordReader; and RecordWriter functions in C++; rest of the infrastructure already present in Java should be reused. 2. Avoid users having to write both Java and C++ for this to work. 3. Avoid users having to work with JNI methods directly by wrapping them in helper functions. 4. The interface should be SWIG'able.,Closed,Fixed,,Owen O'Malley,Sanjay Dahiya,Fri; 19 May 2006 16:28:42 +0000,Thu; 2 May 2013 02:29:06 +0000,Wed; 16 May 2007 19:23:46 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-234
HADOOP-235,Bug,Major,,LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException,openRaw should throw f.toString() on an error; not toString().,Closed,Fixed,,Unassigned,Benjamin Reed,Sat; 20 May 2006 01:46:44 +0000,Thu; 3 Aug 2006 17:46:43 +0000,Fri; 26 May 2006 03:02:38 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-235
HADOOP-236,Bug,Major,,job tracker should refuse connection from a task tracker with a different version number,After one mapred system upgrade; we noticed that all tasks assigned to one task tracker failed. It turned out that for some reason the task tracker was not upgraded.To avoid this; a task tracker should reports its version # when it registers itsself with a job tracker. If the job tracker receives an inconsistent version #; it should refuse the connection.,Closed,Fixed,,Sharad Agarwal,Hairong Kuang,Sat; 20 May 2006 02:17:34 +0000,Wed; 8 Jul 2009 16:51:48 +0000,Fri; 6 Jun 2008 05:40:46 +0000,,0.2.1,,,HADOOP-402,https://issues.apache.org/jira/browse/HADOOP-236
HADOOP-237,Improvement,Major,metrics,Standard set of Performance Metrics for Hadoop,I am starting to use Hadoop's shiny new Metrics API to publish performance (and other) Metrics of running jobs and other daemons.Which performance metrics are people interested in seeing ? If possible; please group them according to modules; such as map-reduce; dfs; general-cluster-related etc. I will follow this process:1. collect this list2. assess feasibility of obtaining metric3. assign context/record/metrics names4. seek approval for names5. instrument the code.,Closed,Fixed,HADOOP-47,Milind Bhandarkar,Milind Bhandarkar,Sat; 20 May 2006 05:42:11 +0000,Fri; 4 Aug 2006 22:22:31 +0000,Thu; 20 Jul 2006 10:30:14 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-237
HADOOP-238,Bug,Blocker,,map outputs transfers fail with EOFException,My patch on Friday unintentionally included part of my explorations with the file transfers; which broke map output transfers.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Sun; 21 May 2006 13:23:21 +0000,Wed; 8 Jul 2009 16:51:48 +0000,Tue; 23 May 2006 06:34:01 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-238
HADOOP-239,Bug,Minor,,job tracker WI drops jobs after 24 hours,The jobtracker's WI; keeps track of jobs executed in the past 24 hours.if the cluster was idle for a day (say Sunday) it drops all its history.Monday morning; the page is empty.Better would be to store a fixed number of jobs (say 10 each of succeeded and failed jobs).Also; if the job tracker is restarted; it loses all its history.The history should be persistent; withstanding restarts and upgrades.,Closed,Fixed,,Sanjay Dahiya,Yoram Arnon,Tue; 23 May 2006 00:28:51 +0000,Thu; 2 May 2013 02:29:00 +0000,Wed; 4 Oct 2006 22:02:19 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-239
HADOOP-240,Bug,Minor,,namenode should not log failed mkdirs at warning level,"when a namenode creates a directory; it also recursively tries to creates its parent directories. If they already exist; the lastSuccess is  false and the ""error"" is logged at the warning level.The right approach is to set the logging leve lower to fine or simply ignore the the ""error"".",Closed,Fixed,,Hairong Kuang,Hairong Kuang,Wed; 24 May 2006 00:20:58 +0000,Wed; 8 Jul 2009 16:41:55 +0000,Thu; 8 Jun 2006 01:16:18 +0000,,0.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-240
HADOOP-241,Bug,Minor,fs,TestCopyFiles fails under cygwin due to incorrect path,Under cygwin TestCopyFiles generates an incorrect url which includes windows style path.This is the result of concatenation of a win path with unix path.File.getPath() should be used to produce a consistent path.,Closed,Fixed,,Milind Bhandarkar,Konstantin Shvachko,Wed; 24 May 2006 01:22:23 +0000,Thu; 3 Aug 2006 17:46:43 +0000,Thu; 25 May 2006 00:55:01 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-241
HADOOP-242,Bug,Major,,"job fails because of ""No valid local directories in property: "" exception","when running a fairly large job; of 70+K map tasks; I get many exceptions as shown below; and eventually the job failes when a task fails four times.The exception doesn't really tell us enough information to debug this properly; so the first thing to do would be to add more information (path) to the exception.The path indicated in the config file exists; is writable and valid; though 'path' may be anything.the exception:java.io.IOException: No valid local directories in property: mapred.local.dir at org.apache.hadoop.conf.Configuration.getLocalPath(Configuration.java:293) at org.apache.hadoop.mapred.JobConf.getLocalPath(JobConf.java:153) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:523) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.launchTask(TaskTracker.java:572) at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:389) at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:303) at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:418) at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:920)the code:  public Path getLocalPath(String dirsProp; String path)    throws IOException {    String[] dirs = getStrings(dirsProp);    int hashCode = path.hashCode();    FileSystem fs = FileSystem.getNamed(""local""; this);    for (int i = 0; i  dirs.length; i++) {  // try each local dir      int index = (hashCode+i  Integer.MAX_VALUE) % dirs.length;      Path file = new Path(dirsindex; path);      Path dir = file.getParent();      if (fs.exists(dir) || fs.mkdirs(dir)) {        return file;      }    }    throw new IOException(""No valid local directories in property: ""+dirsProp);  }",Closed,Fixed,,Owen O'Malley,Yoram Arnon,Wed; 24 May 2006 02:13:55 +0000,Wed; 8 Jul 2009 16:51:49 +0000,Fri; 23 Jun 2006 23:39:16 +0000,,,,,HADOOP-277,https://issues.apache.org/jira/browse/HADOOP-242
HADOOP-243,Bug,Trivial,,WI shows progress as 100.00% before actual completion (rounding error),For jobs of over 50000 tasks; the rounding error in the WI is confusing.When less than 0.005% of the map (or reduce) tasks remain to execute; the WI shows progress as 100.00%; which is misleading.Rounding down to the nearest .01% would be better.,Closed,Fixed,,Owen O'Malley,Yoram Arnon,Wed; 24 May 2006 02:17:46 +0000,Wed; 8 Jul 2009 16:51:49 +0000,Thu; 14 Sep 2006 19:57:30 +0000,,0.5.0;0.6.0,,,,https://issues.apache.org/jira/browse/HADOOP-243
HADOOP-244,Bug,Major,,very long cleanup after a job fails,Eight hours after a job failed (it executed for about 14 hours prior to failing); many task trackers keep throwing the exceptions below:060523 121732 Server handler 0 on 50040 caught: java.io.FileNotFoundException: LocalFSjava.io.FileNotFoundException: LocalFS        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:123)        at org.apache.hadoop.fs.FSDataInputStream$Checker.init(FSDataInputStream.java:46)        at org.apache.hadoop.fs.FSDataInputStream.init(FSDataInputStream.java:228)        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:151)        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)060523 121814 task_0006_r_000123_0 copy failed: task_0006_m_046105_0 from node5:50040java.net.SocketTimeoutException: timed out waiting for rpc response        at org.apache.hadoop.ipc.Client.call(Client.java:305)        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:150)        at org.apache.hadoop.mapred.$Proxy2.getFile(Unknown Source)        at org.apache.hadoop.mapred.ReduceTaskRunner.prepare(ReduceTaskRunner.java:112)        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:67)060523 121814 task_0006_r_000123_0 0.13023989% reduce  copy  task_0006_m_046105_0@node5:50040060523 121814 task_0006_r_000123_0 Copying task_0006_m_048815_0 output from node6060523 121817 SEVERE Can't open map output:/hadoop/mapred/local/task_0006_m_031921_0/part-152.outjava.io.FileNotFoundException: LocalFS        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:123)        at org.apache.hadoop.fs.FSDataInputStream$Checker.init(FSDataInputStream.java:46)        at org.apache.hadoop.fs.FSDataInputStream.init(FSDataInputStream.java:228)        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:151)        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)060523 121817 Unknown child with bad map output: task_0006_m_031921_0. Ignored.060523 121817 Server handler 1 on 50040 caught: java.io.FileNotFoundException: LocalFSjava.io.FileNotFoundException: LocalFS        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:123)        at org.apache.hadoop.fs.FSDataInputStream$Checker.init(FSDataInputStream.java:46)        at org.apache.hadoop.fs.FSDataInputStream.init(FSDataInputStream.java:228)        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:151)        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)060523 121914 task_0006_r_000123_0 copy failed: task_0006_m_048815_0 from node6:50040java.net.SocketTimeoutException: timed out waiting for rpc response        at org.apache.hadoop.ipc.Client.call(Client.java:305)        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:150)        at org.apache.hadoop.mapred.$Proxy2.getFile(Unknown Source)        at org.apache.hadoop.mapred.ReduceTaskRunner.prepare(ReduceTaskRunner.java:112)        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:67),Closed,Duplicate,HADOOP-225,Sameer Paranjpye,Yoram Arnon,Wed; 24 May 2006 02:24:52 +0000,Wed; 8 Jul 2009 16:51:48 +0000,Wed; 31 May 2006 07:19:54 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-244
HADOOP-245,Bug,Major,record,record io translator doesn't strip path names,When I run the record translator with a pathname; the path name is not stripped. So for example:% bin/rcc --language c++ foo/bar/bat.jrgenerates:foo/bar/bat.jr.hh (instead of ./bat.jr.hh)and the first line is #ifndef _FOO/BAR/BAT_JR_HH_the first was unexpected and the second is clearly wrong.,Closed,Fixed,,Milind Bhandarkar,Owen O'Malley,Wed; 24 May 2006 03:41:58 +0000,Thu; 3 Aug 2006 17:46:43 +0000,Tue; 6 Jun 2006 03:46:57 +0000,,0.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-245
HADOOP-246,Bug,Major,record,the record-io generated c++ has wrong comments,The comments on the namespaces on the closing come out backward:} // end namespace org} // end namespace apache} // end namespace hadoop} // end namespace record} // end namespace test,Closed,Duplicate,HADOOP-245,Milind Bhandarkar,Owen O'Malley,Wed; 24 May 2006 03:45:07 +0000,Thu; 3 Aug 2006 17:46:43 +0000,Wed; 31 May 2006 07:23:10 +0000,,0.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-246
HADOOP-247,Bug,Critical,,The Reduce Task thread for reporting progress during the sort exits in case of any IOException,The Reduce task thread for reporting progress during the sort; exits in case of any exception (except InterruptedException). The solution would be to continue the thread in case of an exception.,Closed,Fixed,,Mahadev konar,Mahadev konar,Wed; 24 May 2006 03:52:00 +0000,Wed; 8 Jul 2009 16:51:48 +0000,Wed; 24 May 2006 06:26:04 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-247
HADOOP-248,Improvement,Major,,locating map outputs via random probing is inefficient,"Currently the ReduceTaskRunner polls the JobTracker for a random list of map tasks asking for their output locations. It would be better if the JobTracker kept an ordered log and the interface was changed to:class MapLocationResults {   public int getTimestamp();   public MapOutputLocation[] getLocations();}interface InterTrackerProtocol {  ...  MapLocationResults locateMapOutputs(int prevTimestamp);} with the intention that each time a ReduceTaskRunner calls locateMapOutputs; it passes back the ""timestamp"" that it got from the previous result. That way; reduces can easily find the new MapOutputs. This should help the ""ramp up"" when the maps first start finishing.",Closed,Fixed,,Devaraj Das,Owen O'Malley,Wed; 24 May 2006 06:25:33 +0000,Thu; 2 May 2013 02:29:03 +0000,Thu; 22 Feb 2007 20:22:42 +0000,,0.2.1,,,HADOOP-343,https://issues.apache.org/jira/browse/HADOOP-248
HADOOP-249,Improvement,Major,,Improving Map -> Reduce performance and Task JVM reuse,These patches are really just to make Hadoop start trotting. It is still at least an order of magnitude slower than it should be; but I think these patches are a good start.I've created two patches for clarity. They are not independent; but could easily be made so.The disk-zoom patch is a performance trifecta: less disk IO; less disk space; less CPU; and overall a tremendous improvement. The patch is based on the following observation: every piece of data from a map hits the disk once on the mapper; and 3 (+plus sorting) times on the reducer. Further; the entire input for the reduce step is sorted together maximizing the sort time. This patch causes:1)  the mapper to sort the relatively small fragments at the mapper which causes two hits to the disk; but they are smaller files.2) the reducer copies the map output and may merge (if more than 100 outputs are present) with a couple of other outputs at copy time. No sorting is done since the map outputs are sorted.3) the reducer  will merge the map outputs on the fly in memory at reduce time.I'm attaching the performance graph (with just the disk-zoom patch) to show the results. This benchmark uses a random input and null output to remove any DFS performance influences. The cluster of 49 machines I was running on had limited disk space; so I was only able to run to a certain size on unmodified Hadoop. With the patch we use 1/3 the amount of disk space.The second patch allows the task tracker to reuse processes to avoid the over-head of starting the JVM. While JVM startup is relatively fast; restarting a Task causes disk IO and DFS operations that have a negative impact on the rest of the system. When a Task finishes; rather than exiting; it reads the next task to run from stdin. We still isolate the Task runtime from TaskTracker; but we only pay the startup penalty once.This second patch also fixes two performance issues not related to JVM reuse. (The reuse just makes the problems glaring.) First; the JobTracker counts all jobs not just the running jobs to decide the load on a tracker. Second; the TaskTracker should really ask for a new Task as soon as one finishes rather than wait the 10 secs.I've been benchmarking the code alot; but I don't have access to a really good cluster to try the code out on; so please treat it as experimental. I would love to feedback.There is another obvious thing to change: ReduceTasks should start after the first batch of MapTasks complete; so that 1) they have something to do; and 2) they are running on the fastest machines.,Closed,Fixed,,Devaraj Das,Benjamin Reed,Wed; 24 May 2006 07:50:20 +0000,Tue; 12 Oct 2010 12:13:07 +0000,Fri; 19 Sep 2008 07:32:04 +0000,,0.3.0,,HADOOP-2560,HADOOP-2560;MAPREDUCE-2123;HADOOP-830,https://issues.apache.org/jira/browse/HADOOP-249
HADOOP-250,New Feature,Major,,HTTP Browsing interface for DFS Health/Status,A web interface to view the DFS health/status (name and data nodes) is to be created. User can connect to the webserver on the namenode and a web page will be displayed. The web page will give some details about that namenode (startup time and the total cluster capacity). The web page will contain a table of 'live' and 'dead' datanodes. Each live datanode will be a link to the complete details of the datanode as given by DatanodeInfo (also see DataNodeReport).,Closed,Fixed,,Unassigned,Devaraj Das,Wed; 24 May 2006 12:22:28 +0000,Wed; 8 Jul 2009 16:41:46 +0000,Tue; 20 Jun 2006 02:06:34 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-250
HADOOP-251,Bug,Major,,progress report failures kill task,Communication problems in reporting progress to the task tracker kill the task because the exceptions are not caught. Since reporting progress is not critical; my patch catches the exception; logs it; and ignores it.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Thu; 25 May 2006 04:41:12 +0000,Wed; 8 Jul 2009 16:51:47 +0000,Thu; 25 May 2006 06:17:16 +0000,,0.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-251
HADOOP-252,Improvement,Major,ipc,add versioning to RPC,"currently; any change to any rpc message breaks the protocol; with mysterious exceptions occurring at run time.a versioning sceme would have two benefits:	intelligent error messages; indicating that an upgrade is required	backwards compatibility could be supported.the proposal is to add a ""const version"" for each protocol; and a method: int getVersion(int version) that sends the client's version and receives the server's version. This would be the first method invoked on connection. Both sides then either agree on the lowest version number; providing backwards compatibility support; or abort the connection as ""unsupported version"".",Closed,Fixed,,Unassigned,Yoram Arnon,Thu; 25 May 2006 05:36:37 +0000,Fri; 4 Aug 2006 22:22:31 +0000,Fri; 14 Jul 2006 16:11:16 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-252
HADOOP-253,Improvement,Major,,we need speculative execution for reduces,With my new http-based shuffle (on top of the svn head including sameer's parallel fetch); I just finished sorting 2010 g on 200 nodes in 8:49 with 9 reduce failures. However; the amusing part is that the replacement reduces were not the slow ones. 8 of the original reduces were the only things running for the last hour. The job timings looked like:Job 0001  Total:    Tasks: 16551    Total: 10056104 secs    Average: 607 secs    Worst: task_0001_r_000291_0    Worst time: 31050 secs    Best: task_0001_m_013597_0    Best time: 20 secs  Maps:    Tasks: 16151    Total: 2762635 secs    Average: 171 secs    Worst: task_0001_m_002290_0    Worst time: 2663 secs    Best: task_0001_m_013597_0    Best time: 20 secs  Reduces:    Tasks: 400    Total: 7293469 secs    Average: 18233 secs    Worst: task_0001_r_000291_0    Worst time: 31050 secs    Best: task_0001_r_000263_1    Best time: 5591 secsAnd the number of tasks run per a node was very uneven:#tasks node124 node1161117 node1307117 node1124116 node1253114 node1310111 node1302111 node1299111 node1298111 node1249111 node1221110 node1288110 node1286110 node1211109 node1268108 node1292108 node1202108 node1200107 node1313107 node1277107 node1246107 node1242107 node1231107 node1214106 node1243105 node1251105 node1212105 node1205104 node1272104 node1269104 node1210104 node1203104 node1193104 node1128103 node1300103 node1285103 node1279103 node1209103 node1173103 node1165102 node1276102 node1239102 node1228102 node1204102 node1188101 node1314101 node1303100 node1301100 node125299 node128799 node121399 node120698 node129598 node118697 node129397 node126597 node126297 node126097 node125897 node123597 node122997 node122697 node121597 node120897 node118797 node117597 node117196 node129196 node124896 node122496 node121695 node130595 node128095 node126395 node125495 node115395 node111594 node127194 node126194 node123494 node123394 node122794 node122594 node121794 node114293 node127593 node119893 node110792 node126692 node122092 node121991 node130991 node128991 node127091 node125991 node125691 node123291 node117989 node129089 node125589 node124789 node120789 node120189 node119089 node115489 node114188 node130688 node128288 node125088 node122288 node118488 node114988 node111787 node127887 node125787 node119187 node118587 node118086 node129786 node117885 node119585 node114385 node111284 node128184 node127484 node126483 node129683 node114882 node121882 node116882 node116781 node131181 node124081 node122381 node119681 node116481 node111680 node126780 node123080 node117780 node111979 node129479 node119979 node118179 node117079 node116679 node110378 node124478 node118978 node115777 node130477 node117274 node118271 node116071 node114768 node123668 node118367 node124559 node113958 node131257 node116256 node130856 node119755 node114654 node110653 node111153 node110549 node114549 node112348 node117646 node113644 node113244 node112544 node112244 node110843 node119243 node112142 node119442 node113842 node110441 node115541 node112641 node111440 node115840 node115140 node113740 node111040 node110039 node115638 node114038 node113538 node110937 node114437 node112036 node111834 node113334 node111331 node113426 node112723 node110120 node1131And it should not surprise us that the last 8 reduces were running on nodes 1134; 1127;1101; and 1131. This really demonstrates the need to run speculative reduce runs. I propose that when the list of reduce jobs running is down to 1/2 the cluster size that we start running speculative reduces. I estimate that it would have saved around an hour on this run. Does that sound like a reasonable heuristic?,Closed,Duplicate,HADOOP-76,Owen O'Malley,Owen O'Malley,Thu; 25 May 2006 13:16:43 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Wed; 31 May 2006 07:37:54 +0000,,0.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-253
HADOOP-254,Improvement,Major,,use http to shuffle data between the maps and the reduces,To speed up the shuffle time; I'll use http (via the task tracker's jetty server) to send the map outputs.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Thu; 25 May 2006 23:53:34 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Sat; 27 May 2006 02:44:29 +0000,,0.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-254
HADOOP-255,Bug,Major,ipc,Client Calls are not cancelled after a call timeout,"In ipc/Client.java; if a call times out; a SocketTimeoutException is thrown but the Call object still exists on the queue.What I found was that when transferring very large amounts of data; it's common for queued up calls to timeout. Yet even though the caller has is no longer waiting; the request is still serviced on the server and the data is sent to the client. The client after receiving the full response calls callComplete() which is a noop since nobody is waiting.The problem is that the calls that timeout will retry and the system gets into a situation where data is being transferred around; but it's all data for timed out requests and no progress is ever made.My quick solution to this was to add a ""boolean timedout"" to the Call object which I set to true whenever the queued caller times out. And then when the client starts to pull over the response data (in Connection::run) to first check if the Call is timedout and immediately close the connection.I think a good fix for this is to queue requests on the client; and do a single sendParam only when there is no outstanding request. This will allow closing the connection when receiving a response for a request we no longer have pending; reopen the connection; and resend the next queued request. I can provide a patch for this; but I've seen a lot of recent activity in this area so I'd like to get some feedback first.",Closed,Fixed,,Owen O'Malley,Naveen Nalam,Fri; 26 May 2006 05:30:14 +0000,Wed; 3 Jan 2007 21:40:04 +0000,Fri; 6 Oct 2006 21:14:42 +0000,,0.2.1,,,HADOOP-572,https://issues.apache.org/jira/browse/HADOOP-255
HADOOP-256,New Feature,Major,,Implement a C api for hadoop dfs,Implement a C api for hadoop dfs to ease talking to hadoop's dfs from native C/C++ applications.,Closed,Fixed,,Arun C Murthy,Arun C Murthy,Sat; 27 May 2006 03:33:13 +0000,Mon; 9 Sep 2013 03:39:07 +0000,Sat; 3 Jun 2006 06:20:58 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-256
HADOOP-257,Improvement,Major,,starting one data node thread to manage multiple data directories,If a data node is configured with multiple data directories; current implementation of dfs will start multiple data node threads; each of which manages one data directory and talks to its name node independently. From the name node's point of view; it sees multiple data nodes instead of one.I feel that a more scalable solution should be to start one data node thread that manages multiple data diretories. But the one data node thread needs to take care of the block allocation problem.,Closed,Duplicate,HADOOP-64,Konstantin Shvachko,Hairong Kuang,Sat; 27 May 2006 06:06:45 +0000,Wed; 8 Jul 2009 16:41:54 +0000,Wed; 31 May 2006 07:35:29 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-257
HADOOP-258,Bug,Major,,Cannot obtain block errors,I've been seeing a lot of errors from dfs for the last couple days. The exception that I see is:java.io.IOException: Could not obtain block: blk_2867431916903738534 file=/user/oom/big-rand/part000271 offset=0 at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:532) at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:641) at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:83) at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158) at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at java.io.BufferedInputStream.read1(BufferedInputStream.java:256) at java.io.BufferedInputStream.read(BufferedInputStream.java:313) at java.io.DataInputStream.readFully(DataInputStream.java:176) at java.io.DataInputStream.readFully(DataInputStream.java:152) at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:264) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:248) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:238) at org.apache.hadoop.mapred.SequenceFileRecordReader.(SequenceFileRecordReader.java:36) at org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:53) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:105) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:866)I haven't had my fingers in that code recently; does it ring any bells for anyone?,Closed,Cannot Reproduce,,Sameer Paranjpye,Owen O'Malley,Sat; 27 May 2006 14:16:43 +0000,Wed; 8 Jul 2009 16:41:55 +0000,Tue; 15 Aug 2006 21:34:39 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-258
HADOOP-259,Bug,Major,,map output http client does not timeout,"The new map output http client uses java.net.URLConnection to fetch the data file. However under Java 1.4 there is no way to specify a timeout and it is set to infinite (or if not infinite at least 12 hours).  This causes reduce tasks to get ""stuck"" in the ""reduce  copy"" phase even after the ""Task failed to report status for 600 seconds. Killing."" message.I will add the code in the ReduceTaskRunner to make sure that copies in-flight don't get stuck; but this is another point where a switch to java 1.5 would be helpful. Under 1.5 I could set the timeout on the connection and the read would timeout after the given interval and my entire change would be local to MapOutputLocation.copyFile.For now; I'll assume that we need to maintain support for 1.4 and make the corresponding fix; but I'm grumbling...",Closed,Fixed,,Owen O'Malley,Owen O'Malley,Mon; 29 May 2006 13:14:42 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Fri; 2 Jun 2006 02:51:34 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-259
HADOOP-260,Improvement,Minor,,the start up scripts should take a command line parameter --config making it easy to run multiple hadoop installation on same machines,#NAME?,Closed,Fixed,HADOOP-155,Milind Bhandarkar,Mahadev konar,Wed; 31 May 2006 01:42:21 +0000,Fri; 18 Aug 2006 11:18:01 +0000,Mon; 24 Jul 2006 09:16:16 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-260
HADOOP-261,Bug,Major,,when map outputs are lost; nothing is shown in the webapp about why the map task failed,When a task tracker invokes mapOutputLost; it does not include a diagnostic that lets the user (or the web app) know what failed in the map. I want to add a diagnostic message.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 31 May 2006 01:46:39 +0000,Wed; 8 Jul 2009 16:51:44 +0000,Wed; 20 Sep 2006 20:39:44 +0000,,0.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-261
HADOOP-262,Bug,Major,,the reduce tasks do not report progress if they the map output locations is empty.,The ReduceTaskRunner should report progress even if the number of ouput locations to copy is empty. In case; the last few maps are running on a tasktracker that goes down; all the reduce tasks waiting for these mapoutputs would fail.,Closed,Fixed,,Mahadev konar,Mahadev konar,Wed; 31 May 2006 02:01:48 +0000,Wed; 8 Jul 2009 16:51:45 +0000,Tue; 6 Jun 2006 02:58:36 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-262
HADOOP-263,Improvement,Major,,task status should include timestamps for when a job transitions,It would help users to understand what happened if the task status included information about when the task transitioned:Map:   started   finishedReduce:   started   shuffle finished   sort finished   finished,Closed,Fixed,,Sanjay Dahiya,Owen O'Malley,Thu; 1 Jun 2006 02:12:52 +0000,Thu; 2 May 2013 02:29:00 +0000,Mon; 25 Sep 2006 22:26:12 +0000,,0.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-263
HADOOP-264,Bug,Blocker,,WritableFactory has no permissions to create DatanodeRegistration,The datanode can not come up because the DatanodeRegistration is package local and the factory registration doesn't happen.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Thu; 1 Jun 2006 06:16:33 +0000,Wed; 8 Jul 2009 16:41:55 +0000,Fri; 2 Jun 2006 02:53:59 +0000,,0.3.0,,,HADOOP-217,https://issues.apache.org/jira/browse/HADOOP-264
HADOOP-265,Bug,Major,,Abort tasktracker if it can not write to its local directories,Currently if a task tracker is not able to write to any of its local directories; it continues to run and all the tasks assigned to it fail.A task tracker should not start upif it has a problem reading/writing any of its local directories. It should abort if it gets the problem at run time.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Thu; 1 Jun 2006 07:07:42 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Sat; 3 Jun 2006 02:52:21 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-265
HADOOP-266,Bug,Minor,ipc,RPC doesn not handle exceptions correctly,1. Examining HADOOP-264 bug I realized that not all rpc server exceptions are actually returned to the client. Particularly; if an exception happens in org.apache.hadoop.ipc.Server.Connection.run()for example inside param.readFields(in);then it logged but never returned back to the client.Client simply timeouts in this case; which is not exactly what one would expect.2. On the way back org.apache.hadoop.ipc.Client.call()rpc client always throws a RemoteException; rather than the exception wrapped into this RemoteException.,Closed,Won't Fix,,Owen O'Malley,Konstantin Shvachko,Thu; 1 Jun 2006 09:10:37 +0000,Sat; 29 Mar 2008 03:01:07 +0000,Tue; 10 Jul 2007 16:43:31 +0000,,,,,HADOOP-2841,https://issues.apache.org/jira/browse/HADOOP-266
HDFS-70,Bug,Minor,,"Data node should shutdown when a ""critical"" error is returned by the name node",Currently data node does not distinguish between critical and non critical exceptions.Any exception is treated as a signal to sleep and then try again. Seeorg.apache.hadoop.dfs.DataNode.run()This is happening because RPC always throws the same RemoteException.In some cases (like UnregisteredDatanodeException; IncorrectVersionException) the data node should shutdown rather than retry.This logic naturally belongs to the org.apache.hadoop.dfs.DataNode.offerService()but can be reasonably implemented (without examining the RemoteException.className field) after HADOOP-266 (2) is fixed.,Resolved,Won't Fix,,Sameer Paranjpye,Konstantin Shvachko,Thu; 1 Jun 2006 09:38:32 +0000,Sun; 17 Jul 2011 18:26:10 +0000,Sun; 17 Jul 2011 18:26:10 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-70
HADOOP-268,Improvement,Major,,TaskTracker latency is slowing down maps because progress reporting is done inline,Moving the progress reporter to a separate thread should free up the user application to run faster.,Closed,Duplicate,NULL,Unassigned,Owen O'Malley,Fri; 2 Jun 2006 01:11:58 +0000,Wed; 8 Jul 2009 16:51:47 +0000,Tue; 10 Jul 2007 16:35:46 +0000,,0.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-268
HADOOP-269,Improvement,Major,documentation,add FAQ to Wiki,Hadoop should have an FAQ in the Wiki.  We can bootstrap this by reviewing the mailing list archives.,Closed,Fixed,,Doug Cutting,Doug Cutting,Fri; 2 Jun 2006 02:04:14 +0000,Thu; 3 Aug 2006 17:46:45 +0000,Fri; 9 Jun 2006 04:21:42 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-269
HADOOP-270,Bug,Major,,possible deadlock when shut down a datanode thread,"The DataNode class provides a method ""shutdown"" that can be used to notify a data node thread to abort itself gracefully. In addition this method waits for its data receiving server to terminate. This may cause possible deadlock if the method is called by the data receiving server.",Closed,Fixed,,Hairong Kuang,Hairong Kuang,Fri; 2 Jun 2006 03:45:09 +0000,Wed; 8 Jul 2009 16:41:55 +0000,Sat; 3 Jun 2006 03:12:55 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-270
HADOOP-271,Improvement,Minor,,add links to task tracker http server from task details and failure pages,Make the machine name in the list of task attempts and failures into links to the correct task tracker http server.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 2 Jun 2006 06:31:08 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Wed; 21 Jun 2006 01:51:38 +0000,,0.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-271
HADOOP-272,Bug,Major,,bin/hadoop dfs -rm <dir> crashes in log4j code,"When I run ""bin/hadoop dfs -rm out-dir"" I get the following error messages:log4j:ERROR setFile(null;true) call failed.java.io.FileNotFoundException: /local/owen/hadoop/run/log (Is a directory)        at java.io.FileOutputStream.openAppend(Native Method)        at java.io.FileOutputStream.init(FileOutputStream.java:177)        at java.io.FileOutputStream.init(FileOutputStream.java:102)        at org.apache.log4j.FileAppender.setFile(FileAppender.java:289)        at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163)        at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215)        at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256)        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132)        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96)        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654)        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612)        at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509)        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415)        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441)        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468)        at org.apache.log4j.LogManager.clinit(LogManager.java:122)        at org.apache.log4j.Logger.getLogger(Logger.java:104)        at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229)        at org.apache.commons.logging.impl.Log4JLogger.init(Log4JLogger.java:65)        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)        at java.lang.reflect.Constructor.newInstance(Constructor.java:494)        at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:529)        at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:235)        at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:370)        at org.apache.hadoop.conf.Configuration.clinit(Configuration.java:54)        at org.apache.hadoop.dfs.DFSShell.main(DFSShell.java:307)log4j:ERROR Either File or DatePattern options are not set for appender DRFA.Delete failed",Closed,Fixed,,Owen O'Malley,Owen O'Malley,Sat; 3 Jun 2006 06:03:04 +0000,Thu; 3 Aug 2006 17:46:45 +0000,Sat; 3 Jun 2006 06:41:54 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-272
HADOOP-273,Improvement,Trivial,,Add a interactive shell for admistrative access to the DFS,Implement a shell that allows the user to work in the dfs like on the local fs.cd /user/cd testput text.txt someText.txtcat someText.txtrm someText.txt,Closed,Invalid,HADOOP-6541,Aaron Kimball,Marco Paga,Sat; 3 Jun 2006 18:49:01 +0000,Mon; 20 Feb 2012 12:34:59 +0000,Tue; 29 Jan 2008 17:36:09 +0000,,0.7.1,,,,https://issues.apache.org/jira/browse/HADOOP-273
HADOOP-274,Bug,Major,,The new logging framework puts application logs into server directory in hadoop.log,The new logging infrastructure puts application logs into the server log directory under hadoop.log. I think it would be less confusing to use the old behavior of writing out to stderr (for applications).Thoughts?,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Mon; 5 Jun 2006 23:26:26 +0000,Thu; 3 Aug 2006 17:46:45 +0000,Tue; 6 Jun 2006 01:44:00 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-274
HADOOP-275,New Feature,Major,,log4j changes for hadoopStreaming,A patch for hadoopStreaming. Use log4j Still Using short UTF8 strings,Closed,Fixed,,Doug Cutting,Michel Tourn,Tue; 6 Jun 2006 03:59:58 +0000,Wed; 8 Jul 2009 17:05:36 +0000,Tue; 6 Jun 2006 06:40:56 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-275
HADOOP-276,Bug,Major,,No appenders could be found for logger,When you start the servers with an old configuration directory without the properties files; you get messages about:log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration).log4j:WARN Please initialize the log4j system properly.The problem is that the property files are not included in the jar file.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 6 Jun 2006 04:56:07 +0000,Thu; 3 Aug 2006 17:46:45 +0000,Tue; 6 Jun 2006 05:09:33 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-276
HADOOP-277,Bug,Major,,Race condition in Configuration.getLocalPath(),"(attached: a patch to fix the problem; and a logfile showing the problem occuring twice)There is a race condition in Configuration.java:       Path file = new Path(dirsindex; path);       Path dir = file.getParent();       if (fs.exists(dir) || fs.mkdirs(dir)) {         return file;If two threads simultaneously process this code with the same target directory; fs.exists() will return false; but from fs.mkdirs() only one of the two threads will return true. From the Java documentation: ""returns: true if and only if the directory was created; along with all necessary parent directories; false otherwise""That is; if the first thread successfully creates the directory; the second will not; and therefore return false; even though the directory exists.This was really happening. We use four temporary directories; and we had reducers failing all over the place with  bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem; and the log output is below.Here you can see copies initiated for two files that hash to the same temp directory; simultaneously. map_4.out is created in the correct directory (/data2...); but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later; when the appender tries to locate the file; that race condition does not occur (the directory already exists); and the appender looks for the file map_15.out in the correct directory; where it does not exist.060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05.060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04....060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out...060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/map_15.out...060605 142531 task_0001_r_000009_1 0.31808624% reduce  append  /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out...060605 142725 task_0001_r_000009_1 java.io.FileNotFoundException: /data2/tmp/mapred/local/task_0001_r_000009_1/map_15.out",Closed,Fixed,,Sameer Paranjpye,p sutter,Tue; 6 Jun 2006 08:11:33 +0000,Thu; 3 Aug 2006 17:46:45 +0000,Thu; 8 Jun 2006 02:35:46 +0000,,0.3.1,,,HADOOP-242,https://issues.apache.org/jira/browse/HADOOP-277
HADOOP-278,Bug,Major,,a missing map/reduce input directory does not produce a user-visible error message,If map/reduce's input directory is in DFS and does not exist; the user has to find the problem in the jobtracker logs rather than either the launching program or the webapp.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 7 Jun 2006 00:00:03 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Tue; 27 Jun 2006 00:47:36 +0000,,0.3.1,,,,https://issues.apache.org/jira/browse/HADOOP-278
HADOOP-279,Improvement,Major,,running without the hadoop script causes warnings about log4j not being configured correctly,"Anyone who runs hadoop programs without the bin/hadoop script will get a message about log4j not being configured correctly. The problem is that the config file uses the system property ""hadoop.root.logger""; which is set in the script.",Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 7 Jun 2006 03:14:11 +0000,Wed; 8 Jul 2009 16:51:46 +0000,Wed; 7 Jun 2006 04:28:28 +0000,,0.3.1,,,,https://issues.apache.org/jira/browse/HADOOP-279
HADOOP-280,Bug,Major,,AllTestDriver has incorrect class name for DistributedFSCheck test,It has TestDFSIO instead.,Closed,Fixed,,Unassigned,Konstantin Shvachko,Wed; 7 Jun 2006 03:46:44 +0000,Thu; 3 Aug 2006 17:46:46 +0000,Wed; 7 Jun 2006 04:41:31 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-280
HADOOP-281,Bug,Major,,dfs.FSDirectory.mkdirs can create sub-directories of a file!,dfs.FSDirectory.mkdirs will merrily adds children to a directory tree node without checking whether it represents a directory. So it is possible to create a subdirectories of a file.,Closed,Fixed,,Wendy Chien,Sameer Paranjpye,Wed; 7 Jun 2006 06:05:21 +0000,Fri; 8 Sep 2006 21:19:44 +0000,Mon; 28 Aug 2006 20:40:21 +0000,,0.3.1,,,,https://issues.apache.org/jira/browse/HADOOP-281
HADOOP-282,Bug,Critical,,the datanode crashes if it starts before the namenode,If the datanode tries to register before the namenode is offering service; it crashes with a uncaught exception.java.net.ConnectException: Connection refused        at java.net.PlainSocketImpl.socketConnect(Native Method)        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)        at java.net.Socket.connect(Socket.java:507)        at java.net.Socket.connect(Socket.java:457)        at java.net.Socket.init(Socket.java:365)        at java.net.Socket.init(Socket.java:207)        at org.apache.hadoop.ipc.Client$Connection.init(Client.java:112)        at org.apache.hadoop.ipc.Client.getConnection(Client.java:351)        at org.apache.hadoop.ipc.Client.call(Client.java:289)        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:150)        at org.apache.hadoop.dfs.$Proxy0.register(Unknown Source)        at org.apache.hadoop.dfs.DataNode.register(DataNode.java:176)        at org.apache.hadoop.dfs.DataNode.init(DataNode.java:109)        at org.apache.hadoop.dfs.DataNode.makeInstanceForDir(DataNode.java:892)        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:846)        at org.apache.hadoop.dfs.DataNode.runAndWait(DataNode.java:862)        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:917),Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 7 Jun 2006 06:07:12 +0000,Wed; 8 Jul 2009 16:41:54 +0000,Thu; 8 Jun 2006 04:09:11 +0000,,0.3.1,,,,https://issues.apache.org/jira/browse/HADOOP-282
HADOOP-283,Improvement,Minor,,Counting of running maps/reduces in tasktrackerstatus,The functions countreducetasks and countmaptasks currently return the total number of maps/reduces on the tasktracker. They should only return the number of running maps/reduces. Because they return the total count; the jobtracker is not able to schedule tasks on these tasktrackers in the current cycle; when a task is finished and the tasktracker is runnning max number of maps/reduces.,Closed,Fixed,,Mahadev konar,Mahadev konar,Wed; 7 Jun 2006 08:37:26 +0000,Fri; 15 Dec 2006 23:02:17 +0000,Thu; 29 Jun 2006 04:16:31 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-283
HADOOP-284,Bug,Major,,dfs timeout on open,In my sort benchmark; I've started seeing hundred's of maps dying with timeout exceptions in the dfs.open() code:java.net.SocketTimeoutException: timed out waiting for rpc response at org.apache.hadoop.ipc.Client.call(Client.java:305) at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:153) at org.apache.hadoop.dfs.$Proxy1.open(Unknown Source) at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:457) at org.apache.hadoop.dfs.DFSClient$DFSInputStream.(DFSClient.java:444) at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:207) at org.apache.hadoop.dfs.DistributedFileSystem.openRaw(DistributedFileSystem.java:90) at org.apache.hadoop.fs.FSDataInputStream$Checker.(FSDataInputStream.java:46) at org.apache.hadoop.fs.FSDataInputStream.(FSDataInputStream.java:228) at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:72) at org.apache.hadoop.dfs.DistributedFileSystem.copyToLocalFile(DistributedFileSystem.java:182) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:535) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.launchTask(TaskTracker.java:584) at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:395) at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:308) at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:424) at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:961),Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 7 Jun 2006 12:38:18 +0000,Wed; 8 Jul 2009 16:41:54 +0000,Fri; 30 Jun 2006 05:34:00 +0000,,0.3.1,,,,https://issues.apache.org/jira/browse/HADOOP-284
HADOOP-285,Bug,Blocker,,Data nodes cannot re-join the cluster once connection is lost,A data node looses connection to a name node and then tries to offerService() again.HADOOP-270 changes force it to start dataXceiveServer; which is already started and in this casethrows IllegalThreadStateException; which goes on in a loop; and never reaches the heartbeat section.So the data node never re-joins the cluster; while from the out side it looks it's still running.This is another reason why we see missing data; and don't see failed data nodes.,Closed,Fixed,,Hairong Kuang,Konstantin Shvachko,Wed; 7 Jun 2006 15:26:00 +0000,Wed; 8 Jul 2009 16:41:55 +0000,Thu; 8 Jun 2006 02:29:02 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-285
HADOOP-286,Bug,Major,,copyFromLocal throws LeaseExpiredException,Loading local files to dfs through hadoop dfs -copyFromLocal failed due to the following exception:copyFromLocal: org.apache.hadoop.dfs.LeaseExpiredException: No lease on output_crawled.1.txt        at org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:414)        at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:190)        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        at java.lang.reflect.Method.invoke(Method.java:585)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:231),Closed,Fixed,,Konstantin Shvachko,Runping Qi,Wed; 7 Jun 2006 23:50:34 +0000,Wed; 8 Jul 2009 16:41:54 +0000,Wed; 6 Sep 2006 22:36:48 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-286
HADOOP-287,Improvement,Major,io,Speed up SequenceFile sort with memory reduction,I replaced the merge sort with a quick sort and it yielded approx 30% improvement in sort time. It also reduced the memory requirement for sorting because the sort is done in place.,Closed,Duplicate,HADOOP-2919,Chris Douglas,Benjamin Reed,Thu; 8 Jun 2006 04:40:37 +0000,Thu; 17 Apr 2008 05:26:38 +0000,Mon; 31 Mar 2008 22:53:12 +0000,,0.17.0,,HADOOP-1346,,https://issues.apache.org/jira/browse/HADOOP-287
HADOOP-288,Bug,Major,,RFC: Efficient file caching,"RFC: Efficient file caching (on Hadoop Task nodes; for benefit of MapReduce Tasks)------------------------------------------------------We will start implementing this soon. Please provide feedback and improvements to this plan.The header ""Options:"" indicates places where simple choices must be made.Problem:-------o MapReduce tasks require access to additional out-of-band data (""dictionaries"")This out-of-band data is:o in addition to the map/reduce inputs.o large (1GB+)o broadcast (same data is required on all the Task nodes)o changes ""infrequently""; in particular:oo it is always constant for all the Tasks in a Job. oo it is often constant for a month at a time oo it may be shared across team memberso sometimes used by pure-Java MapReduce programso sometimes used by non-Java MapReduce programs (using Hadoop-Streaming)o (future) used by programs that use HDFS and Task-trackers but not MapReduce.Existing Solutions to the problem:---------------------------------These solutions are not good enough. The present proposal is to do Sol 1 with caching.Sol 1: Pure Hadoop: package the out-of-band data in the MapReduce Job jar file.Sol 2: Non  Hadoop: for each task node run rsync from single source for data.Sol 3: Non  Hadoop: use BitTorrent; etc.Sol.1 is correct but slow for many reasons: The Job submitter must recreate a large jar(tar) file for every Job.  (The jar contains both changing programs and stable dictionaries) The large Jar file must be propagated from the client to HDFS with  a large replication factor.  At the beginning of every Task; the Task tracker gets the job jar from HDFS  and unjars it in the working directory. This can dominate task execution time.Sol.2 has nice properties but also some problems. It does not scale well with large clusters (many concurrent rsync read requests i.e. single-source broadcast) It assumes that Hadoop users can upload data using rsync to the cluster nodes. As a policy; this is not allowed. It requires rsync.Sol.3 alleviates the rsync scalability problems but       It is a dependency on an external system.       We want something simpler and more tightly integrated with Hadoop.Staging (uploading) out-of-band data:------------------------------------The out-of-band data will often originate on the local filesystem of a user machine  (i.e. a MapReduce job submitter)Nevertheless it makes sense to use HDFS to store the original out-of-band data because:o HDFS has (wide) replication. This enables scalable broadcast later.o HDFS is an available channel to move data from clients to all task machines.o HDFS is convenient as a shared location among Hadoop team members.Accessing (downloading) out-of-band data:----------------------------------------The non-Java MapReduce programs do not have or want1 APIs for HDFS.Instead these programs just want to access out-of-band data as  local files at predefined paths.(1 Existing programs should be reusable with no changes.  This is often possible bec. communication is over stdin/stdout.)Job's jar file as a special case:--------------------------------One use case is to allow users to make the job jar itself cachable.This is only useful in cases where NOTHING changes when a job is resubmitted (no MapRed code changes and no changes in shipped data)This situation might occur with an 'extractor' job (gets data from an external source: like Nutch crawler)Currently the Hadoop mapred-jar mechanism works in this way: the job jar data is unjarred in the ""working directory"" of the Task  the jar contains both MapRed java code (added to classpath)Cache synchronization:---------------------The efficient implementation of the out-of-band data distributionis mostly a cache synchronization problem.A list of the various aspects where choices must be made follows.Cache key:---------How do you test that the cached copy is out-of-date?Options: 1. the archive/file timestamp 2. the MD5 of the archive/file contentComparing source and destination Timestamps is problematic bec. it assumes synchronized clocks.Also there is no last-modif metadata in HDFS (for good reasons; like scalability of metadata ops)Timestamps stored with the source ('last-propagate-time') do  not require synchronized clocks; only locally monotonic time. (and the worse which can happen at daylight-savings switch is a missed update or an extra-update)The cache code could store a copy of the local timestamp in the same way that it caches the value of the content hash along with the source data.Cachable unit:-------------Options: individual files or archives or both.Note:At the API level; directories will be processed recursively (and the local FS directories will parallel HDFS directories)So bulk operations are always possible using directories.The question here is whether to handle archives as an additional bulk mechanism.Archives are special because:o unarchiving occurs transparently as part of the cache synco The cache key is computed on the archive and preserved although   the archive itself is not preserved.Supported archive format will be: tar (maybe tgz or compressed jar)Archive detection test: by filename extension "".tar"" or "".jar""Suppose we don't handle archives as special files:Pros: o less code; no discussion about which archive formats are supported o fine for large dictionary files. And when files are not large; user may as well   put them in the Job jar as usual. o user code could always check and unarchive specific cached files   (as a side-effect of MapRed task initialization)Cons: o handling small files may be inefficient   (multiple HDFS operations; multiple hash computation;    one 'metadata' hash file along with each small file) o It will not be possible to handle the Job's jar file as a special case of caching Cache isolation: ---------------In some cases it may be a problem if the cached HDFS files are updated while a Job is in progress:The file may become unavailable for a short period of time and some tasks fail.The file may change (atomically) and different tasks use a different version.This isolation problem is not addressed in this proposal.Standard solutions to the isolation problem are:o Assume that Jobs and interfering cache updates won't occur concurrently.o Put a version number in the HDFS file paths and refer to a hard-coded version in the Job code.o Before running the MapRed job; run a non-distributed application that tests  what is the latest available version of the out-of-band data.   Then make this version available to the MapRed job.  Two ways to do this.   o either set a job property just-in-time:    addCachePathPair(""/mydata/v1234/""; ""localcache/mydata_latest"");     (see Job Configuration for meaning of this)  o or publish the decision as an HDFS file containing the version.    then rely on user code to read the version; and manually populate the cache:    Cache.syncCache(""/hdfs/path/fileordir""; ""relative/local/fileordir"");    (see MapReduce API for meaning of this)Cache synchronization stages:----------------------------There are two stages: Client-to-HDFS and HDFS-to-TaskTrackero Client-to-HDFS stage.Options: A simple option is to not do anything here; i.e. rely on the user.This is a reasonable option given previous remarks on the role of HDFS: HDFS is a staging/publishing area and a natural shared location.In particular this means that the system need not track where the client files come from.o HDFS-to-TaskTracker:Client-to-HDFS synchronization (if done at all) should happen before this.Then HDFS-to-TaskTracker synchronization must happen right before the data is needed on a node.MapReduce cache API:-------------------Options:1. No change in MapReduce framework code:require the user to put this logic in map() (or reduce) function: in MyMapper constructor (or in map() on first record) user is asked to add:    Cache.syncCache(""/hdfs/path/fileordir""; ""relative/local/fileordir"");    Cache.syncCache(""...""); //etc.2. Put this logic in MapReduce framework and use Job properties to   communicate the list of pairs (hdfs path; local path)Directories are processed recursively.If archives are treated specially then they are unarchived on destination.MapReduce Job Configuration:---------------------------Options:with No change in MapReduce framework code (see above) no special Job configuration:    it is up to the MapRed writer to configure and run the cache operations.with Logic in MapReduce framework (see above) some simple Job configurationJobConf.addCachePathPair(String; String)JobConf.addCachePathPair(""/hdfs/path/fileordir""; ""relative/local/fileordir"");",Closed,Fixed,,Mahadev konar,Michel Tourn,Thu; 8 Jun 2006 08:28:11 +0000,Fri; 6 Oct 2006 21:48:56 +0000,Thu; 14 Sep 2006 22:11:37 +0000,,0.6.0,,,MAPREDUCE-458,https://issues.apache.org/jira/browse/HADOOP-288
HADOOP-289,Bug,Major,,Datanodes need to catch SocketTimeoutException and UnregisteredDatanodeException,"Datanode needs to catch SocketTimeoutException when registering otherwise it goes downthe same way as when the namenode is not available (HADOOP-282).	UnregisteredDatanodeException need to be caught for all non-registering requests. The datanode should be shutdown in this case. Otherwise it will loop infinitely and consume namenode resources.",Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Thu; 8 Jun 2006 09:34:01 +0000,Wed; 8 Jul 2009 16:41:54 +0000,Sat; 10 Jun 2006 00:16:56 +0000,,0.3.1,,,,https://issues.apache.org/jira/browse/HADOOP-289
HADOOP-290,Bug,Minor,,Fix Datanode transfer thread logging,"Under the current datanode the logging of ""Starting thread to transfer block"" doesn't print out the hosts that it is transferring to; it prints out a java array toString.  This patch fixes this and prints out the hostnames and ports for all hosts we are tranferring to.",Closed,Fixed,,dhruba borthakur,Dennis Kubes,Thu; 8 Jun 2006 11:19:27 +0000,Wed; 8 Jul 2009 16:41:55 +0000,Fri; 18 Jan 2008 05:23:48 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-290
HADOOP-291,New Feature,Major,util,Hadoop Log Archiver/Analyzer utility,"Overview of the log archiver/analyzer utility...1. Input  The tool takes as input a list of directory URLs; each url could also we associated with a file-pattern to specify what pattern of files in that directory are to be used.  e.g. http://g1015:50030/logs/hadoop-sameer-jobtracker-*         file:///export/crawlspace/sanjay/hadoop/trunk/run/logs/haddop-sanjay-namenode-* (local disk on the machine on which the job was submitted)2. The tool supports 2 main functions:  a) Archival    Archive the logs in the DFS in the following hierarchy:   /users/username/log-archive/YYYY/mm/dd/HHMMSS.log by default    Or a user-specified directory and then:    input-dir/YYYY/mm/dd/HHMMSS.log  b) Processing with simple sort/grep primitives    Archive the logs as above and then grep for lines with given pattern (e.g. INFO) and then sort with spec e.g. logger&lt;level&lt;date. (Note: This is proposed with current log4j based logging in mind... do we need anything more generic?). The sort/grep specs are user-provided; along with directory URLs.3. Thoughts on implementation...  a) Archival    Current idea is to put a .jsp page (src/webapps) on each of the nodes; which then does a copyFromLocal of the log-file into the DFS. The jobtracker will fire n map-tasks which only hit the jsp page as per the directory URLs. The reduce-task is a no-op and only collects statistics on failures (if any).  b) Processing with sort/grep    Here; the tool first archives the files as above and then another set of map-reduce tasks will do the sort/grep on the files in DFS with given specs.	* - * - Suggestions/corrections welcome...thanks;Arun",Closed,Duplicate,NULL,Unassigned,Arun C Murthy,Thu; 8 Jun 2006 17:40:06 +0000,Tue; 1 May 2007 20:46:15 +0000,Tue; 3 Apr 2007 10:30:55 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-291
HADOOP-292,Bug,Minor,,hadoop dfs commands should not output superfluous data to stdout,running a command such as hadoop dfs -ls /dataproduces output such as the following:06/06/08 17:42:32 INFO conf.Configuration: parsing jar:file: /hadoop/hadoop-0.4-dev/hadoop-0.4-dev.jar!/hadoop-default.xml06/06/08 17:42:32 INFO conf.Configuration: parsing file:hadoop/hadoop-site.xml06/06/08 17:42:32 INFO dfs.DistributedFileSystem: No FS indicated; using default:kry1200:802006/06/08 17:42:32 INFO ipc.Client: Client connection to 172.30.111.134:8020: startingFound 2 items/data/a dir/data/b     dirthe first few lines shouldn't be there.it's especially annoying when running -cat into a file or into some post processing program; but in general; the output should be clean.,Closed,Fixed,,Owen O'Malley,Yoram Arnon,Fri; 9 Jun 2006 07:46:01 +0000,Wed; 8 Jul 2009 16:41:56 +0000,Sat; 10 Jun 2006 01:03:38 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-292
HADOOP-293,Bug,Major,,map reduce job fail without reporting a reason,Often I see in the WI reports of tasks failing without information reported as to the reason of the failure.It makes analysis and fixing the problem much harder.The reason for the failure should always be reported in the WI.,Closed,Fixed,,Owen O'Malley,Yoram Arnon,Fri; 9 Jun 2006 07:48:23 +0000,Wed; 8 Jul 2009 16:51:49 +0000,Wed; 20 Sep 2006 21:03:07 +0000,,0.3.1,,,,https://issues.apache.org/jira/browse/HADOOP-293
HADOOP-294,Bug,Major,,dfs client error retries aren't happening (already being created and not replicated yet),The conditions for catching the dfs error conditions that need to be retried were broken in a previous patch.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Sat; 10 Jun 2006 01:11:48 +0000,Wed; 8 Jul 2009 16:41:55 +0000,Sat; 10 Jun 2006 01:39:33 +0000,,0.3.1,,,,https://issues.apache.org/jira/browse/HADOOP-294
HADOOP-295,Bug,Major,,jobs don't get executed in parallel,we expect tasks to be assigned to nodes up to their configured capacity; with separate slots for map and reduce tasks.we're observing jobs queueing up; with the map slots free; as exemplified by this copy-paste (converted to text) of the JT WI page.Cluster SummaryMaps Reduces Tasks/Node Nodes 0           205             2                 156 --------------------------------------------------------------------------------Running Jobs  Jobid        User           Name          Map % complete Map total Maps completed Reduce % complete Reduce total Reduces completed job_0088 runping  DocUpdater   100.00%                  3597                3597                  99.92%                       256                        253 job_0089 murthij   Term Count        0.00%                  243                        0                     0.00%                        200                            0 job_0091 zshao     sort                       0.00%                  444                        0                     0.00%                        100                           0 --------------------------------------------------------------------------------,Closed,Fixed,HADOOP-299,Owen O'Malley,Yoram Arnon,Tue; 13 Jun 2006 07:26:32 +0000,Wed; 8 Jul 2009 16:51:47 +0000,Fri; 23 Jun 2006 23:02:32 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-295
HADOOP-296,New Feature,Major,,Do not assign blocks to a datanode with < x mb free,We're running a smallish cluster with very different machines; some with only 60 gb harddrivesThis creates a problem when inserting files into the dfs; these machines run out of space quickly and then they cannot run any map reduce operationsA solution would be to not assign any new blocks once the space is below a certain user configurable thresholdThis free space could then be used by the map reduce operations instead (if that's on the same disk),Closed,Fixed,,Johan Oskarsson,Johan Oskarsson,Tue; 13 Jun 2006 17:58:18 +0000,Wed; 8 Jul 2009 16:41:55 +0000,Tue; 27 Jun 2006 04:58:27 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-296
HDFS-382,Improvement,Minor,,When selecting node to put new block on; give priority to those with more free space/less blocks,As mentioned in previous bug report:We're running a smallish cluster with very different machines; some with only 60 gb harddrivesThis creates a problem when inserting files into the dfs; these machines run out of space quickly while some have plenty of space free.So instead of just shuffling the nodes; I've created a quick patch that first sorts the target nodes by (freespace / blocks).It then randomizes the position of the first third of the nodes (so we don't put all the blocks in the file on the same machine)I'll let you guys figure out how to improve this./Johan,Open,Unresolved,,Sameer Paranjpye,Johan Oskarsson,Tue; 13 Jun 2006 23:23:27 +0000,Sun; 17 Jul 2011 19:57:41 +0000,,,,,,,https://issues.apache.org/jira/browse/HDFS-382
HADOOP-298,Improvement,Minor,util,nicer reports of progress for distcp,The unformatted number of bytes in distcp is difficult to read.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 14 Jun 2006 02:31:52 +0000,Thu; 3 Aug 2006 17:46:47 +0000,Wed; 14 Jun 2006 04:56:06 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-298
HADOOP-299,Bug,Major,,maps from second jobs will not run until the first job finishes completely,Because of the logic in the JobTracker's pollForNewTask; second jobs will rarely start running maps until the first job finishes completely. The JobTracker leaves room to re-run failed maps from the first job and it reserves the total number of maps for the first job. Thus; if you have more maps in the first job than your cluster capacity; none of the second job maps will ever run.I propose setting the reserve to 1% of the first job's maps.,Closed,Fixed,HADOOP-295,Owen O'Malley,Owen O'Malley,Wed; 14 Jun 2006 04:23:34 +0000,Wed; 8 Jul 2009 16:51:47 +0000,Tue; 20 Jun 2006 02:03:39 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-299
MAPREDUCE-474,Improvement,Trivial,,ability to report numeric progress within a particular key,I'd like some mechanism to report numeric progress in maps that have side effects. In the common case; there is exactly one key/value for each map and they do a lot of work (minutes to hours) while working on that key. It would be nice if mapred.Reporter had a progress(float) method that set the progress within the current key.,Resolved,Not A Problem,,Owen O'Malley,Owen O'Malley,Wed; 14 Jun 2006 04:33:59 +0000,Mon; 16 Jan 2012 09:09:40 +0000,Mon; 16 Jan 2012 09:09:40 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-474
HADOOP-301,Improvement,Major,,the randomwriter example will clobber the output file,The randomwriter will automatically clobber the output directory; which is dangerous given the lack of permissions in  dfs.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 14 Jun 2006 06:03:13 +0000,Wed; 8 Jul 2009 16:51:47 +0000,Tue; 27 Jun 2006 01:21:46 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-301
HADOOP-302,Improvement,Major,io,class Text (replacement for class UTF8) was: HADOOP-136,"Just to verify; which length-encoding scheme are we using for class Text (aka LargeUTF8) a) The ""UTF-8/Lucene"" scheme? (highest bit of each byte is an extension bit; which I think is what Doug is describing in his last comment) or b) the record-IO scheme in o.a.h.record.Utils.java:readInt Either way; note that: 1. UTF8.java and its successor Text.java need to read the length in two ways:   1a. consume 1+ bytes from a DataInput and   1b. parse the length within a byte array at a given offset (1.b is used for the ""WritableComparator optimized for UTF8 keys"" ). o.a.h.record.Utils only supports the DataInput mode. It is not clear to me what is the best way to extend this Utils code when you need to support both reading modes 2 Methods like UTF8's WritableComparator are to be low overhead; in partic. there should be no Object allocation. For the byte array case; the varlen-reader utility needs to be extended to return both:  the decoded length and the length of the encoded length.  (so that the caller can do offset += encodedlength) 3. A String length does not need (small) negative integers. 4. One advantage of a) is that it is standard (or at least well-known and natural) and there are no magic constants (like -120; -121 -124)",Closed,Fixed,,Hairong Kuang,Michel Tourn,Wed; 14 Jun 2006 07:32:27 +0000,Fri; 4 Aug 2006 22:22:32 +0000,Wed; 26 Jul 2006 08:05:00 +0000,,,,,LUCENE-510,https://issues.apache.org/jira/browse/HADOOP-302
MAPREDUCE-407,Bug,Minor,,JobClient looking for classes for submitted job in the wrong place,JobClient does some checking of the job being submitted when it submits a jar file along with the job. The problem is that the JobClient pulls classes from the classpath rather than the submitted jar file. Because the jar file may contain newer (or older) versions of classes on the classpath this behavior leads to confusing errors when the job is run. It is also a pain to ensure that the jar file being submitted is on the classpath. Further; if the JobClient uses the submitted jar file rather than the classpath; missing classes from the jar file can be detected earlier.This patch will cause the JobClient to load the classes for a job from the jar file rather than the classpath. Because of the class loading precedence rules in Java; if the class is on the system class path; it will be loaded from there rather than the submitted jar file; but now users need not (and should not) put job classfiles on the system classpath.This patch also allows config files to be put in a configuration directory rather than on the classpath; which also eliminates some confusing behavior when there are duplicate instances of config files in different parts of the classpath.,Resolved,Invalid,,Owen O'Malley,Benjamin Reed,Thu; 15 Jun 2006 02:07:18 +0000,Mon; 16 Jan 2012 09:08:22 +0000,Mon; 16 Jan 2012 09:08:22 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-407
HADOOP-304,Bug,Trivial,,UnregisteredDatanodeException message correction,,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Thu; 15 Jun 2006 05:35:05 +0000,Thu; 3 Aug 2006 17:46:47 +0000,Tue; 27 Jun 2006 01:28:03 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-304
HADOOP-305,Improvement,Minor,,tasktracker waits for 10 seconds for asking for a task.,the tasktracker should ask for a job as soon as a job running on it is finished.,Closed,Fixed,HADOOP-99,Mahadev konar,Mahadev konar,Fri; 16 Jun 2006 05:46:13 +0000,Wed; 8 Jul 2009 16:51:49 +0000,Tue; 27 Jun 2006 01:36:44 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-305
HADOOP-306,New Feature,Major,,Safe mode and name node startup procedures,"This is a proposal to improve DFS cluster startup process.The data node startup procedures were described and implemented in HADOOP-124.I'm trying to extend them to the name node here.The main idea is to introduce safe mode; which can be entered manually for administrationpurposes; or automatically when a configurable threshold of active data nodes is breached;or at startup when the node stays in safe mode until the minimal limit of activenodes is reached.This are high level requirements intended to improve the name node and cluster reliability.    = The name node safe mode means that the name node is not changing the state of the       file system. Meta data is read-only; and block replication / removal is not taking place.    = In safe mode the name node accepts data node registrations and       processes their block reports.    = The name node always starts in safe mode and stays safe until the majority        (a configurable parameter: safemode.threshold) of data nodes (or blocks?)        is reported.    = The name node can also fall into safe mode when the number of non-active        (heartbeats stopped coming in) data nodes becomes critical.    = The startup ""silent period""; when the name node is in safe mode and is        not issuing any block requests to the data nodes; is initially set to a        configurable value safemode.timeout.increment. By the end of the timeout        the name node checks the safemode.threshold and decides whether to switch        to the normal mode or to stay in safe. If the normal mode criteria is not        met; then the silent period is extended by incrementing the safemode timeout.    = The name node stays in safe mode not longer than a configurable value of        safemode.timeout.max; in which case it logs missing data nodes and shuts        itself down.    = When the name node switches to normal mode it checks whether all required        data nodes have actually registered; based on the list of active data storages        from the last session. Then it logs missing nodes; if any; and starts        replicating and/or deleting blocks as required.    = A historical list of data storages (nodes) ever registered with the cluster is        persistently stored in the image and log files. The list is used in two ways:        a) at startup to verify whether all nodes have registered; and to report        missing nodes;        b) at runtime if a data node registers with a new storage id the        name node verifies that no new blocks are reported from that storage;        which would prevent us from accidentally connecting data nodes from a        different cluster.    = The name node should have an option to run in safe mode. Starting with        that option would mean it never leaves safe mode.        This is useful for testing the cluster.    = Data nodes that can not connect to the name node for a long time (configurable)        should shut down themselves.",Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Sat; 17 Jun 2006 07:57:35 +0000,Tue; 13 Oct 2009 21:33:10 +0000,Wed; 20 Sep 2006 22:19:53 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-306
HADOOP-307,Task,Minor,,Many small jobs benchmark for MapReduce,A benchmark that runs many small MapReduce tasks in sequence. A single map reduce implementation is used; it is invoked multiple times with input as the output from previous run. The input to first Map is a TextInputFormat ( a text file with few hundred KBs). Input records are passed to output without much processing. The idea is to benchmark the time taken by initialization of Mapper and Reducer. An initial prototyping on a single machine with 20 MR tasks in sequence took ~47 seconds per task. Looking for suggestions on what else can be included in the benchmark.,Closed,Fixed,,Sanjay Dahiya,Sanjay Dahiya,Sun; 18 Jun 2006 20:14:33 +0000,Wed; 8 Jul 2009 16:51:48 +0000,Tue; 18 Jul 2006 11:03:44 +0000,,,,HADOOP-460,,https://issues.apache.org/jira/browse/HADOOP-307
HADOOP-308,Bug,Major,,Task Tracker does not handle the case of read only local dir  case correctly,In case that the local dir is not writable on a node; the tasks on the  node will fail as expected; with an exception like:(Read-only file system) at java.io.FileOutputStream.open(Native Method) at java.io.FileOutputStream.(FileOutputStream.java:179) at java.io.FileOutputStream.(FileOutputStream.java:131) at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.(DFSClient.java:723) at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:241) at org.apache.hadoop.dfs.DistributedFileSystem.createRaw(DistributedFileSystem.java:96) at org.apache.hadoop.fs.FSDataOutputStream$Summer.(FSDataOutputStream.java:44) at org.apache.hadoop.fs.FSDataOutputStream.(FSDataOutputStream.java:134) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:224) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:176) ....at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:265) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:847)However; the task tracker will continue accept new tasks and continue to fail.The runloop of tasktracker should detect such a problem and exits.,Closed,Duplicate,NULL,Owen O'Malley,Runping Qi,Tue; 20 Jun 2006 01:05:28 +0000,Wed; 8 Jul 2009 16:51:56 +0000,Fri; 4 Jan 2008 06:48:41 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-308
HADOOP-309,Bug,Minor,,NullPointerException in StatusHttpServer,"The NullPointerException happens in the constructor of StatusHttpServer because the System property ""hadoop.log.dir"" is undefined.I see it trying to start the Namenode directly without using any scripts.Exception in thread ""main"" java.lang.NullPointerException	at org.mortbay.util.Resource.newResource(Resource.java:99)	at org.mortbay.http.ResourceCache.setResourceBase(ResourceCache.java:140)	at org.mortbay.http.HttpContext.setResourceBase(HttpContext.java:2207)	at org.apache.hadoop.mapred.StatusHttpServer.init(StatusHttpServer.java:66)	at org.apache.hadoop.dfs.FSNamesystem.init(FSNamesystem.java:172)	at org.apache.hadoop.dfs.NameNode.init(NameNode.java:97)	at org.apache.hadoop.dfs.NameNode.init(NameNode.java:88)	at org.apache.hadoop.dfs.NameNode.main(NameNode.java:496)In general I think the sources should not rely on the system properties and environment variables defined in hadoop scripts.",Closed,Fixed,,navychen,Konstantin Shvachko,Tue; 20 Jun 2006 07:37:19 +0000,Wed; 8 Jul 2009 16:51:50 +0000,Fri; 2 Feb 2007 21:06:47 +0000,,0.10.1,,,,https://issues.apache.org/jira/browse/HADOOP-309
HADOOP-310,Improvement,Minor,io,Additional constructor requested in BytesWritable,"It would be grand if BytesWritable.java had an additional constructor as below. This allows me to use the BytesWritable class without doing a buffer copy; since we have a less-than-fully-utilized byte array holding our key.Thanks! /**	Create a BytesWritable using the byte array as the initial value.	@param bytes This array becomes the backing storage for the object.   */  public BytesWritable(byte[] bytes; int size) {    this.bytes = bytes;    this.size = size;  }",Resolved,Fixed,,Brock Noland,p sutter,Wed; 21 Jun 2006 01:13:19 +0000,Thu; 30 Jun 2011 07:24:56 +0000,Mon; 27 Jun 2011 03:39:22 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-310
HADOOP-311,Bug,Major,,dfs client timeout on read kills task,If a DFS client reads a file and times out; it immediately throws an exception to the client; which will kill the task.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 21 Jun 2006 03:28:08 +0000,Wed; 8 Jul 2009 16:41:54 +0000,Thu; 22 Jun 2006 01:34:54 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-311
HADOOP-312,Improvement,Major,ipc,Connections should not be cached,Servers and clients (client include datanodes; tasktrackers; DFSClients  tasks) should not cache connections or maybe cache them for very short periods of time. Clients should set up  tear down connections to the servers everytime they need to contact the servers (including the heartbeats). If connection is cached; then reuse the existing connection for a few subsequent transactions until the connection expires. The heartbeat interval should be more so that many more clients (order of  tens of thousands) can be accomodated within 1 heartbeat interval.,Closed,Fixed,,Devaraj Das,Devaraj Das,Thu; 22 Jun 2006 01:55:24 +0000,Fri; 8 Sep 2006 21:19:45 +0000,Wed; 6 Sep 2006 22:49:58 +0000,,,,HADOOP-362,,https://issues.apache.org/jira/browse/HADOOP-312
HADOOP-313,Bug,Major,,A stand alone driver for individual tasks,This is a tool to reproduce problems and to run unit tests involving either a map or reduce task.You just give it a reduce directory on the command line.Usage: java org.apache.hadoop.mapred.StandaloneReduceTask taskdir &lt;limitmaps&#93;taskdir name encodes: task_jobidrpartition_attempttaskdir contains job.xml and one or more input files named: map_dddd.outYou should run with the same -Xmx option as the TaskTracker child JVM,Closed,Fixed,,Michel Tourn,Michel Tourn,Thu; 22 Jun 2006 06:30:19 +0000,Fri; 4 Aug 2006 22:22:33 +0000,Tue; 11 Jul 2006 15:16:17 +0000,,,,,MAPREDUCE-443,https://issues.apache.org/jira/browse/HADOOP-313
HADOOP-314,Improvement,Major,,remove the append phase in sorting the reduce inputs,This patch creates a new interface to SequenceFile.sort that allows both a list of files to be passed as input and specifying whether the inputs should be deleted. The ReduceTask is changed to pass the list of map inputs and to delete them as they are sorted.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Thu; 22 Jun 2006 22:49:53 +0000,Wed; 8 Jul 2009 16:51:49 +0000,Fri; 23 Jun 2006 02:14:18 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-314
HADOOP-315,Bug,Minor,ipc,bobo Exception in TestRPC,"We are getting a bobo exceptions in TestRPC.It looks like the test should fail in this case after throwing bobo.By the way; does anybody know whether ""bobo"" means anything?error: java.io.IOException: bobojava.io.IOException: bobo    at org.apache.hadoop.ipc.TestRPC$TestImpl.error(TestRPC.java:84)    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    at java.lang.reflect.Method.invoke(Method.java:585)    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:467)",Closed,Won't Fix,,Unassigned,Konstantin Shvachko,Fri; 23 Jun 2006 01:15:29 +0000,Fri; 15 Dec 2006 23:02:18 +0000,Wed; 26 Jul 2006 20:55:29 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-315
HADOOP-316,Bug,Major,,job tracker has a deadlock,The JobTracker has a deadlock in the ExpireLaunchingTasks stuff.In particular; it locks the JobTracker and launchingTasks inconsistently.This deadlocks has been observed in the wild and causes the JobTracker to stop responding to rpc and http (other than the root page).,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 23 Jun 2006 01:27:01 +0000,Wed; 8 Jul 2009 16:51:47 +0000,Fri; 23 Jun 2006 02:37:53 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-316
HADOOP-317,Bug,Major,ipc,connection was forcibly closed Exception in RPC on Windows,I see a lot of exceptions caused by RPC in the nightly build on Windows.The most often is thrown by RPC on the namenode; saying06/06/21 19:28:36 INFO ipc.Server: Server listener on port 7017: readAndProcess threw exception java.io.IOException: An existing connection was forcibly closed by the remote host. Count of bytes read: 0java.io.IOException: An existing connection was forcibly closed by the remote host    at sun.nio.ch.SocketDispatcher.read0(Native Method)    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:25)    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233)    at sun.nio.ch.IOUtil.read(IOUtil.java:200)    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:207)    at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:374)    at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:289)    at org.apache.hadoop.ipc.Server$Listener.run(Server.java:210)I am not sure how serious that is; since the tests do not fail; and the name node does not crash.Besides the RPC we should probably also check that the unit tests actually fail if they need to.,Closed,Fixed,,Doug Cutting,Konstantin Shvachko,Fri; 23 Jun 2006 01:51:36 +0000,Thu; 3 Aug 2006 17:46:48 +0000,Fri; 23 Jun 2006 04:06:42 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-317
HADOOP-318,Bug,Major,,Progress in writing a DFS file does not count towards Job progress and can make the task timeout,When a task writes to DFS file; depending on how busy the cluster is; it can timeout after 10 minutes by default; because the progress towards writing a DFS file does not count as progress of the task. The solution (patch is forthcoming) is to provide a way to callback reporter to report task progress from DFSOutputStream.,Closed,Fixed,,Milind Bhandarkar,Milind Bhandarkar,Fri; 23 Jun 2006 03:14:57 +0000,Wed; 8 Jul 2009 16:51:48 +0000,Thu; 29 Jun 2006 04:04:40 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-318
HADOOP-319,Bug,Major,fs,"FileSystem ""close"" does not remove the closed fs from the fs map",The close methods of both DistributedFileSystem and LocalFileSystem do not remove the closed file system from the fs map in FileSystem. As a result; a subsequent call to FileSystem.getNamed may return the handle to the closed file system and receive errors if perform any operation on the fs handle.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Fri; 23 Jun 2006 03:48:37 +0000,Thu; 3 Aug 2006 17:46:48 +0000,Tue; 27 Jun 2006 00:31:33 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-319
HADOOP-320,Bug,Major,,bin/hadoop dfs -mv does not mv  source's checksum file if source is a file,None of the rename operation of DistributedSystem or DFSClient checks if the source is a file or not. Need also to rename the checksum file accordingly if the source is a file.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Fri; 23 Jun 2006 06:03:31 +0000,Wed; 8 Jul 2009 16:41:55 +0000,Wed; 6 Sep 2006 21:31:00 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-320
HADOOP-321,Improvement,Major,,DatanodeInfo refactoring,I'm trying to refactor some name node classes; which seem to be similar.So DatanodeInfo is a public api now for purely external ( to name node) use.The name node class that stores information about data nodes including theset of its blocks is called DatanodeDescriptor.The DatanodeReport is removed since it was a variation of DatanodeInfo.Previously DatanodeInfo and DatanodeDescriptor were the same class; andDatanodeReport was used for reporting node statistics only.This is a preparation step for HADOOP-306.,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Fri; 23 Jun 2006 08:08:54 +0000,Wed; 8 Jul 2009 16:41:55 +0000,Wed; 26 Jul 2006 06:56:59 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-321
HADOOP-322,New Feature,Major,,Need a job control utility to submit and monitor a group of jobs which have DAG dependency,"In my applications; some jobs depend on the outputs of other jobs. Therefore; job dependency forms a DAG. A job is ready to run if and only if it does not have any dependency or all the jobs it depends are finished successfully. To help schedule and monitor a group of jobs like that; I am thinking of implementing a utility that:	accept jobs with dependency specification	monitor job status	submit jobs when they are readyWith such a utility; the application can construct its jobs; specify their dependency and then hand the jobs to the utility class. The utility takes care of the details of job submission.I'll post my design skech for comments/suggestion.Eventually; I'll submit a patch for the utility.",Closed,Fixed,,Runping Qi,Runping Qi,Sat; 24 Jun 2006 12:15:08 +0000,Fri; 8 Sep 2006 21:19:46 +0000,Tue; 15 Aug 2006 21:54:01 +0000,,0.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-322
HADOOP-323,Bug,Major,fs,IO Exception at LocalFileSystem.renameRaw; when running Nutch nightly builds (0.8-dev).,"IO Exception at LocalFileSystem.renameRaw; when running Nutch nightly builds (0.8-dev).Please see the deatil descriptions in:http://issues.apache.org/jira/browse/NUTCH-266Not knowing how to reclassify an existing bug; I am opening this new bug under Hadoop.The version number is 0.3.3 but because I don't see it in the jira list; I chose the closest matching version.  The Nutch-with-GUI build was running with hadoop-0.2 but stopped running; exhibiting the same symptom with other nightly builds; when switched to use hadoop-0.3.3.I checked ""fs"" as component but this bug could also be caused by the order in which jobs are scheduled; I suspect.",Resolved,Invalid,,Unassigned,Kuro Kurosaka,Tue; 27 Jun 2006 00:45:48 +0000,Sat; 16 Jul 2011 16:30:25 +0000,Sat; 16 Jul 2011 16:30:25 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-323
HADOOP-324,Bug,Major,,IOException: No space left on device is handled incorrectly,When a data node disk is almost full the name node still assigns blocks to the data node.By the time the data node actually tries to write that data to disk the disk may become full.Current implementation forces the data node to shutdown after that.The expected behavior is to report the block write failure and continue.The Exception looks as follows:java.io.IOException: No space left on deviceat java.io.FileOutputStream.writeBytes(Native Method)at java.io.FileOutputStream.write(FileOutputStream.java:260)at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)at java.io.DataOutputStream.write(DataOutputStream.java:90)at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:623)at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:410)at java.lang.Thread.run(Thread.java:595)2006-06-26 08:26:04;751 INFO org.apache.hadoop.dfs.DataNode: Finishing DataNode in: /tmp/hadoop/dfs/data/data,Closed,Fixed,,Wendy Chien,Konstantin Shvachko,Tue; 27 Jun 2006 07:59:17 +0000,Fri; 8 Sep 2006 21:19:47 +0000,Wed; 9 Aug 2006 16:23:47 +0000,,0.3.2,,,HADOOP-336,https://issues.apache.org/jira/browse/HADOOP-324
HADOOP-325,Bug,Major,ipc,ClassNotFoundException under jvm 1.6,We have been having problems with classes that are returned by RPC methods not being loaded/initialized correctly. The work around has been to put in the servers; code of the form:static { new FooBar(); }  // to resolve the ClassNotFoundException for class FooBar.When I tried running under java 1.6; that stopped working because one of the classes had to be instantiated from a package that didn't have visibility to create an instance. So I tracked the problem down to how the classes were being loaded via reflection.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 27 Jun 2006 12:57:52 +0000,Thu; 3 Aug 2006 17:46:48 +0000,Wed; 28 Jun 2006 02:41:20 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-325
HADOOP-326,Improvement,Minor,,cleanup of dead field (map ouput port),The TaskTrackerStatus retains a field that records the port of the map outuput server; even though that server no longer exists in the current code base.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 27 Jun 2006 13:10:24 +0000,Wed; 8 Jul 2009 16:51:49 +0000,Wed; 28 Jun 2006 02:54:54 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-326
HADOOP-327,Bug,Major,util,ToolBase calls System.exit,The new ToolBase class calls System.exit when the main routine finishes. That will break if the application uses threads that need to finish before the jvm exits. The normal semantics is that the program doesn't finish execution until all of the non-daemon threads exit (including the main one) and System.exit should never be called except for critical errors.,Closed,Fixed,,Hairong Kuang,Owen O'Malley,Tue; 27 Jun 2006 13:15:11 +0000,Fri; 4 Aug 2006 22:22:34 +0000,Wed; 12 Jul 2006 15:38:21 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-327
HADOOP-328,Improvement,Major,util,add a -i option to distcp to ignore read errors of the input files,"Add an option ""-i"" to ignore problems reading files and just copy what can be copied.",Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 27 Jun 2006 13:41:35 +0000,Thu; 3 Aug 2006 17:46:48 +0000,Wed; 28 Jun 2006 21:14:29 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-328
HADOOP-329,Bug,Major,,ClassCastException in DFSClient,"I'm getting the following message back to my launching application:Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.ClassCastException: org.apache.hadoop.dfs.DatanodeInfo cannot be cast to java.lang.Comparable        at java.util.TreeMap.getEntry(TreeMap.java:325)        at java.util.TreeMap.containsKey(TreeMap.java:209)        at java.util.TreeSet.contains(TreeSet.java:217)        at org.apache.hadoop.dfs.DFSClient.bestNode(DFSClient.java:373)        at org.apache.hadoop.dfs.DFSClient.access$100(DFSClient.java:42)        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:520)        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:638)        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:167)        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)        at java.io.BufferedInputStream.read(BufferedInputStream.java:313)        at java.io.DataInputStream.readFully(DataInputStream.java:174)        at java.io.DataInputStream.readFully(DataInputStream.java:150)        at org.apache.hadoop.fs.FSDataInputStream$Checker.init(FSDataInputStream.java:55)        at org.apache.hadoop.fs.FSDataInputStream.init(FSDataInputStream.java:237)        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:72)        at org.apache.hadoop.dfs.DistributedFileSystem.copyToLocalFile(DistributedFileSystem.java:182)        at org.apache.hadoop.mapred.JobInProgress.init(JobInProgress.java:83)        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:935)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        at java.lang.reflect.Method.invoke(Method.java:589)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:469)        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:159)",Closed,Fixed,,Unassigned,Owen O'Malley,Wed; 28 Jun 2006 07:07:48 +0000,Wed; 8 Jul 2009 16:41:57 +0000,Wed; 28 Jun 2006 21:08:05 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-329
HADOOP-330,Improvement,Major,,Raw SequenceFile Input/Output formats,We'd like a raw input/output format for Map/Reduce that allows jobs to get the raw bytes for keys/values. This would allow things like the IdentityMap to be much faster because the values would not be decompressed/compressed or serialized/deserialized.,Closed,Duplicate,NULL,Chris Douglas,Owen O'Malley,Thu; 29 Jun 2006 01:01:05 +0000,Wed; 8 Jul 2009 16:51:50 +0000,Fri; 16 Jan 2009 05:12:32 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-330
HADOOP-331,Improvement,Major,,map outputs should be written to a single output file with an index,The current strategy of writing a file per target map is consuming a lot of unused buffer space (causing out of memory crashes) and puts a lot of burden on the FS (many opens; inodes used; etc).  I propose that we write a single file containing all output and also write an index file IDing which byte range in the file goes to each reduce.  This will remove the issue of buffer waste; address scaling issues with number of open files and generally set us up better for scaling.  It will also have advantages with very small inputs; since the buffer cache will reduce the number of seeks needed and the data serving node can open a single file and just keep it open rather than needing to do directory and open ops on every request.The only issue I see is that in cases where the task output is substantiallyu larger than its input; we may need to spill multiple times.  In this case; we can do a merge after all spills are complete (or during the final spill).,Closed,Fixed,,Devaraj Das,eric baldeschwieler,Thu; 29 Jun 2006 03:50:27 +0000,Wed; 8 Jul 2009 16:51:50 +0000,Fri; 8 Dec 2006 01:58:33 +0000,,0.3.2,,,,https://issues.apache.org/jira/browse/HADOOP-331
HADOOP-332,Improvement,Major,,Implement remote replication of dfs namespace images and transaction logs,The namespace information (image and transaction logs) needs to be replicated on hosts other than the namenode to prevent data loss if a namenode crashes. In the short term we will add a new protocol to the datanodes to receive and store the namespace information.In the long term; it would be nice to have read-only namenodes that could receive the information and serve it up for users that want to read data; but do not need to write to the namespace.,Closed,Duplicate,HADOOP-90,Konstantin Shvachko,Owen O'Malley,Thu; 29 Jun 2006 13:28:03 +0000,Wed; 8 Jul 2009 16:41:56 +0000,Tue; 19 Dec 2006 21:11:52 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-332
HADOOP-333,Improvement,Major,,we should have some checks that the sort benchmark generates correct outputs,We should implement some checks of the input versus output of the sort benchmark to get some correctness guarantees:1. the number of records2. the number of bytes3. the output records are in fact sorted4. the xor of the md5 of each record's key/value pair,Closed,Fixed,,Arun C Murthy,Owen O'Malley,Fri; 30 Jun 2006 00:20:39 +0000,Wed; 8 Jul 2009 16:51:55 +0000,Fri; 16 Feb 2007 22:04:10 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-333
HADOOP-334,Improvement,Major,,Redesign the dfs namespace datastructures to be copy on write,The namespace datastructures should be copy on write so that the namespace does not need to be completely locked down from user changes while the checkpoint is being made.,Closed,Duplicate,HADOOP-227,Konstantin Shvachko,Owen O'Malley,Fri; 30 Jun 2006 00:26:50 +0000,Wed; 8 Jul 2009 16:41:57 +0000,Thu; 11 Oct 2007 19:13:00 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-334
HADOOP-335,Improvement,Major,,factor out the namespace image/transaction log writing,Factor the checkpoint/transaction log handling code out of FSNameSystem into its own class.,Closed,Fixed,,Konstantin Shvachko,Owen O'Malley,Fri; 30 Jun 2006 00:30:14 +0000,Fri; 4 Aug 2006 22:22:35 +0000,Wed; 26 Jul 2006 08:26:48 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-335
HADOOP-336,Improvement,Major,,The task tracker should track disk space used; and have a configurable cap,We've been having problems where the task tracker is destabilizing HDFS by filling disks and then releasing them.  Various tasks can also interfere with each other this way.We should have a configurable max working space for a task (and allow the setting of lower per task limits).  The task tracker should track use against this limit and terminate a job if it overruns it.,Closed,Duplicate,HADOOP-27,Unassigned,eric baldeschwieler,Sat; 1 Jul 2006 03:21:29 +0000,Wed; 8 Jul 2009 16:51:50 +0000,Thu; 31 Aug 2006 20:00:39 +0000,,,,,HADOOP-324,https://issues.apache.org/jira/browse/HADOOP-336
HADOOP-337,New Feature,Major,,DFS files should be appendable,Actually two related issues1. One should be able to open an existing DFS file; to seek to a position and truncate the rest; and to append starting at the end (or where trancation happens) .2. One should be able to read the writen data of a DFS file while other is writing/appending to the file,Closed,Duplicate,NULL,Sameer Paranjpye,Runping Qi,Sat; 1 Jul 2006 04:19:23 +0000,Wed; 8 Jul 2009 16:41:56 +0000,Mon; 28 Apr 2008 20:11:08 +0000,,0.1.0;0.1.1;0.2.0;0.2.1;0.3.0;0.3.1;0.3.2;0.4.0,,,HADOOP-1700,https://issues.apache.org/jira/browse/HADOOP-337
HADOOP-338,Bug,Major,,the number of maps in the JobConf does not match reality,The config value JobConf.getNumMapTasks() returns the hint of how many maps should be run rather than the actual number of input splits that were generated.The job tracker should fix the job.xml with the actual number of input splits.,Resolved,Fixed,,Owen O'Malley,Owen O'Malley,Sat; 1 Jul 2006 04:56:44 +0000,Wed; 8 Jul 2009 16:51:55 +0000,Tue; 10 Apr 2007 18:00:13 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-338
HADOOP-339,New Feature,Minor,,making improvements to the jobclients to get information on currenlyl running jobs and the jobqueue,adding a few methods to the joblcient and jobsubmission protocol to support queries to the jobtracker for currently running jobs,Closed,Fixed,,Mahadev konar,Mahadev konar,Sat; 1 Jul 2006 06:37:33 +0000,Fri; 4 Aug 2006 22:22:35 +0000,Tue; 11 Jul 2006 15:41:54 +0000,,0.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-339
HADOOP-340,Improvement,Minor,conf,Using wildcards in config pathnames,In our cluster there's machines with very different disk setupsI've solved this by not rsyncing hadoop-site.xml; but as you probably understand this means new settings will not get copied properly.I'd like to be able to use wildcards in the dfs.data.dir path for example:property  namedfs.data.dir/name    value/home/hadoop/disk*/dfs/data/value/propertythen every disk mounted in that directory would be used,Closed,Fixed,,Doug Cutting,Johan Oskarsson,Sun; 2 Jul 2006 16:40:00 +0000,Fri; 4 Aug 2006 22:22:35 +0000,Sat; 8 Jul 2006 14:15:24 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-340
HADOOP-341,Improvement,Major,util,Enhance distcp to handle *http* as a 'source protocol'.,Requirements:  Presently distcp recursively copies a directory from one dfs to another i.e. both source and destination of of the dfs protocol.  Enhance it to handle http as the source protocol i.e. support copying files from arbitrary http-based sources into the dfs.Design:  Follow distcp's current design: one map task per file which needs to be copied.  Caveat: distcp handles recursive copying by listing sub-directories; this is not as feasible with a http-based source since things like 'fancy-indexing' might not be enabled on the web-server (for all sub-locations recursively too); and even if it is enabled it will mean tedious parsing of the html served to glean the sub-directories etc. Hence the idea is to support an input file (via a -f option) which contains a list of the http-based urls which represent multiple source files.,Closed,Fixed,,Arun C Murthy,Arun C Murthy,Mon; 3 Jul 2006 18:49:37 +0000,Thu; 2 May 2013 02:28:59 +0000,Tue; 18 Jul 2006 12:00:42 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-341
HADOOP-342,New Feature,Major,,Design/Implement a tool to support archival and analysis of logfiles.,Requirements:  a) Create a tool support archival of logfiles (from diverse sources) in hadoop's dfs.  b) The tool should also support analysis of the logfiles via grep/sort primitives. The tool should allow for fairly generic pattern 'grep's and let users 'sort' the matching lines (from grep) on 'columns' of their choice.  E.g. from hadoop logs: Look for all log-lines with 'FATAL' and sort them based on timestamps (column x)  and then on column y (column x; followed by column y).Design/Implementation:  a) Log Archival    Archival of logs from diverse sources can be accomplished using the distcp tool (HADOOP-341).  b) Log analysis    The idea is to enable users of the tool to perform analysis of logs via grep/sort primitives.    This can be accomplished via a relatively simple Map-Reduce task where the map does the grep for the given pattern via RegexMapper and then the implicit sort (reducer) is used with a custom Comparator which performs the user-specified comparision (columns).     The sort/grep specs can be fairly powerful by letting the user of the tool use java's in-built regex patterns (java.util.regex).,Closed,Fixed,,Unassigned,Arun C Murthy,Mon; 3 Jul 2006 19:26:17 +0000,Thu; 2 May 2013 02:28:59 +0000,Tue; 18 Jul 2006 12:06:28 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-342
HADOOP-343,Bug,Major,,In case of dead task tracker; the copy mapouts try copying all mapoutputs from this tasktracker,In case of a dead task tracker; the reduces which do not have the updated map out locations try copygin files from this node and since there are failures on copying; this leads to backoff and slowing down of the copy pahse.,Closed,Fixed,,Sameer Paranjpye,Mahadev konar,Thu; 6 Jul 2006 01:07:46 +0000,Wed; 8 Jul 2009 16:51:50 +0000,Wed; 4 Oct 2006 17:25:57 +0000,,0.6.2,,,HADOOP-248,https://issues.apache.org/jira/browse/HADOOP-343
HADOOP-344,Bug,Major,,TaskTracker passes incorrect file path to DF under cygwin,The path that is passed is OS dependent. The File abstraction should be used in order to make it universal.I'm attaching a patch that does that.We might want to change the path parameter for DF. Making it File rather than String should prevent us from bugs like that in the future.,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Thu; 6 Jul 2006 05:12:06 +0000,Fri; 4 Aug 2006 22:22:37 +0000,Tue; 25 Jul 2006 08:59:49 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-344
HADOOP-345,Improvement,Major,,JobConf access to name-values,"class JobConf (or its base class Configuration) should be extended to enable enumeration of all its key-value pairs.( more precisely: the Properties returned by Configuration.getProps() )This will be useful to ""export"" all JobConf properties to environment variables.We use env.vars to expose some Hadoop context to non-Java MapReduce applications.Note that the typed properties are also represented as Strings (getInt; getStrings; getClass; etc.)So a single enumeration exposes everything as (untyped) environment variables.The proposed escaping rules from JobConf properties to env.var are:1. values are left as-is.2. keys are escaped as follows:A-Za-z0-9 -- unchanged.all other chars -- underscore.For exampleset(""mapred.input.key.class""; ""com.example.MyKey"")becomes env.var:export mapred_input_key_class=com.example.MyKeyJustification:1. Environment variables are case-sensitive. (Although uppercase is the preferred convention)  So no need to uppercase everything.2. Some characters are forbidden in env.vars; or at least not shell-friendly:For example period; colon are problematic.3. The Hadoop conventions are already hierarchical and provide some namespace protection.   This means we don't need an additional prefix as protection.   For example all exported environment variables will start with ""mapred."" ; ""dfs."" ; ""ipc."" etc.  This means they will not conflict with standard environemnt variables like PATH; USER; etc.  And they will not conflict with standard hadoop env.vars because those are upper-case. (like HADOOP_CONF_DIR)",Closed,Fixed,,Michel Tourn,Michel Tourn,Thu; 6 Jul 2006 07:41:37 +0000,Fri; 4 Aug 2006 22:22:37 +0000,Tue; 1 Aug 2006 20:14:02 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-345
MAPREDUCE-221,New Feature,Major,,Generic 'Sort' Infrastructure for Map-Reduce framework.,"It would be useful to add a generic sort infrastructure to the Map-Reduce framework to ease usage.Specifically the idea to add a fairly generic and powerful comparator which can be configured by the user to meet his specific needs.Spec:--------  The proposal is to model generic (uber) comparator along the lines of the the standard unix sort command. The comparator provides the following (configurable) functionality:  a) Separator for breaking up the data (stream) into 'columns'.  b) Multiple key ranges for specifying priorities of 'columns'. (ala --keys/-k option of unix sort i.e. -k 2;3 -k 1;4 etc.)  c) A variant of a) to let user specify byte range-boundaries without using a separator for 'columns'.  d) Option to sort 'reverse'.  e) Option to do a 'stable' sort i.e. don't do a last-ditch comparision of all bytes if all key ranges match.  f) Option to do 'numeric' comparisions instead of lexicographical comparisions?  Of course all these are optional with the default behaviour as-is today.	* - * - Anything more/less?thanks;Arun",Resolved,Incomplete,,Arun C Murthy,Arun C Murthy,Thu; 6 Jul 2006 12:57:13 +0000,Mon; 16 Jan 2012 09:16:08 +0000,Mon; 16 Jan 2012 09:16:08 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-221
HADOOP-347,New Feature,Major,,Implement HDFS content browsing interface,Implement HDFS content browsing interface over HTTP. Clients would connect to the NameNode and this would send a redirect to a random DataNode. The DataNode; via dfs client; would proxy to namenode for metadata browsing and to other datanodes for content. One can also view the local blocks on any DataNode. Head; Tail will be provided as shorthands for viewing the first block and the last block of a file. For full file viewing; the data displayed per HTTP request will be a block with a PREV/NEXT link. The block size for viewing can be a configurable parameter (the user sets it via the web browser) to the HTTP server (e.g.; 256 KB can be the default block size for viewing files).,Closed,Fixed,,Devaraj Das,Devaraj Das,Thu; 6 Jul 2006 17:37:56 +0000,Thu; 2 May 2013 02:28:58 +0000,Wed; 19 Jul 2006 09:39:45 +0000,,0.1.0;0.1.1;0.2.0;0.2.1;0.3.0;0.3.1;0.3.2;0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-347
HADOOP-348,Bug,Minor,,fs.default.name default not working,"in NameNode:84 and FSNamesystem:176 there is a line:InetSocketAddress addr = DataNode.createSocketAddr(conf.get(""fs.default.name""; ""local""));This won't ever work as createSocketAddr first checks to make sure there is a : in the string to get the server name and port. It is not a super simple fix of just putting in a :50000 (or something like that) because the NameNode constructor allows you to pass in a port that would need to match the port the in the conf setting. I think the method public NameNode(File dir; int port; Configuration conf) should simply be deprecated.",Closed,Invalid,,Sameer Paranjpye,Barry Kaplan,Fri; 7 Jul 2006 01:26:41 +0000,Wed; 8 Jul 2009 16:41:57 +0000,Thu; 18 Dec 2008 23:23:14 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-348
HADOOP-349,Improvement,Major,conf,Allow info server to be turned off/on by conf file,"Since I am using hadoop within my own servlet which is not Jetty; it would be nice to not have to need Jetty to run my servlet. As such I propose adding an if statement/donf setting at FSNamesystem:171 as such         if(conf.getBoolean(""dfs.info.active"";true)){            this.infoPort = conf.getInt(""dfs.info.port""; 50070);            this.infoServer = new StatusHttpServer(""dfs""; infoPort; false);            this.infoServer.start();}",Closed,Won't Fix,,Doug Cutting,Barry Kaplan,Fri; 7 Jul 2006 01:29:18 +0000,Thu; 23 Apr 2009 19:24:57 +0000,Fri; 16 Jan 2009 05:14:05 +0000,,,,HADOOP-353,,https://issues.apache.org/jira/browse/HADOOP-349
HADOOP-350,Bug,Minor,,In standalone mode; 'org.apache.commons.cli cannot be resolved',"Standalone works fine in 0.4.0 but if I use TRUNK; 'Last Changed Date: 2006-06-28 14:27:33 -0700 (Wed; 28 Jun 2006)'; and pass it my fat job jar; I get following exception.06/07/06 13:18:02 INFO conf.Configuration: parsing file:/tmp/hadoop-unjar29923/nutch-site.xml06/07/06 13:18:02 INFO conf.Configuration: parsing file:/home/stack/workspace/hadoop-local-conf/hadoop-site.xmlException in thread ""main"" java.lang.Error: Unresolved compilation problems:           The import org.apache.commons.cli cannot be resolved        The import org.apache.commons.cli cannot be resolved        The import org.apache.commons.cli cannot be resolved        The import org.apache.commons.cli cannot be resolved               The import org.apache.commons.cli cannot be resolved        The import org.apache.commons.cli cannot be resolved        The import org.apache.commons.cli cannot be resolved        The import org.apache.commons.cli cannot be resolved        The import org.apache.commons.cli cannot be resolved        Options cannot be resolved to a type        Option cannot be resolved to a type        OptionBuilder cannot be resolved        Option cannot be resolved to a type        OptionBuilder cannot be resolved        Option cannot be resolved to a type        OptionBuilder cannot be resolved        Option cannot be resolved to a type        OptionBuilder cannot be resolved        Options cannot be resolved to a type        Options cannot be resolved to a type        CommandLine cannot be resolved to a type        Options cannot be resolved to a type        The method buildGeneralOptions() is undefined for the type ToolBase        CommandLineParser cannot be resolved to a type        GnuParser cannot be resolved to a type        CommandLine cannot be resolved to a type        ParseException cannot be resolved to a type        e cannot be resolved        HelpFormatter cannot be resolved to a type        HelpFormatter cannot be resolved to a type        at org.apache.hadoop.util.ToolBase.init(ToolBase.java:21)        at org.apache.hadoop.mapred.JobClient.init(JobClient.java:178)        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:320)        at org.archive.access.nutch.ImportArcs.importArcs(ImportArcs.java:576)        at org.archive.access.nutch.Nutchwax.doImport(Nutchwax.java:159)        at org.archive.access.nutch.Nutchwax.doAll(Nutchwax.java:144)        at org.archive.access.nutch.Nutchwax.doJob(Nutchwax.java:368)        at org.archive.access.nutch.Nutchwax.main(Nutchwax.java:621)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        at java.lang.reflect.Method.invoke(Method.java:585)        at org.apache.hadoop.util.RunJar.main(RunJar.java:128)I took a bit of a look.  Odd is that command-cli.jar is in hadoop/lib dir so should be found no problem (and we seem to be loading other stuff out of the uncompressed job jar fine  see mention of nutch-site.xml above).   I tried adding the command-cli to my job jar.  I printed out all that RunJar  see below  was adding to its CLASSPATH and saw mention of command-cli but still no workee.file:/tmp/hadoop-unjar29923/lib/archive-commons-1.8.0.jarfile:/tmp/hadoop-unjar29923/lib/commons-codec-1.3.jarfile:/tmp/hadoop-unjar29923/lib/commons-httpclient-3.0-rc3.jarfile:/tmp/hadoop-unjar29923/lib/commons-logging-1.0.4.jarfile:/tmp/hadoop-unjar29923/lib/dsi.unimi.it-1.2.0.jarfile:/tmp/hadoop-unjar29923/lib/commons-lang-2.1.jarfile:/tmp/hadoop-unjar29923/lib/commons-cli-2.0-SNAPSHOT.jarfile:/tmp/hadoop-unjar29923/lib/lucene-core-1.9.1.jarfile:/tmp/hadoop-unjar29923/lib/lucene-misc-1.9.1.jarfile:/tmp/hadoop-unjar29923/lib/jakarta-oro-2.0.7.jarfile:/tmp/hadoop-unjar29923/lib/xerces-2_6_2-apis.jarfile:/tmp/hadoop-unjar29923/lib/xerces-2_6_2.jarfile:/tmp/hadoop-unjar29923/lib/concurrent-1.3.4.jar06/07/06 13:18:01 INFO conf.Configuration: parsing file:/home/stack/workspace/hadoop-local-conf/hadoop-default.xml Any ideas?Thanks.",Closed,Fixed,,Unassigned,stack,Fri; 7 Jul 2006 03:42:34 +0000,Fri; 4 Aug 2006 22:22:38 +0000,Tue; 18 Jul 2006 09:18:08 +0000,,0.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-350
HADOOP-351,Wish,Major,ipc,Remove Jetty dependency,Somewhat related to HADOOP-349; it would be nice to not have a Jetty dependency for those of us embedding Hadoop within our own web applications. In particular the Server object is using SocketChannelOutputStream from the Jetty code base. It seems to me that for an object like this it would be better to simply have a Hadoop version of a blocking output stream on an nio channel if necessary vs. using the Jetty version requiring a Hadoop user's to package yet another jar file (and all the complications that are associated with that).,Closed,Fixed,,Devaraj Das,Barry Kaplan,Fri; 7 Jul 2006 05:25:02 +0000,Fri; 4 Aug 2006 22:22:38 +0000,Wed; 19 Jul 2006 12:11:30 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-351
HADOOP-352,Bug,Major,,Portability of hadoop shell scripts for deployment,"Hadoop shell scripts are based on /bin/bash; which is a ""standard"" shell only on GNU/Linux.  On other Unix systems like FreeBSD however the ""standard"" shell is /bin/sh.  The attached patch addresses these compatiblity issues.  Note that Solaris support is not yet tested.Also; the best way to set the HADOOP_HOME variable upon ssh connection in a portable way is to set it in .ssh/environment.  The bash startup script "".bashrc"" is not an option on systems where ""bash"" is not installed.",Closed,Fixed,,Unassigned,Jean-Baptiste Quenot,Fri; 7 Jul 2006 16:09:20 +0000,Mon; 21 Aug 2006 19:35:54 +0000,Tue; 11 Jul 2006 15:06:47 +0000,,0.4.0,,,HADOOP-469,https://issues.apache.org/jira/browse/HADOOP-352
HADOOP-353,Improvement,Major,,Run datanode (or other hadoop servers) inside tomcat,Barry Kaplan is running hadoop data nodes inside tomcat and encountering some issues.http://issues.apache.org/jira/browse/HADOOP-211#action_12419360I'm filing this bug to capture discussion about the pros and cons of such an approach.  I'd be curious to know what others on the list (who know more about java/tomcat) think about this proposal.,Resolved,Won't Fix,,Unassigned,eric baldeschwieler,Fri; 7 Jul 2006 23:27:49 +0000,Fri; 18 Jul 2014 04:58:44 +0000,Fri; 18 Jul 2014 04:58:44 +0000,,,,HADOOP-349,,https://issues.apache.org/jira/browse/HADOOP-353
HADOOP-354,Bug,Major,,All daemons should have public methods to start and stop them,To get tomcat working shutdown working properly I needed to make the NameNode stop() method public as well as the DataNode shutdown() method public. (will attach patch file).Furthermore I noticed that the RPC object has a static client reference that you have no way of stopping; so I added one. (will attach patch),Closed,Fixed,,Unassigned,Barry Kaplan,Sat; 8 Jul 2006 00:25:02 +0000,Wed; 8 Jul 2009 16:41:57 +0000,Fri; 14 Jul 2006 15:45:31 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-354
HADOOP-355,Improvement,Major,,hadoopStreaming: fix APIs; -reduce NONE; StreamSequenceRecordReader,This patch fixes outstanding problems with streaming:fix APIsoption -reduce NONEsupport StreamSequenceRecordReader as an input,Closed,Fixed,,Unassigned,Michel Tourn,Sat; 8 Jul 2006 04:57:30 +0000,Wed; 8 Jul 2009 17:05:35 +0000,Tue; 11 Jul 2006 15:56:48 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-355
HADOOP-356,Improvement,Major,,Build and test hadoopStreaming nightly,hadoopStreaming was originally built and tested separately.It makes sense to keep building the hadoopStreaming code as a separate jar file:this is all client code that can be uploaded in the Job jar.this allows quick turnaround on production clusters (hadoopStreaming code updates do not require to bring down the MapReduce system.If necessary users can use their own modified versions of the hadoopStreaming jar but still run on the production cluster.)On the other hand it makes sense to build this code nightly.Many recent changes broke either compilation or correctness.All the problems would have been caught if the hadoopStreaming compilation and tests were run nighlty.Conclusion:the updated top-level build.xml adds the following dependencies:1. Target compile calls target compile in contrib/streaming/build.xml2. Target test calls target test in contrib/streaming/build.xml,Closed,Fixed,,Unassigned,Michel Tourn,Sat; 8 Jul 2006 05:14:38 +0000,Fri; 4 Aug 2006 22:22:41 +0000,Tue; 18 Jul 2006 10:37:21 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-356
HADOOP-357,Bug,Major,,hadoop doesn't handle 0 reduces,We have cases where we want to use maps for submitting parallel jobs and don't need any reduces. Currently; you are required to have a single reducer. It would be nicer if the framework would let you specify that you don't need any reducers.,Closed,Duplicate,NULL,Unassigned,Owen O'Malley,Sat; 8 Jul 2006 06:25:50 +0000,Wed; 8 Jul 2009 16:51:54 +0000,Thu; 24 May 2007 06:22:24 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-357
HADOOP-358,Bug,Major,fs,NPE in Path.equals,An NPE is raised in Path.equals when testing the method with two unequal pathes and with the first one having no drive.This is due to operator precedence: &amp; has a higher priority level than ?:See http://java.sun.com/docs/books/tutorial/java/nutsandbolts/expressions.htmlSee attached patch (just added some parenthesis and a testcase).,Closed,Fixed,,Doug Cutting,Fr√©d√©ric Bertin,Mon; 10 Jul 2006 22:52:31 +0000,Fri; 4 Aug 2006 22:22:41 +0000,Tue; 11 Jul 2006 16:24:30 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-358
HADOOP-359,New Feature,Major,,add optional compression of map outputs,Currently; there is no way to request that the transient data be compressed.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 12 Jul 2006 10:18:47 +0000,Wed; 8 Jul 2009 16:51:50 +0000,Wed; 12 Jul 2006 18:49:46 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-359
HADOOP-360,Bug,Major,,hadoop-daemon starts but does not stop servers under cygWin,Latest changes HADOOP-352 to the hadoop scripts made them incompatible with cygwin.The servers start fine; but when  I try to stop them the script reports there is nothing to stop. Which in turn messes up consequent server starts.,Closed,Fixed,,Unassigned,Konstantin Shvachko,Wed; 12 Jul 2006 14:02:50 +0000,Fri; 4 Aug 2006 22:22:42 +0000,Wed; 12 Jul 2006 18:48:42 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-360
HADOOP-361,Improvement,Major,,junit with pure-Java hadoopStreaming combiner; remove CRLF in some files,"junit with pure-Java hadoopStreaming combinerremove CRLF in some filesWhen this is committed; the hadoopStreaming tests should be pure-Java.This implies that hadoopStreaming can safely be built nightly without plaftorm dependencies.After I check that this is the case I will update the build.xml in this issue:HADOOP-356  	 Build and test hadoopStreaming nightly",Closed,Fixed,,Unassigned,Michel Tourn,Thu; 13 Jul 2006 04:29:44 +0000,Wed; 8 Jul 2009 17:05:35 +0000,Fri; 14 Jul 2006 14:53:55 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-361
HADOOP-362,Bug,Major,,tasks can get lost when reporting task completion to the JobTracker has an error,Basically; the JobTracker used to lose some updates about successful map tasks and it would assume that the tasks are still running (the old progress report is what it used to display in the web page). Now this would cause the reduces to also wait for the map output and they would never receive the output. This would cause the job to appear as if it was hung.The following piece of code sends the status of tasks to the JobTracker:            synchronized (this) {                for (Iterator it = runningTasks.values().iterator();                     it.hasNext(); ) {                    TaskInProgress tip = (TaskInProgress) it.next();                    TaskStatus status = tip.createStatus();                    taskReports.add(status);                    if (status.getRunState() != TaskStatus.RUNNING) {                        if (tip.getTask().isMapTask()) {                            mapTotal--;                        } else {                            reduceTotal--;                        }                        it.remove();                    }                }            }             //            // Xmit the heartbeat            //                       TaskTrackerStatus status =              new TaskTrackerStatus(taskTrackerName; localHostname;                                    httpPort; taskReports;                                    failures);            int resultCode = jobClient.emitHeartbeat(status; justStarted);  Notice that the completed TIPs are removed from runningTasks data structure. Now; if the emitHeartBeat threw an exception (if it could not communicate with the JobTracker till the IPC timeout expires) then this update is lost. And the next time it sends the hearbeat this completed task's status is missing and hence the JobTracker doesn't know about this completed task. So; one solution to this is to remove the completed TIPs from runningTasks after emitHeartbeat returns. Here is how the new code would look like:              synchronized (this) {                for (Iterator it = runningTasks.values().iterator();                     it.hasNext(); ) {                    TaskInProgress tip = (TaskInProgress) it.next();                    TaskStatus status = tip.createStatus();                    taskReports.add(status);                }            }             //            // Xmit the heartbeat            //             TaskTrackerStatus status =              new TaskTrackerStatus(taskTrackerName; localHostname;                                    httpPort; taskReports;                                    failures);            int resultCode = jobClient.emitHeartbeat(status; justStarted);            synchronized (this) {                for (Iterator it = runningTasks.values().iterator();                     it.hasNext(); ) {                    TaskInProgress tip = (TaskInProgress) it.next();                    if (tip.runstate != TaskStatus.RUNNING) {                        if (tip.getTask().isMapTask()) {                            mapTotal--;                        } else {                            reduceTotal--;                        }                        it.remove();                    }                }            },Closed,Fixed,,Owen O'Malley,Devaraj Das,Thu; 13 Jul 2006 05:39:21 +0000,Wed; 8 Jul 2009 16:51:50 +0000,Mon; 31 Jul 2006 20:07:02 +0000,,,,HADOOP-312,,https://issues.apache.org/jira/browse/HADOOP-362
MAPREDUCE-264,Improvement,Critical,,When combiners exist; postpone mappers' spills of map output to disk until combiners are unsuccessful.,When a map/reduce job is set up with a combiner; the mapper tasks each build up an in-heap collection of 100K key/value pairs  and then apply the combiner to reduce that to whatever it becomes by applying the combiner to sets with like keys before spilling to disk to send it to the reducers.Typically running the combiner consumes a lot less resources than shipping the data; especially since the data end up in a reducer where probably the same code will be run anyway.I would like to see this changed so that when the combiner shrinks the 100K key/value pairs to less than; say; 90K; we just keep running the mapper and combiner alternately until we get enough distinct keys to make this unlikely to be worthwhile or until we run out of input; of course.This has two costs: the whole internal buffer has to be re-sorted so we can apply the combiner even though as few as 10K new elements have been added; and in some cases we'll call the combiner on many singletons.  The first of these costs can be avoided by doing a mini-sort in the new pairs section and doing a merge to develop the combiner sets and the new sorted retained elements section.The second of these costs can be avoided by detecting what would otherwise be singleton combiner calls and not making them; which is a good idea in itself even if we don't decide to do this reform.The two techniques combine well; recycled elements of the buffer need not be combined if there's no new element with the same key.-dk,Resolved,Won't Fix,,Owen O'Malley,Dick King,Fri; 14 Jul 2006 17:15:12 +0000,Mon; 16 Jan 2012 09:19:39 +0000,Mon; 16 Jan 2012 09:19:39 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-264
HADOOP-364,Bug,Major,ipc,rpc versioning broke out-of-order server launches,The change to check the RPCs broke the ability to bring up the datanodes before the namenode and the tasktracker before the jobtracker.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 14 Jul 2006 22:10:02 +0000,Fri; 4 Aug 2006 22:22:43 +0000,Tue; 18 Jul 2006 09:05:52 +0000,,0.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-364
HADOOP-365,Bug,Major,,datanode crashes on startup with ClassCastException,"When I bring up a DataNode it gets a ClassCastException:Exception in thread ""main"" java.lang.ClassCastException: java.util.HashMap$Entry        at org.apache.hadoop.dfs.DataNode.runAndWait(DataNode.java:907)        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:954)This was introduced by the patch for HADOOP-354.",Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 14 Jul 2006 23:33:16 +0000,Wed; 8 Jul 2009 16:41:57 +0000,Tue; 18 Jul 2006 09:14:24 +0000,,0.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-365
HADOOP-366,Improvement,Major,,Should be able to specify more than one jar into a JobConf file,A job should be able to specify more than one jar file into its JobConf file because sometimes custom Map and Reduce classes or just InputFormat classes uses objects coming from other jar files. For now; we have to build a unique jar to make Hadoop mapreduce operations works.,Closed,Fixed,HADOOP-1622,Owen O'Malley,Thomas Friol,Mon; 17 Jul 2006 12:13:24 +0000,Wed; 8 Jul 2009 16:51:50 +0000,Tue; 24 Jul 2007 07:46:54 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-366
HADOOP-367,Bug,Major,,Static blocks do not automatically run when a class is loaded in Java 5.0,"There seems to be a change that happened between 1.4 and 1.5 with respect to static initializers. I can't find this documented; but I can reproduce with a very simple program. Basically; a static initializer is not called unless a static member/method of the class is accessed or an instance is created. This is actually what the JLS says; but until 1.5 the static initializers ran when the class was loaded. Note that this behavior only occurs when running with the 1.5 JRE AND compiling for 1.5.For many Writables this isn't an issue; so the fallback behavior of the WritableFactory works; but Block is package private; so loadEdits fails when called from org.apache.hadoop.io.ArrayWritable.readFields() yielding the following trace:Caused by: java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)        at org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:81)        at org.apache.hadoop.dfs.FSDirectory.loadFSEdits(FSDirectory.java:532)        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:470)        at org.apache.hadoop.dfs.FSDirectory.init(FSDirectory.java:307)        at org.apache.hadoop.dfs.FSNamesystem.init(FSNamesystem.java:177)        at org.apache.hadoop.dfs.NameNode.init(NameNode.java:91)        at org.apache.hadoop.dfs.NameNode.init(NameNode.java:84)        at org.apache.hadoop.dfs.NameNode.main(NameNode.java:491)",Closed,Duplicate,HADOOP-104,Unassigned,Benjamin Reed,Mon; 17 Jul 2006 22:12:26 +0000,Thu; 2 May 2013 02:28:59 +0000,Fri; 8 Dec 2006 04:46:34 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-367
HADOOP-368,Bug,Minor,fs,DistributedFSCheck should cleanup; seek; and report missing files.,,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Tue; 18 Jul 2006 02:26:33 +0000,Fri; 4 Aug 2006 22:22:44 +0000,Tue; 18 Jul 2006 11:37:36 +0000,,0.3.2;0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-368
HADOOP-369,New Feature,Trivial,,Added ability to copy all part-files into one output file,Since we use the hadoop output in non-hadoop applications it's nice to be able to merge the part-files into one output file on the local filesystem.So I've added a dfsshell feature that streams from all files in a directory to one output file.,Closed,Fixed,,Johan Oskarsson,Johan Oskarsson,Tue; 18 Jul 2006 16:39:26 +0000,Wed; 8 Jul 2009 16:41:58 +0000,Tue; 1 Aug 2006 20:39:31 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-369
MAPREDUCE-134,Bug,Major,,TaskTracker startup fails if any mapred.local.dir entries don't exist,"This appears to have been introduced with the ""check for enough free space"" before startup.It's debatable how best to fix this bug. I will submit a patch which ignores directories for which the DF utility fails. This is letting me continue operation on my cluster (where the number of drives varies; so there are entries in mapred.local.dir for drives that aren't on all cluster nodes); but a cleaner solution is probably better. I'd lean towards ""check for existence""; and ignore the dir if it doesn't  - but don't depend on DF to fail; since DF could fail for other reasons without meaning you're out of disk space. I argue that a TaskTracker should start up if all directories that can be written to in the list have enough space. Otherwise; a failed drive per cluster machine means no work ever gets done.",Resolved,Duplicate,MAPREDUCE-2413,Ravi Gummadi,Bryan Pendleton,Tue; 18 Jul 2006 18:10:53 +0000,Mon; 16 Jan 2012 09:21:39 +0000,Mon; 16 Jan 2012 09:21:03 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-134
HADOOP-371,Improvement,Major,build,ant tar should package contrib jars,"From Hadoop-356:I note that the contrib packages are not included in distributions (the ""tar"" target).  They probably should be.  Michel; would you like to modify ""tar"" to include the contrib code?  This should be done in a separate bug.OK.This packaging is done in target deploy-contrib.So I can just add a dependency to the top-level tar target:!-- Make release tarball(s)                                               --target name=""tar"" depends=""package; deploy-contrib""",Closed,Fixed,,Nigel Daley,Michel Tourn,Tue; 18 Jul 2006 18:51:43 +0000,Sat; 3 Feb 2007 03:26:57 +0000,Thu; 4 Jan 2007 17:45:35 +0000,,0.9.2,,,,https://issues.apache.org/jira/browse/HADOOP-371
HADOOP-372,New Feature,Major,,should allow to specify different inputformat classes for different input dirs for Map/Reduce jobs,Right now; the user can specify multiple input directories for a map reduce job. However; the files under all the directories are assumed to be in the same format; with the same key/value classes. This proves to be  a serious limit in many situations. Here is an example. Suppose I have three simple tables: one has URLs and their rank values (page ranks); another has URLs and their classification values; and the third one has the URL meta data such as crawl status; last crawl time; etc. Suppose now I need a job to generate a list of URLs to be crawled next. The decision depends on the info in all the three tables.Right now; there is no easy way to accomplish this.However; this job can be done if the framework allows to specify different inputformats for different input dirs.Suppose my three tables are in the following directory respectively: rankTable; classificationTable. and metaDataTable. If we extend JobConf class with the following method (as Owen suggested to me):    addInputPath(aPath; anInputFormatClass; anInputKeyClass; anInputValueClass)Then I can specify my job as follows:    addInputPath(rankTable; SequenceFileInputFormat.class; UTF8.class; DoubleWritable.class)    addInputPath(classificationTable; TextInputFormat.class; UTF8;class; UTF8.class)    addInputPath(metaDataTable; SequenceFileInputFormat.class; UTF8.class; MyRecord.class)If an input directory is added through the current API; it will have the same meaning as it is now. Thus this extension will not affect any applications that do not need this new feature.It is relatively easy for the M/R framework to create an appropriate record reader for a map task based on the above information.And that is the only change needed for supporting this extension.,Closed,Fixed,HADOOP-450,Chris Smith,Runping Qi,Wed; 19 Jul 2006 18:02:05 +0000,Thu; 2 May 2013 02:29:00 +0000,Fri; 18 Jul 2008 10:28:16 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-372
HADOOP-373,Bug,Major,,Some calls to mkdirs do not check return value,Some calls to mkdirs do not check if mkdirs failed.  Arun noticed this while looking at HADOOP-281.  Since this issue is actually a separate issue from the mkdirs implementation issue of HADOOP-281; I created a new bug for it.,Closed,Fixed,,Wendy Chien,Wendy Chien,Wed; 19 Jul 2006 23:25:36 +0000,Wed; 8 Jul 2009 16:41:58 +0000,Thu; 2 Nov 2006 02:37:06 +0000,,0.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-373
HADOOP-374,New Feature,Major,,native support for gzipped text files,in many cases it is convenient to store text files in dfs as gzip compressed files.It would be good to have built in support for processing these files in a mapreduce job.The getSplits implementation should return a single split per input file; ignoring the numSplits parameter.One can probably subclass InputFormatBase; and the getSplits method can simply call listPaths() and then construct and return a single split per path returned.The code for reading would look something like (courtesy of Vijay Murthy):   public RecordReader getRecordReader(FileSystem fs; FileSplit split;                                       JobConf job; Reporter reporter)     throws IOException {     final BufferedReader in =       new BufferedReader(new InputStreamReader         (new GZIPInputStream(fs.open(split.getPath()))));     return new RecordReader() {         long position;         public synchronized boolean next(Writable key; Writable value)           throws IOException {           String line = in.readLine();           if (line != null) {             position += line.length();             ((UTF8)value).set(line);             return true;           }           return false;         }         public synchronized long getPos() throws IOException {           return position;         }        public synchronized void close() throws IOException {           in.close();         }       };   },Closed,Fixed,HADOOP-474,Unassigned,Yoram Arnon,Thu; 20 Jul 2006 18:14:44 +0000,Wed; 8 Jul 2009 16:51:50 +0000,Tue; 26 Sep 2006 05:42:00 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-374
HADOOP-375,Bug,Major,,Introduce a way for datanodes to register their HTTP info ports with the NameNode,"If we have multiple datanodes within a single machine the Jetty servers (other than the first one) won't be able to bind to the fixed HTTP port. So; one solution is to have the datanodes pick a free port (starting from a configured port value) and then inform namenode about it so that the namenode can then do redirects; etc.Johan Oskarson reported this problem. If a computer have a second dfs data dir in the config it doesn't start properly because of:Exception in thread ""main"" java.io.IOException: Problem starting http server        at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:182)        at org.apache.hadoop.dfs.DataNode.init(DataNode.java:170)        at org.apache.hadoop.dfs.DataNode.makeInstanceForDir(DataNode.java:1045)        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:999)        at org.apache.hadoop.dfs.DataNode.runAndWait(DataNode.java:1015)        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:1066)Caused by: org.mortbay.util.MultiExceptionjava.net.BindException: Address already in use        at org.mortbay.http.HttpServer.doStart(HttpServer.java:731)        at org.mortbay.util.Container.start(Container.java:72)        at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:159)        ... 5 more",Closed,Fixed,,Devaraj Das,Devaraj Das,Fri; 21 Jul 2006 10:48:37 +0000,Wed; 8 Jul 2009 16:41:58 +0000,Wed; 26 Jul 2006 12:13:19 +0000,,0.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-375
HADOOP-376,Bug,Major,,Datanode does not scan for an open http port,The DataNode does not scan for an open http port. Only the singleton servers are allowed to have required port assignments.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Fri; 21 Jul 2006 15:24:57 +0000,Thu; 2 May 2013 02:28:58 +0000,Mon; 24 Jul 2006 08:54:18 +0000,,0.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-376
HADOOP-377,Bug,Major,,Configuration does not handle URL,"Current Configuration allows:	String pointing to a resource in the classpath	Path local path on the file systemThe attached patch handles java.net.URL.  We use it to load hadoop-client.xml from a JAR.Thanks in advance!",Closed,Fixed,,Unassigned,Jean-Baptiste Quenot,Fri; 21 Jul 2006 17:49:33 +0000,Fri; 4 Aug 2006 22:22:46 +0000,Tue; 1 Aug 2006 19:17:30 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-377
HADOOP-378,Bug,Trivial,conf,too many files are distributed to slaves nodes,currently all the svn tree is sync'ed to the slaves on startup; excluding only .svn files.that includes the example jar file; sources; and other files that are not required.It would  be better to pick and choose the files that get sync'ed. It will improve sync times on startup; and can prevent some name conflicts; since all the jar files are also included in the classpath by the hadoop script.,Resolved,Incomplete,,Unassigned,Yoram Arnon,Fri; 21 Jul 2006 19:08:22 +0000,Sat; 16 Jul 2011 16:46:48 +0000,Sat; 16 Jul 2011 16:46:47 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-378
MAPREDUCE-321,Improvement,Major,,provide progress feedback while the reducer is sorting,during the sort phase of reduce; the progress is stuck at 33%; like:tip_0132_r_000000 0.33333334 reduce  sort For some jobs this takes a long time; during which it's not clear if the job is making any progress.Some progress indication would be good.,Open,Unresolved,,Unassigned,Yoram Arnon,Fri; 21 Jul 2006 22:47:04 +0000,Mon; 16 Jan 2012 09:38:16 +0000,,,,newbie,,,https://issues.apache.org/jira/browse/MAPREDUCE-321
HADOOP-380,Bug,Major,,The reduce tasks poll for mapoutputs in a loop,The Reduce tasks poll for the mapoutputs in a loop. The polling thread should be sleeping for 5 seconds before polling again but there is a bug in updating the timestamps which make the reduce task poll in a loop without sleeping.,Closed,Fixed,,Mahadev konar,Mahadev konar,Fri; 21 Jul 2006 23:18:22 +0000,Wed; 8 Jul 2009 16:51:51 +0000,Tue; 25 Jul 2006 09:21:18 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-380
HADOOP-381,New Feature,Major,,keeping files for tasks that match regex on task id,For debugging map/reduce jobs; if a single task is producing bad results; but not failing; it is hard to debug the problem. This patch lets you set a pattern for task ids that will keep their files from being deleted when the task and job complete. This allows the developer to run the task in the IsolationRunner under the debugger.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Sat; 22 Jul 2006 00:03:20 +0000,Wed; 8 Jul 2009 16:51:51 +0000,Tue; 25 Jul 2006 07:55:59 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-381
HADOOP-382,Test,Major,,add a unit test for multiple datanodes in a machine,recently we saw a bug present itself only when multiple data nodes are started on a single machine.A unit test that starts multiple data nodes would expose such bugs before they happen in a real installation.,Closed,Fixed,,Milind Bhandarkar,Yoram Arnon,Mon; 24 Jul 2006 18:41:00 +0000,Wed; 8 Jul 2009 16:41:59 +0000,Wed; 8 Nov 2006 21:35:13 +0000,,0.8.0,,,,https://issues.apache.org/jira/browse/HADOOP-382
HADOOP-383,Bug,Major,,unit tests fail on windows,"When I run the ""ant test"" target under Windows; I get the following exception in the logs for the streaming unit tests. The unit tests need to run under Windows; if it is going to be a supported platform. So we either need to remove the streaming unit tests or make them work.Testsuite: org.apache.hadoop.streaming.TestStreamingTests run: 1; Failures: 1; Errors: 0; Time elapsed: 0.172 secTestcase: testCommandLine took 0.172 sec        FAILEDjava.lang.RuntimeException: Operating system Windows XP not supported by this class        at org.apache.hadoop.streaming.Environment.init(Environment.java:47)        at org.apache.hadoop.streaming.StreamJob.init(StreamJob.java:68)        at org.apache.hadoop.streaming.StreamJob.go(StreamJob.java:55)        at org.apache.hadoop.streaming.TestStreaming.testCommandLine(Unknown Source)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        at java.lang.reflect.Method.invoke(Method.java:585)        at junit.framework.TestCase.runTest(TestCase.java:154)        at junit.framework.TestCase.runBare(TestCase.java:127)        at junit.framework.TestResult$1.protect(TestResult.java:106)        at junit.framework.TestResult.runProtected(TestResult.java:124)        at junit.framework.TestResult.run(TestResult.java:109)        at junit.framework.TestCase.run(TestCase.java:118)        at junit.framework.TestSuite.runTest(TestSuite.java:208)        at junit.framework.TestSuite.run(TestSuite.java:203)        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)",Closed,Fixed,,Michel Tourn,Owen O'Malley,Mon; 24 Jul 2006 20:48:56 +0000,Wed; 8 Jul 2009 17:05:35 +0000,Tue; 25 Jul 2006 09:04:33 +0000,,0.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-383
HADOOP-384,Bug,Minor,fs,improved error messages for file checksum errors,Improves the messages on a couple of the failures we've been seeing to try and get enough information to identifiy the problem.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Mon; 24 Jul 2006 21:43:47 +0000,Fri; 4 Aug 2006 22:22:48 +0000,Tue; 25 Jul 2006 08:25:59 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-384
HADOOP-385,Bug,Major,,rcc does not generate correct Java code for the field of a record type,"If  a field of a Jute record  is also a Jute record and the target language is Java; rcc does not generate correct field name; correct code for deserializing the field. The access modifier ""private"" of the validation method causes problem for validating nested records.In addition; rcc can not handle the case when a comment started with ""//"" also contains the string ""//"".",Closed,Fixed,,Milind Bhandarkar,Hairong Kuang,Tue; 25 Jul 2006 00:38:00 +0000,Fri; 4 Aug 2006 22:22:48 +0000,Wed; 26 Jul 2006 07:40:08 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-385
HADOOP-386,New Feature,Major,,Periodically move blocks from full nodes to those with space,I'm still having a lot of problems with some nodes filling up quickly and others hardly being touched; mostly because of the hardware beingvery different.As someone suggested; there should be a thread that periodically checks the dfs for nodes with little or no free space and schedules blocksto be moved off that node.,Closed,Fixed,,Johan Oskarsson,Johan Oskarsson,Tue; 25 Jul 2006 10:48:55 +0000,Wed; 8 Jul 2009 16:41:58 +0000,Wed; 26 Jul 2006 12:25:54 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-386
HADOOP-387,Bug,Major,,LocalJobRunner assigns duplicate mapid's,"While hunting down nutch issue NUTCH-266 i discovered that id's are generated with following fragment of code:    private String newId() {      return Integer.toString(Math.abs(new Random().nextInt());36);    }and the related Javadoc:""public Random()    Creates a new random number generator. Its seed is initialized to a value based on the current time:         public Random() { this(System.currentTimeMillis()); }    Two Random objects created within the same millisecond will have the same sequence of random numbers.""it appears that in this case there are more than one Random pobject generated at the same millisecond and id's areno longer unique.",Closed,Fixed,,Unassigned,Sami Siren,Tue; 25 Jul 2006 18:39:30 +0000,Wed; 8 Jul 2009 16:51:50 +0000,Wed; 26 Jul 2006 07:29:34 +0000,,0.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-387
HADOOP-388,Bug,Major,,"the hadoop-daemons.sh fails with ""no such file or directory"" when used from a relative path",The new shell scripts fail with a relative directory:% current/bin/hadoop-daemons.sh start datanodenode1: current/bin/..: No such file or directorynode2: current/bin/..: No such file or directoryThe problem is that HADOOP_HOME is set to a relative directory and hadoop-daemons does a cd; breaking the other scripts.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 25 Jul 2006 21:10:10 +0000,Mon; 2 Oct 2006 06:29:18 +0000,Wed; 26 Jul 2006 08:09:53 +0000,,0.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-388
HADOOP-389,Bug,Major,,MiniMapReduce tests get stuck because of some timing issues with initialization of tasktrackers.,The MiniMapReduce tests sometimes calls shutdown before the tasktrackers have been initialized. This makes the TestMiniMRBringup run for ever.,Closed,Fixed,HADOOP-390,Mahadev konar,Mahadev konar,Wed; 26 Jul 2006 18:50:15 +0000,Wed; 8 Jul 2009 16:51:51 +0000,Mon; 31 Jul 2006 19:43:41 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-389
HADOOP-3,,,,,,,,,,,,,,,,,,,
HADOOP-5000,Bug,Major,,A Configuration instance cannot be reloaded if the configuration has InputStream resources,"If a Configuration instance as a stream resource; on an reload of configuration values; the loadResource method will fail because the stream has been already read in the first loadResource invocation.For example: This example will fail on conf.set(""b""; ""B""); because it a loadResources() will be done and the InputStream has been already consumed.",Open,Unresolved,,Unassigned,Alejandro Abdelnur,Fri; 9 Jan 2009 07:20:25 +0000,Mon; 12 Jan 2009 07:57:37 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-5000
HADOOP-5001,Bug,Major,test,Junit tests that time out don't write any test progress related logs,Some junit tests time out frequently possibly because of a bug. When such tests time out; the log4j appender isn't writing anything to the log files. It seems that all the log statements  are buffered in the memory till test completion. The logs get written to the log file only after the test goes to completion.This is seriously limiting debugging in presence of a test time out. If possible; we should try to flush logs regularly so that we can find out the extent to which a test has progressed before timing out.,Closed,Won't Fix,,Vinod Kumar Vavilapalli,Vinod Kumar Vavilapalli,Fri; 9 Jan 2009 08:08:10 +0000,Thu; 23 Apr 2009 19:25:02 +0000,Fri; 16 Jan 2009 11:17:41 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5001
HADOOP-5002,Bug,Blocker,,2 core tests TestFileOutputFormat and TestHarFileSystem are failing in branch 19,"I see 2 of the core tests(TestHarFileSystem and TestFileOutputFormat) failing in branch 19. These are passing in branch 20 and in trunk.I see nullpointer exceptions for the attempts of reducde tasks in the logs(for example; see the following):2009-01-09 13:53:34;112 INFO  mapred.TaskInProgress (TaskInProgress.java:updateStatus(484)) - Error from attempt_200901091353_0001_r_000000_0: java.lang.NullPointerException	at org.apache.hadoop.fs.Path.init(Path.java:61)	at org.apache.hadoop.fs.Path.init(Path.java:50)	at org.apache.hadoop.tools.HadoopArchives$HArchivesReducer.configure(HadoopArchives.java:552)	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:58)	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:83)	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:337)	at org.apache.hadoop.mapred.Child.main(Child.java:155)",Closed,Fixed,,Amareshwari Sriramadasu,Ravi Gummadi,Fri; 9 Jan 2009 09:13:14 +0000,Wed; 8 Jul 2009 16:53:13 +0000,Wed; 21 Jan 2009 06:31:24 +0000,,0.19.1,,,,https://issues.apache.org/jira/browse/HADOOP-5002
HADOOP-5003,Bug,Minor,,When computing absoluet guaranteed capacity (GC) from a percent value; Capacity Scheduler should round up floats; rather than truncate them.,The Capacity Scheduler calculates a queue's absolute GC value by getting its percent of the total cluster capacity (which is a float; since the configured GC% is a float) and casting it to an int. Casting a float to an int always rounds down. For very small clusters; this can result in the GC of a queue being one lower than what it should be. For example; if Q1 has a GC of 50%; Q2 has a GC of 40%; and Q3 has a GC of 10%; and if the cluster capacity is 4 (as we have; in our test cases); Q1's GC works out to 2; Q2's to 1; and Q3's to 0 with today's code. Q2's capacity should really be 2; as 40% of 4; rounded up; should be 2. Simple fix is to use Math.round() rather than cast to an int.,Closed,Won't Fix,,Unassigned,Vivek Ratan,Fri; 9 Jan 2009 13:13:08 +0000,Wed; 8 Jul 2009 16:40:29 +0000,Mon; 12 Jan 2009 15:32:33 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5003
MAPREDUCE-59,Bug,Major,,'job -kill' from command line should inform if the job doesn't exist,"Killing an invalid job from command line succeeds with a message stating ""job killed"".",Resolved,Invalid,,Unassigned,Amar Kamat,Fri; 9 Jan 2009 15:22:26 +0000,Sat; 7 Jul 2012 16:21:20 +0000,Sat; 7 Jul 2012 16:21:19 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-59
HADOOP-5005,Test,Major,test,Create a mock MapReduce cluster simulator to test schedulers,Currently the Hadoop schedulers use a FakeTaskTrackerManager to run tests which is both messy and doesn't really simulate trackers going up and down; tasks finishing at different times; faiulres; etc. It would be nice to have a simulated MapReduce cluster where tasks really do take different amounts of (simulated) time; trackers may be slow; tasks can be made to fail; etc. The existing TaskTrackerManager interface given to the schedulers; plus perhaps a mockable clock (e.g. the FairScheduler.Clock class) should be enough to do all this. The end result will be easier-to-write and more complex scheduler tests.,Open,Unresolved,,Unassigned,Matei Zaharia,Fri; 9 Jan 2009 17:50:12 +0000,Sat; 10 Jan 2009 07:55:31 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-5005
HADOOP-5006,Bug,Trivial,contrib/cloud,ganglia is now showing any graphs,Ganglia is not showing any graphs since the rdd tool require installed fonts; though the fedora core image used as basis to build the hadoop image does not come with fonts. To fix this just install dejavu-fonts as another package.The line in create-hadoop-image-remote.sh should look like this:yum -y install rsync lynx screen ganglia-gmetad ganglia-gmond ganglia-web dejavu-fonts httpd php,Resolved,Fixed,,Unassigned,Stefan Groschupf,Fri; 9 Jan 2009 22:14:49 +0000,Wed; 24 Oct 2012 18:21:32 +0000,Wed; 24 Oct 2012 18:21:32 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5006
HADOOP-5007,Bug,Major,,can't hard-stop Chukwa adaptors,There are two ways to stop a Chukwa adaptor  gracefully; via Adaptor.shutdown(); and abruptly; via Adaptor.hardStop(). Agent.stopAdaptor() should call hardStop() when the user passes in the appropriate flag; instead of just leaving the adaptor running.This got broken by HADOOP-4709.  I assume this was inadvertent; if not; could someone explain the rationale for this change?,Closed,Duplicate,NULL,Ari Rabkin,Ari Rabkin,Fri; 9 Jan 2009 23:12:08 +0000,Wed; 8 Jul 2009 16:40:48 +0000,Wed; 4 Mar 2009 20:42:02 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5007
HADOOP-5008,Bug,Major,test,TestReplication#testPendingReplicationRetry leaves an opened fd unclosed,The unit test opens a block file to overwrite but does not close it; So subsequent test would fail because the data directory is not able to be removed.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Fri; 9 Jan 2009 23:30:30 +0000,Tue; 24 Feb 2009 23:37:36 +0000,Mon; 12 Jan 2009 20:25:17 +0000,,0.18.0,,,,https://issues.apache.org/jira/browse/HADOOP-5008
HADOOP-5009,Bug,Major,,DataNode#shutdown sometimes leaves data block scanner verification log unclosed,When datanode gets shutdown by calling DataNode#shutdown; it occasionally leaves the data block scanner verification log unclosed. There are two possible causes:1. DataNode does not wait until block scanner thread to exit.2. DataBlockScanner does not guarantee that the verification log is closed if the scanner is interrupted.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Fri; 9 Jan 2009 23:57:13 +0000,Wed; 8 Jul 2009 16:43:29 +0000,Wed; 21 Jan 2009 21:59:54 +0000,,0.17.0,,,,https://issues.apache.org/jira/browse/HADOOP-5009
HADOOP-5010,Improvement,Trivial,documentation,Document HTTP/HTTPS methods to read directory and file data,In HADOOP-1563; Doug Cutting wrote:The URI for this should be something like hftp://host:port/a/b/c; since; while HTTP will be used as the transport; this will not be a FileSystem for arbitrary HTTP urls.Recently; we've been talking about implementing an HDFS proxy (HADOOP-4575) which would be a secure way to make HFTP/HSFTP available. In so doing; we may even remove HFTP/HSFTP from being offered on the HDFS itself (that's another discussion).In the case of the HDFS proxy; does it make sense to do away with the artificial HFTP/HSFTP protocols; and instead simply offer standard HTTP and HTTPS? That would allow non-HDFS-specific clients; as well as using various standard HTTP infrastructure; such as load balancers; etc.NB; to the best of my knowledge; HFTP is only documented on the distcp page; and HSFTP is not documented at all?,Resolved,Fixed,,Unassigned,Marco Nicosia,Sat; 10 Jan 2009 05:19:54 +0000,Thu; 11 Aug 2011 18:17:29 +0000,Thu; 11 Aug 2011 18:17:29 +0000,,0.18.0,,,,https://issues.apache.org/jira/browse/HADOOP-5010
HADOOP-5011,Bug,Major,,Scanner setup takes too long...,posix4? and dr_ryan are trying to figure why setup of a scanner takes so long.  Use case is fetch of a hundred or a thousand or so rows at a time.,Closed,Invalid,,Unassigned,stack,Sat; 10 Jan 2009 06:09:41 +0000,Thu; 23 Apr 2009 19:25:02 +0000,Sat; 10 Jan 2009 06:11:37 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5011
HADOOP-5012,Bug,Major,,addStoredBlock should take into account corrupted blocks when determining excesses,I found another source of corruption on our cluster.0) Three replicas of a block exist1) One is recognized as corrupt (3 reps total)2) Namenode decides to create a new replica.  Replication done and addStoredBlock is called (4 reps total)3) There are too many replicas; so processOverReplicatedBlock is called by addStoredBlock.4) processOverReplicatedBlock is called; and it decides do invalidate the newly created replica.  Oddly enough; it decides to invalidate the newly created one instead of the one in the corrupted replicas map!5) We are in the same state as (1)  3 replicas total; 1 of which is still bad.I believe we can fix this easily  change numCurrentReplica variable to take into account the number of corrupt replicas.,Closed,Duplicate,NULL,Unassigned,Brian Bockelman,Sat; 10 Jan 2009 17:44:42 +0000,Thu; 23 Apr 2009 19:25:02 +0000,Wed; 14 Jan 2009 04:30:14 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5012
MAPREDUCE-289,Improvement,Major,,JobTracker should not expand jobs if its running low on memory,When the JobTracker detects that its running low on memory it should not expand new jobs if the job has the potential to bring it down. Consider and example where the JobTracker runs on 60% of the max memory and a new job is submitted which can take upto 40% of the max memory.  Ideally the JobTracker should queue the job for expansion and expand when sufficient memory is available.,Resolved,Fixed,,Unassigned,Amar Kamat,Mon; 12 Jan 2009 07:56:23 +0000,Mon; 21 Jul 2014 17:58:28 +0000,Mon; 21 Jul 2014 17:58:28 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-289
HADOOP-6835,Improvement,Major,io,Support concatenated gzip files,When running MapReduce with concatenated gzip files as input only the first part is read; which is confusing; to say the least. Concatenated gzip is described in http://www.gnu.org/software/gzip/manual/gzip.html#Advanced-usage and in http://www.ietf.org/rfc/rfc1952.txt. (See original report at http://www.nabble.com/Problem-with-Hadoop-and-concatenated-gzip-files-to21383097.html),Closed,Fixed,,Greg Roelofs,Tom White,Mon; 12 Jan 2009 13:16:32 +0000,Mon; 12 Dec 2011 06:19:09 +0000,Wed; 7 Jul 2010 23:23:16 +0000,,0.20.2,,HADOOP-7386,MAPREDUCE-1795;HADOOP-6335;PIG-42,https://issues.apache.org/jira/browse/HADOOP-6835
HADOOP-5015,Improvement,Major,,Separate block/replica management code from FSNamesystem,Currently FSNamesystem contains a big amount of code that manages blocks and replicas. The code scatters in FSNamesystem and it is hard to read and maintain. It would be nice to move the code to a separate class called; for example; BlockManager.,Closed,Fixed,,Suresh Srinivas,Hairong Kuang,Mon; 12 Jan 2009 23:41:12 +0000,Tue; 24 Aug 2010 20:34:50 +0000,Wed; 6 May 2009 22:34:19 +0000,,,,HADOOP-5782,,https://issues.apache.org/jira/browse/HADOOP-5015
HDFS-275,Improvement,Major,,FSNamesystem should have an InvalidateBlockMap class to manage blocks scheduled to remove,This jira intends to move the code that handles recentInvalideSet to a separate class InvalidateBlockMap.,Open,Unresolved,,Hairong Kuang,Hairong Kuang,Tue; 13 Jan 2009 00:28:39 +0000,Mon; 21 Jul 2014 18:00:21 +0000,,,,,,HADOOP-5124;HDFS-333,https://issues.apache.org/jira/browse/HDFS-275
HADOOP-5017,Bug,Minor,,NameNode.namesystem should be private,As stated in the comment:,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Tue; 13 Jan 2009 01:05:09 +0000,Tue; 24 Aug 2010 20:34:51 +0000,Thu; 15 Jan 2009 01:42:57 +0000,,,,,HADOOP-2413,https://issues.apache.org/jira/browse/HADOOP-5017
HADOOP-5018,New Feature,Major,,Chukwa should support pipelined writers,We ought to support chaining together writers; this will radically increase flexibility and make it practical to add new features without major surgery by putting them in pass-through or filter classes.,Closed,Fixed,,Ari Rabkin,Ari Rabkin,Tue; 13 Jan 2009 02:09:17 +0000,Thu; 2 May 2013 02:29:20 +0000,Tue; 10 Feb 2009 00:15:52 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5018
HDFS-207,New Feature,Minor,namenode,add querying block's info in the fsck facility,"As now the fsck can do pretty well;but when the developer happened to the log such Block blk_28622148 is not valid.etcWe wish to know which file and the datanodes the block belongs to.It  can be solved by running ""bin/hadoop fsck -files -blocks -locations / | grep blockid"" ;but as mentioned early in the HADOOP-4945 ;it's not an effective way in a big product cluster.so maybe we could do something to let the fsck more convenience .",Open,Unresolved,,zhangwei,zhangwei,Tue; 13 Jan 2009 09:33:21 +0000,Mon; 21 Jul 2014 18:02:04 +0000,,,,,,,https://issues.apache.org/jira/browse/HDFS-207
MAPREDUCE-45,Bug,Major,,JobStatus should contain user name and carry forward start time when job is killed.,If an initialized job is killed; the jobs start time is set to zero. Job status don't seem to have user name set properly.,Resolved,Cannot Reproduce,,Amar Kamat,Sreekanth Ramakrishnan,Tue; 13 Jan 2009 10:18:55 +0000,Sat; 31 Dec 2011 09:31:21 +0000,Sat; 31 Dec 2011 09:31:20 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-45
HDFS-135,Bug,Major,,TestEditLog assumes that FSNamesystem.getFSNamesystem().dir is non-null; even after the FSNameSystem is closed,"In my modified services; I'm setting FSNameSystem.dir to null on close(): This breaks TestEditLog There are two possible conclusions here. 	Setting dir=null in FSNameSystem.close() is a regression and should be fixed	The test contains some assumptions that are not validI will leave it to others to decide; I will try and fix the code whichever approach is chosen. Personally; I'd go for setting dir=null as it is cleaner; but there is clearly some risk of backward's compatibility problems; at least in test code",Resolved,Cannot Reproduce,,Steve Loughran,Steve Loughran,Tue; 13 Jan 2009 14:38:10 +0000,Mon; 26 Jan 2015 22:52:25 +0000,Mon; 26 Jan 2015 22:52:25 +0000,,,,,HADOOP-5037,https://issues.apache.org/jira/browse/HDFS-135
HADOOP-5022,Bug,Blocker,contrib/hod,[HOD] logcondense should delete all hod logs for a user; including jobtracker logs,Currently; logcondense.py does not delete jobtracker logs that it uploads to the DFS when the HOD cluster is deallocated. This will result in the hod-logs directory to slowly accumulate a whole bunch of jobtracker logs. Particularly for users who run a lot of user jobs; this could fill up the namespace.  Further these directories will cause the logcondense program to keep repeatedly looking at these directories stressing out the namenode. So; logcondense.py should optionally also delete the jobtracker logs.,Closed,Fixed,,Peeyush Bishnoi,Hemanth Yamijala,Tue; 13 Jan 2009 17:39:07 +0000,Tue; 24 Aug 2010 20:34:52 +0000,Sat; 17 Jan 2009 00:49:44 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5022
HADOOP-5023,Improvement,Major,,Add Tomcat support to hdfsproxy,We plan to add Tomcat support to hdfsproxy since Tomcat has good production support at Yahoo.,Closed,Fixed,,zhiyong zhang,Kan Zhang,Tue; 13 Jan 2009 18:42:38 +0000,Tue; 24 Aug 2010 20:34:52 +0000,Sat; 28 Feb 2009 01:58:33 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5023
HADOOP-5024,Bug,Major,,DFS chmod does not correctly parse some multiple-mode permission specifications,The current implementation of chmod attempts to combine multiple permission specifications into one omnibus specification that represents the end result of all the specifications.  However; this fails for some specifications; essentially allowing the latest specified mode to override the prior ones.  For example the following chmod in unix results in: while the current implementation results in: (adapted from TestDFSShell.java) Unix appears to apply each specified mode sequentially; and this approach would correct the problem in DFSShell as well at the cost of a separate rpc call for each mode specification.,Closed,Invalid,,Unassigned,Jakob Homan,Tue; 13 Jan 2009 22:31:27 +0000,Wed; 8 Jul 2009 16:43:29 +0000,Tue; 13 Jan 2009 23:05:47 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5024
CHUKWA-12,New Feature,Blocker,,Add instrumentation Api for Chukwa components,Chukwa Components should be able to emit some metrics in an easy way.I'm thinking of reusing the new HADOOP JMX instrumentation API to do that + MetricsContext to output/collect them using chukwa.,Closed,Fixed,,Jerome Boulon,Jerome Boulon,Wed; 14 Jan 2009 00:05:26 +0000,Fri; 15 May 2009 06:28:12 +0000,Wed; 1 Apr 2009 18:15:34 +0000,,,,,CHUKWA-53,https://issues.apache.org/jira/browse/CHUKWA-12
HADOOP-5026,Bug,Minor,,Startup scripts should be svn-executable,When you check out Chukwa; not all of the shell scripts in bin that need to be executable are so. In particular; I believe that all files ending in .sh that exist in bin should be executable. This will simply require svn:executable properties to be set.,Closed,Fixed,,Andy Konwinski,Andy Konwinski,Wed; 14 Jan 2009 00:09:02 +0000,Wed; 8 Jul 2009 16:40:47 +0000,Wed; 14 Jan 2009 21:43:53 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5026
HDFS-168,Bug,Major,,Block report processing should compare gneration stamp,If a reported block has a different generation stamp then the one stored in the NameNode; the reported block will be considered as invalid.  This is incorrect since blocks with larger generation stamp are valid.,Resolved,Won't Fix,,Unassigned,Tsz Wo Nicholas Sze,Wed; 14 Jan 2009 02:14:50 +0000,Thu; 25 Mar 2010 22:50:10 +0000,Thu; 25 Mar 2010 22:50:10 +0000,,,,HADOOP-4692,,https://issues.apache.org/jira/browse/HDFS-168
MAPREDUCE-2816,Bug,Major,,SortedMapWritable: inkonsistent put() and putAll() behaviour,The current SortedMapWritable implementation is breaking support for custom classes in case putAll() is used. Its important for putAll() that addToMap() will called to register all used classes. Please consider to have putAll() call put() for each map entry.trunk:  public Writable put(WritableComparable key; Writable value) {    addToMap(key.getClass());    addToMap(value.getClass());    return instance.put(key; value);  }  public void putAll(Map? extends WritableComparable; ? extends Writable t) {    for (Map.Entry? extends WritableComparable; ? extends Writable e:      t.entrySet()) {            instance.put(e.getKey(); e.getValue());    }  },Open,Unresolved,,Unassigned,Stefan Podkowinski,Wed; 14 Jan 2009 13:10:10 +0000,Thu; 11 Aug 2011 18:16:47 +0000,,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-2816
HADOOP-5029,Improvement,Major,,Add utilities to load chukwa sequence file to database,When data need to be reprocessed in the database; there is currently no manual method to reload the chukwa sequence files into database.A few minor tweaks to MetricsDataLoader should be possible to create a command line utility to do this.,Resolved,Fixed,,Eric Yang,Eric Yang,Wed; 14 Jan 2009 17:59:08 +0000,Wed; 8 Jul 2009 16:40:48 +0000,Fri; 27 Feb 2009 18:23:11 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5029
HADOOP-5030,Improvement,Major,,Chukwa RPM build improvements,When user defines location of Chukwa RPM as /usr/local/chukwa; the RPM should install into the defined directory.The rpm build currently creates /usr/local/chukwa/chukwa.  This should be changed.,Closed,Fixed,,Eric Yang,Eric Yang,Wed; 14 Jan 2009 18:47:16 +0000,Wed; 8 Jul 2009 16:40:47 +0000,Sat; 17 Jan 2009 03:26:20 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5030
HADOOP-5031,Bug,Major,,metrics aggregation is incorrect in database,A few problem with the aggregation SQL statements:hdfs throughput should be calculated by doing two level aggregation:First; calculate the rate for hadoop datanode metrics with accumulated vales.Second; sum up all datanode rate to provide a single number to represent the current cluster performance.Disable hod jobs utilization measurement - The data provide a rough view of the cluster performance but mostly inaccurate.Disable user utilization measurement generated from hod job - The data is generated from hod job metrics; and it's mostly inaccurate.,Closed,Fixed,,Eric Yang,Eric Yang,Wed; 14 Jan 2009 19:01:49 +0000,Tue; 24 Aug 2010 20:34:53 +0000,Fri; 6 Mar 2009 18:13:07 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5031
HADOOP-5032,Bug,Major,,CHUKWA_CONF_DIR environment variable needs to be exported to shell script,CHUKWA_CONF_DIR is used by chukwa command line scripts; like agent.sh; jettyCollector.sh  When the command line scripts are called through chukwa-daemon.sh wrapper; the CHUKWA_CONF_DIR environment is not passed to the command line script.  This variable should be exported.,Closed,Fixed,,Eric Yang,Eric Yang,Wed; 14 Jan 2009 19:10:12 +0000,Tue; 24 Aug 2010 20:34:53 +0000,Thu; 5 Feb 2009 22:45:02 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5032
HADOOP-5033,Improvement,Minor,,chukwa writer API is confusing,The ChukwaWriter interface has both add(Chunk) and add(ListChunk).   The code doesn't actually use the former.  I'd like to remove it.  Thoughts?,Closed,Fixed,,Ari Rabkin,Ari Rabkin,Wed; 14 Jan 2009 19:27:42 +0000,Thu; 2 May 2013 02:29:20 +0000,Thu; 26 Feb 2009 00:49:45 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5033
HADOOP-5034,New Feature,Major,,NameNode should send both replication and deletion requests to DataNode in one reply to a heartbeat,Currently NameNode favors block replication requests over deletion requests. On reply to a heartbeat; NameNode does not send a block deletion request unless there is no block replication request. This brings a problem when a near-full cluster loses a bunch of DataNodes. In react to the DataNode loss; NameNode starts to replicate blocks. However; replication takes a lot of cpu and a lot of replications fail because of the lack of disk space. So the administrator tries to delete some DFS files to free up space. However; block deletion requests get delayed for very long time because it takes a long time to drain the block replication requests for most DataNodes.I'd like to propose to let NameNode to send both replication requests and deletion requests to DataNodes in one reply to a heartbeat. This also implies that the replication monitor should schedule both replication and deletion work in one iteration.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Wed; 14 Jan 2009 19:30:13 +0000,Wed; 8 Jul 2009 16:43:29 +0000,Mon; 2 Feb 2009 19:16:50 +0000,,0.18.0,,,,https://issues.apache.org/jira/browse/HADOOP-5034
HADOOP-5035,Improvement,Major,,Support non Time Series charting and handle data gap more gracefully for Chukwa charts,Chukwa charting only support time series data because the data structure is a HashMap of Long; Double.Long is the timestamp (xaxis); and Double is the yaxis value.  The data structure will be changed to HashMap of String; Double.This will add capability to plot xaxis with any data.If the yaxis value is not a number; the charting system should skip plotting for this data point.,Closed,Duplicate,NULL,Eric Yang,Eric Yang,Wed; 14 Jan 2009 19:30:22 +0000,Wed; 8 Jul 2009 16:40:47 +0000,Wed; 4 Mar 2009 02:15:50 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5035
HADOOP-5036,Bug,Major,,chukwa agent controller remove file does not work ,Test case: Start the agentAdd a fileTailing adaptor to a file  validate that this adaptor is written to the checkpointremove the adaptor using the shutdown command on port 9093 (default)wait for the next checkpoint file to be writtenThe adaptor should no longer be there but it is,Closed,Duplicate,HADOOP-5397,Jerome Boulon,Jerome Boulon,Wed; 14 Jan 2009 19:55:45 +0000,Wed; 8 Jul 2009 16:40:47 +0000,Wed; 4 Mar 2009 20:34:26 +0000,,,,HADOOP-5087,,https://issues.apache.org/jira/browse/HADOOP-5036
HADOOP-5037,Sub-task,Minor,,Deprecate FSNamesystem.getFSNamesystem() and change fsNamesystemObject to private,HADOOP-2413 involves quite many codes.  This is the first step to fix it.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Wed; 14 Jan 2009 20:05:29 +0000,Tue; 24 Aug 2010 20:34:54 +0000,Fri; 23 Jan 2009 18:47:50 +0000,,,,,HDFS-135,https://issues.apache.org/jira/browse/HADOOP-5037
HADOOP-5038,Improvement,Major,,remove System.out.println statement,Starting the agent using the chuka-daemon.sh script redirect the standard output to a file. Only critical information should be outputted to stdout,Closed,Fixed,,Jerome Boulon,Jerome Boulon,Wed; 14 Jan 2009 20:29:37 +0000,Tue; 24 Aug 2010 20:34:58 +0000,Tue; 10 Feb 2009 00:30:02 +0000,,,,HADOOP-5138,,https://issues.apache.org/jira/browse/HADOOP-5038
HADOOP-5039,Bug,Major,,Hourly&daily rolling are not using the right path,"The path for Rolling is build like this: chukwaMainRepository + ""/"" + cluster + ""/"" + dataSource + ""/"" + workingDay + ""/"" + workingHour + ""//.evt"";If there's a spill file in ""/"" + cluster + ""/"" + dataSource + ""/"" + workingDay + ""/"" + workingHour it will be part of the rolling but it shouldn't. Only data from subDirectories should be part of the rolling.",Closed,Fixed,,Jerome Boulon,Jerome Boulon,Wed; 14 Jan 2009 21:41:28 +0000,Tue; 24 Aug 2010 20:34:59 +0000,Tue; 10 Feb 2009 00:38:02 +0000,,,,HADOOP-5138,,https://issues.apache.org/jira/browse/HADOOP-5039
CHUKWA-22,New Feature,Major,Data Processors,Need index for chukwa sequence files,Chukwa has ability to collect large volume of data; but the lack of index prevents Chukwa front end to serve data straight from HDFS.  This jira is the place holder for designing a indexing service for Chukwa.  The plan is to create indexing service base on available software like lucene or katta.,Open,Unresolved,,Eric Yang,Eric Yang,Wed; 14 Jan 2009 21:43:25 +0000,Thu; 2 May 2013 02:29:20 +0000,,,,,,HIVE-417,https://issues.apache.org/jira/browse/CHUKWA-22
CHUKWA-15,New Feature,Major,Data Processors,Need search service for Chukwa data,In order to search log files efficiently; Chukwa needs to provide a search service base on index created by HADOOP-5040.,Open,Unresolved,,Unassigned,Eric Yang,Wed; 14 Jan 2009 21:46:35 +0000,Thu; 2 May 2013 02:29:20 +0000,,,,,,,https://issues.apache.org/jira/browse/CHUKWA-15
HADOOP-5042,New Feature,Major,, Add expiration handling to the chukwa log4j appender,Chukwa log4j appender is not doing any sort of cleanup. The idea here is to keep only n rotate files and delete the older ones. This way we don't have to worry about manually cleaning old files,Closed,Fixed,,Jerome Boulon,Jerome Boulon,Wed; 14 Jan 2009 22:18:56 +0000,Tue; 24 Aug 2010 20:34:59 +0000,Tue; 24 Feb 2009 21:12:46 +0000,,,,CHUKWA-28;HADOOP-5138,,https://issues.apache.org/jira/browse/HADOOP-5042
CHUKWA-26,Improvement,Blocker,Data Processors,Rewrite processSinkFiles.sh in java to have a better error handling,,Resolved,Fixed,,Jerome Boulon,Jerome Boulon,Thu; 15 Jan 2009 00:15:16 +0000,Thu; 9 Apr 2009 04:21:07 +0000,Thu; 9 Apr 2009 04:21:07 +0000,,,,,,https://issues.apache.org/jira/browse/CHUKWA-26
CHUKWA-30,Improvement,Major,Data Collection,Remove HDFS flush & connection holding (Collector),,Resolved,Fixed,,Jerome Boulon,Jerome Boulon,Thu; 15 Jan 2009 00:17:34 +0000,Sat; 6 Jun 2009 23:06:33 +0000,Tue; 26 May 2009 18:22:20 +0000,,0.2.0,,,,https://issues.apache.org/jira/browse/CHUKWA-30
HADOOP-5045,Sub-task,Major,fs,FileSystem.isDirectory() should not be deprecated.,We should remove FileSystem.isDirectory().,Closed,Fixed,,Suresh Srinivas,Tsz Wo Nicholas Sze,Thu; 15 Jan 2009 02:30:42 +0000,Tue; 24 Aug 2010 20:35:00 +0000,Thu; 29 Jan 2009 17:29:09 +0000,,,,,HADOOP-5953,https://issues.apache.org/jira/browse/HADOOP-5045
CHUKWA-6,Bug,Major,,Chukwa log4j appender logs corrupted data if the system is under high stress,Data from Iostat indicates that log files did not write properly when system is under high stress.2008-12-29 03:03:48;510 INFO org.apache.hadoop.chukwa.inputtools.plugin.metrics.Exec: Linux 2.6.9-55.ELsmp (example1002)  12/29/08^D^Davg-cpu:  %user   %nice    %sys %iowait   %idle^D           1.19    0.35    0.85    2.63   94.99^D^DDevice:    rrqm/s wrqm/s   r/s   w/s  rsec/s  wsec/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await  svctm  %util^Dsda          0.13  33.31  3.02  3.53  281.94  311.02   140.97   155.51    90.52     0.56   86.19   2.30   1.51^Dsdb          4.52   3.45  5.93  1.51  107.12   39.67    53.56    19.83    19.74     0.07    9.98   3.53   2.63^Dsdc          4.57  22.76  7.12  1.71  395.58  195.76   197.79    97.88    66.93     0.24   27.13   3.90   3.44^Dsdd          4.52  18.17  6.13  1.65  151.17  158.59    75.58    79.30    39.81     0.45   57.96   3.84   2.98^D^Davg-cpu:  %user   %nice    %sys %iowait   %idle^D          24.83    0.00    0.29    0.07   74.81^D^DDevice:    rrqm/s wrqm/s   r/s   w/s  rsec/s  wsec/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await  svctm  %util^Dsda          1.04   1.78 41.99  3.64 8706.75   45.10  4353.37    22.55   191.82     0.21    4.50   3.70  16.89^Dsdb          0.00   0.00  0.00  0.00    0.00    0.00     0.00   2008-12-29 03:08:48;513 INFO org.apache.hadoop.chukwa.inputtools.plugin.metrics.Exec: Linux 2.6.9-55.ELsmp (example1002)  12/29/08^D^DThe most probable reason is that disk buffer got paged out before it is written to disk.  Exec plugin can be configured to always flush on every output.  For hadoop logs; this need to be fine tuned.,Closed,Unresolved,HADOOP-5301,Ari Rabkin,Eric Yang,Thu; 15 Jan 2009 08:03:18 +0000,Mon; 13 Jul 2009 18:09:29 +0000,,,,,,,https://issues.apache.org/jira/browse/CHUKWA-6
HADOOP-5047,Bug,Major,contrib/hod,Hod should be modified to generate core-site.xml; mapred-site.xml and hdfs-site.xml,Hod is currently generating hadoop-site.xml but as per the modifications done in hadoop (ref: JIRA-4631); hod should also be modified to be in synch with this change. It should also generate core-site.xml; mapred-site.xml and hdfs-site.xml instead of hadoop-site.xml from 0.20.0 onwards.,Resolved,Won't Fix,,Peeyush Bishnoi,Suman Sehgal,Thu; 15 Jan 2009 08:25:38 +0000,Wed; 18 May 2011 22:03:37 +0000,Wed; 18 May 2011 22:03:37 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5047
HADOOP-5048,Bug,Major,,Sometimes job is still displayed in jobqueue_details page for long time after job was killed.,"When I tried kill all running job; I noticed that were two jobs were listed on jobqueue_details.jsp page page as well as they were also listed under failed job on jobtracker.jsp page.When I checked status of each that was displayed ""killed"" and Cleanup task status as ""Successful""; but both jobs were also being on jobqueue_details.jsp page for longtime e.g up to 10 -15 mins after I restarted JobTracker.Before killing the jobs; status of both jobs was running and no task of from them was scheduled.I noticed this behavior on 3 different occasions. But is this random; not always reproducible.",Closed,Fixed,,Sreekanth Ramakrishnan,Karam Singh,Thu; 15 Jan 2009 08:49:11 +0000,Wed; 8 Jul 2009 16:40:29 +0000,Tue; 20 Jan 2009 19:21:36 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5048
MAPREDUCE-27,Bug,Major,,Jobs with 0 maps will never get removed from the default scheduler,Jobs' with 0 maps finish/succeed in the init phase i.e while the job is in the PREP state. EagerTaskInitializationListener removes the job after initing but JobQueueJobInProgressListener waits for a job-state change event to be raised and aonly then removes the job from the queue and hence the job will stay forever with the JobQueueJobInProgressListener. Looks like FairScheduler periodically scans the job list and removes completed jobs. CapacityScheduler has a concept of waiting jobs and scans waiting queue for completed jobs and purges them.,Resolved,Duplicate,MAPREDUCE-805,Amar Kamat,Amar Kamat,Thu; 15 Jan 2009 08:56:17 +0000,Thu; 20 Aug 2009 11:37:35 +0000,Wed; 19 Aug 2009 11:16:38 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-27
HADOOP-5050,Bug,Major,,TestDFSShell fails intermittently,"TestDFSShell.testFilePermissions fails intermittently with following assertion failure :Testcase: testFilePermissions took 0.299 sec	FAILEDexpected:...-... but was:...w...junit.framework.ComparisonFailure: expected:...-... but was:...w...	at org.apache.hadoop.hdfs.TestDFSShell.testChmod(TestDFSShell.java:781)	at org.apache.hadoop.hdfs.TestDFSShell.testFilePermissions(TestDFSShell.java:832)",Closed,Fixed,,Jakob Homan,Amareshwari Sriramadasu,Thu; 15 Jan 2009 10:19:57 +0000,Tue; 24 Aug 2010 20:35:00 +0000,Wed; 21 Jan 2009 21:36:46 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5050
HADOOP-5051,Bug,Major,,hdfs throughput calculation is incorrect in chukwa database,The SQL statement to calculate hdfs throughput is incorrect.  The correct algorithm is to calculate metrics rate for individual datanode then sum up of all datanode's rate to get the total throughput for the cluster.,Resolved,Fixed,,Eric Yang,Eric Yang,Thu; 15 Jan 2009 17:57:47 +0000,Wed; 8 Jul 2009 16:40:45 +0000,Fri; 27 Feb 2009 18:34:53 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5051
HADOOP-5052,New Feature,Major,,Add an example for computing exact digits of Pi,It would be useful to add an example showing how to use Hadoop to do scientific computing.  We should add an example for computing exact digits of Pi.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Thu; 15 Jan 2009 17:59:07 +0000,Thu; 7 Jul 2016 23:01:43 +0000,Mon; 23 Feb 2009 02:52:29 +0000,,,,,MAPREDUCE-637,https://issues.apache.org/jira/browse/HADOOP-5052
HADOOP-5053,Bug,Major,,Collector does not shutdown properly,"Using ""ps ax"" to determine process status is more reliable than jps; and chukwa has changed to use ps ax as part of shutdown.  However; the shell script does not handle the case where there are blank space in front of pid number; though running ""jettyCollector.sh stop"" does not shutdown the collector process.",Closed,Duplicate,NULL,Eric Yang,Eric Yang,Thu; 15 Jan 2009 18:09:21 +0000,Wed; 8 Jul 2009 16:40:46 +0000,Fri; 16 Jan 2009 23:42:51 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5053
HADOOP-5054,Bug,Major,,Chukwa database partitions does not create/expire automatically,dbAdmin.sh should create database partitions base on time; but this is not working because the timestamp variable is not inside the loop for creating database partitions.  When the script is running; it would create partitions corresponding to the execution time once only.The database expiration script need to handle the case where partition number is invalid.,Resolved,Fixed,,Eric Yang,Eric Yang,Thu; 15 Jan 2009 18:22:49 +0000,Wed; 8 Jul 2009 16:40:46 +0000,Fri; 27 Feb 2009 19:54:47 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5054
HADOOP-5055,Bug,Major,,chukwa alert configuration should be loaded from CHUKWA_CONF_DIR,chukwa-daemon.sh is expecting alert.conf from CHUKWA_HOME/conf/alert.conf; but this should be changed toCHUKWA_CONF_DIR.  This change will satisfy the recent changes of chukwa config rpm.,Resolved,Fixed,,Eric Yang,Eric Yang,Thu; 15 Jan 2009 18:30:07 +0000,Wed; 8 Jul 2009 16:40:46 +0000,Fri; 6 Mar 2009 01:59:34 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5055
HADOOP-5056,Bug,Major,,chukwa init.d script can't run over pdsh ,On Redhat 5.1; the sudo script does not allow commands to be executed as another user over ssh.  This means chukwainit.d script can not be executed via pdsh.  The error message was: sudo: sorry; you must have a tty to run sudo,Closed,Invalid,,Eric Yang,Eric Yang,Thu; 15 Jan 2009 18:43:50 +0000,Wed; 8 Jul 2009 16:40:47 +0000,Fri; 16 Jan 2009 01:53:51 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5056
HADOOP-5057,Bug,Major,, Chukwa agent crash on startup with missing check point file,When check point file doesn't exist; the agent startup script crashes.,Resolved,Fixed,,Ari Rabkin,Eric Yang,Thu; 15 Jan 2009 18:46:49 +0000,Wed; 8 Jul 2009 16:40:47 +0000,Wed; 4 Mar 2009 20:27:40 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5057
CHUKWA-28,Improvement,Blocker,, Add late initialization to the chukwa log4j appender,With the current Chukwa log4j implementation; if you define a static log4j configuration the appender is initialized at creation time.This may cause some problems for example with Hadoop Audit log or Metrics context; if the log4j properties are statically defined then a userX running an hadoop command will trigger a permission denied exception since the appender will try to initialize itself and therefore try to create a file under userX ownership for each appender even if not data is going to be written to it.The goal is to delay this initialization until the first message is actually written to that log so this kind of issue could easily be avoided.,Resolved,Fixed,,Jerome Boulon,Jerome Boulon,Thu; 15 Jan 2009 19:12:02 +0000,Fri; 27 Mar 2009 22:40:27 +0000,Fri; 27 Mar 2009 22:40:27 +0000,,,,HADOOP-5042,,https://issues.apache.org/jira/browse/CHUKWA-28
HADOOP-5059,Bug,Major,util,'whoami'; 'topologyscript' calls failing with java.io.IOException: error=12; Cannot allocate memory,We've seen primary/secondary namenodes fail when calling whoami or topologyscripts.(Discussed as part of HADOOP-4998)Sample stack traces.Primary Namenode Secondary Namenode,Resolved,Fixed,,Unassigned,Koji Noguchi,Thu; 15 Jan 2009 19:36:04 +0000,Mon; 21 Jul 2014 18:08:30 +0000,Mon; 21 Jul 2014 18:08:30 +0000,,,,,HADOOP-4998,https://issues.apache.org/jira/browse/HADOOP-5059
CHUKWA-21,New Feature,Major,Data Processors,Create a generic aggregator for Chukwa,"Create a generic way to compute aggregation on top of chukwaRecords based on a config fileShould be able:	work on several Chukwa streams	To aggregate by time period	Group by values for specific keys	Provide a redefine list of functions (AVG;MIN;MAX;Counter-Rate conversion...)	work with new functions",Open,Unresolved,,Unassigned,Jerome Boulon,Thu; 15 Jan 2009 23:58:58 +0000,Tue; 22 Mar 2016 03:00:26 +0000,,,,,,,https://issues.apache.org/jira/browse/CHUKWA-21
HADOOP-5061,Improvement,Major,documentation,Update chinese documentation for default configuration,The chinese documentation needs to be updated as per HADOOP-4828,Resolved,Incomplete,,Unassigned,Sharad Agarwal,Fri; 16 Jan 2009 01:17:53 +0000,Mon; 21 Jul 2014 18:08:51 +0000,Mon; 21 Jul 2014 18:08:51 +0000,,,,HADOOP-4828,,https://issues.apache.org/jira/browse/HADOOP-5061
HDFS-118,Bug,Major,,Namenode clients should recover from connection or Namenode restarts,This Jira discusses the client side recovery from namenode restarts; fail overs and network connectivity issues. This does not address Namenode high availability and tracks only the client side recovery.,Resolved,Fixed,,Suresh Srinivas,Suresh Srinivas,Fri; 16 Jan 2009 02:26:29 +0000,Mon; 21 Jul 2014 18:09:49 +0000,Mon; 21 Jul 2014 18:09:49 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-118
MAPREDUCE-56,Bug,Major,,Client recovery from Job tracker restarts and connectivity failures,This Jira addresses the client side recovery from Jobtracker restarts; fail overs and network connectivity issues. This does not address Jobtracker high availability and tracks only the client side recovery.,Resolved,Not A Problem,,Unassigned,Suresh Srinivas,Fri; 16 Jan 2009 02:33:37 +0000,Sat; 31 Dec 2011 09:40:20 +0000,Sat; 31 Dec 2011 09:40:20 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-56
HADOOP-5064,New Feature,Major,,Hadoop 1.0,"Hadoop 1.0 has been discussed in email alias core-dev@hadoop.apache.organd in the wiki http://wiki.apache.org/hadoop/Release1.0RequirementsThe discussion is summarized in this and in sub jiras. Further discussions will occur here rather than on email.Below are the 4 main Hadoop 1.0 task categories that have been identified so far.	Decide on the Hadoop 1.0 compatibility requirements (HADOOP-xxx)	interface Classification - visibility (public/private) and stability (HADOOP-xxx)	Interface cleanup (HADOOP-xxx)	Mechanisms to support versioning and compatibility	1.0 Features   A few features that we need before 2.0 and are likely to break 1.0 compatibility; hence we should do them by 1.0			Security (HADOOP-4487)		Client recovery from NN (HADOOP-5062) retarts and JT restarts (HADOOP-5063)",Resolved,Won't Fix,,Sanjay Radia,Sanjay Radia,Fri; 16 Jan 2009 02:39:23 +0000,Sat; 31 Dec 2011 17:45:34 +0000,Sat; 31 Dec 2011 17:45:34 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5064
HADOOP-5065,Bug,Major,,setOutputFormatClass in mapreduce.Job fails for SequenceFileOutputFormat,The signatures for get{Input;Output}FormatClass are probably too restrictive:,Closed,Fixed,,Chris Douglas,Chris Douglas,Fri; 16 Jan 2009 03:01:04 +0000,Thu; 23 Apr 2009 19:18:02 +0000,Fri; 16 Jan 2009 06:10:33 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5065
HADOOP-5066,Bug,Major,build,ant binary should not compile docs,ant binary now compiles docs. The compilation of binary itself takes around 6 minutes. Since the tar ball does not include docs; they need not be compiled.The size of binary is 17MB on my system. I could see duplicate library copies in the tar contents.For example :rw-rw-r- /             26202 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/commons-logging-api-1.0.4.jarrw-rw-r- /           2532573 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/hadoop-0.21.0-dev-core.jarrw-rw-r- /             69850 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/hadoop-0.21.0-dev-tools.jarrw-rw-r- /            516429 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/jetty-6.1.14.jarrw-rw-r- /            121070 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/junit-3.8.1.jarrw-rw-r- /            391834 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/log4j-1.2.15.jarrw-rw-r- /             15345 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/slf4j-api-1.4.3.jarrw-rw-r- /             15010 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/xmlenc-0.52.jar----------------------------------------------rw-rw-r- /           2532573 2009-01-16 11:53:51 hadoop-0.21.0-dev/hadoop-0.21.0-dev-core.jarrw-rw-r- /             69850 2009-01-16 11:53:51 hadoop-0.21.0-dev/hadoop-0.21.0-dev-tools.jarrw-rw-r- /            516429 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/jetty-6.1.14.jarrw-rw-r- /             26202 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/commons-logging-api-1.0.4.jarrw-rw-r- /            121070 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/junit-3.8.1.jarrw-rw-r- /            391834 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/log4j-1.2.15.jarrw-rw-r- /             15345 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/slf4j-api-1.4.3.jarrw-rw-r- /             15010 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/xmlenc-0.52.jar,Closed,Fixed,,Giridharan Kesavan,Amareshwari Sriramadasu,Fri; 16 Jan 2009 06:17:30 +0000,Thu; 23 Apr 2009 19:18:02 +0000,Wed; 4 Mar 2009 06:03:22 +0000,,0.21.0,,,HADOOP-5390,https://issues.apache.org/jira/browse/HADOOP-5066
HADOOP-5067,Bug,Major,,Failed/Killed attempts column in jobdetails.jsp does not show the number of failed/killed attempts correctly,I see one of the task failures when i see it from the taskdetails.jsp page; but  failed/killed attempts column show it as zero.,Closed,Fixed,,Amareshwari Sriramadasu,Amareshwari Sriramadasu,Fri; 16 Jan 2009 08:05:38 +0000,Wed; 8 Jul 2009 16:53:14 +0000,Fri; 13 Feb 2009 04:02:21 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5067
HADOOP-5068,Bug,Major,,testClusterBlockingForLackOfMemory in TestCapacityScheduler fails randomly,testClusterBlockingForLackOfMemory fails randomly when TestCapacityScheduler is run.,Closed,Fixed,,Vinod Kumar Vavilapalli,Sreekanth Ramakrishnan,Fri; 16 Jan 2009 10:40:22 +0000,Wed; 8 Jul 2009 16:40:29 +0000,Tue; 7 Apr 2009 18:14:26 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5068
HADOOP-5069,New Feature,Minor,test,add a Hadoop-centric junit test result listener,"People are encountering different problems with hadoop's unit tests; defects currently being WONTFIX'd	HADOOP-5001 : Junit tests that time out don't write any test progress related logs	HADOOP-4721 : OOM in .TestSetupAndCleanupFailureThere is a root cause here; the XmlResultFormatter of Ant buffers everything before writing out a DOM. Too much logged: OOM and no output. Timeout: kill and no output.We could add a new logger class to hadoop and then push it back into Ant once we were happy; or keep it separate if we had specific dependencies (like on hadoop-dfs API) that they lacked. Some ideas	stream XML to disk. We would have to put the test summary at the end; could use XSL to generate HTML and the classic XML content	stream XHTML to disk. Makes it readable as you go along; makes the XSL work afterwards harder.	push out results as records to a DFS. There's a problem here in that this needs to be a different DFS from that you are testing; yet it needs to be compatible with the client.Item #3 would be interesting but doing it inside JUnit is too dangerous classpath and config wise. Better to have Ant do the copy afterwards. What is needed then is a way to easily append different tests to the same DFS file in a way that tools can analyse them all afterwards. The copy is easy add a new Ant resource for that but the choice of format is trickier.Here's some work I did on this a couple of years back; I've not done much since then: http://people.apache.org/~stevel/slides/distributed_testing_with_smartfrog_slides.pdfIs anyone else interested in exploring this?",Resolved,Fixed,,Unassigned,Steve Loughran,Fri; 16 Jan 2009 12:04:44 +0000,Mon; 21 Jul 2014 18:10:51 +0000,Mon; 21 Jul 2014 18:10:51 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5069
HADOOP-5070,Bug,Blocker,documentation,Update the year for the copyright to 2009,The year should be updated to 2009 before any new release comes out.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Fri; 16 Jan 2009 17:25:24 +0000,Tue; 24 Aug 2010 20:35:02 +0000,Wed; 21 Jan 2009 00:10:02 +0000,,,,,HADOOP-3543,https://issues.apache.org/jira/browse/HADOOP-5070
HADOOP-5071,Sub-task,Major,,Hadoop 1.0 Compatibility Requirements,"The purpose of this Jira is to decide on  Hadoop 1.0 Compatibility requirementsA proposal is described below that was discussed on email alias core-dev@hadoop.apache.orgRelease terminology used below:Standard release numbering: major; minor; dot releases	Only bug fixes in dot releases: m.x.y			no changes to API; disk format; protocols or config etc. in a dot release			new features in major (m.0) and minor (m.x.0) releasesHadoop Compatibility Proposal	1 API CompatibilityNo need for client recompilation when upgrading across minor releases (ie. from m.x to m.y; where x = y)Classes or methods deprecated in m.x can be removed in (m+1).0Note that this is stronger than what we have been doing in Hadoop 0.x releases.	This is fairly standard compatibility rules for major and minor releases.	2 Data Compatibility			Motivation: Users expect File systems preserve data transparently across releases.		2.a HDFS metadata and data can change across minor or major releases ; but such changes are transparent to user application. That is release upgrade must automatically convert the metadata and data as needed. Further; a release upgrade must allow a cluster to roll back to the older version and its older disk format. (rollback needs to restore the orignal data not any updated data).		2.a-WeakerAutomaticConversion:Automatic conversion is support across a small number of releases. If a user wants to jump across multiple releases he may be forced to go through a few intermediate release to get to the final desired release.			3 Wire Protocol CompatibilityWe offer no wire compatibility in our 0.x release today.			Motivation: The motivation isn't to make the hadoop protocols public. Applications will not call the protocol directly but through a library (in our case FileSystem class and its implementations). Instead the motivation is that customers run multiple clusters and have apps that access data across clusters. Customers cannot be expected to update all clusters simultaneously.		3.a Old m.x clients can connect to new m.y servers; where x = y but the old clients might get reduced functionality or performance. m.x clients might not be able to connect to (m+1).z servers		3.b. New m.y clients must be able to connect to old m.x server; where x y but only for old m.x functionality.Comment: Generally old API methods continue to use old rpc methods. However; it is legal to have new implementations of old API methods call newrpcs methods; as long as the library transparently handles the fallback case for old servers.		3.c. At any major release transition [ ie from a release m.x to a release (m+1).0]; a user should be able to read data from the cluster running the old version.					Motivation: data copying across clusters is a common operation for many customers. For example this is routinely at done at Yahoo; another use case is HADOOP-4058. Today; http (or hftp) provides a guaranteed compatible way of copying data across versions. Clearly one cannot force a customer to simultaneously update all its Hadoop clusters on to a new major release.  We can satisfy this requirement via the http/hftp mechanism or some other mechanism.						3.c-StrongerShall we add a stronger requirement for 1. 0 : wire compatibility across major versions? That is not just for reading but for all operations. This can be supported by class loading or other games.Note we can wait to provide this when 2. 0 happens. If Hadoop provided this guarantee then it would allow customers to partition their data across clusters without risking apps breaking across major releases due to wire incompatibility issues.					Motivation: Data copying is a compromise. Customers really want to run apps across clusters running different versions.",Resolved,Won't Fix,,Sanjay Radia,Sanjay Radia,Fri; 16 Jan 2009 18:34:18 +0000,Sat; 31 Dec 2011 17:45:13 +0000,Sat; 31 Dec 2011 17:45:13 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5071
HADOOP-5072,Bug,Major,test,testSequenceFileGzipCodec won't pass without native gzip codec,Somehow; SequenceFile requires native gzip codec. We should remove it from the test cases since that may not pass on all platforms.,Closed,Fixed,,Zheng Shao,Zheng Shao,Fri; 16 Jan 2009 19:25:29 +0000,Tue; 24 Aug 2010 20:35:03 +0000,Fri; 16 Jan 2009 19:46:57 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5072
HADOOP-5073,Sub-task,Major,,Hadoop 1.0 Interface Classification - scope (visibility - public/private) and stability,This jira proposes an interface classification for hadoop interfaces.The discussion was started in email alias core-dev@hadoop.apache.org in Nov 2008.,Closed,Fixed,,Jakob Homan,Sanjay Radia,Fri; 16 Jan 2009 20:59:47 +0000,Fri; 31 May 2013 14:07:47 +0000,Wed; 9 Sep 2009 00:45:05 +0000,,0.21.0,,,HADOOP-6658;MAPREDUCE-1650;HDFS-752;HADOOP-6289;HADOOP-6668;HBASE-4403,https://issues.apache.org/jira/browse/HADOOP-5073
HDFS-125,Bug,Major,,Consistency of different replicas of the same block is not checked.,HDFS currently detects corrupted replicas by verifying that its contents matches the checksum stored in the block meta-file. This is done independently for each replica of the block on the data-node it belongs to. But we do not check that the replicas are identical across data-nodes as long as they have the same size.This is not common but can happen as a result of a software bug or an operator mismanagement. And in this case different clients will read different data from the same file.,Open,Unresolved,HDFS-591,Unassigned,Konstantin Shvachko,Fri; 16 Jan 2009 22:26:39 +0000,Wed; 23 Jul 2014 22:49:01 +0000,,,,,,HDFS-366,https://issues.apache.org/jira/browse/HDFS-125
HADOOP-5075,Bug,Blocker,,Potential infinite loop in updateMinSlots,We ran into a problem at Facebook where the updateMinSlots loop in the scheduler was repeating infinitely. This might happen if; due to rounding; we are unable to assign the last few slots in a pool. This patch adds a break statement to ensure that the loop exists if it hasn't managed to assign any slots.,Closed,Fixed,,Matei Zaharia,Matei Zaharia,Sat; 17 Jan 2009 01:42:06 +0000,Wed; 8 Jul 2009 16:41:04 +0000,Fri; 23 Jan 2009 05:09:56 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5075
HADOOP-5076,Bug,Major,,chukwa metrics file get overwritten when process launch,In Log4jMetricsContext; the log4j appender always rewrite the file instead of append to the log file.  This should bechanged to append to ensure the metrics log file is streamed correctly.,Closed,Duplicate,HADOOP-5100,Unassigned,Eric Yang,Sat; 17 Jan 2009 01:42:28 +0000,Wed; 8 Jul 2009 16:40:47 +0000,Tue; 10 Feb 2009 02:40:06 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5076
HADOOP-5077,Bug,Blocker,util,JavaDoc errors in 0.18.3,There are JavaDoc errors in 0.18.3. These are not present in 0.19 and above thus went undetected by Hudson and others.,Closed,Fixed,,Raghu Angadi,Raghu Angadi,Sat; 17 Jan 2009 01:53:32 +0000,Fri; 30 Jan 2009 20:14:50 +0000,Sat; 17 Jan 2009 02:59:38 +0000,,0.18.3,,,,https://issues.apache.org/jira/browse/HADOOP-5077
HADOOP-5078,Bug,Major,contrib/cloud,Broken AMI/AKI for ec2 on hadoop,"c1.xlarge and m1.large instances fail to boot. ec2-get-console-output show them stuck at ""Creating /dev"" step.",Closed,Fixed,,Tom White,Mathieu Poumeyrol,Mon; 19 Jan 2009 16:22:22 +0000,Tue; 24 Aug 2010 20:35:04 +0000,Thu; 29 Jan 2009 12:37:22 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5078
HADOOP-5079,Bug,Major,util, HashFunction inadvertently destroys some randomness,HashFunction.hash restricts initval for the next hash to the [0; maxValue) range of the hash indexes returned. This is suboptimal; particularly for larger nbHash and smaller maxValue.  Rather we should first set initval; then restrict the range for the result assignment.,Closed,Fixed,,Jonathan Ellis,Jonathan Ellis,Mon; 19 Jan 2009 21:47:00 +0000,Thu; 23 Apr 2009 19:18:02 +0000,Tue; 10 Feb 2009 01:53:08 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5079
HADOOP-5080,Test,Minor,test,Update TestCLI with additional test cases.,Currently TestCLI contains few of the dfs commands and verifies some of the error messages for quota and refreshServiceAcl.. Here is a proposal to add additional test cases to TestCLI to cover an exhaustive list of Hadoop commands. Here is a list of action items for the same:1) Complete the test cases for dfs commands which are not yet automated such as count; chmod; chown; chgrp etc2) Verify help messages in fs; dfsadmin; mradmin3) Add other Hadoop commands such as archives; dfsadmin; balancer; job; queue; version; jar; distcp; daemonlog etc to the command line test.,Closed,Fixed,,Unassigned,Ramya Sunil,Tue; 20 Jan 2009 04:36:27 +0000,Tue; 24 Aug 2010 20:35:04 +0000,Thu; 7 May 2009 16:02:28 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5080
HADOOP-5081,Test,Minor,test,Split TestCLI into HDFS; Mapred and Core tests,At present; TestCLI contains command line tests for both hdfs and mapred. Going forward; this test has to be broken up into separate hdfs; mapred and core tests.,Closed,Fixed,,Sharad Agarwal,Ramya Sunil,Tue; 20 Jan 2009 05:48:39 +0000,Tue; 24 Aug 2010 20:35:05 +0000,Tue; 5 May 2009 09:18:45 +0000,,0.20.0,,HADOOP-5135,,https://issues.apache.org/jira/browse/HADOOP-5081
MAPREDUCE-118,Bug,Blocker,client,Job.getJobID() will always return null,JobContext is used for a read-only view of job's info. Hence all the readonly fields in JobContext are set in the constructor. Job extends JobContext. When a Job is created; jobid is not known and hence there is no way to set JobID once Job is created. JobID is obtained only when the JobClient queries the jobTracker for a job-id.; which happens later i.e upon job submission.,Closed,Fixed,,Amareshwari Sriramadasu,Amar Kamat,Tue; 20 Jan 2009 07:34:21 +0000,Fri; 2 Sep 2011 22:13:20 +0000,Tue; 1 Jun 2010 05:26:10 +0000,,0.20.1,,,,https://issues.apache.org/jira/browse/MAPREDUCE-118
MAPREDUCE-291,Improvement,Major,jobtracker,Optionally a separate daemon should serve JobHistory,Currently the JobTracker serves the JobHistory to end-users off files local-disk/hdfs. While running very large clusters with a large user-base might result in lots of traffic for job-history which needlessly taxes the JobTracker. The proposal is to have an optional daemon which handles serving of job-history requests.,Resolved,Fixed,,Amar Kamat,Arun C Murthy,Tue; 20 Jan 2009 09:28:41 +0000,Mon; 21 Jul 2014 18:13:44 +0000,Mon; 21 Jul 2014 18:13:44 +0000,,,,,MAPREDUCE-177,https://issues.apache.org/jira/browse/MAPREDUCE-291
MAPREDUCE-3753,Bug,Major,,Reduce output data is not written to disk,"I run into a critical issue with Hadoop 18.2 on my Linux boxes:The jobs executes without any complains and they are listed in thesucceeded list but there is no output data beside the ""_logs"" directory.The same code works with .17.2.1Here are some sections of the logs:logfilehadoop@bock:~/logs$ tail hadoop-hadoop-jobtracker-bock.log2008-12-23 13:30:56;707 INFO org.apache.hadoop.mapred.JobInProgress:Choosing a data-local task task_200812231229_0031_m_000001 forspeculation2008-12-23 13:30:56;707 INFO org.apache.hadoop.mapred.JobTracker: Addingtask 'attempt_200812231229_0031_m_000001_1' to tiptask_200812231229_0031_m_000001; for tracker'tracker_bock:localhost/127.0.0.1:15260'2008-12-23 13:31:01;065 INFO org.apache.hadoop.mapred.JobInProgress:Task 'attempt_200812231229_0031_m_000001_1' has completedtask_200812231229_0031_m_000001 successfully.2008-12-23 13:31:03;177 INFO org.apache.hadoop.mapred.TaskRunner: Savedoutput of task 'attempt_200812231229_0031_r_000000_0' tohdfs://BOCK:9000/ana/oiprocessed/2008/12/23/Sen1/92a74190-2038-4c79-82c4-2de6fdc615db/logfileBut the folder contains only a ""_logs"" folder which has a history filewhich contains:logfileJob JOBID=""job_200812231415_0001"" FINISH_TIME=""1230038377844""JOB_STATUS=""SUCCESS"" FINISHED_MAPS=""2"" FINISHED_REDUCES=""1""FAILED_MAPS=""0"" FAILED_REDUCES=""0"" COUNTERS=""Job Counters .Data-localmap tasks:2;Job Counters .Launched reduce tasks:1;Job Counters .Launchedmap tasks:3;Map-Reduce Framework.Reduce input records:61;Map-ReduceFramework.Map output records:61;Map-Reduce Framework.Map outputbytes:7194;Map-Reduce Framework.Combine output records:0;Map-ReduceFramework.Map input records:61;Map-Reduce Framework.Reduce inputgroups:12;Map-Reduce Framework.Combine input records:0;Map-ReduceFramework.Map input bytes:36396;Map-Reduce Framework.Reduce outputrecords:12;File Systems.HDFS bytes written:1533;File Systems.Local byteswritten:14858;File Systems.HDFS bytes read:38679;File Systems.Localbytesread:7388;com..ana.scheduling.HadoopTask$Counter.MAPPEED:61""/logfileSo what I see is that the system runs successful and it even says itwrites data! (""Map-Reduce Framework.Reduce output records:12;File Systems.HDFS bytes written:1533"")If I run the same code with .17.2.1 or in local mode with .18.2 it worksand I get a part-0000 file with the expected data.Please tell me if you need additional information.",Resolved,Incomplete,,Unassigned,Michael Fuchs,Tue; 20 Jan 2009 09:44:26 +0000,Mon; 21 Jul 2014 18:15:49 +0000,Mon; 21 Jul 2014 18:15:49 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-3753
HADOOP-5085,Bug,Major,fs,Copying a file to local with Crc throws an exception,$ hadoop dfs -get -crc /user/aa/test.txt test.txtget: org.apache.hadoop.dfs.DistributedFileSystem cannot be cast toorg.apache.hadoop.fs.ChecksumFileSystemThe problem seems to be caused by the line 251 in FsShell#copyToLocal: Copying crc files to local should not require the source file system to be ChecksumFileSystem.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Tue; 20 Jan 2009 18:47:59 +0000,Thu; 23 Apr 2009 19:18:02 +0000,Mon; 2 Feb 2009 23:51:08 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5085
HADOOP-5086,Improvement,Minor,fs,Trash URI semantics can be relaxed,When using fully qualified URIs with FsShell; the authority element of the URI must match the default filesystem exactly; or else one may get an error message when the trash is enabled: It should be possible to use the FileSystem for the Path provided rather than the default FileSystem. 0.17 was less particular about this.,Closed,Fixed,,Chris Douglas,Chris Douglas,Tue; 20 Jan 2009 23:42:20 +0000,Tue; 24 Feb 2009 23:37:36 +0000,Fri; 23 Jan 2009 22:52:23 +0000,,0.18.0,,,,https://issues.apache.org/jira/browse/HADOOP-5086
HADOOP-5087,Bug,Major,,Regex for Cmd parsing contains an error,,Resolved,Fixed,,Ari Rabkin,Jerome Boulon,Tue; 20 Jan 2009 23:56:49 +0000,Wed; 8 Jul 2009 16:40:46 +0000,Tue; 3 Mar 2009 23:11:25 +0000,,,,HADOOP-5036;HADOOP-5138,,https://issues.apache.org/jira/browse/HADOOP-5087
HADOOP-5088,Improvement,Major,build,include releaseaudit as part of  test-patch.sh script ,Existing test-patch.sh script doesn't seem to execute releaseaudit target as part of patch testing.We need to call releaseaudit target from test-patch.sh scriptThanks;Giri,Closed,Fixed,,Giridharan Kesavan,Giridharan Kesavan,Wed; 21 Jan 2009 05:21:56 +0000,Tue; 24 Aug 2010 20:35:07 +0000,Fri; 30 Jan 2009 06:02:56 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5088
MAPREDUCE-312,Improvement,Major,,Port HADOOP-4667 to the default Map-Reduce scheduler,HADOOP-4667 has implemented 'global scheduling' for the fair-share scheduler with very promising results - we should port the same to the default o.a.h.mapred.JobQueueTaskScheduler.,Resolved,Won't Fix,,Arun C Murthy,Arun C Murthy,Wed; 21 Jan 2009 06:48:51 +0000,Mon; 21 Jul 2014 18:18:08 +0000,Mon; 21 Jul 2014 18:18:08 +0000,,,,,MAPREDUCE-548,https://issues.apache.org/jira/browse/MAPREDUCE-312
MAPREDUCE-517,Bug,Critical,,The capacity-scheduler should assign multiple tasks per heartbeat,HADOOP-3136 changed the default o.a.h.mapred.JobQueueTaskScheduler to assign multiple tasks per TaskTracker heartbeat; the capacity-scheduler should do the same.,Closed,Fixed,,Arun C Murthy,Arun C Murthy,Wed; 21 Jan 2009 06:53:25 +0000,Thu; 24 Nov 2011 22:04:04 +0000,Thu; 25 Aug 2011 20:08:50 +0000,,,,HADOOP-5884,,https://issues.apache.org/jira/browse/MAPREDUCE-517
MAPREDUCE-538,Improvement,Major,,Port HADOOP-4667 to teh capacity scheduler,HADOOP-4667 has implemented 'global scheduling' for the fair-share scheduler with very promising results - we should port the same to the capacity scheduler.,Resolved,Incomplete,,Unassigned,Arun C Murthy,Wed; 21 Jan 2009 06:53:33 +0000,Mon; 21 Jul 2014 18:18:35 +0000,Mon; 21 Jul 2014 18:18:35 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-538
HADOOP-5092,Bug,Major,build,Branch 0.18 doesn't display version info,hadoop version doesn't display the version of hadoop for branch 0.18 rather it shows version unknown. Same behaviour is observed in svn as well as in git.,Closed,Invalid,,Unassigned,Suman Sehgal,Wed; 21 Jan 2009 11:23:42 +0000,Thu; 23 Apr 2009 19:25:03 +0000,Wed; 21 Jan 2009 16:36:26 +0000,,0.18.0,,,,https://issues.apache.org/jira/browse/HADOOP-5092
HADOOP-5093,Bug,Minor,conf,Configuration default resource handling needs to be able to remove default resources ,There's a way to add default resources; but not remove them. This allows someone to push an invalid resource into the default list; and for the rest of the JVM's life; any Conf file loaded with quietMode set will fail.,Closed,Won't Fix,,Unassigned,Steve Loughran,Wed; 21 Jan 2009 14:04:36 +0000,Tue; 30 Jun 2015 07:22:35 +0000,Sun; 8 Feb 2015 17:24:53 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5093
HADOOP-5094,Improvement,Minor,,Show dead nodes information in dfsadmin -report,"As part of operations responsibility to bring back dead nodes; it will be good to have a quick way to obtain a list of dead data nodes.  The current way is to scrape the namenode web UI page and parse that information; but this creates load on the namenode.   In search of a less costly way; I noticed dfsadmin -report only reports data nodes with State: ""In Service"" and ""Decommission in progress"" get listed.Asking for a cheap way to obtain a list of dead nodes.  In addition; can the following requests be reviewed for additional enhancement and changes to dfsadmin -report.	Consistent formatting output in ""Remaining raw bytes:"" for the data nodes should have a space between the exact value and the parenthesized value.Sample:Total raw bytes: 3842232975360 (3.49 TB)Remaining raw bytes: 146090593065(136.06 GB)Used raw bytes: 3240864964620 (2.95 TB)	Include the running version of Hadoop.	What is the meaning of ""Total effective bytes""?	Display the hostname instead of the IP address for the data node (toggle option?)",Closed,Fixed,HDFS-363,Jakob Homan,Jim Huang,Wed; 21 Jan 2009 18:30:33 +0000,Thu; 17 Jul 2014 21:27:30 +0000,Wed; 4 Feb 2009 00:31:13 +0000,,0.18.2,,,HADOOP-4281,https://issues.apache.org/jira/browse/HADOOP-5094
HADOOP-5095,Bug,Major,,chukwa watchdog does not monitor the system correctly,watchdog depends on environment variables to locate pid files correctly.  Due to the recent path enhancement for build.xml; watchdog needs to be updated with the correct CHUKWA_LOG_DIR and CHUKWA_CONF_DIR at build time.,Closed,Fixed,,Jerome Boulon,Eric Yang,Wed; 21 Jan 2009 19:36:50 +0000,Tue; 24 Aug 2010 20:35:09 +0000,Tue; 10 Feb 2009 00:48:56 +0000,,,,HADOOP-5138,,https://issues.apache.org/jira/browse/HADOOP-5095
CHUKWA-11,Bug,Blocker,,conf files with templates should be ignored; not tracked; by SVN,Files with conf/*.template file should not be tracked by SVN. They should be ignored instead.These files were incorrectly added to SVN as part of HADOOP 4792.These files include:*chukwa-env.sh*chukwa-agent-conf.xml*chukwa-collector-conf.xml*collectors*chukwa-demux-conf.xml (a corresponding chukwa-demux.xml.template should be created for this file),Closed,Fixed,,Ari Rabkin,Andy Konwinski,Wed; 21 Jan 2009 22:17:40 +0000,Fri; 15 May 2009 06:27:58 +0000,Tue; 10 Mar 2009 20:36:08 +0000,,,,,HADOOP-4792,https://issues.apache.org/jira/browse/CHUKWA-11
HADOOP-5097,Sub-task,Major,,Remove static variable JspHelper.fsn,There is another static FSNamesystem variable; fsn; declared in JspHelper.  We should remove it.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Wed; 21 Jan 2009 22:37:59 +0000,Tue; 24 Aug 2010 20:35:10 +0000,Mon; 2 Feb 2009 18:43:39 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5097
MAPREDUCE-226,New Feature,Major,,Support detailed timing for MapReduce job,It would be nice to break down the time each individual Map task or Reduce task spends on reading input; writing output; and executing the map() or reduce() calls.,Open,Unresolved,,Unassigned,Hong Tang,Wed; 21 Jan 2009 22:52:53 +0000,Sat; 20 Jun 2009 07:51:02 +0000,,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-226
HDFS-327,Improvement,Major,,DataNode should warn about unknown files in storage,"DataNode currently just ignores the files it does not know about. There could be a lot of files left in DataNode's storage that never get noticed or deleted. These files could be left because of bugs or by a misconfiguration. E.g. while upgrading from 0.17; DN left a lot of metada files that were not named in correct format for 0.18 (HADOOP-4663).The proposal here is simply to make DN print a warning for each of the unknown files at the start up. This at least gives a way to list all the unknown files and  (equally importantly) forces a notion of ""known"" and ""unknown"" files in the storage.",Open,Unresolved,,Unassigned,Raghu Angadi,Thu; 22 Jan 2009 00:07:35 +0000,Mon; 21 Jul 2014 18:24:58 +0000,,,,newbie,,,https://issues.apache.org/jira/browse/HDFS-327
HADOOP-5100,Bug,Major,,Chukwa Log4JMetricsContext class should append new log to current log file,Log4JMetricsContext is setting is own appender dynamically and by doing so it's truncating the current log file. It should append to it if the file exist.,Closed,Fixed,HADOOP-5076,Jerome Boulon,Jerome Boulon,Thu; 22 Jan 2009 01:10:01 +0000,Tue; 24 Aug 2010 20:35:10 +0000,Tue; 10 Feb 2009 02:39:40 +0000,,,,HADOOP-5138,,https://issues.apache.org/jira/browse/HADOOP-5100
HADOOP-5101,Improvement,Major,build,optimizing build.xml target dependencies,Need to optimize build.xmlFor ex: findbugs target depends on package target and package target depends on doc; jar; cn-docs ; etc...Though findbugs is run on three of the jar files for which we have three different targets; jar; tools-jar ; examplesLikewise different targets could be optimized. Thanks;Giri,Closed,Fixed,,Giridharan Kesavan,Giridharan Kesavan,Thu; 22 Jan 2009 04:20:14 +0000,Tue; 24 Aug 2010 20:35:10 +0000,Tue; 10 Feb 2009 01:39:32 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5101
HADOOP-5102,Improvement,Major,build,Split build script for building core; hdfs and mapred separately,,Resolved,Fixed,,Unassigned,Sharad Agarwal,Thu; 22 Jan 2009 06:07:05 +0000,Mon; 21 Jul 2014 18:22:35 +0000,Mon; 21 Jul 2014 18:22:35 +0000,,,,HDFS-256;MAPREDUCE-231;HADOOP-5107;HADOOP-5135,,https://issues.apache.org/jira/browse/HADOOP-5102
HADOOP-5103,Bug,Major,,"Too many logs saying ""Adding new node"" on JobClient console","JobClient's console has logs saying ""Adding a new node rackname/node-ip-addr:port"" for all the hosts; where each split resides.For jobs with more #maps; these logs just fill up client's space.This is introduced by HADOOP-3293",Closed,Fixed,,Jothi Padmanabhan,Amareshwari Sriramadasu,Thu; 22 Jan 2009 06:17:01 +0000,Tue; 24 Aug 2010 20:35:11 +0000,Mon; 23 Feb 2009 08:43:01 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5103
MAPREDUCE-519,Bug,Major,,Fix capacity scheduler's documentation,Parent jira for all documentation related issues in capacity scheduler.,Resolved,Won't Fix,,Chen He,Vinod Kumar Vavilapalli,Thu; 22 Jan 2009 06:24:33 +0000,Mon; 21 Jul 2014 18:30:09 +0000,Mon; 21 Jul 2014 18:30:09 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-519
HADOOP-5105,Improvement,Major,build,to optimize hudsonBuildHadoopNightly.sh script,,Resolved,Fixed,,Giridharan Kesavan,Giridharan Kesavan,Thu; 22 Jan 2009 11:32:56 +0000,Mon; 16 Feb 2009 17:00:44 +0000,Fri; 30 Jan 2009 05:45:43 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5105
HADOOP-5106,Bug,Major,,hdfs-default.xml option names are not consistent with DataNode ,Some of the options that are named in hdfs-default.xml do not match the options which are read in DataNode. One of the two files needs to be changed to be consistent -ideally whichever breaks the least installations.,Closed,Invalid,,Unassigned,Steve Loughran,Thu; 22 Jan 2009 13:20:58 +0000,Thu; 29 Apr 2010 08:07:31 +0000,Thu; 22 Jan 2009 13:26:06 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5106
HADOOP-5107,Improvement,Major,build,split the core; hdfs; and mapred jars from each other and publish them independently to the Maven repository,I think to support splitting the projects; we should publish the jars for 0.20.0 as independent jars to the Maven repository,Closed,Fixed,,Giridharan Kesavan,Owen O'Malley,Thu; 22 Jan 2009 18:59:26 +0000,Tue; 24 Aug 2010 20:35:11 +0000,Tue; 24 Nov 2009 07:55:39 +0000,,0.20.0,,HDFS-749;HADOOP-5102;HDFS-256;MAPREDUCE-231;HDFS-623,,https://issues.apache.org/jira/browse/HADOOP-5107
MAPREDUCE-64,Bug,Major,performance;task,Map-side sort is hampered by io.sort.record.percent,Currently io.sort.record.percent is a fairly obscure; per-job configurable; expert-level parameter which controls how much accounting space is available for records in the map-side sort buffer (io.sort.mb). Typically values for io.sort.mb (100) and io.sort.record.percent (0.05) imply that we can store ~350;000 records in the buffer before necessitating a sort/combine/spill.However for many applications which deal with small records e.g. the world-famous wordcount and it's family this implies we can only use 5-10% of io.sort.mb i.e. (5-10M) before we spill inspite of having much more memory available in the sort-buffer. The word-count for e.g. results in ~12 spills (given hdfs block size of 64M). The presence of a combiner exacerbates the problem by piling serialization/deserialization of records too...Sure; jobs can configure io.sort.record.percent; but it's tedious and obscure; we really can do better by getting the framework to automagically pick it by using all available memory (upto io.sort.mb) for either the data or accounting.,Closed,Fixed,,Chris Douglas,Arun C Murthy,Thu; 22 Jan 2009 22:02:22 +0000,Sat; 30 Mar 2013 00:22:12 +0000,Fri; 5 Feb 2010 05:43:27 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-64
HADOOP-5109,Bug,Major,test,TestKillCompletedJob is failing intermittetnly when run as part of test-core,TestKillCompletedJob fails most times when run as part of test-core; but succeeds when run by itself.,Resolved,Fixed,,Unassigned,Jakob Homan,Thu; 22 Jan 2009 23:47:10 +0000,Mon; 21 Jul 2014 18:30:23 +0000,Mon; 21 Jul 2014 18:30:23 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5109
MAPREDUCE-520,Bug,Major,,The capacity schedule must start a timer when a queue is underserved,It is important to the user that the timer that controls preemption be started when a queue is underserved; regardless of whether any other queues are over allocation.,Resolved,Won't Fix,,Unassigned,Owen O'Malley,Fri; 23 Jan 2009 00:03:10 +0000,Mon; 21 Jul 2014 18:30:46 +0000,Mon; 21 Jul 2014 18:30:46 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-520
HADOOP-5111,Bug,Major,,Generic mapreduce classes cannot be used with Job::set* methods,The set methods on a Job take Class instances whose parameterized type is an unbounded type (e.g. Class? extends RawComparator?&gt;). Attempting to pass a Class whose parameterized types are not explicit will cause compile-time errors; as in HADOOP-5065.,Closed,Fixed,,Chris Douglas,Chris Douglas,Fri; 23 Jan 2009 00:27:08 +0000,Wed; 8 Jul 2009 16:53:14 +0000,Sat; 28 Feb 2009 01:36:48 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5111
HADOOP-5112,Task,Major,build,Upgrade Clover to 2.4.2 and enable Test Optimization in HADOOP,The current Hadoop build on Hudson is using Clover 1.3.13. I will attach a patch to the build.xml and the clover 2.4.2 jar to this issue.Test Optimization works by only running tests for which code has changed. This can be used both in a CI environment and on a developers machine to make running tests faster.Clover will also run tests which previously failed. Modified source code is detected at compile time and then used to select the tests that get run.,Resolved,Incomplete,,Unassigned,Nick Pellow,Fri; 23 Jan 2009 01:22:20 +0000,Mon; 21 Jul 2014 18:31:11 +0000,Mon; 21 Jul 2014 18:31:11 +0000,,,,,HADOOP-3921,https://issues.apache.org/jira/browse/HADOOP-5112
HADOOP-5113,Bug,Major,contrib/hod,"logcondense should delete hod logs for a user ; whose username has any of the characters in the value passed to ""-l"" options ","Logcondense script is not able to delete the completed job directories of hadoop logs in hod-logs inside HDFS  for the the users ; whose username has any of the characters ; in the value passed to ""-l"" options  or in '/user' as set default . This happened because logcondense script use python 'lstrip' method ; which returns copy of the string after removing leading characters in the value passed to ""-l"" options or in ""/user"" instead of just stripping value from the given string .",Closed,Fixed,,Peeyush Bishnoi,Peeyush Bishnoi,Fri; 23 Jan 2009 07:49:26 +0000,Tue; 24 Aug 2010 20:35:13 +0000,Thu; 29 Jan 2009 03:51:58 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5113
HADOOP-5114,Test,Minor,test,A bunch of mapred unit tests are failing on Windows,"A bunch of unit tests are consistently failing when run on Windows. Below are a list of unit tests which are failing and the corresponding exceptions thrown:Exception: ""java.net.ConnectException: Connection refused: no further information""Failing tests:	TestMiniMRMapRedDebugScript - testMapDebugScript	TestNoDefaultsJobConf - testNoDefaults	TestQueueManager - testAllEnabledACLForJobSubmission	TestCompressedEmptyMapOutputs - testMapReduceSortWithCompressedEmptyMapOutputs	TestJobInProgressListener - testJobQueueChanges	TestKillCompletedJob - testKillCompJob	TestMiniMRClasspath - testClassPath	TestMiniMRDFSCaching - testWithDFS	TestMiniMRWithDFSWithDistinctUsers - testDistinctUsers	TestSetupAndCleanupFailure - testWithDFS	TestDBJob - testRun	TestMiniMRWithDFS - testWithDFS	TestJobStatusPersistency - testNonPersistency	TestSpecialCharactersInOutputPath - testJobWithDFS	TestUserDefinedCounters - testMapReduceJob	TestDelegatingInputFormat - testSplitting	TestEmptyJobWithDFS - testEmptyJobWithDFS	TestJavaSerialization - testMapReduceJob	TestClusterMapReduceTestCase - testMapReduceException: java.lang.IllegalArgumentException: Pathname /path from path is not a valid DFS filename.Failing tests:	TestJobInProgress - testRunningTaskCount	TestJobQueueInformation - testJobQueues	TestJobTrackerRestart - testJobTrackerRestartException: java.io.IOException: Bad connect ack with firstBadLink 127.0.0.1:port numberFailing tests:	TestJobSysDirWithDFS - testWithDFS	TestJobInProgress - testPendingMapTaskCount	TestMiniMRDFSSort - testMapReduceSortException: junit.framework.AssertionFailedErrorFailing tests:	TestMRServerPorts - testJobTrackerPorts	TestMRServerPorts - testTaskTrackerPorts	TestMiniMRTaskTempDir - testTaskTempDirException: java.io.IOException: Job failed!Failing tests:	TestMiniMRLocalFS - testWithLocal",Resolved,Fixed,,Raghu Angadi,Ramya Sunil,Fri; 23 Jan 2009 08:08:39 +0000,Wed; 8 Jul 2009 16:53:14 +0000,Fri; 6 Feb 2009 20:31:08 +0000,,0.18.3,,,HDFS-11;HADOOP-4679;HDFS-405,https://issues.apache.org/jira/browse/HADOOP-5114
HADOOP-5115,Test,Minor,test,TestLocalDirAllocator fails under AIX ,TestLocalDirAllocator fails when running under AIX for the same reasons as CYGWIN under Windows (as noted in the test source code comments).  AIX allows the writing of a file in a directory that is marked read-only. This breaks the test. If the test is changed to sense for AIX (as it does for windows) then the usefulness of this unit test is questionable other than exposing an interesting anomoly in the native file system.,Resolved,Incomplete,,Unassigned,Bill Habermaas,Fri; 23 Jan 2009 15:23:06 +0000,Mon; 21 Jul 2014 18:31:30 +0000,Mon; 21 Jul 2014 18:31:30 +0000,,0.18.2,,,,https://issues.apache.org/jira/browse/HADOOP-5115
HADOOP-5116,Test,Minor,test,TestSocketIOWithTimeout fails under AIX - TIMEOUT error. ,This test expects an exception to occur when read/writing a closed socket.  Under AIX this does not occur and results in a loop.,Resolved,Incomplete,,Unassigned,Bill Habermaas,Fri; 23 Jan 2009 15:27:46 +0000,Mon; 21 Jul 2014 18:31:54 +0000,Mon; 21 Jul 2014 18:31:54 +0000,,0.18.2,,,,https://issues.apache.org/jira/browse/HADOOP-5116
HDFS-83,Bug,Major,,Move fs.checkpoint.* properties to hdfs-default.xml,The fs.checkpoint.* properties in core-default.xml are actually HDFS properties (for the secondary namenode) so they belong in hdfs-default.xml and should be moved. Would this be a good time to rename them to start with dfs for consistency (deprecating them first; of course)?,Resolved,Duplicate,NULL,Unassigned,Tom White,Fri; 23 Jan 2009 17:10:05 +0000,Thu; 5 Jan 2012 04:58:05 +0000,Thu; 5 Jan 2012 04:58:05 +0000,,,newbie,,,https://issues.apache.org/jira/browse/HDFS-83
HADOOP-5118,Improvement,Major,,ChukwaAgent controller should retry to register for a longer period but not as frequent as now ,Watchdog is watching for ChukwaAgent only once every 5 minutes; so there's no point in retrying more than once every 5 mins.In practice; if the watchdog is not able to automatically restart the agent; it will take more than 20 minutes to get Ops to restart it.Also Ops want us to limit the number of communications between Hadoop and Chukwa; that's why 30 minutes.,Resolved,Fixed,,Jerome Boulon,Jerome Boulon,Fri; 23 Jan 2009 18:07:10 +0000,Wed; 8 Jul 2009 16:40:46 +0000,Fri; 27 Feb 2009 01:03:00 +0000,,,,HADOOP-5138,,https://issues.apache.org/jira/browse/HADOOP-5118
HADOOP-5119,Sub-task,Major,,FSEditLog should not use FSNamesystem.getFSNamesystem(),FSEditLog should not access the namespace by the static method FSNamesystem getFSNamesystem().,Closed,Duplicate,NULL,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Fri; 23 Jan 2009 22:06:07 +0000,Wed; 8 Jul 2009 16:43:30 +0000,Wed; 1 Apr 2009 18:46:32 +0000,,,,,HADOOP-4539,https://issues.apache.org/jira/browse/HADOOP-5119
HADOOP-5120,Sub-task,Major,,UpgradeManagerNamenode and UpgradeObjectNamenode should not use FSNamesystem.getFSNamesystem(),UpgradeManagerNamenode and UpgradeObjectNamenode should not access the namespace by the static method FSNamesystem.getFSNamesystem(),Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Fri; 23 Jan 2009 22:11:19 +0000,Tue; 24 Aug 2010 20:35:14 +0000,Fri; 20 Feb 2009 21:48:44 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5120
HBASE-1152,Bug,Major,,Fix shell usage for format.width,,Closed,Fixed,,stack,stack,Sat; 24 Jan 2009 07:10:47 +0000,Sun; 13 Sep 2009 22:24:22 +0000,Sat; 24 Jan 2009 07:11:44 +0000,,,,,,https://issues.apache.org/jira/browse/HBASE-1152
HADOOP-5122,Improvement,Trivial,test,libhdfs test conf uses deprecated fs.default.name value,src/c++/libhdfs/tests/conf/core-site.xml contains a deprecated layout for fs.default.name:,Closed,Fixed,,Craig Macdonald,Craig Macdonald,Sat; 24 Jan 2009 22:05:09 +0000,Wed; 8 Jul 2009 17:05:56 +0000,Mon; 9 Feb 2009 17:02:36 +0000,,,,,HADOOP-4631,https://issues.apache.org/jira/browse/HADOOP-5122
MAPREDUCE-1332,New Feature,Minor,,Ant tasks for job submission,"Ant tasks to make it easy to work with hadoop filesystem and submit jobs. submit : uploads JAR; submits job as user; with various settingsfilesystem operations: mkdir; copyin; copyout; delete -We could maybe use Ant1.7 ""resources"" here; and so use hdfs as a source or dest in Ant's own tasks	security. Need to specify user; pick up user.name from JVM as default?	cluster binding: namenode/job tracker (hostname;port) or url are all that is needed?#job conf: how to configure the job that is submitted? support a list of property name=""name"" value=""something"" children	testing. AntUnit to generate junitreport compatible XML files	Documentation. With an example using Ivy to fetch the JARs for the tasks and hadoop client.	Polling: ant task to block for a job finished?",Resolved,Won't Fix,,Steve Loughran,Steve Loughran,Mon; 26 Jan 2009 15:30:04 +0000,Thu; 2 May 2013 02:29:25 +0000,Sat; 18 Feb 2012 16:24:23 +0000,,0.22.0,,,HADOOP-2778;HADOOP-1508,https://issues.apache.org/jira/browse/MAPREDUCE-1332
HADOOP-5124,Improvement,Major,,A few optimizations to FsNamesystem#RecentInvalidateSets,This jira proposes a few optimization to FsNamesystem#RecentInvalidateSets:1. when removing all replicas of a block; it does not traverse all nodes in the map. Instead it traverse only the nodes that the block is located.2. When dispatching blocks to datanodes in ReplicationMonitor. It randomly chooses a predefined number of datanodes and dispatches blocks to those datanodes. This strategy provides fairness to all datanodes. The current strategy always starts from the first datanode.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Mon; 26 Jan 2009 18:53:53 +0000,Tue; 8 Nov 2011 21:07:34 +0000,Fri; 6 Feb 2009 23:16:41 +0000,,,,,HDFS-1257;HDFS-275,https://issues.apache.org/jira/browse/HADOOP-5124
HADOOP-5125,Improvement,Major,ipc,Remove deprecated call method from RPC,public static Object[] call(Method method; Object[][] params; InetSocketAddress[] addrs; Configuration conf) is deprecated and only called from the unit test.  It should be removed.,Resolved,Later,,Jakob Homan,Jakob Homan,Mon; 26 Jan 2009 23:53:49 +0000,Fri; 30 Jan 2009 22:47:11 +0000,Fri; 30 Jan 2009 22:47:11 +0000,,,,,HADOOP-4348,https://issues.apache.org/jira/browse/HADOOP-5125
HADOOP-5126,Improvement,Major,,Empty file BlocksWithLocations.java should be removed,File org.apache.hadoop.hdfs.protocol.BlocksWithLocations.java is empty and should be removed.,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Tue; 27 Jan 2009 00:00:29 +0000,Tue; 24 Aug 2010 20:35:15 +0000,Tue; 27 Jan 2009 00:09:22 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5126
HADOOP-5127,Improvement,Major,,FSDirectory should not have public methods.,FSDirectory class contains public constructors and methods. All of them except for one close() can be converted into package private.,Closed,Fixed,,Jakob Homan,Konstantin Shvachko,Tue; 27 Jan 2009 00:28:45 +0000,Wed; 8 Jul 2009 16:43:32 +0000,Tue; 10 Feb 2009 18:47:27 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5127
MAPREDUCE-241,Improvement,Major,,JobQueueTaskScheduler could assign multiple reduces per heartbeat,Currently the JobQueueTaskScheduler assigns only 1 reduce per heartbeat; for applications where latency is important we could assign more reduces (upto available slots).,Resolved,Won't Fix,,Arun C Murthy,Arun C Murthy,Tue; 27 Jan 2009 08:16:47 +0000,Mon; 21 Jul 2014 18:32:36 +0000,Mon; 21 Jul 2014 18:32:36 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-241
MAPREDUCE-270,Improvement,Major,,TaskTracker could send an out-of-band heartbeat when the last running map/reduce completes,Currently the TaskTracker strictly respects the heartbeat interval; this causes utilization issues when all running tasks complete. We could send an out-of-band heartbeat in that case.,Closed,Fixed,,Arun C Murthy,Arun C Murthy,Tue; 27 Jan 2009 08:18:40 +0000,Tue; 24 Aug 2010 21:13:24 +0000,Mon; 28 Sep 2009 21:32:25 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/MAPREDUCE-270
HADOOP-5130,Bug,Blocker,,TaskTracker seems to hold onto the assigned task for a long while before launching it,I saw atleast a couple of instances where the task assigned to the TaskTracker is launched several minutes after the receipt of the LaunchTaskAction:,Closed,Won't Fix,,Unassigned,Arun C Murthy,Tue; 27 Jan 2009 08:22:06 +0000,Tue; 23 Mar 2010 04:07:38 +0000,Wed; 18 Feb 2009 16:25:45 +0000,,0.20.0,,,MAPREDUCE-1617,https://issues.apache.org/jira/browse/HADOOP-5130
MAPREDUCE-117,Bug,Major,,ReduceCopier sleeps for a hardcoded interval of 5secs,Currently the ReduceCopier.run has a hard-coded 5s sleep which hurts jobs where latency is important; we really should have a mechanism where the thread fetching task-completion events should notify the ReduceCopier.run.,Open,Unresolved,,Arun C Murthy,Arun C Murthy,Tue; 27 Jan 2009 08:34:55 +0000,Sat; 20 Jun 2009 07:50:54 +0000,,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-117
HDFS-340,Improvement,Major,,Allow MAX_XCEIVER_COUNT to be configured,Currently DataXcieverServer.java hardcodes MAX_XCEIVER_COUNT to 256. I ran a randomwriter with 4k maps and output replication set to 40 - all write pipelines failed since each DataNode hit the above limit and refused connections.Should we consider making it configurable?,Resolved,Not A Problem,,Unassigned,Arun C Murthy,Tue; 27 Jan 2009 08:40:02 +0000,Sat; 22 May 2010 04:05:05 +0000,Sat; 22 May 2010 03:25:13 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-340
HDFS-145,Bug,Major,,FSNameSystem#addStoredBlock does not handle inconsistent block length correctly,Currently NameNode treats either the new replica or existing replicas as corrupt if the new replica's length is inconsistent with NN recorded block length. The correct behavior should be1. For a block that is not under construction; the new replica should be marked as corrupt if its length is inconsistent (no matter shorter or longer) with the NN recorded block length;2. For an under construction block; if the new replica's length is shorter than the NN recorded block length; the new replica could be marked as corrupt; if the new replica's length is longer; NN should update its recorded block length. But it should not mark existing replicas as corrupt. This is because NN recorded length for an under construction block does not accurately match the block length on datanode disk. NN should not judge an under construction replica to be corrupt by looking at the inaccurate information:  its recorded block length.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Wed; 28 Jan 2009 00:39:26 +0000,Tue; 24 Aug 2010 20:47:44 +0000,Tue; 12 Jan 2010 19:10:22 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HDFS-145
HADOOP-5134,Bug,Blocker,,FSNamesystem#commitBlockSynchronization adds under-construction block locations to blocksMap,From my understanding of sync/append design; an under construction block should not have any block locations associated with it in the blocksMap. So an under construction block will not be managed by ReplicationMonitor.However; if there is an error in the write pipeline; a lease recovery will trigger a call; commitBlockSynchronization; to NN. This call will add the successfully-recovered datanodes to blocksMap. This seems to violate the design. It should update the targets of the last block at INode instead.,Closed,Fixed,,dhruba borthakur,Hairong Kuang,Wed; 28 Jan 2009 00:56:14 +0000,Wed; 8 Jul 2009 16:43:30 +0000,Tue; 17 Feb 2009 18:23:48 +0000,,0.18.2,,,,https://issues.apache.org/jira/browse/HADOOP-5134
HADOOP-5135,Improvement,Major,build,Separate the core; hdfs and mapred junit tests,To support splitting of projects; the tests should be separated into different directories.,Closed,Fixed,,Sharad Agarwal,Sharad Agarwal,Wed; 28 Jan 2009 08:21:24 +0000,Tue; 24 Aug 2010 20:35:15 +0000,Fri; 8 May 2009 12:27:27 +0000,,,,HADOOP-5102;HADOOP-5219;HADOOP-5081,HADOOP-5962;HADOOP-6119,https://issues.apache.org/jira/browse/HADOOP-5135
MAPREDUCE-425,Bug,Minor,,NPE in TaskInProgress.cleanup,This may be something that only my code triggers; an NPE in TaskTracker$TaskInProgress.cleanup Looking at the code; the only source of NPE's on that line is localJobConf It looks like if TaskInProgress.cleanup() ever gets called with no valid localJobConf; then an NPE is the result. The exception gets logged and discarded; but it does appear in the logs.,Resolved,Duplicate,NULL,Unassigned,Steve Loughran,Wed; 28 Jan 2009 11:48:23 +0000,Thu; 29 Apr 2010 08:09:24 +0000,Wed; 29 Jul 2009 11:24:55 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-425
HDFS-286,Improvement,Minor,,Move Datanode packet IO logging to its own log,If the Datanode is set to log at info; then the log fills up with lots of details about packet sending and receivingsf-startdaemon-debug 09/01/28 13:15:42 org.apache.hadoop.hdfs.server.datanode.DataXceiver@83efe6e INFO datanode.DataNode : Receiving block blk_-3185775405544105186_1757 src: /127.0.0.1:41218 dest: /127.0.0.1:48017sf-startdaemon-debug 09/01/28 13:15:42 PacketResponder 0 for Block blk_-3185775405544105186_1757 INFO datanode.DataNode : Received block blk_-3185775405544105186_1757 of size 3647 from /127.0.0.1:41218sf-startdaemon-debug 09/01/28 13:15:42 PacketResponder 0 for Block blk_-3185775405544105186_1757 INFO datanode.DataNode : PacketResponder 0 for block blk_-3185775405544105186_1757 terminatingsf-startdaemon-debug 09/01/28 13:15:42 org.apache.hadoop.hdfs.server.datanode.DataXceiver@83f0029 INFO datanode.DataNode : Receiving block blk_-1511363731410268168_1758 src: /127.0.0.1:41219 dest: /127.0.0.1:48017sf-startdaemon-debug 09/01/28 13:15:42 PacketResponder 0 for Block blk_-1511363731410268168_1758 INFO datanode.DataNode : Received block blk_-1511363731410268168_1758 of size 940 from /127.0.0.1:41219sf-startdaemon-debug 09/01/28 13:15:42 PacketResponder 0 for Block blk_-1511363731410268168_1758 INFO datanode.DataNode : PacketResponder 0 for block blk_-1511363731410268168_1758 terminatingsf-startdaemon-debug 09/01/28 13:15:42 org.apache.hadoop.hdfs.server.datanode.DataXceiver@83f01e4 INFO datanode.DataNode : Receiving block blk_-967265843864311176_1759 src: /127.0.0.1:41220 dest: /127.0.0.1:48017sf-startdaemon-debug 09/01/28 13:15:42 PacketResponder 0 for Block blk_-967265843864311176_1759 INFO datanode.DataNode : Received block blk_-967265843864311176_1759 of size 36948 from /127.0.0.1:41220It would be convenient for those people who only want to see errors in communication but monitor other DataNode operations to have a separate logger for DataNode communications; one to view at a separate log level.,Open,Unresolved,,Unassigned,Steve Loughran,Wed; 28 Jan 2009 13:25:37 +0000,Mon; 21 Jul 2014 18:33:23 +0000,,,,newbie,,,https://issues.apache.org/jira/browse/HDFS-286
HADOOP-5138,Bug,Critical,,Current Chukwa Trunk failed contrib unit tests.,"junit.framework.AssertionFailedError: org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent$AlreadyRunningException: Agent already running; aborting	at org.apache.hadoop.chukwa.datacollection.agent.TestAgentConfig.testInitAdaptors_vs_Checkpoint(TestAgentConfig.java:73)Test org.apache.hadoop.chukwa.datacollection.adaptor.filetailer.TestStartAtOffset FAILED (timeout)",Closed,Fixed,,Jerome Boulon,Jerome Boulon,Wed; 28 Jan 2009 17:25:28 +0000,Tue; 24 Aug 2010 20:35:16 +0000,Tue; 3 Feb 2009 07:07:02 +0000,,,,HADOOP-5087;HADOOP-5039;HADOOP-5095;HADOOP-5100;HADOOP-5042;HADOOP-4839;HADOOP-5118;HADOOP-4859;HADOOP-5038,,https://issues.apache.org/jira/browse/HADOOP-5138
HADOOP-5139,Bug,Major,ipc,RPC call throws IllegalArgumentException complaining duplicate metrics registration,Here is the error log:    INFO  ipc.Server (Server.java:run(968)) - IPC Server handler 7 on 51017; call addBlock(/file7; DFSClient_-2132593831) from 127.0.0.1:51030: error: java.io.IOException: java.lang.IllegalArgumentException: Duplicate metricsName:addBlock    java.io.IOException: java.lang.IllegalArgumentException: DuplicatemetricsName:addBlock         at org.apache.hadoop.metrics.util.MetricsRegistry.add(MetricsRegistry.java:56)         at org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.init(MetricsTimeVaryingRate.java:89)         at org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.init(MetricsTimeVaryingRate.java:99)         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:522)         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)         at java.security.AccessController.doPrivileged(Native Method)         at javax.security.auth.Subject.doAs(Subject.java:396)         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953),Closed,Fixed,,Hairong Kuang,Hairong Kuang,Wed; 28 Jan 2009 23:02:40 +0000,Thu; 23 Apr 2009 19:18:02 +0000,Thu; 29 Jan 2009 17:53:31 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5139
MAPREDUCE-406,Bug,Minor,,setting fs.default.name to an invalid URI format kills init thread in JobTracker,If you set fs.default.name in a JobConf object to something that causes java.net.URI to throw an IllegalArgumentException; the job not only fails initalization; but kills the JobInitThread,Resolved,Fixed,,Unassigned,sam rash,Thu; 29 Jan 2009 02:25:39 +0000,Mon; 21 Jul 2014 19:22:54 +0000,Mon; 21 Jul 2014 19:22:54 +0000,,,,,HDFS-301;HADOOP-5687,https://issues.apache.org/jira/browse/MAPREDUCE-406
HADOOP-5141,Sub-task,Major,,Resolving json.jar through ivy ,using json.jar (snapshot) from mvn repository and resolve the chukwa dependency through ivy,Resolved,Won't Fix,,Giridharan Kesavan,Giridharan Kesavan,Thu; 29 Jan 2009 09:08:08 +0000,Thu; 1 Oct 2009 07:37:54 +0000,Thu; 1 Oct 2009 07:37:54 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5141
HADOOP-5142,Bug,Minor,io,MapWritable#putAll does not store classes,MapWritable's putAll method does not call addToMap for keys and values as MapWritable#put does. So new classes will not be storedin class-id maps and will lead to problems during readFields.,Closed,Fixed,,Unassigned,Doƒüacan G√ºney,Thu; 29 Jan 2009 19:39:25 +0000,Thu; 23 Apr 2009 19:18:02 +0000,Mon; 23 Feb 2009 10:55:58 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5142
HDFS-246,New Feature,Major,,Add a method to get file length for Seekable; FSDataInputStream and libhdfs,When open any seekable file; it should be able to get the length of the file via Seekable interface; since the seek method should be able to detect seeking beyond the end of file. Such interface can benefit distributed file systems by saving a network round-trip of FileSystem.getFileStatus(Path).getLen() for any open file.In libhdfs; such interface should also be exposed to make native program taking advantage of this change.I have the changes locally for all FSInputStream concrete classes. The change can be considered trivial; since some of the FSInputStream classes already have a method named getFileLength(); or a member field named size/length/end.,Open,Unresolved,,Qi Liu,Qi Liu,Thu; 29 Jan 2009 23:30:25 +0000,Tue; 3 May 2011 11:58:50 +0000,,,,,,HDFS-691;HDFS-814,https://issues.apache.org/jira/browse/HDFS-246
HADOOP-5144,Improvement,Major,,manual way of turning on restore of failed storage replicas for namenode,"when HADOOP-4885 is implemented and committed we will automatic facility to restore failed storage replicas for namenode. Currently it is controlled by configuration file.But since it is quite a ""sensitive"" feature we might want to have a manual way of turning it on. (i.e. starting with the feature turned off; and turn it on when needed).we can use hadoop dfsadmin script to do this.",Closed,Fixed,,Boris Shkolnik,Boris Shkolnik,Fri; 30 Jan 2009 00:55:29 +0000,Fri; 30 Aug 2013 14:45:34 +0000,Fri; 27 Feb 2009 22:50:03 +0000,,,,HADOOP-4885,,https://issues.apache.org/jira/browse/HADOOP-5144
HADOOP-5145,Bug,Major,,Balancer sometimes runs out of memory after days or weeks running,The culprit is a HashMap called MovedBlocks. By design this map does not get cleaned up between iterations. This is because the deletion of source replicas is done by NN. When next iteration starts; source replicas may not have been deleted; Balancer does not want to schedule them to move again. To prevent running out of memory; Balancer should expire/clean the movedBlocks from some iterations back.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Fri; 30 Jan 2009 01:22:28 +0000,Wed; 8 Jul 2009 16:43:30 +0000,Thu; 5 Mar 2009 18:32:13 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5145
HADOOP-5146,Bug,Blocker,,LocalDirAllocator misses files on the local filesystem,For some reason the LocalDirAllocator.getLocaPathToRead doesn't find files which are present; extra logging shows:,Closed,Fixed,,Devaraj Das,Arun C Murthy,Fri; 30 Jan 2009 02:37:18 +0000,Wed; 8 Jul 2009 16:53:18 +0000,Sat; 28 Feb 2009 07:28:24 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5146
HADOOP-5147,Improvement,Minor,,remove refs to slaves file,The Chukwa bin scripts refer to conf/slaves.  This should be conf/chukwa-agents.Further; stop-agents should not touch cron; since start-agents doesn't.,Closed,Fixed,,Ari Rabkin,Ari Rabkin,Fri; 30 Jan 2009 04:23:42 +0000,Tue; 24 Aug 2010 20:35:20 +0000,Tue; 10 Feb 2009 00:55:26 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5147
HADOOP-5148,Bug,Minor,,make watchdog disable-able,A scripting bug leaves watchdogging always on.,Closed,Fixed,,Ari Rabkin,Ari Rabkin,Fri; 30 Jan 2009 04:26:07 +0000,Tue; 24 Aug 2010 20:35:22 +0000,Tue; 10 Feb 2009 01:02:29 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5148
HADOOP-5149,Bug,Minor,,HistoryViewer throws IndexOutOfBoundsException when there are files or directories not confrming to log file name convention,When running history viewer in local mode (specifying file:///path/to/hodlogs as path to logs); it throws IndexOutOfBoundsException due to the following code: The reason is because there are some directories under the log directories that do not conform to the log file naming convention; and the length of the jobDetails array is 1.History viewer should be more defensive and ignore (possibly with warning) files or directories that it does not recognize.,Closed,Fixed,,Hong Tang,Hong Tang,Fri; 30 Jan 2009 18:23:15 +0000,Tue; 24 Aug 2010 20:35:22 +0000,Thu; 26 Mar 2009 09:14:48 +0000,,0.21.0,,,HADOOP-5550,https://issues.apache.org/jira/browse/HADOOP-5149
HADOOP-5150,Bug,Minor,build,ant binary wastes time building documentations that are not needed in the packaging,,Closed,Duplicate,NULL,Unassigned,Hong Tang,Fri; 30 Jan 2009 19:36:32 +0000,Thu; 23 Apr 2009 19:25:03 +0000,Wed; 18 Feb 2009 07:27:35 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5150
HADOOP-5151,Bug,Trivial,documentation,hdfs_quota_admin_guide.html 'q' in setQuota; etc not properly capitalized,"This is trivial; but unfortunately significant. The quota admin guide has entries like; ""dfsadmin -setquota N directory...directory"" and ""dfsadmin -clrquota directory...director"" (oh hey that's missing a 'y').The dfsadmin command is case-sensitive:$ hadoop dfsadmin -setquota 200 /user/marcosetquota: Unknown commandbut setQuota works:$ hadoop dfsadmin -setQuota 200 /user/marco$",Closed,Duplicate,NULL,Unassigned,Marco Nicosia,Fri; 30 Jan 2009 23:09:25 +0000,Thu; 23 Apr 2009 19:25:03 +0000,Fri; 30 Jan 2009 23:20:20 +0000,,0.18.0,,,,https://issues.apache.org/jira/browse/HADOOP-5151
HADOOP-5152,Improvement,Minor,io,better messaging when tasktacker cannot access userlogs,When the tasktracker cannot access the userlogs directory all the tasks fail but the only message in the logs is:2009-01-28 16:00:37;024 WARN org.apache.hadoop.mapred.TaskRunner: attempt_200901280756_0001_m_000534_1 Child Errorjava.io.IOException: Task process exit with nonzero status of 1.        at org.apache.hadoop.mapred.TaskRunner.runChild(TaskRunner.java:462)        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:403)It should write to the log what the actual problem is.,Open,Unresolved,,Unassigned,David O'Dell,Fri; 30 Jan 2009 23:36:37 +0000,Mon; 21 Jul 2014 19:24:12 +0000,,,0.18.2,newbie,,,https://issues.apache.org/jira/browse/HADOOP-5152
MAPREDUCE-183,Bug,Major,,The description for some of the configuration entries in the default xml files are outdated and needs to be updated,Description for some configuration entries in mapred-default.xml are outdated. For example; io.sort.mb. This file needs to be revisited and descriptions updated. The same is probably true for the other defaults as well (core-defaults and hdfs-defaults),Resolved,Fixed,,Unassigned,Jothi Padmanabhan,Mon; 2 Feb 2009 07:02:52 +0000,Mon; 21 Jul 2014 19:25:09 +0000,Mon; 21 Jul 2014 19:25:09 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-183
HADOOP-5154,Bug,Blocker,,4-way deadlock in FairShare scheduler,This happened while trying to change the priority of a job from the scheduler servlet.,Closed,Fixed,,Matei Zaharia,Vinod Kumar Vavilapalli,Mon; 2 Feb 2009 07:41:21 +0000,Wed; 8 Jul 2009 16:41:04 +0000,Wed; 25 Feb 2009 15:05:27 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5154
HADOOP-7713,Improvement,Trivial,,dfs -count -q should label output column,These commands should label the output columns: Current output of the 2nd command above: It is not obvious what these columns mean.,Resolved,Fixed,HDFS-2049;HDFS-11801,Jonathan Allen,Nigel Daley,Mon; 2 Feb 2009 19:15:45 +0000,Thu; 11 May 2017 01:10:16 +0000,Thu; 5 Feb 2015 15:45:17 +0000,,,newbie,,HDFS-7701,https://issues.apache.org/jira/browse/HADOOP-7713
HADOOP-5156,Bug,Major,test,TestHeartbeatHandling uses MiniDFSCluster.getNamesystem() which does not exist in branch 0.20,This breaks branch 0.20 build; which currently does not compile.This will probably require promoting HADOOP-5017 to branch 0.20 or simply using cluster.getNameNode().getNamesystem() in this test.,Closed,Fixed,,Hairong Kuang,Konstantin Shvachko,Mon; 2 Feb 2009 21:30:53 +0000,Wed; 8 Jul 2009 16:43:31 +0000,Mon; 2 Feb 2009 22:38:16 +0000,,0.19.1;0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5156
HDFS-196,Bug,Major,,File length not reported correctly after application crash,Our application (Hypertable) creates a transaction log in HDFS.  This log is written with the following pattern:out_stream.write(header; 0; 7);out_stream.sync()out_stream.write(data; 0; amount);out_stream.sync()...However; if the application crashes and then comes back up again; the following statementlength = mFilesystem.getFileStatus(new Path(fileName)).getLen();returns the wrong length.  Apparently this is because this method fetches length information from the NameNode which is stale.  Ideally; a call to getFileStatus() would return the accurate file length by fetching the size of the last block from the primary datanode.,Reopened,Unresolved,,Unassigned,Doug Judd,Mon; 2 Feb 2009 22:12:39 +0000,Wed; 10 Jun 2015 06:15:54 +0000,,,,,,,https://issues.apache.org/jira/browse/HDFS-196
HADOOP-5158,Improvement,Major,,Port HDFS space quotas to 0.18,0.18 already has quotas for HDFS namespace (HADOOP-3187). HADOOP-3938 implements similar quotas for disk space on HDFS in 0.19. This jira proposes to port HADOOP-3938 to 0.18.4.,Resolved,Fixed,,Raghu Angadi,Raghu Angadi,Mon; 2 Feb 2009 23:13:42 +0000,Wed; 8 Jul 2009 16:43:31 +0000,Mon; 23 Feb 2009 23:42:46 +0000,,0.18.3,,,HADOOP-3938;HADOOP-4254;HADOOP-4449,https://issues.apache.org/jira/browse/HADOOP-5158
HDFS-402,Improvement,Minor,,Display the server version in dfsadmin -report,As part of HADOOP-5094; it was requested to include the server version in the dfsadmin -report; to avoid the need to screen scrape to get this information:Please do provide the server version; so there is a quick and non-taxing way of determine what is the current running version on the namenode.Currently there is nothing in the dfs client protocol to query this information.,Reopened,Unresolved,,Uma Maheswara Rao G,Jakob Homan,Tue; 3 Feb 2009 01:18:29 +0000,Wed; 16 Dec 2015 18:43:31 +0000,,,,newbie,,,https://issues.apache.org/jira/browse/HDFS-402
MAPREDUCE-68,Bug,Major,,Hadoop reduce scheduler sometimes leaves machines idle,I have a MapReduce application with number of reducers equal to the number of machines in the cluster (and with speculative execution turned off). However; Hadoop schedules multiple reduces to run on single machines and leaves other machines idle. This causes contention and seriously slows down the job. Hadoop should employ the simple heuristic of utilizing as many machines as possible when scheduling reduces.,Resolved,Not A Problem,,Unassigned,Nathan Marz,Tue; 3 Feb 2009 03:22:55 +0000,Sat; 31 Dec 2011 09:53:21 +0000,Sat; 31 Dec 2011 09:53:21 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-68
HADOOP-5161,Bug,Major,,Accepted sockets do not get placed in DataXceiverServer#childSockets,DAtaXceiver#childSockets is a map that keeps track of all open sockets that are accepted by DataXceiverServer but no socket does get added to this map.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Tue; 3 Feb 2009 20:11:39 +0000,Wed; 8 Jul 2009 16:43:31 +0000,Fri; 6 Feb 2009 22:50:43 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5161
CHUKWA-18,New Feature,Major,MR Data Processors,Add alias to the reducer side,Demux can run the same Mapper class for more than one DataType using alias.The goal is todo the same thing on the reducer side.,Open,Unresolved,,Unassigned,Jerome Boulon,Wed; 4 Feb 2009 00:16:00 +0000,Tue; 22 Mar 2016 03:00:40 +0000,,,,,,CHUKWA-292,https://issues.apache.org/jira/browse/CHUKWA-18
HADOOP-5163,Improvement,Major,,FSNamesystem#getRandomDatanode() should not use Replicator to choose a random datanode,Below is the code:  public DatanodeDescriptor getRandomDatanode() {    return replicator.chooseTarget(1; null; null; 0)[0];  }Using Replicator to choose a random datanode is an overkill. It is very expensive and unnecessary.,Closed,Duplicate,NULL,Jakob Homan,Hairong Kuang,Wed; 4 Feb 2009 00:44:48 +0000,Tue; 24 Aug 2010 20:35:24 +0000,Tue; 19 May 2009 22:54:12 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5163
HADOOP-5164,Bug,Major,test,Subclasses of ClusterMapReduceTestCase can't easily add new configuration parameters,Currently there is not a clean way for subclasses of ClusterMapReduceTestCase to add to the JobConf used to start the cluster daemons.The startCluster() method does take a Properties object that is added to the JobConf used to the start the daemons.  However; startCluster() is called from JUnit inside the setUp() method; which sets this parameter to be null.If you try to override setUp() in a subclass of ClusterMapReduceTestCase; then you won't be able to invoke the TestCase.setUp() ancestor without calling ClusterMapReduceTestCase's setUp() (which will pass in the null parameter).  On the other hand; if you just call startCluster() within your test method; then you would be starting up a cluster that was already started.,Open,Unresolved,,Unassigned,George Porter,Wed; 4 Feb 2009 02:59:45 +0000,Thu; 26 Mar 2009 08:56:03 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-5164
MAPREDUCE-132,Bug,Major,jobtracker,getSetupAndCleanupTasks should return multiple tasks in a heartbeat,getSetupAndCleanupTasks in JobTracker returns one task per heartbeat. With HADOOP-3136; schedulers give multiple tasks  per heartbeat; getSetupAndCleanupTasks should also give multiple tasks per heartbeat.,Resolved,Incomplete,,Amareshwari Sriramadasu,Amareshwari Sriramadasu,Wed; 4 Feb 2009 03:24:50 +0000,Mon; 21 Jul 2014 19:38:17 +0000,Mon; 21 Jul 2014 19:38:17 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-132
HADOOP-5166,Bug,Blocker,,JobTracker fails to restart if recovery and ACLs are enabled,JobTracker fails to restart and throw NullPointerException when mapred.jobtracker.restart.recover and mapred.acls.enabled are set to true2009-02-04 12:28:19;834 FATAL org.apache.hadoop.mapred.JobTracker: java.lang.NullPointerException        at org.apache.hadoop.mapred.QueueManager.hasAccess(QueueManager.java:185)        at org.apache.hadoop.mapred.JobTracker.checkAccess(JobTracker.java:2694)        at org.apache.hadoop.mapred.JobTracker.addJob(JobTracker.java:2663)        at org.apache.hadoop.mapred.JobTracker.access$2300(JobTracker.java:86)        at org.apache.hadoop.mapred.JobTracker$RecoveryManager.recover(JobTracker.java:1099)        at org.apache.hadoop.mapred.JobTracker.offerS,Closed,Fixed,,Amar Kamat,Karam Singh,Wed; 4 Feb 2009 13:27:12 +0000,Wed; 8 Jul 2009 16:53:15 +0000,Thu; 12 Feb 2009 14:17:02 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5166
MAPREDUCE-190,Bug,Major,,MultipleOutputs should use newer Hadoop serialization interface since 0.19,We have a system based on Hadoop 0.18 / Cascading 0.8.1 and now I'm trying to port it to Hadoop 0.19 / Cascading 1.0. The first serious problem I've got into that we're extensively using MultipleOutputs in our jobs dealing with sequence files that store Cascading's Tuples.Since Cascading 0.9; Tuples stopped being WritableComparable and implemented generic Hadoop serialization interface and framework. However; in Hadoop 0.19; MultipleOutputs require use of older WritableComparable interface. Thus; trying to do something like:  yields an error: MultipleOutputs should eventually be ported to use more generic Hadoop serialization; as I understand.,Resolved,Incomplete,,Unassigned,Mikhail Yakshin,Wed; 4 Feb 2009 17:16:18 +0000,Mon; 21 Jul 2014 19:40:00 +0000,Mon; 21 Jul 2014 19:40:00 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-190
CHUKWA-16,New Feature,Major,Data Processors,Load different dataType in parallele,DatabaseLoader is loading all sequence files in sequence.Instead it should only load sequence files for the same cluster/stream in sequence.,Resolved,Duplicate,NULL,Jerome Boulon,Jerome Boulon,Wed; 4 Feb 2009 17:52:47 +0000,Fri; 26 Jun 2009 01:04:28 +0000,Fri; 26 Jun 2009 01:04:28 +0000,,,,,,https://issues.apache.org/jira/browse/CHUKWA-16
MAPREDUCE-354,Improvement,Major,,the map output servlet should only locate the index if it isn't in the cache,Currently; the map output servlet locates the cache file using the local dir allocator; before it determines whether the information is in the cache. It would avoid a lot of filesystem lookups if it waited.,Resolved,Incomplete,,Owen O'Malley,Owen O'Malley,Wed; 4 Feb 2009 21:19:32 +0000,Mon; 21 Jul 2014 19:41:42 +0000,Mon; 21 Jul 2014 19:41:42 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-354
HADOOP-5170,New Feature,Major,,Set max map/reduce tasks on a per-job basis; either per-node or cluster-wide,"There are a number of use cases for being able to do this.  The focus of this jira should be on finding what would be the simplest to implement that would satisfy the most use cases.This could be implemented as either a per-node maximum or a cluster-wide maximum.  It seems that for most uses; the former is preferable however either would fulfill the requirements of this jira.Some of the reasons for allowing this feature (mine and from others on list):	I have some very large CPU-bound jobs.  I am forced to keep the max map/node limit at 2 or 3 (on a 4 core node) so that I do not starve the Datanode and Regionserver.  I have other jobs that are network latency bound and would like to be able to run high numbers of them concurrently on each node.  Though I can thread some jobs; there are some use cases that are difficult to thread (scanning from hbase) and there's significant complexity added to the job rather than letting hadoop handle the concurrency.	Poor assignment of tasks to nodes creates some situations where you have multiple reducers on a single node but other nodes that received none.  A limit of 1 reducer per node for that job would prevent that from happening. (only works with per-node limit)	Poor mans MR job virtualization.  Since we can limit a jobs resources; this gives much more control in allocating and dividing up resources of a large cluster.  (makes most sense w/ cluster-wide limit)",Closed,Won't Fix,,Matei Zaharia,Jonathan Gray,Wed; 4 Feb 2009 21:23:21 +0000,Tue; 15 Oct 2013 17:30:41 +0000,Thu; 9 Jul 2009 21:21:23 +0000,,,,,MAPREDUCE-704;MAPREDUCE-698;MAPREDUCE-5583,https://issues.apache.org/jira/browse/HADOOP-5170
MAPREDUCE-512,Test,Minor,,Add tests for the DFS HTML and JSP pages,"Add some basic tests to look for the standard JSP pages on a locally deployed MiniMR cluster1. namenode: check that dfshealth is present2. datanode: check that all the datanode JSPs load3. GET the standard servlets.The initial checks can just use httpclient to GET the pages; no need (yet) for HtmlUnit. If the tests were designed to take optional URLs  (e.g test.namenode.url and test.datanode.url) they could be run against processes brought up externally/remotelyThey would	help test that the JSP pages are being compiled down and bundled into the JARS	verify the classpath is getting set up right	check that the Jasper engine is working	check the servlets are all registeringI've effectively had to do this in my own code; having a set of these tests inside hadoop would make it easier to point the blame at the classpath setup or something else.",Resolved,Won't Fix,,Steve Loughran,Steve Loughran,Wed; 4 Feb 2009 23:28:13 +0000,Mon; 26 Jan 2015 22:49:14 +0000,Mon; 26 Jan 2015 22:49:14 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-512
HADOOP-5172,Bug,Major,test,Chukwa : TestAgentConfig.testInitAdaptors_vs_Checkpoint regularly fails ,org.apache.hadoop.chukwa.datacollection.agent.TestAgentConfig.testInitAdaptors_vs_Checkpoint regularly fails in Hudson builds. I am not sure which branches it affects. I will attach one of the failure logs.,Closed,Fixed,,Jerome Boulon,Raghu Angadi,Thu; 5 Feb 2009 06:59:34 +0000,Tue; 24 Aug 2010 20:35:26 +0000,Fri; 13 Feb 2009 00:44:49 +0000,,,,,CHUKWA-10,https://issues.apache.org/jira/browse/HADOOP-5172
MAPREDUCE-645,Bug,Minor,distcp,When disctp is used to overwrite a file; it should return immediately with an error message,"When disctp is triggered to copy a directory to an already existing file; it just shows a ""copy failed"" error message after 4 attempts without showing any useful error message. This is extremely time consuming on a large cluster and especially when the directory being copied contains several sub-directories.Instead; it would be an improvement if distcp could return immediately displaying a useful error message when an user attempts such an operation. (This is an unlikely situation but still a valid test case)",Closed,Fixed,,Ravi Gummadi,Ramya Sunil,Thu; 5 Feb 2009 12:08:28 +0000,Tue; 24 Aug 2010 21:14:01 +0000,Fri; 18 Sep 2009 06:54:56 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-645
HADOOP-5174,Bug,Major,,Pipes example throws NullPointerException on trunk when run with LocalJobRunner,Running the wordcount example (http://wiki.apache.org/hadoop/C++WordCount) fails with the following exception when run with the LocalJobRunner:,Closed,Duplicate,MAPREDUCE-476,Unassigned,Vinod Kumar Vavilapalli,Thu; 5 Feb 2009 12:15:42 +0000,Wed; 8 Jul 2009 16:41:17 +0000,Wed; 11 Feb 2009 04:36:17 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5174
HADOOP-5175,New Feature,Major,,Option to prohibit jars unpacking,"I've noticed that task tracker moves all unpacked jars into ${hadoop.tmp.dir}/mapred/local/taskTracker.We are using a lot of external libraries; that are deployed via ""-libjars"" option. The total number of files after unpacking is about 20 thousands.After running a number of jobs; tasks start to be killed with timeout reason (""Task attempt_200901281518_0011_m_000173_2 failed to report status for 601 seconds. Killing!""). All killed tasks are in ""initializing"" state. I've watched the tasktracker logs and found such messages:Thread 20926 (Thread-10368):  State: BLOCKED  Blocked count: 3611  Waited count: 24  Blocked on java.lang.ref.Reference$Lock@e48ed6  Blocked by 20882 (Thread-10341)  Stack:    java.lang.StringCoding$StringEncoder.encode(StringCoding.java:232)    java.lang.StringCoding.encode(StringCoding.java:272)    java.lang.String.getBytes(String.java:947)    java.io.UnixFileSystem.getBooleanAttributes0(Native Method)    java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:228)    java.io.File.isDirectory(File.java:754)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:427)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)HADOOP-4780 patch brings the code which stores map of directories along with their DU's; thus reducing the number of calls to DU. However; the delete operation takes too long. I've manually deleted archive after 10 jobs had run and it took over 30 minutes on XFS.I suppose that an option to prohibit jars unpacking would be helpfull in my situation.",Closed,Fixed,,Todd Lipcon,Andrew Gudkov,Thu; 5 Feb 2009 15:57:22 +0000,Tue; 24 Aug 2010 20:35:27 +0000,Wed; 27 May 2009 11:01:04 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5175
HADOOP-5176,Improvement,Trivial,benchmarks,TestDFSIO reports itself as TestFDSIO,"When TestDFSIO starts up; it reports itself as ""TestFSDIO""; which would seem to be a typo.",Closed,Fixed,,Ravi Phulari,Bryan Duxbury,Thu; 5 Feb 2009 19:07:31 +0000,Tue; 24 Aug 2010 20:35:29 +0000,Mon; 9 Feb 2009 19:30:32 +0000,,0.18.1;0.18.2;0.18.3;0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5176
HADOOP-5177,Bug,Major,,Default number of xceivers should be increased dramatically,"On our cluster; unless we manually increase the number of xcievers via the property dfs.datanode.max.xcievers; our jobs fail with the typical exception being ""Could not get block locations..."". We have the xcievers property set to 8000 now which resolved the issue for us. Clearly; the Hadoop default should be much higher than what it is now (256 I believe).",Resolved,Duplicate,NULL,Unassigned,Nathan Marz,Thu; 5 Feb 2009 19:10:06 +0000,Mon; 25 Apr 2011 17:14:15 +0000,Mon; 25 Apr 2011 17:14:15 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5177
MAPREDUCE-74,Bug,Major,,JobClient hangs when getting map-task reports,a call to JobClient.getMapTaskReports() has been hung for 6+ days,Resolved,Not A Problem,,Unassigned,sam rash,Thu; 5 Feb 2009 22:44:39 +0000,Sat; 31 Dec 2011 09:54:01 +0000,Sat; 31 Dec 2011 09:54:01 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-74
HADOOP-5179,Bug,Major,,FileSystem#copyToLocalFile shouldn't copy .crc files,The .crc files shouldn't be copied locally; as they are an internal Hadoop filesystem thing. This is causing the following problem for me:I sometimes copy a directory from HDFS locally; modify those files; and then reupload them somewhere else in HDFS. I then get checksum errors on the re-upload.,Resolved,Duplicate,HADOOP-7178,Unassigned,Nathan Marz,Thu; 5 Feb 2009 23:59:19 +0000,Mon; 21 Jul 2014 19:43:04 +0000,Mon; 21 Jul 2014 19:43:04 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5179
MAPREDUCE-420,Bug,Trivial,,FileNotFoundException when finishing a profiled task that doesn't generate an output file,"When running a hadoop job with mapred.task.profile=true; hadoop assumes that mapred.task.profile.params contains a ""%s"" that can be changed for a file name that will be created by the profiler containing its text output. If the profiler doesn't generate such an output file; profiled tasks will raise a FileNotFound exception at the end of their execution:09/02/05 15:39:10 INFO mapred.JobClient:  map 100% reduce 0%09/02/05 15:39:10 INFO mapred.JobClient: Communication problem with server: java.io.FileNotFoundException: http://machine1:50060/tasklog?plaintext=truetaskid=attempt_200901221838_0043_m_000000_0filter=profile        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1239)        at org.apache.hadoop.mapred.JobClient.downloadProfile(JobClient.java:1089)        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1166)        at JavaCat.main(JavaCat.java:46)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        at java.lang.reflect.Method.invoke(Method.java:597)        at org.apache.hadoop.util.RunJar.main(RunJar.java:165)        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)This happens because it's assumed users will use the default hprof profiler; which actually allows you to set the output file name. Not all profilers are like that and some don't have any option to create a text output log at all or don't allow you to change the name of the output file. In my case; I'm a profiler that doesn't generate text output and only allows you to set the output directory for the snapshot files it generates.",Resolved,Duplicate,MAPREDUCE-105,Rodrigo Schmidt,Rodrigo Schmidt,Fri; 6 Feb 2009 00:59:20 +0000,Tue; 13 Sep 2011 15:55:24 +0000,Wed; 6 Jan 2010 19:27:39 +0000,,,,,HADOOP-2367,https://issues.apache.org/jira/browse/MAPREDUCE-420
HDFS-219,New Feature,Major,,Add md5sum facility in dfsshell,I think it would be usefull to add md5sum (or anyone else) to dfsshell ;and the facility can verify the file on hdfs.It can confirm the file is integrity after copyFromLocal or copyToLocal.,Resolved,Duplicate,HADOOP-9209,Unassigned,zhangwei,Fri; 6 Feb 2009 02:07:18 +0000,Mon; 11 May 2015 14:52:23 +0000,Sat; 9 May 2015 14:21:47 +0000,,,newbie,,,https://issues.apache.org/jira/browse/HDFS-219
HADOOP-5182,Bug,Major,,Task processes not exiting due to ackQueue bug in DFSClient,"I was running some gridmix tests on a 10 node cluster on EC2 and ran into an issue with unmodified Hadoop trunk revision (SVN revision#  734915). After running gridmix multiple times; I noticed several mapreduce jobs stuck in the running state. They remained in that hung state for several days; while other gridmixes of that size finished in approximately 8 hours; and are still in that state actually.I saw that the slave nodes had a bunch of hung task processes running; for tasks that the JobTracker log said were completed. These were hanging because the SIGTERM handler was waiting on DFSClient to close existing streams; but this was never finishing because DFSOutputStream waits on an ackQueue from the datanodes that was apparently getting no acks. The tasks did finish their work; but the processes hung around.I'll attached a sample jstack trace - note how the SIGTERM handler is blocked on ""thread-5""; which is waiting for a monitor on the DFSOutputStream; but this stream's monitor is held by main; which is trying to flush the stream (last trace in the file).Has anyone seen this issue before?Another thing I saw was cleanup tasks never running (they are stuck in the initializing state on the web UI and can't be seen as running processes on the nodes). Not sure if that is actually related.",Resolved,Duplicate,NULL,Unassigned,Andy Konwinski,Fri; 6 Feb 2009 02:27:29 +0000,Wed; 8 Jul 2009 16:43:31 +0000,Wed; 13 May 2009 05:55:50 +0000,,0.20.0;0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5182
HDFS-61,Bug,Major,,Datanode shutdown is called multiple times ,"When DataNode gets IncorrectVersionException in DataNode.offerService() DataNode.shutdown() is called	In DataNode.processCommand() when DataNode gets DNA_SHUTDOWN; DataNode.shutdown() is calledDataNode.shutdown() is again called in DataNode.run() method",Resolved,Cannot Reproduce,,Suresh Srinivas,Suresh Srinivas,Fri; 6 Feb 2009 03:32:03 +0000,Thu; 29 Dec 2011 12:42:16 +0000,Thu; 29 Dec 2011 12:42:15 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-61
MAPREDUCE-143,Bug,Major,,OOM in the TaskTracker while serving map outputs,Saw this exception in the TT logs:2009-02-06 06:18:08;553 ERROR org.mortbay.log: EXCEPTIONjava.lang.OutOfMemoryError: GC overhead limit exceeded2009-02-06 06:18:11;247 ERROR org.mortbay.log: Error for /mapOutputjava.lang.OutOfMemoryError: GC overhead limit exceeded2009-02-06 06:18:11;247 ERROR org.mortbay.log: Error for /mapOutputjava.lang.OutOfMemoryError: Java heap space        at java.nio.HeapByteBuffer.init(HeapByteBuffer.java:39)        at java.nio.ByteBuffer.allocate(ByteBuffer.java:312)        at org.mortbay.io.nio.IndirectNIOBuffer.init(IndirectNIOBuffer.java:28)        at org.mortbay.jetty.nio.AbstractNIOConnector.newBuffer(AbstractNIOConnector.java:71)        at org.mortbay.jetty.AbstractBuffers.getBuffer(AbstractBuffers.java:131)        at org.mortbay.jetty.HttpGenerator.addContent(HttpGenerator.java:145)        at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:642)        at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:577)        at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:2879),Resolved,Incomplete,,Unassigned,Devaraj Das,Fri; 6 Feb 2009 07:49:45 +0000,Mon; 21 Jul 2014 19:44:43 +0000,Mon; 21 Jul 2014 19:44:43 +0000,,,,MAPREDUCE-2510,,https://issues.apache.org/jira/browse/MAPREDUCE-143
MAPREDUCE-540,Bug,Major,,Upate thread in FairScheduler runs too frequently,The UpdateThread in FairScheduler runs every 500ms (hardcoded). This proves to be very costly when running large clusters. UpdateThread tries to acquire lock on JT object every that often and so seriously affects HeartBeat processing besides everything else. The update interval should be a function of the cluster size. Or in the minimum it should be configurable and by default should be set to a reasonably high default value.,Resolved,Fixed,,Matei Zaharia,Vinod Kumar Vavilapalli,Fri; 6 Feb 2009 09:38:27 +0000,Tue; 30 Jun 2009 22:00:21 +0000,Tue; 30 Jun 2009 20:49:54 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-540
MAPREDUCE-554,Improvement,Minor,,Improve limit handling in fairshare scheduler,The fairshare scheduler has a way by which it can limit the number of jobs in a pool by setting the maxRunningJobs parameter in its allocations definition. This limit is treated as a hard limit; and comes into effect even if the cluster is free to run more jobs; resulting in underutilization. Possibly the same thing happens with the parameter maxRunningJobs for user and userMaxJobsDefault. It may help to treat these as a soft limit and run additional jobs to keep the cluster fully utilized.,Open,Unresolved,,Unassigned,Hemanth Yamijala,Fri; 6 Feb 2009 11:18:44 +0000,Sat; 20 Jun 2009 08:01:45 +0000,,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-554
MAPREDUCE-555,Improvement,Minor,,Provide an option to turn off priorities in jobs,The fairshare scheduler can define pools mapping to queues (as defined in the capacity scheduler - HADOOP-3445). When used in this manner; one can imagine queues set up to be used by users who come from disparate teams or organizations (say a default queue). For such a queue; it makes sense to ignore job priorities and consider the queue as strict FIFO; as it is difficult to compare priorities of jobs from different users.,Resolved,Fixed,,Unassigned,Hemanth Yamijala,Fri; 6 Feb 2009 11:27:50 +0000,Mon; 21 Jul 2014 19:46:05 +0000,Mon; 21 Jul 2014 19:46:04 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-555
HDFS-311,Improvement,Major,,Modifications to enable multiple types of logging ,,Resolved,Duplicate,NULL,Luca Telloli,Luca Telloli,Fri; 6 Feb 2009 12:16:06 +0000,Thu; 2 May 2013 02:29:26 +0000,Wed; 14 Sep 2011 11:31:49 +0000,,,,HDFS-397,HADOOP-4539,https://issues.apache.org/jira/browse/HDFS-311
HDFS-234,New Feature,Major,,Integration with BookKeeper logging system,BookKeeper is a system to reliably log streams of records (https://issues.apache.org/jira/browse/ZOOKEEPER-276). The NameNode is a natural target for such a system for being the metadata repository of the entire file system for HDFS.,Closed,Fixed,,Ivan Kelly,Luca Telloli,Fri; 6 Feb 2009 14:55:48 +0000,Mon; 28 Sep 2015 20:58:20 +0000,Wed; 9 May 2012 12:10:43 +0000,,,,HADOOP-5721,,https://issues.apache.org/jira/browse/HDFS-234
HADOOP-5190,Task,Major,documentation,Create a privacy policy for the Hadoop website,It would be great to collect analytics about the visitors to the website and to do so; we need to create a privacy policy that tells visitors what we will collect.,Resolved,Fixed,,Tom White,Owen O'Malley,Fri; 6 Feb 2009 17:54:45 +0000,Mon; 9 Mar 2009 18:52:22 +0000,Mon; 9 Mar 2009 18:51:15 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5190
HADOOP-5191,Bug,Minor,,After creation and startup of the hadoop namenode on AIX or Solaris; you will only be allowed to connect to the namenode via hostname but not IP.,"After creation and startup of the hadoop namenode on AIX or Solaris; you will only be allowed to connect to the namenode via hostname but not IP.fs.default.name=hdfs://p520aix61.mydomain.com:9000Hostname for box is p520aix and the IP is 10.120.16.68If you use the following url; ""hdfs://10.120.16.68""; to connect to the namenode; the exception that appears below occurs. You can only connect successfully if ""hdfs://p520aix61.mydomain.com:9000"" is used. Exception in thread ""Thread-0"" java.lang.IllegalArgumentException: Wrong FS: hdfs://10.120.16.68:9000/testdata; expected: hdfs://p520aix61.mydomain.com:9000	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:320)	at org.apache.hadoop.dfs.DistributedFileSystem.checkPath(DistributedFileSystem.java:84)	at org.apache.hadoop.dfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:122)	at org.apache.hadoop.dfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:390)	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:667)	at TestHadoopHDFS.run(TestHadoopHDFS.java:116)",Closed,Fixed,,Raghu Angadi,Bill Habermaas,Fri; 6 Feb 2009 18:04:00 +0000,Tue; 24 Aug 2010 20:35:30 +0000,Wed; 25 Mar 2009 21:01:39 +0000,,0.19.1,,,MAPREDUCE-438,https://issues.apache.org/jira/browse/HADOOP-5191
HADOOP-5192,Bug,Blocker,,Block reciever should not remove a finalized block when block replication fails,HADOOP-4702 makes block receivers to remove the received block in case of a block replication failure. But the block should not be removed if the cause of the failure is that the block to be received already exists. The key is that a block receiver should allow to remove only partial blocks in case of block replication failures.,Closed,Fixed,,Hairong Kuang,Hairong Kuang,Fri; 6 Feb 2009 18:55:25 +0000,Wed; 8 Jul 2009 16:43:31 +0000,Fri; 13 Feb 2009 00:24:34 +0000,,0.18.3,,,,https://issues.apache.org/jira/browse/HADOOP-5192
HADOOP-5193,Bug,Major,,SecondaryNameNode does not rollImage because of incorrect calculation of edits modification time.,Secondary name-node cannot complete the second phase of the checkpoint because getFsEditsTime() returns the mod time of edits.new rather than edits file.The difference is that edits remains unchanged during the whole checkpoint process an therefore can serve as an invariant. On the contrary edits.new is changing all the time since it is the target of the edits log during checkpoint. So comparison of the mod time of edits.new before and after checkpoint fail and name-node does not upload new image file from the secondary node and does not truncate edits files.,Closed,Fixed,,Konstantin Shvachko,Konstantin Shvachko,Fri; 6 Feb 2009 19:21:34 +0000,Wed; 8 Jul 2009 16:43:32 +0000,Sun; 8 Feb 2009 21:45:30 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5193
HADOOP-5194,Bug,Blocker,,DiskErrorException in TaskTracker when running a job,In particular; this can be reproduced in Windows by running a hadoop example such as PiEstimator. (Have changed TaskTracker.java to print out the trace.)This patch disables usage of setsid and pidfiles on Windows.,Closed,Fixed,,Ravi Gummadi,Tsz Wo Nicholas Sze,Fri; 6 Feb 2009 21:18:19 +0000,Tue; 24 Aug 2010 20:35:31 +0000,Thu; 26 Mar 2009 10:26:05 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5194
HADOOP-5195,Bug,Minor,test,Unit tests for TestProxyUgiManager and TestHdfsProxy consistently failing on trunk builds,of the last 10 trunk builds; these unit tests have failed in about 50% of the builds.   Trunk builds have been failing unit tests consistently for as far back as I can see in hudson.,Resolved,Fixed,,Unassigned,Lee Tucker,Fri; 6 Feb 2009 22:09:07 +0000,Mon; 21 Jul 2014 19:46:33 +0000,Mon; 21 Jul 2014 19:46:33 +0000,,0.21.0,,,HADOOP-5956,https://issues.apache.org/jira/browse/HADOOP-5195
HADOOP-5196,Improvement,Minor,io,avoiding unnecessary byte[] allocation in SequenceFile.CompressedBytes and SequenceFile.UncompressedBytes,SequenceFile.CompressedBytes and SequenceFile.UncompressedBytes are used by the SequenceFile's raw bytes reading/writing API. The current implementation does not reuse the internal byte[] and causes unnecessary buffer allocation and initializaiton (zeroing the buffer).,Closed,Fixed,,Hong Tang,Hong Tang,Sat; 7 Feb 2009 02:41:25 +0000,Tue; 24 Aug 2010 20:35:32 +0000,Thu; 19 Mar 2009 17:44:03 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5196
HDFS-405,Test,Minor,,Several unit tests failing on Windows frequently,"This issue is similar to HADOOP-5114. A huge number of unit tests are failing on Windows on branch  18 consistently. 0.21 is showing the maximum number of failures. Failures on other branches are a subset of failures observed in 0.21. Below is the list of failures observed on 0.21.	java.io.IOException: Job failed!			TestJobName - testComplexNameWithRegex		TestJobStatusPersistency - testNonPersistency; testPersistency		TestJobSysDirWithDFS - testWithDFS		TestKillCompletedJob - testKillCompJob		TestMiniMRClasspath - testClassPath; testExternalWritable		TestMiniMRDFSCaching - testWithDFS		TestMiniMRDFSSort - testMapReduceSort; testMapReduceSortWithJvmReuse		TestMiniMRLocalFS - testWithLocal		TestMiniMRWithDFS - testWithDFS; testWithDFSWithDefaultPort		TestMiniMRWithDFSWithDistinctUsers - testDistinctUsers		TestMultipleLevelCaching - testMultiLevelCaching		TestQueueManager - testAllEnabledACLForJobSubmission; testEnabledACLForNonDefaultQueue;  testUserEnabledACLForJobSubmission;  testGroupsEnabledACLForJobSubmission		TestRackAwareTaskPlacement - testTaskPlacement		TestReduceFetch - testReduceFromDisk; testReduceFromPartialMem; testReduceFromMem		TestSpecialCharactersInOutputPath - testJobWithDFS		TestTTMemoryReporting - testDefaultMemoryValues; testConfiguredMemoryValues		TestTrackerBlacklistAcrossJobs - testBlacklistAcrossJobs		TestUserDefinedCounters - testMapReduceJob		TestDBJob - testRun		TestServiceLevelAuthorization - testServiceLevelAuthorization		TestNoDefaultsJobConf - testNoDefaults		TestBadRecords - testBadMapRed		TestClusterMRNotification - testMR		TestClusterMapReduceTestCase - testMapReduce; testMapReduceRestarting		TestCommandLineJobSubmission - testJobShell		TestCompressedEmptyMapOutputs - testMapReduceSortWithCompressedEmptyMapOutputs		TestCustomOutputCommitter - testCommitter		TestJavaSerialization - testMapReduceJob; testWriteToSequencefile		TestJobClient - testGetCounter; testJobList; testChangingJobPriority		TestJobName - testComplexName			java.lang.IllegalArgumentException: Pathname /path from Cpath is not a valid DFS filename.			TestJobQueueInformation - testJobQueues		TestJobInProgress - testRunningTaskCount		TestJobTrackerRestart - testJobTrackerRestart			Timeout			TestKillSubProcesses - testJobKill		TestMiniMRMapRedDebugScript - testMapDebugScript		TestControlledMapReduceJob - testControlledMapReduceJob		TestJobInProgressListener - testJobQueueChanges		TestJobKillAndFail - testJobFailAndKill			junit.framework.AssertionFailedError			TestMRServerPorts - testJobTrackerPorts; testTaskTrackerPorts		TestMiniMRTaskTempDir - testTaskTempDir		TestTaskFail - testWithDFS		TestTaskLimits - testTaskLimits		TestMapReduceLocal - testWithLocal		TestCLI - testAll		TestHarFileSystem - testArchives		TestTrash - testTrash; testNonDefaultFS		TestHDFSServerPorts - testNameNodePorts; testDataNodePorts; testSecondaryNodePorts		TestHDFSTrash - testNonDefaultFS		TestFileOutputFormat - testCustomFile			org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.security.authorize.AuthorizationException: java.security.AccessControlException: access denied ConnectionPermission(org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol)			TestServiceLevelAuthorization - testRefresh			junit.framework.ComparisonFailure			TestDistCh - testDistCh			java.io.FileNotFoundException			TestCopyFiles - testMapCount",Resolved,Fixed,,Unassigned,Ramya Sunil,Mon; 9 Feb 2009 06:36:16 +0000,Mon; 21 Jul 2014 19:46:49 +0000,Mon; 21 Jul 2014 19:46:49 +0000,,,,,HADOOP-5114,https://issues.apache.org/jira/browse/HDFS-405
HADOOP-5198,Bug,Blocker,util,NPE in Shell.runCommand(),"I have seen one of the task failures with following exception:java.lang.NullPointerException	at java.lang.ProcessBuilder.start(ProcessBuilder.java:441)	at org.apache.hadoop.util.Shell.runCommand(Shell.java:149)	at org.apache.hadoop.util.Shell.run(Shell.java:134)	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:286)	at org.apache.hadoop.util.ProcessTree.isAlive(ProcessTree.java:244)	at org.apache.hadoop.util.ProcessTree.sigKillInCurrentThread(ProcessTree.java:67)	at org.apache.hadoop.util.ProcessTree.sigKill(ProcessTree.java:115)	at org.apache.hadoop.util.ProcessTree.destroyProcessGroup(ProcessTree.java:164)	at org.apache.hadoop.util.ProcessTree.destroy(ProcessTree.java:180)	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType$JvmRunner.kill(JvmManager.java:377)	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:249)	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:113)	at org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:76)	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:411)",Closed,Fixed,,Amareshwari Sriramadasu,Amareshwari Sriramadasu,Mon; 9 Feb 2009 10:47:49 +0000,Tue; 24 Aug 2010 20:35:33 +0000,Tue; 31 Mar 2009 09:08:59 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5198
MAPREDUCE-348,Improvement,Major,,A proposal to merge common functionality of various Schedulers,There are at least 3 Schedulers in Hadoop today: Default; Capacity; and Fairshare. Over time; we're seeing a lot of functionality common to all three. Many bug fixes; improvements to existing functionality; and new functionality are applicable to all three schedulers. This trend seems to be getting stronger; as we notice similar problems; solutions; and ideas. This is a proposal to detect and consolidate such common functionality.,Resolved,Won't Fix,,Unassigned,Vivek Ratan,Mon; 9 Feb 2009 12:23:31 +0000,Mon; 21 Jul 2014 19:48:40 +0000,Mon; 21 Jul 2014 19:48:40 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-348
HADOOP-5200,Bug,Minor,,NPE when the namenode comes up but the filesystem is set to file://,If you bring up a namenode and the conf file points to file:/// as the URI; then the authority is null; breaking code that follows,Closed,Duplicate,HADOOP-5687,Steve Loughran,Steve Loughran,Mon; 9 Feb 2009 13:11:20 +0000,Tue; 24 Aug 2010 20:35:33 +0000,Fri; 22 May 2009 13:57:10 +0000,,0.21.0,,,HDFS-301;HADOOP-5687,https://issues.apache.org/jira/browse/HADOOP-5200
MAPREDUCE-253,Improvement,Major,,getDiagnostics in TaskReport should return exceptions,Currently; getDiagnostics() returns Strings. When exceptions are thrown in user code and/or Hadoop; it would be cleaner to propagate the exception back to the application for better error handling. Hadoop should return the exceptions instead of returning string representations that correspond to printStackTrace() output.,Resolved,Won't Fix,,Unassigned,Santhosh Srinivasan,Mon; 9 Feb 2009 17:24:52 +0000,Mon; 20 Jan 2014 18:53:03 +0000,Mon; 20 Jan 2014 18:53:03 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-253
MAPREDUCE-76,Bug,Major,,JobControl should handle exceptions,If the JobControl encounters non IOExceptions; then JobControl fails without reporting failures. JobControl should handle all exceptions and report the failure to launch jobs. In addition; an API to support the querying of failure to launch jobs should be supported.,Resolved,Not A Problem,,Unassigned,Santhosh Srinivasan,Mon; 9 Feb 2009 17:28:46 +0000,Wed; 11 Jul 2012 03:19:05 +0000,Wed; 11 Jul 2012 03:19:05 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-76
HADOOP-5203,Bug,Major,,TT's version build is too restrictive,"At start time; TT checks whether its version is compatible with JT.The condition is too restrictive. It will shut down itself if one of the following conditions fail:	the version numbers must match	the revision numbers must match	the user ids who build the jar must match	the build times must matchI think it should check the major part of the version numbers only (thus any version like 0.19.xxxx should be compatible).",Closed,Fixed,,Rick Cox,Runping Qi,Mon; 9 Feb 2009 17:34:07 +0000,Sun; 25 Mar 2012 22:20:07 +0000,Wed; 13 May 2009 06:31:28 +0000,,0.19.0,,,HADOOP-8209,https://issues.apache.org/jira/browse/HADOOP-5203
HADOOP-5204,Bug,Blocker,build,hudson trunk build failure due to autoheader failure in create-c++-configure-libhdfs task,create-c++-configure-libhdfs:     exec autoheader: warning: missing template: HADOOP_CONF_DIR     exec autoheader: Use AC_DEFINE(HADOOP_CONF_DIR; []; Description)     exec autoreconf: /usr/bin/autoheader failed with exit status: 1See output at: http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-trunk/746/,Closed,Fixed,,Sreekanth Ramakrishnan,Lee Tucker,Mon; 9 Feb 2009 19:35:48 +0000,Tue; 24 Aug 2010 20:35:35 +0000,Tue; 10 Feb 2009 15:46:03 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5204
HADOOP-5205,Improvement,Major,,"Change CHUKWA_IDENT_STRING from ""demo"" to ""TODO-AGENTS-INSTANCE-NAME""",,Closed,Fixed,,Jerome Boulon,Jerome Boulon,Mon; 9 Feb 2009 19:57:35 +0000,Tue; 24 Aug 2010 20:35:36 +0000,Thu; 26 Feb 2009 00:31:15 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5205
HADOOP-5206,Bug,Major,,"All ""unprotected*"" methods of FSDirectory should synchronize on the root.","Synchronization on rootDir is missing for two (relatively new) methods:	unprotectedSetQuota()	unprotectedSetTimes()",Closed,Fixed,,Jakob Homan,Konstantin Shvachko,Mon; 9 Feb 2009 20:01:24 +0000,Tue; 24 Aug 2010 20:35:37 +0000,Tue; 17 Feb 2009 18:12:21 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5206
HADOOP-5207,Bug,Major,,Some core tests not executed by Hudson,Since Hudson build #3816; there are 670 tests not executed by Hudson compared with build #3815.,Closed,Duplicate,NULL,Unassigned,Tsz Wo Nicholas Sze,Mon; 9 Feb 2009 22:18:08 +0000,Thu; 23 Apr 2009 19:25:03 +0000,Tue; 10 Feb 2009 07:07:14 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5207
HADOOP-5208,Bug,Major,build,"SAXParseException: ""id"" must not contain the '<' character",Found the following exception in recent Hudson builds (e.g. see build #3821):,Closed,Duplicate,NULL,Giridharan Kesavan,Tsz Wo Nicholas Sze,Mon; 9 Feb 2009 22:32:35 +0000,Thu; 23 Apr 2009 19:25:03 +0000,Mon; 23 Feb 2009 03:39:31 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5208
HADOOP-5209,Bug,Minor,documentation,Update year to 2009 for javadoc,The year is still 2008 in the generated javadoc.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Mon; 9 Feb 2009 23:41:12 +0000,Tue; 24 Aug 2010 20:35:38 +0000,Thu; 19 Feb 2009 01:14:01 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5209
HADOOP-5210,Bug,Minor,,Reduce Task Progress shows > 100% when the total size of map outputs (for a single reducer) is high ,When the total map outputs size (reduce input size) is high; the reported progress is greater than 100%.,Resolved,Fixed,,Ravi Gummadi,Jothi Padmanabhan,Tue; 10 Feb 2009 07:36:29 +0000,Fri; 14 Jan 2011 06:34:09 +0000,Wed; 25 Mar 2009 09:00:15 +0000,,,,,MAPREDUCE-2264;MAPREDUCE-7,https://issues.apache.org/jira/browse/HADOOP-5210
HADOOP-5211,Bug,Major,,TestSetupAndCleanupFailure fails with timeout,TestSetupAndCleanupFailure fails with timeout on my machine. The cause might be the while statements hanging the CPU at 100% for checking job completion.,Closed,Fixed,,Enis Soztutar,Enis Soztutar,Tue; 10 Feb 2009 10:28:02 +0000,Thu; 23 Apr 2009 19:18:03 +0000,Wed; 11 Feb 2009 17:48:20 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5211
HADOOP-5212,Bug,Major,scripts,cygwin path translation not happening correctly after Hadoop-4868,,Closed,Fixed,,Sharad Agarwal,Sharad Agarwal,Tue; 10 Feb 2009 13:31:50 +0000,Tue; 24 Aug 2010 20:35:39 +0000,Tue; 10 Feb 2009 18:15:13 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5212
HADOOP-5213,Bug,Blocker,io,BZip2CompressionOutputStream NullPointerException,BZip2CompressionOutputStream will throw a NullPointerException if the user creates a BZip2CompressionOutputStream and close it without writing out any data.,Closed,Fixed,,Zheng Shao,Zheng Shao,Wed; 11 Feb 2009 04:45:45 +0000,Thu; 2 May 2013 02:29:23 +0000,Fri; 1 May 2009 06:52:36 +0000,,0.19.1,,HADOOP-4918,,https://issues.apache.org/jira/browse/HADOOP-5213
HADOOP-5214,Bug,Blocker,,ConcurrentModificationException in FairScheduler.getTotalSlots,,Closed,Fixed,,rahul k singh,Vinod Kumar Vavilapalli,Wed; 11 Feb 2009 04:48:43 +0000,Wed; 8 Jul 2009 16:41:04 +0000,Fri; 20 Feb 2009 11:17:06 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5214
HADOOP-5215,New Feature,Major,,Add seek functionality for writes into file interfaces,We need a seek functionality; which could be able to let clients seek to a position and start writing from there. For writing; a seek can leave a hole in the file.,Open,Unresolved,,Unassigned,He Yongqiang,Wed; 11 Feb 2009 05:00:07 +0000,Tue; 26 Jan 2016 03:13:25 +0000,,,0.19.0,,,HDFS-294;HDFS-214,https://issues.apache.org/jira/browse/HADOOP-5215
MAPREDUCE-346,Improvement,Major,,Report Map-Reduce Framework Counters in pipeline order,Currently there is no order in which counters are printed. It would be more user friendly if Map-Reduce Framework counters are reported in the pipeline order.,Resolved,Fixed,,Sharad Agarwal,Sharad Agarwal,Wed; 11 Feb 2009 06:21:42 +0000,Mon; 21 Jul 2014 19:50:40 +0000,Mon; 21 Jul 2014 19:50:40 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-346
HADOOP-5217,Sub-task,Major,test,Split the AllTestDriver for core; hdfs and mapred,The sub projects would have individual test jar. This would require separate driver class for each.,Closed,Fixed,,Sharad Agarwal,Sharad Agarwal,Wed; 11 Feb 2009 10:29:50 +0000,Tue; 24 Aug 2010 20:35:42 +0000,Mon; 4 May 2009 08:04:58 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5217
HADOOP-5218,Bug,Major,,libhdfs unit test failed because it was unable to start namenode/datanode,The libhdfs unit tests fail because the start-daemon.sh script has changed. The unit tests fail to start/stop namenpde/datanode.,Closed,Fixed,,dhruba borthakur,dhruba borthakur,Wed; 11 Feb 2009 10:43:36 +0000,Tue; 24 Aug 2010 20:35:42 +0000,Thu; 26 Feb 2009 05:42:42 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5218
HADOOP-5219,Bug,Major,io,SequenceFile is using mapred property,"SequenceFile is using ""mapred.local.dir"". It should not depend on mapred as it is part of the core.",Closed,Fixed,,Sharad Agarwal,Sharad Agarwal,Wed; 11 Feb 2009 12:03:36 +0000,Tue; 24 Aug 2010 20:35:43 +0000,Mon; 23 Feb 2009 08:47:37 +0000,,,,HADOOP-5135,,https://issues.apache.org/jira/browse/HADOOP-5219
HADOOP-5220,Improvement,Trivial,,Provide link from subproject mailing_lists.html to TLP mailing_lists.html,To make folks more aware of general@hadoop and get them to subscribe; all subproject mailing_lists.html pages should link to the TLP mailing_lists.html page.  For example:http://hadoop.apache.org/core/mailing_lists.htmlshould link tohttp://hadoop.apache.org/mailing_lists.html,Resolved,Fixed,,Unassigned,Nigel Daley,Wed; 11 Feb 2009 16:58:04 +0000,Wed; 11 Feb 2009 23:40:24 +0000,Wed; 11 Feb 2009 23:40:24 +0000,,site,,,,https://issues.apache.org/jira/browse/HADOOP-5220
CHUKWA-32,Improvement,Major,Build and Test Code,Add init.d scripts to start chukwa's torque data loader daemons,,Resolved,Won't Fix,,Jerome Boulon,Jerome Boulon,Wed; 11 Feb 2009 18:03:32 +0000,Tue; 28 Apr 2009 22:35:30 +0000,Tue; 28 Apr 2009 22:35:30 +0000,,,,,,https://issues.apache.org/jira/browse/CHUKWA-32
HADOOP-5222,Improvement,Minor,,Add offset in client trace,By adding offset in client trace; the client trace information can provide more accurately information about I/O.It is useful for performance analyzing.Since there is  no random write now; the offset of writing is always zero.,Closed,Fixed,,Lei (Eddy) Xu,Lei (Eddy) Xu,Wed; 11 Feb 2009 18:06:21 +0000,Tue; 24 Aug 2010 20:35:44 +0000,Mon; 23 Feb 2009 01:59:39 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5222
MAPREDUCE-318,Improvement,Major,performance;task,Refactor reduce shuffle code,The reduce shuffle code has become very complex and entangled. I think we should move it out of ReduceTask and into a separate package (org.apache.hadoop.mapred.task.reduce). Details to follow.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 11 Feb 2009 18:34:32 +0000,Sun; 6 Jan 2013 06:24:18 +0000,Thu; 3 Sep 2009 13:56:31 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-318
HADOOP-5224,Bug,Blocker,,Disable append,As discussed on core-dev@ (http://www.nabble.com/Hadoop-0.19.1-td21739202.html) we will disable append API in Hadoop 0.19.1 by throwing UnsupportedOperationException from FileSystem.append(...) methods.Does append in libhdfs (HADOOP-4494) need attention too?,Closed,Fixed,,Unassigned,Nigel Daley,Wed; 11 Feb 2009 20:39:31 +0000,Wed; 8 Jul 2009 17:05:56 +0000,Wed; 18 Feb 2009 05:42:43 +0000,,0.19.0,,,HADOOP-5332,https://issues.apache.org/jira/browse/HADOOP-5224
HADOOP-5225,Bug,Blocker,,workaround for tmp file handling on DataNodes in 0.19.1 (HADOOP-4663),"As discussed on core-dev@ (http://www.nabble.com/Hadoop-0.19.1-td21739202.html) we will reduce the semantics of sync in Hadoop 0.19.1.  This requires the same ""fix"" as HADOOP-4997.",Closed,Fixed,,Raghu Angadi,Nigel Daley,Wed; 11 Feb 2009 20:43:06 +0000,Wed; 8 Jul 2009 16:43:32 +0000,Fri; 13 Feb 2009 22:26:39 +0000,,0.19.0,,,HADOOP-4997,https://issues.apache.org/jira/browse/HADOOP-5225
HADOOP-5226,Bug,Major,documentation,Add license headers to html and jsp files,License headers are missing in some html and jsp files.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Wed; 11 Feb 2009 21:19:32 +0000,Tue; 24 Aug 2010 20:35:45 +0000,Thu; 12 Feb 2009 22:26:42 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5226
HADOOP-5227,Bug,Major,,distcp -delete option deletes all files from the destination directory,distcp -delete option deletes all the files from the destination even though they are present on the source.,Closed,Fixed,,Tsz Wo Nicholas Sze,Suhas Gogate,Wed; 11 Feb 2009 21:39:51 +0000,Wed; 8 Jul 2009 16:51:08 +0000,Fri; 3 Apr 2009 22:55:36 +0000,,,,,HADOOP-3939,https://issues.apache.org/jira/browse/HADOOP-5227
HADOOP-5228,Bug,Major,,Chukwa Tests should not write to /tmp,From http://wiki.apache.org/hadoop/HowToContributeNew unit tests should be provided ...By default; do not let tests write any temporary files to /tmp. Instead; the tests should write to the location specified by the test.build.data system property.,Resolved,Fixed,,Ari Rabkin,Jerome Boulon,Thu; 12 Feb 2009 00:43:11 +0000,Wed; 8 Jul 2009 16:40:45 +0000,Thu; 5 Mar 2009 20:46:31 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5228
HADOOP-5229,Bug,Trivial,build,duplicate variables in build.xml hadoop.version vs version let build fails at assert-hadoop-jar-exists,"property name=""hadoop.jar"" location=""${build.dir}/hadoop-${hadoop.version}-core.jar"" / where hadoop.version is defined in ${ivy.dir}/libraries.properties as hadoop.version=0.20.0. Though jar jarfile=""${build.dir}/${final.name}-core.jar"" where property name=""final.name"" value=""${name}-${version}""/.Means there is a hadoop-0.21.0-dev-core.jar builded though the assert-hadoop-jar-exists target looks for a  hadoop-0.2.0-core.jar.",Closed,Fixed,,Stefan Groschupf,Stefan Groschupf,Thu; 12 Feb 2009 01:04:58 +0000,Tue; 24 Aug 2010 20:35:46 +0000,Sat; 28 Feb 2009 14:44:30 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5229
HDFS-6,Bug,Minor,,in FSNamesystem.registerDatanode; dnAddress should be resolved (rarely occured),In FSNamesystem.java registerDatanode(); if the datanode address cannot begot from the RPC Server; it will use that from the datanode report:    String dnAddress = Server.getRemoteAddress();    if (dnAddress == null) {      // Mostly called inside an RPC.      // But if not; use address passed by the data-node.      dnAddress = nodeReg.getHost();    }      The getHost() may return the hostname or address; while the Server.getRemoteAddress() will return the IP address; which is the dnAddress should be. Thus I think the it should be    if (dnAddress == null) {      // Mostly called inside an RPC.      // But if not; use address passed by the data-node.      dnAddress = InetAddress.getByName(nodeReg.getHost()).getHostAddress();    }      I know it should not be called in most situation; but I indeed use that; and I suppose the dnAddress should be an IP address.,Resolved,Won't Fix,,Unassigned,Wang Xu,Thu; 12 Feb 2009 07:25:08 +0000,Sat; 19 Nov 2011 20:51:05 +0000,Sat; 19 Nov 2011 20:51:05 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-6
HADOOP-5231,Bug,Major,,Negative number of maps in cluster summary,I observed -ve number of maps in cluster summary; when running MRReliability test. (job with large number of failures),Closed,Fixed,,Amareshwari Sriramadasu,Amareshwari Sriramadasu,Thu; 12 Feb 2009 07:51:10 +0000,Mon; 3 Aug 2009 12:59:03 +0000,Tue; 24 Mar 2009 16:37:12 +0000,,0.19.1,,,,https://issues.apache.org/jira/browse/HADOOP-5231
HADOOP-5232,New Feature,Major,build,preparing HadoopPatchQueueAdmin.sh;test-patch.sh scripts to run builds on hudson slaves.,To modify hadoopPatchQueueAdmin.sh and test-patch.sh script to run patch builds on hudson slaves.,Closed,Fixed,,Giridharan Kesavan,Giridharan Kesavan,Thu; 12 Feb 2009 10:42:58 +0000,Tue; 24 Aug 2010 20:35:47 +0000,Thu; 26 Feb 2009 06:34:56 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5232
HADOOP-5233,Bug,Blocker,,Reducer not Succeded after 100%,I have seen a reducer in RUNNING state; when it was waiting for the commitResponse.Task logs show :2009-02-12 08:35:36;414 INFO org.apache.hadoop.mapred.TaskRunner: Task:attempt_200902120746_0297_r_000033_0 is done. And is in the process of commitingand no logs after that.TT logs say:2009-02-12 08:35:36;417 INFO org.apache.hadoop.mapred.TaskTracker: Task attempt_200902120746_0297_r_000033_0 is in COMMIT_PENDING2009-02-12 08:35:36;417 INFO org.apache.hadoop.mapred.TaskTracker: attempt_200902120746_0297_r_000033_0 0.33333334% reduce  sortThis looks like; the task progress went from COMMIT_PENDING to RUNNING again.,Closed,Fixed,,Amareshwari Sriramadasu,Amareshwari Sriramadasu,Thu; 12 Feb 2009 10:43:20 +0000,Wed; 8 Jul 2009 16:53:15 +0000,Fri; 20 Feb 2009 13:18:36 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5233
HADOOP-5234,Bug,Major,,NPE in TaskTracker reinit action,I have seen an NPE in TT reinit action.TT logs : corresponding code is,Closed,Duplicate,HADOOP-5233,Amareshwari Sriramadasu,Amareshwari Sriramadasu,Thu; 12 Feb 2009 10:46:59 +0000,Wed; 8 Jul 2009 16:53:15 +0000,Wed; 25 Feb 2009 04:05:12 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5234
HADOOP-5235,Bug,Major,,possible NPE in tip.kill(),There is possibility of NPE with runner.kill() code in TaskTracker.TaskInProgress.kill().If a killTaskAction is issued for cleanup attempt of a task; forwhich runner is not created; the existing code would through NPE.,Closed,Duplicate,HADOOP-5233,Amareshwari Sriramadasu,Amareshwari Sriramadasu,Thu; 12 Feb 2009 11:04:28 +0000,Wed; 8 Jul 2009 16:53:15 +0000,Wed; 25 Feb 2009 04:04:20 +0000,,0.19.1,,,,https://issues.apache.org/jira/browse/HADOOP-5235
MAPREDUCE-424,Bug,Minor,jobtracker,Unreachable code in TaskInProgress.inCompleteSubTask,TaskInProgress.incompleteSubTask has the following unreachable-code : It would never reach this code; because it is never called from other places; where taskState is not FAILED; KILLED; FAILED_UNCLEAN and KILLED_UNCLEAN.Also status != null is not required; since status is never null.,Resolved,Incomplete,,Amareshwari Sriramadasu,Amareshwari Sriramadasu,Thu; 12 Feb 2009 11:29:39 +0000,Mon; 21 Jul 2014 19:51:33 +0000,Mon; 21 Jul 2014 19:51:33 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-424
HADOOP-5237,Improvement,Major,metrics,MBeanUtil should log errors using the logging API,MBeanUtil prints stack traces to standard error. It should use commons logging instead.,Open,Unresolved,,Unassigned,Tom White,Thu; 12 Feb 2009 16:13:21 +0000,Thu; 5 Feb 2015 17:32:41 +0000,,,,jmx;mbean;newbie;patch,,,https://issues.apache.org/jira/browse/HADOOP-5237
HADOOP-5238,Bug,Minor,build,ivy publish and ivy integration does not cleanly work,"As far I understand the goal using ivy for hadoop is to be able intgrating hadoop easily in thirdparty builds that uses transient dependency tools like ivy or maven. The way ivy is currently integrated a couple hick ups. + the generated artifact files have names that can't be used for maven or ivy. e.g. hadoop-version-core but standard would be hadoop-core-version. This effects all those generated artifact files like hadoop-version-example etc. This is caused by the use of ${final.name}-core.jar+ This conflicts with the use of ""${version}"" in ivy.xml info organisation=""org.apache.hadoop"" module=""${ant.project.name}"" revision=""${version}"". The result will be a error report by ivy that found artifact and defined artifact name are different.",Resolved,Fixed,,Giridharan Kesavan,Stefan Groschupf,Thu; 12 Feb 2009 19:56:12 +0000,Mon; 21 Jul 2014 19:58:15 +0000,Mon; 21 Jul 2014 19:58:15 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5238
HDFS-146,Bug,Major,,Regression: TestInjectionForSimulatedStorage fails with IllegalMonitorStateException,"org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage.testInjection fails with IllegalMonitorStateExceptionStacktracejava.lang.IllegalMonitorStateException	at java.lang.Object.notifyAll(Native Method)	at org.apache.hadoop.ipc.Server.stop(Server.java:1110)	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:574)	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:569)	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:553)	at org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage.testInjection(TestInjectionForSimulatedStorage.java:195)No errors show up in the standard output; but there are a few warnings.http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-trunk/749/testReport/org.apache.hadoop.hdfs/TestInjectionForSimulatedStorage/testInjection/",Resolved,Cannot Reproduce,,Unassigned,gary murry,Thu; 12 Feb 2009 20:49:36 +0000,Thu; 29 Dec 2011 14:29:06 +0000,Thu; 29 Dec 2011 14:29:06 +0000,,,,,HDFS-44;HDFS-104,https://issues.apache.org/jira/browse/HDFS-146
HADOOP-5240,Improvement,Major,build,'ant javadoc' does not check whether outputs are up to date and always rebuilds,Running 'ant javadoc' twice in a row calls the javadoc program both times; it doesn't check to see whether this is redundant work.,Closed,Fixed,,Aaron Kimball,Aaron Kimball,Thu; 12 Feb 2009 21:02:02 +0000,Tue; 24 Aug 2010 20:35:48 +0000,Mon; 23 Feb 2009 22:30:28 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5240
HADOOP-5241,Bug,Blocker,,Reduce tasks get stuck because of over-estimated task size (regression from 0.18),I have a simple MR benchmark job that computes PageRank on about 600 GB of HTML files using a 100 node cluster. For some reason; my reduce tasks get caught in a pending state. The JobTracker's log gets filled with the following messages:2009-02-12 15:47:29;839 WARN org.apache.hadoop.mapred.JobInProgress: No room for reduce task. Node tracker_d-59.cs.wisc.edu:localhost/127.0.0.1:33227 has 110125027328 bytes free; but we expect reduce input to take 3996421982352009-02-12 15:47:29;852 WARN org.apache.hadoop.mapred.JobInProgress: No room for reduce task. Node tracker_d-67.cs.wisc.edu:localhost/127.0.0.1:48626 has 107537776640 bytes free; but we expect reduce input to take 3996421982352009-02-12 15:47:29;885 WARN org.apache.hadoop.mapred.JobInProgress: No room for reduce task. Node tracker_d-73.cs.wisc.edu:localhost/127.0.0.1:58849 has 113631690752 bytes free; but we expect reduce input to take 399642198235SNIPThe weird thing is that I get through about 70 reduce tasks completing before it hangs. If I reduce the amount of the input data on 100 nodes down to 200GB; then it seems to work. As I scale the amount of input to the number of nodes; I can get it work some of the times on 50 nodes and without any problems on 25 nodes and less.Note that it worked without any problems on Hadoop 0.18 late last year without changing any of the input data or the actual MR code.,Closed,Fixed,,Sharad Agarwal,Andy Pavlo,Thu; 12 Feb 2009 22:01:40 +0000,Wed; 8 Jul 2009 16:53:16 +0000,Mon; 23 Feb 2009 10:02:11 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5241
HDFS-332,Improvement,Major,,hadoop fs -put should return different code for different failures,hadoop fs -put may fail due to different reasons; such as the source file does not exist; the destination file already exists; permission denied; or exceptions during writing.However; it returns the same code (-1); making it impossible to tell what is the actual cause of the failure.,Resolved,Won't Fix,,Ravi Phulari,Runping Qi,Thu; 12 Feb 2009 22:25:15 +0000,Mon; 21 Jul 2014 20:00:43 +0000,Mon; 21 Jul 2014 20:00:43 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-332
CHUKWA-10,Bug,Minor,Build and Test Code,Fix TestAgentConfig ,"testInitAdaptors_vs_Checkpoint regularly fails in Hudson builds (solaris only); this class has been exclude for now but we still need to find/confirm the root cause.Right now the assumption is that there's a delay on solaris before being able to reuse a port.See Jira: HADOOP-5172 for detailsError Message============org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent$AlreadyRunningException: Agent already running; abortingStacktrace========junit.framework.AssertionFailedError: org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent$AlreadyRunningException: Agent already running; aborting	at org.apache.hadoop.chukwa.datacollection.agent.TestAgentConfig.testInitAdaptors_vs_Checkpoint(TestAgentConfig.java:73)",Resolved,Fixed,,Ari Rabkin,Jerome Boulon,Fri; 13 Feb 2009 00:13:35 +0000,Tue; 16 Jun 2009 18:45:24 +0000,Mon; 15 Jun 2009 22:54:18 +0000,,0.2.0,,,HADOOP-5172,https://issues.apache.org/jira/browse/CHUKWA-10
HADOOP-5244,Bug,Major,,Distributed cache spends a lot of time runing du -s,"When running a MapReduce job that has a large jar on the class path (eg; jruby from hbase); the task tracker takes a large amount of CPU time during startup. Using jstack; I got the following stack trace:""Thread-8941"" daemon prio=10 tid=0x00002aab08005c00 nid=0x2807 waiting on condition 0x0000000043eca000..0x0000000043ecbc90   java.lang.Thread.State: RUNNABLE	at java.lang.StringCoding$StringEncoder.encode(StringCoding.java:232)	at java.lang.StringCoding.encode(StringCoding.java:272)	at java.lang.String.getBytes(String.java:947)	at java.io.UnixFileSystem.getLength(Native Method)	at java.io.File.length(File.java:848)	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:428)	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)	at org.apache.hadoop.filecache.DistributedCache.getLocalCache(DistributedCache.java:210)	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:178)Running the system ""du"" returns very quickly on the directory.",Closed,Duplicate,HADOOP-4780,Unassigned,Ben Maurer,Fri; 13 Feb 2009 02:36:39 +0000,Wed; 8 Jul 2009 16:53:16 +0000,Fri; 13 Feb 2009 18:50:32 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5244
MAPREDUCE-192,Bug,Major,,In TaskTracker; the notification for waking up the completion-events fetcher thread may be lost,In the TaskTracker; there is a completion events fetcher thread that fetches new events from the JobTracker. Normally; the fetcher thread would sleep for heartbeatInterval amount of time per cycle. When a reduce task asks for completion events from the corresponding TaskTracker and the TaskTracker currently doesn't have anything to hand out; a notification is sent to the completion events fetcher thread to wake up and fetch new events if any. Sometimes this notification could be lost and the reduce task would be idle for a few seconds. This hurts the performance of jobs like terasort.,Resolved,Incomplete,,Devaraj Das,Devaraj Das,Fri; 13 Feb 2009 04:31:04 +0000,Mon; 21 Jul 2014 20:03:59 +0000,Mon; 21 Jul 2014 20:03:59 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-192
MAPREDUCE-112,Bug,Blocker,,Reduce Input Records and Reduce Output Records counters are not being set when using the new Mapreduce reducer API,After running the examples/wordcount (which uses the new API); the reduce input and output record counters always show 0. This is because these counters are not getting updated in the new API,Resolved,Fixed,,Jothi Padmanabhan,Jothi Padmanabhan,Fri; 13 Feb 2009 05:55:25 +0000,Thu; 1 Oct 2009 08:04:01 +0000,Tue; 15 Sep 2009 06:31:22 +0000,,0.20.1,,,,https://issues.apache.org/jira/browse/MAPREDUCE-112
HADOOP-5247,Bug,Blocker,,NPEs in JobTracker and JobClient when mapred.jobtracker.completeuserjobs.maximum is set to zero.,,Closed,Fixed,,Amar Kamat,Vinod Kumar Vavilapalli,Fri; 13 Feb 2009 06:27:05 +0000,Wed; 24 Mar 2010 04:42:39 +0000,Fri; 20 Feb 2009 13:41:12 +0000,,,,HADOOP-5248,MAPREDUCE-193,https://issues.apache.org/jira/browse/HADOOP-5247
HADOOP-5248,Bug,Blocker,,Job directories could remain undeleted in some scenarios after job completes.,I observed a couple of times that when a job has completed; its job directories were not cleaned up. In discussion; it seems like there is a condition when only reduces from a job are run on a machine and no maps; the TT does not get a signal from the JT to delete the files and could be left behind. FYI; JVM reuse was enabled at the time. I can confirm that 'KillJobAction' was not received by the TT.,Closed,Fixed,,Devaraj Das,Hemanth Yamijala,Fri; 13 Feb 2009 08:33:16 +0000,Wed; 8 Jul 2009 16:53:16 +0000,Thu; 26 Feb 2009 11:25:48 +0000,,,,HADOOP-5247,,https://issues.apache.org/jira/browse/HADOOP-5248
MAPREDUCE-423,Bug,Trivial,,Remove getNumResolvedTaskTrackers() api from JobTracker,,Resolved,Not A Problem,,Harsh J,Amar Kamat,Fri; 13 Feb 2009 09:30:21 +0000,Sun; 18 Dec 2011 11:32:09 +0000,Sun; 18 Dec 2011 11:32:09 +0000,,0.23.0,,,,https://issues.apache.org/jira/browse/MAPREDUCE-423
MAPREDUCE-35,Bug,Major,,There can be more than 'mapred.jobtracker.completeuserjobs.maximum' jobs for a user in the jobtracker,The check for max-completed-jobs-per-user is made in finalize job and hence there can be a case where the user finishes more than 'mapred.jobtracker.completeuserjobs.maximum' jobs within  JobTracker.MIN_TIME_BEFORE_RETIRE units of time and doesnt submit any job after that which can cause more number of jobs to be in memory than allowed. There is no check made for this limit anywhere else.,Resolved,Invalid,,Unassigned,Amar Kamat,Fri; 13 Feb 2009 09:56:36 +0000,Wed; 7 Oct 2009 06:35:02 +0000,Wed; 7 Oct 2009 06:35:02 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-35
HADOOP-5251,Bug,Critical,,TestHdfsProxy and TestProxyUgiManager frequently fail,Had a look at the Hudson nightly builds for Hadoop and all of them have failed lately.Two of the tests that fail frequently are TestHdfsProxy and TestProxyUgiManager.Example:http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/743/testReport/http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/744/testReport/http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/750/testReport/It seems to be related to Clover; perhaps also related to HADOOP-3921?,Closed,Fixed,,Nigel Daley,Johan Oskarsson,Fri; 13 Feb 2009 10:30:09 +0000,Tue; 24 Aug 2010 20:35:49 +0000,Sat; 14 Feb 2009 06:36:24 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5251
HADOOP-5252,Bug,Major,,Streaming overrides -inputformat option,For some reason; streaming currently falls back to StreamInputFormat when it is asked to use SequenceFileInputFormat or KeyValueTextInputFormat via the -inputformat option; even when no -inputreader option is specified. In case of KeyValueTextInputFormat this is not really a problem and for SequenceFileInputFormat the AutoInputFormat added by HADOOP-1722 provides a way around this; but it would be better to get this fixed...,Closed,Fixed,,Klaas Bosteels,Klaas Bosteels,Fri; 13 Feb 2009 11:04:07 +0000,Tue; 24 Aug 2010 20:35:50 +0000,Mon; 25 May 2009 05:59:36 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5252
HADOOP-5253,Bug,Minor,build,to remove duplicate calls to the cn-docs target.,package target depends on docs; cn-docs and ....etc...and doc target intern calls cn-docs which results in call calling cn-docs target twice when ant package executed.,Closed,Fixed,,Giridharan Kesavan,Giridharan Kesavan,Fri; 13 Feb 2009 11:43:27 +0000,Tue; 24 Aug 2010 20:35:51 +0000,Fri; 13 Feb 2009 19:35:48 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5253
HADOOP-5254,Bug,Major,conf,Xinclude setup results in a stack trace,"seen this in SVN_HEAD; and it was mentioned on the user list in the week. It explains why my health tests are failing on class.ForName(FSConstants)gistration(127.0.0.1:8024; storageID=DS-1466307248-127.0.1.1-8024-1234537374021; infoPort=8022; ipcPort=50020):DataXceiversf-startdaemon-debug java.lang.ExceptionInInitializerErrorsf-startdaemon-debug 	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:76)sf-startdaemon-debug 	at java.lang.Thread.run(Thread.java:619)sf-startdaemon-debug Caused by: java.lang.UnsupportedOperationException: This parser does not support specification ""null"" version ""null""sf-startdaemon-debug 	at javax.xml.parsers.DocumentBuilderFactory.setXIncludeAware(DocumentBuilderFactory.java:590)",Resolved,Fixed,MAPREDUCE-5664,Steve Loughran,Steve Loughran,Fri; 13 Feb 2009 15:05:24 +0000,Fri; 10 Jan 2014 07:36:44 +0000,Tue; 17 Feb 2009 22:04:41 +0000,,0.20.0;0.21.0,,,MAPREDUCE-5664;HADOOP-4944,https://issues.apache.org/jira/browse/HADOOP-5254
HADOOP-5255,Bug,Minor,io,Fix for HADOOP-5079 HashFunction inadvertently destroys some randomness,"HADOOP-5079 did this ""HashFunction.hash restricts initval for the next hash to the [0; maxValue) range of the hash indexes returned. This is suboptimal; particularly for larger nbHash and smaller maxValue. Rather we should first set initval; then restrict the range for the result assignment.""  The patch committed on that issue introduced a new bug: ""My first patch contained a regression: you have to take the remainder before calling Math.abs; since Math.abs(Integer.MIN_VALUE) == Integer.MIN_VALUE still"" (Jonathan Ellis).",Closed,Fixed,,Jonathan Ellis,stack,Fri; 13 Feb 2009 15:56:35 +0000,Thu; 23 Apr 2009 19:18:03 +0000,Fri; 20 Feb 2009 06:29:17 +0000,,0.20.0;0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5255
HADOOP-5256,Bug,Major,build;test,Some tests not run by default,"To be run by the 'test' target; test file names must start with ""Test"" and end in "".java"".  One example that violates this is src/test/org/apache/hadoop/fs/FileSystemContractBaseTest.java.  Is this on purpose to that it's not run automatically?  Are there other tests like this?",Resolved,Not A Problem,,gary murry,Nigel Daley,Fri; 13 Feb 2009 18:01:40 +0000,Thu; 11 Aug 2011 18:14:14 +0000,Thu; 11 Aug 2011 18:14:14 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5256
HADOOP-5257,New Feature,Minor,,Export namenode/datanode functionality through a pluggable RPC layer,Adding support for pluggable components would allow exporting DFS functionallity using arbitrary protocols; like Thirft or Protocol Buffers. I'm opening this issue on Dhruba's suggestion in HADOOP-4707.Plug-in implementations would extend this base class: Name node instances would then start the plug-ins according to a configuration object; and would also shut them down when the node goes down: Data nodes would do a similar thing in DataNode.startDatanode() and DataNode.shutdown,Closed,Fixed,,Carlos Valiente,Carlos Valiente,Fri; 13 Feb 2009 18:42:18 +0000,Mon; 6 May 2013 04:56:50 +0000,Tue; 7 Apr 2009 19:12:55 +0000,,,,HDFS-417,HDFS-217;MAPREDUCE-461;HADOOP-8832;HDFS-3963,https://issues.apache.org/jira/browse/HADOOP-5257
HADOOP-5258,New Feature,Major,,Provide dfsadmin functionality to report on namenode's view of network topology,As discussed in HADOOP-4954; it would be useful to be able to query the namenode to its current view on the network topology of racks and datanodes.  This would allow ops to compare what the namenode sees with what they expect it to see.,Closed,Fixed,,Jakob Homan,Jakob Homan,Fri; 13 Feb 2009 19:56:01 +0000,Tue; 24 Aug 2010 20:35:53 +0000,Tue; 10 Mar 2009 23:03:34 +0000,,,,,HADOOP-5477;HADOOP-4954,https://issues.apache.org/jira/browse/HADOOP-5258
HADOOP-5259,Bug,Major,fs,Job with output hdfs:/user/<username>/outputpath (no authority) fails with Wrong FS,Using namenode with default port of 8020.When starting a job with output hdfs:/user/knoguchi/outputpath; my job fails with Wrong FS: hdfs:/user/knoguchi/outputpath; expected: hdfs://aaa.bbb.cc,Closed,Fixed,,Doug Cutting,Koji Noguchi,Fri; 13 Feb 2009 22:58:14 +0000,Wed; 8 Jul 2009 16:43:32 +0000,Tue; 17 Mar 2009 22:27:15 +0000,,0.18.0;0.18.1;0.18.2;0.18.3;0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5259
MAPREDUCE-588,Bug,Minor,contrib/streaming,streaming failed without printing out the cause (IllegalArgumentException),When looking at HADOOP-5259; streaming failed as $ $HADOOP_HOME/bin/hadoop  jar $HADOOP_HOME/contrib/streaming/hadoop-0.21.0-dev-streaming.jar -input (inputpath) -output hdfs:/user/knoguchi/outputpath -mapper cat -reducer catpackageJobJar: /tmp/hadoop-unjar27612/ [] /tmp/streamjob27613.jar tmpDir=nullStreaming Command Failed!$ echo $?1,Resolved,Duplicate,NULL,Unassigned,Koji Noguchi,Fri; 13 Feb 2009 23:32:53 +0000,Wed; 9 Jun 2010 05:16:41 +0000,Wed; 9 Jun 2010 05:16:41 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-588
HADOOP-5261,Bug,Major,,HostsFileReader does not properly implement concurrency support,As currently implemented; the class HostsFileReader does not properly allow concurrent access. It maintains two Sets and manipulates them within synchronized fields; but provides accessor methods that publish unsynchronized access to the sets' references (getHosts() and getExcludedHosts()).  The sets are implemented as HashSets; which are not thread safe.  This can allow a method to obtain a reference to a set that may be modified concurrently by the HostsFileReader.,Resolved,Fixed,,Jakob Homan,Jakob Homan,Sat; 14 Feb 2009 03:50:57 +0000,Mon; 21 Jul 2014 20:26:04 +0000,Mon; 21 Jul 2014 20:26:04 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5261
MAPREDUCE-549,New Feature,Minor,,Allow specifying min shares as percentage of cluster,Currently the guaranteed shares for pools in the fair scheduler are specified as a number of slots. For organizations where a group pays X% of the cluster and the actual number of nodes in the cluster varies due to failures; expansion; etc over time; it would be useful to support a guaranteed share given as a percentage too. This would just let you write in the config file something like minMaps5%/minMaps instead of minMaps42/minMaps. The scheduler would need to recompute what this means in terms of number of slots on every update (probably through some kind of update(ClusterStatus) method in PoolManager).,Open,Unresolved,,Unassigned,Matei Zaharia,Sat; 14 Feb 2009 08:10:26 +0000,Sat; 20 Jun 2009 08:01:44 +0000,,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-549
HADOOP-5263,Improvement,Minor,build;documentation,Documentation: Chinese (cn) doc structure placed in the middle of the English doc structure,The Chinese doc structure was plopped into the middle of the English doc structure. You need to figure out where you are going to put translations of the Hadoop core docs.- src     - docs ------------------ English docs              + .svn              + build                    changes              + cn  ------------- Chinese docs              + src,Resolved,Won't Fix,,Unassigned,Corinne Chandel,Sun; 15 Feb 2009 03:58:55 +0000,Thu; 11 Aug 2011 18:10:53 +0000,Thu; 11 Aug 2011 18:10:53 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5263
HADOOP-5264,Improvement,Major,,TaskTracker should have single conf reference,Tasktracker has 2 conf member variables - originalConf and fConf. This is not required. It should only have one.,Closed,Fixed,,Sharad Agarwal,Sharad Agarwal,Mon; 16 Feb 2009 08:24:26 +0000,Tue; 24 Aug 2010 20:35:53 +0000,Wed; 25 Feb 2009 06:11:34 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5264
HADOOP-5265,Bug,Blocker,build,Builds failing - Cannot run program ant,"The last couple of builds have failed with the following error error message; taken from http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/754/BUILD SUCCESSFULTotal time: 177 minutes 27 secondstrunk $ antFATAL: command execution failed.Maybe you need to configure the job to choose one of your Ant installations?java.io.IOException: Cannot run program ""ant"" (in directory ""/home/hudson/hudson-slave/workspace/Hadoop-trunk/trunk""): java.io.IOException: error=2; No such file or directorysnip",Resolved,Fixed,,Nigel Daley,Johan Oskarsson,Mon; 16 Feb 2009 11:04:40 +0000,Tue; 17 Feb 2009 05:59:12 +0000,Tue; 17 Feb 2009 05:59:12 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5265
HADOOP-5266,Improvement,Major,,"Values Iterator should support ""mark"" and ""reset""",Some users have expressed interest in having a mark-reset functionality on values iterator. Users can call mark() at any point during the iteration process and a subsequent reset() should move the iterator to the last value emitted when mark() was called.,Closed,Fixed,,Jothi Padmanabhan,Jothi Padmanabhan,Mon; 16 Feb 2009 11:14:00 +0000,Tue; 24 Aug 2010 20:35:55 +0000,Mon; 4 May 2009 05:35:10 +0000,,,,HADOOP-475,,https://issues.apache.org/jira/browse/HADOOP-5266
HDFS-353,Improvement,Minor,,DFSClient does not always throw a FileNotFound exception when a file could not be opened,"DfsClient.openInit() throws an IOE when a file can't be found; that is; it has no blockssf-startdaemon-debug 09/02/16 12:38:47 IPC Server handler 0 on 8012 INFO mapred.TaskInProgress : Error from attempt_200902161238_0001_m_000000_2: java.io.IOException: Cannot open filename /tests/mrtestsequence/in/in.txtsf-startdaemon-debug 	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1352)sf-startdaemon-debug 	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.init(DFSClient.java:1343)sf-startdaemon-debug 	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:312)sf-startdaemon-debug 	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:177)sf-startdaemon-debug 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:347)I propose turning this into a FileNotFoundException; which is more specific about the underlying problem. Including the full dfs URL would be useful too.",Open,Unresolved,,Chu Tong,Steve Loughran,Mon; 16 Feb 2009 14:05:39 +0000,Tue; 22 Jul 2014 18:19:16 +0000,,,,newbie,,HADOOP-9361,https://issues.apache.org/jira/browse/HDFS-353
HADOOP-5268,Bug,Major,,Using MultipleOutputFormat and setting reducers to 0 causes org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException and job to fail,"Hi;I'm trying to save the sorting step by only runnign the map phase (setting the reducers to 0); but my job will fail then. The job runs fine when the reduce phase is activated.I'm using MultipleInputFormat and MultipleOutputFormat. Here is my outputformat class; below is the exception. public class MultipleSequenceFileOutputFormatK extends WritableComparable; V extends Writable extends MultipleOutputFormatK; V {   private SequenceFileOutputFormatK; V sequencefileoutputformat = null;   private String uniqueprefix = """";   private boolean set = false;   private static Random r = new Random();   @Override   protected RecordWriterK; V getBaseRecordWriter(FileSystem fs; JobConf job; String name; Progressable arg3) throws IOException {	if (sequencefileoutputformat == null) {	   sequencefileoutputformat = new SequenceFileOutputFormatK; V();	}	return sequencefileoutputformat.getRecordWriter(fs; job; name; arg3);   }   @Override   protected String generateFileNameForKeyValue(K key; V value; String name) {	if (!set) {	   synchronized (r) {		uniqueprefix = new Long(System.currentTimeMillis()).toString() + ""_"" + r.nextInt();		set = true;	   }	}	return ""prefix....."" + uniqueprefix + ""_"" + name;   }   @Override   public void checkOutputSpecs(FileSystem fs; JobConf conf) {   }org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file ......1234809836818_-1723031414_part-00000 for DFSClient_attempt_200902111714_0492_m_000000_0 on client 192.168.0.6 because current leaseholder is trying to recreate file.	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1052)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:995)	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:301)	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)	at java.lang.reflect.Method.invoke(Method.java:597)	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)	at org.apache.hadoop.ipc.Client.call(Client.java:696)	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)	at $Proxy1.create(Unknown Source)	at sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)	at java.lang.reflect.Method.invoke(Method.java:597)	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)	at $Proxy1.create(Unknown Source)	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.(DFSClient.java:2587)	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:454)	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:169)	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:487)	at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.(SequenceFile.java:1198)	at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:401)	at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:354)	at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:427)	at org.apache.hadoop.mapred.SequenceFileOutputFormat.getRecordWriter(SequenceFileOutputFormat.java:57)	at MultipleSequenceFileOutputFormat.getBaseRecordWriter(MultipleSequenceFileOutputFormat.java:33)	at org.apache.hadoop.mapred.lib.MultipleOutputFormat$1.write(MultipleOutputFormat.java:99)	at org.apache.hadoop.mapred.MapTask$DirectMapOutputCollector.collect(MapTask.java:385)	at ...",Resolved,Fixed,,Unassigned,Thibaut,Mon; 16 Feb 2009 20:51:37 +0000,Wed; 8 Jul 2009 16:53:16 +0000,Wed; 25 Feb 2009 15:00:14 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5268
HADOOP-5269,Bug,Blocker,,TaskTracker.runningTasks holding FAILED_UNCLEAN and KILLED_UNCLEAN taskStatuses forever in some cases.,Tasktracker is holdingup TaskStatus objects in runningTasks forever in somecases. This happens in the following scenario.- Task got an exception- Sets the phase to CLEANUP- The task tries to do cleanup. and it doesn't respond after that.- TaskTracker marks the task unresponsive and makes the task FAILED_UNCLEAN- TaskTracker doesn't remove it from runningTasks data structure; since phase is CLEANUP and state is FAILED_UNCLEAN (it treats this as cleanupAttempt).I would propose that once the task goes to CLEANUP phase; kill on the task should mark it a clean failure i.e. The task state should be FAILED/KILLED.,Closed,Fixed,,Amareshwari Sriramadasu,Amareshwari Sriramadasu,Tue; 17 Feb 2009 05:35:26 +0000,Wed; 8 Jul 2009 16:53:17 +0000,Fri; 20 Feb 2009 10:24:50 +0000,,0.19.1,,,,https://issues.apache.org/jira/browse/HADOOP-5269
MAPREDUCE-55,Bug,Major,,Reduce task should stop shuffle-retrying in case of out-of-memory errors,In ReduceTask; MapOutputCopier threads catch Throwble and retry happens for the shuffle. It should not retry incase of Errors suchas OutOfMemoryError etc.May be it should retry only in case of Connect/Read failures and die in all other cases. Thoughts?,Resolved,Duplicate,NULL,Unassigned,Amareshwari Sriramadasu,Tue; 17 Feb 2009 05:44:37 +0000,Wed; 7 Oct 2009 06:16:12 +0000,Wed; 7 Oct 2009 06:16:12 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-55
HADOOP-5271,Improvement,Major,,JobConf option for minimum progress threshold before reducers are assigned,"A specific sub-case of the general priority inversion problem noted in HADOOP-4557 is when many lower priority jobs are submitted and are waiting for mappers to free up.  Even though they haven't actually done any work; they will be assigned any free reducers.  If a higher priority job is submitted; priority inversion results not just due to the lower priority tasks that are in the midst of completing; but also due to the ones that haven't yet started but have claimed all the free reducers.  A simple workaround is to require a job to complete some useful work before assigning it a reducer.  This can be done in a tunable and backwards compatible manner by adding a ""minimum map progress percentage before assigning a reducer"" option to the JobConf.  Setting this to 0 would eliminate the common case above; and setting it to 100 would technically eliminate the inversion of HADOOP-4557; though likely at an unacceptably high cost.",Resolved,Fixed,,Unassigned,Tim Williamson,Tue; 17 Feb 2009 06:36:46 +0000,Tue; 25 May 2010 04:58:53 +0000,Tue; 25 May 2010 04:58:53 +0000,,,,,MAPREDUCE-314,https://issues.apache.org/jira/browse/HADOOP-5271
HADOOP-5272,Bug,Critical,,JobTracker does not log TIP start information after restart,In case of JobTracker restart; attempt_0123456789_0001_m_000000_0 might not be the first attempt to get scheduled as the attempt id offset changes after every restart. For example; upon first restart the new attempt id will be attempt_0123456789_0001_m_000000_1000. Hence TIP start line never gets logged to the job history after the restart; TaskInProgress.isFirstAttempt() always returns false.,Closed,Fixed,,Amar Kamat,Amar Kamat,Tue; 17 Feb 2009 07:10:10 +0000,Wed; 8 Jul 2009 16:53:16 +0000,Tue; 24 Feb 2009 05:27:16 +0000,,,,HADOOP-4191,,https://issues.apache.org/jira/browse/HADOOP-5272
HADOOP-5273,Bug,Minor,documentation,License header missing in TestJobInProgress.java,,Closed,Fixed,,Jakob Homan,Amar Kamat,Tue; 17 Feb 2009 08:10:32 +0000,Tue; 24 Aug 2010 20:35:55 +0000,Fri; 27 Feb 2009 02:30:30 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5273
HADOOP-5274,Bug,Critical,benchmarks,gridmix2 is not getting compiled to generate gridmix.jar,"Not able to compile gridmix2 to generate gridmix.jar. Compilation gets failed giving build failed message.It seems that problem is with mapper class and reduce class specified in CombinerJobCreator.java. Changed mapper class from ""MapClass.class"" to ""Mapper.class"" and reduce class  from ""Reduce.class"" to ""Reducer.class"" then it started working and gridmix.jar was generated.",Closed,Fixed,,Chris Douglas,Suman Sehgal,Tue; 17 Feb 2009 10:08:09 +0000,Thu; 23 Apr 2009 19:18:04 +0000,Wed; 4 Mar 2009 04:53:11 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5274
HADOOP-5275,Improvement,Major,build,ivy directory should be there in hadoop tar ball,Ivy directory should also be in hadoop tar ball. It is required when we are using hadoop (untarred from hadoop tar ball) and need to compile any of the compnent individually (e.g. gridmix2).,Closed,Fixed,,Giridharan Kesavan,Suman Sehgal,Tue; 17 Feb 2009 11:07:33 +0000,Thu; 23 Apr 2009 19:18:04 +0000,Fri; 20 Feb 2009 18:56:14 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5275
HADOOP-5276,Bug,Critical,,Upon a lost tracker; the task's start time is reset to 0,Upon lost tracker; the JIP (via JobTracker.lostTracker() - JobInProgress.failedTask()) hand crafts task status (marking the attempt as KILLED) and updates the JobInProgress (via JobInProgress.updateTaskStatus()). This status contains the attempt start time as 0. One major impact would be that the listener will overwrite the last start time value with the new one and hence the start tiem will get garbled.,Closed,Fixed,,Amar Kamat,Amar Kamat,Tue; 17 Feb 2009 11:51:58 +0000,Tue; 24 Aug 2010 20:35:56 +0000,Tue; 24 Feb 2009 06:49:14 +0000,,,,HADOOP-4191,,https://issues.apache.org/jira/browse/HADOOP-5276
HADOOP-5277,New Feature,Minor,,new InputFormat for use with Chukwa,An InputFormat for parsing Chukwa's sequence files. Designed to be a nearly drop-in replacement for the Hadoop default TextInputFormat so that existing analytics jobs can be ported to use Chukwa with minimal modification.,Resolved,Fixed,,Ari Rabkin,Ari Rabkin,Wed; 18 Feb 2009 05:15:06 +0000,Wed; 8 Jul 2009 16:40:45 +0000,Thu; 26 Feb 2009 05:38:59 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5277
HADOOP-5278,Bug,Major,,Finish time of a TIP is incorrectly logged to the jobhistory upon jobtracker restart,Upon recovery; the jobtracker replays the attempt information to the JIP. Upon seeing the attempt-end info; the JIP completes the TIP (which sets the finish time to current time) and logs it to the new jobhistory file (the .recover file). After seeing the task(tip) finish line; the recovery manager changes the finish time to correct finish time but the new jobhistory file still contains the wrong value for the finish time seen while completing the TIP.,Closed,Fixed,,Amar Kamat,Amar Kamat,Wed; 18 Feb 2009 11:02:16 +0000,Tue; 24 Aug 2010 20:35:57 +0000,Tue; 24 Feb 2009 06:55:47 +0000,,,,HADOOP-4191,,https://issues.apache.org/jira/browse/HADOOP-5278
HADOOP-5279,Improvement,Major,,test-patch.sh scirpt should just call the test-core target as part of runtestcore function.,checkReleaseAuditWarnings function just calls the releaseaudit target.checkFindbugsWarnings function just calls findbugs. runCoreTests function is calling docs tar and test-core.If we just want to execute the test-core target then we should skip calling docs and tar target ..,Closed,Fixed,,Giridharan Kesavan,Giridharan Kesavan,Wed; 18 Feb 2009 11:45:50 +0000,Tue; 24 Aug 2010 20:35:57 +0000,Thu; 19 Feb 2009 17:24:22 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5279
HADOOP-5280,Bug,Blocker,,When expiring a lost launched task; JT doesn't remove the attempt from the taskidToTIPMap.,,Closed,Fixed,,Devaraj Das,Vinod Kumar Vavilapalli,Wed; 18 Feb 2009 12:32:18 +0000,Wed; 8 Jul 2009 16:53:16 +0000,Mon; 23 Feb 2009 12:03:57 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5280
HADOOP-5281,Bug,Blocker,,GzipCodec fails second time it is used in a process,The attached code (GZt.java) raises:,Closed,Fixed,,Chris Douglas,Ben Maurer,Thu; 19 Feb 2009 02:55:49 +0000,Thu; 23 Apr 2009 19:18:04 +0000,Tue; 17 Mar 2009 18:40:54 +0000,,0.18.0,,,,https://issues.apache.org/jira/browse/HADOOP-5281
HADOOP-5282,Bug,Blocker,,Running tasks on lost trackers are incorrectly logging the attempt and task failures,If a running attempt is lost on a tracker; a attempt-level cleanup with the same attempt-id is launched for it. In such cases the events are not logged correctly and instead of logging it logs,Closed,Fixed,,Amar Kamat,Amar Kamat,Thu; 19 Feb 2009 04:37:56 +0000,Wed; 8 Jul 2009 16:53:16 +0000,Fri; 20 Feb 2009 15:23:13 +0000,,0.20.0,,HADOOP-4191,,https://issues.apache.org/jira/browse/HADOOP-5282
MAPREDUCE-28,Bug,Major,jobtracker;test,TestQueueManager takes too long and times out some times,TestQueueManager takes long time for the run and timeouts sometimes.See the failure at http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3875/testReport/.Looking at the console output; before the test finsihes; it was timed-out.On my machine; the test takes about 5 minutes.,Closed,Fixed,,V.V.Chaitanya Krishna,Amareshwari Sriramadasu,Thu; 19 Feb 2009 06:47:40 +0000,Tue; 24 Aug 2010 21:13:17 +0000,Fri; 27 Nov 2009 04:51:37 +0000,,0.21.0,,MAPREDUCE-1075,,https://issues.apache.org/jira/browse/MAPREDUCE-28
MAPREDUCE-509,Wish,Minor,,Setting the network interface for TaskTracker connection to task tracker http server,I ma running TestDFSIO using different network interfaces.For that; I am modifying the IP address in hadoop-default.xml; hadoop-site.xml and slaves files in the conf directory.Unfortunately; I can still observed connection from TaskTracker to task tracker http server over the default network interface (eth0) when it is configured to use different network interface.Can you tell me what should be done to be sure that all connection used a unique network interfaces?,Resolved,Not A Problem,,Unassigned,Amnon Katz,Thu; 19 Feb 2009 11:22:11 +0000,Mon; 21 Jul 2014 20:28:24 +0000,Mon; 21 Jul 2014 20:28:24 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-509
HADOOP-5285,Bug,Blocker,,JobTracker hangs for long periods of time,On one of the larger clusters of 2000 nodes; JT hanged quite often; sometimes for times in the order of 10-15 minutes and once for one and a half hours. The stack trace shows that JobInProgress.obtainTaskCleanupTask() is waiting for lock on JobInProgress object which JobInProgress.initTasks() is holding for a long time waiting for DFS operations.,Closed,Fixed,,Devaraj Das,Vinod Kumar Vavilapalli,Thu; 19 Feb 2009 12:15:05 +0000,Wed; 8 Jul 2009 16:53:16 +0000,Mon; 23 Feb 2009 07:37:20 +0000,,0.19.1,,,HADOOP-5483,https://issues.apache.org/jira/browse/HADOOP-5285
HADOOP-5286,Bug,Major,,DFS client blocked for a long time reading blocks of a file on the JobTracker,On a large cluster; we've observed that DFS client was blocked on reading a block of a file for almost 1 and half hours. The file was being read by the JobTracker of the cluster; and was a split file of a job. On the NameNode logs; we observed that the block had a message as follows:Inconsistent size for block blk_2044238107768440002_840946 reported from ip:port current size is 195072 reported size is 1318567Details follow.,Closed,Duplicate,HADOOP-4664,Unassigned,Hemanth Yamijala,Thu; 19 Feb 2009 12:23:30 +0000,Wed; 8 Jul 2009 16:43:33 +0000,Fri; 27 Feb 2009 21:49:30 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5286
MAPREDUCE-32,Bug,Major,,Killed task is logged as Failed upon a lost tracker,Upon a lost tracker; the successful attempt is killed but the tip is marked as failed.,Resolved,Not A Problem,,Unassigned,Amar Kamat,Thu; 19 Feb 2009 14:03:05 +0000,Sat; 31 Dec 2011 09:18:19 +0000,Sat; 31 Dec 2011 09:18:19 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-32
MAPREDUCE-542,Bug,Major,,FairScheduler.getJobs(queue) should return the jobs in the order of scheduling.,Without this; it is difficult to know what the order of preference of running tasks from jobs at any time is.,Open,Unresolved,,Unassigned,Vinod Kumar Vavilapalli,Thu; 19 Feb 2009 14:13:21 +0000,Sat; 20 Jun 2009 08:01:43 +0000,,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-542
MAPREDUCE-161,Bug,Major,,Tasks are not scheduled even though task trackers have extra slots,I ran a job with 51 reduce tasks on a cluster with 13 task trackers running Hadoop 0.19. Each task tracker has 5 reduce slots.Initially; each task tracker accepted 4 reduce tasks as expected. However;  3 task trackers were put into blacklist because many tasks failed on them.However; those failed tasks stayed in pending state; not being scheduled to other task trackers; even though each of the other healthy tracker has one free slot.,Resolved,Incomplete,,Unassigned,Runping Qi,Thu; 19 Feb 2009 19:44:04 +0000,Mon; 21 Jul 2014 20:29:07 +0000,Mon; 21 Jul 2014 20:29:07 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-161
HDFS-253,New Feature,Major,namenode,Method to retrieve all quotas active on HDFS,"Currently the only way to view quota information on an HDFS is via dfs -count -q; which is fine when an admin is examining a specific directory for quota status.It would also be good to do full HDFS quota audits; by pulling all HDFS quotas currently set on the system. This is especially important when trying to do capacity management (""OK; how much quota have we allotted so far?""). I think the only way to do this now is via lsr | count -q; which is pretty cumbersome.",Patch Available,Unresolved,HDFS-2711,Surendra Singh Lilhore,Marco Nicosia,Thu; 19 Feb 2009 21:54:21 +0000,Tue; 7 Jul 2015 09:47:39 +0000,,,,newbie,,,https://issues.apache.org/jira/browse/HDFS-253
CHUKWA-7,Bug,Trivial,Data Collection,chukwa agent should do something sensible if CHUKWA_HOME isn't defined,If Chukwa is started in such a way that CHUKWA_HOME doesn't get set; then pid files are stored in chukwa/null/var....  This comes up if; e.g.; the agent is started within eclipse.  We can make this case look prettier by defaulting to /tmp.,Resolved,Fixed,,Ari Rabkin,Ari Rabkin,Thu; 19 Feb 2009 22:13:27 +0000,Sun; 5 Dec 2010 02:54:40 +0000,Sun; 5 Dec 2010 02:54:40 +0000,,,,,,https://issues.apache.org/jira/browse/CHUKWA-7
HADOOP-5292,Bug,Major,,KFS: calling getFileBlockLocations() on 0-length file causes a NPE,When getFileBlockLocations() in KosmosFileSystem.java is called on a file with 0-length; there is a NPE.,Closed,Fixed,,Sriram Rao,Sriram Rao,Thu; 19 Feb 2009 22:47:35 +0000,Thu; 23 Apr 2009 19:18:04 +0000,Mon; 23 Feb 2009 05:45:23 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5292
MAPREDUCE-193,Bug,Major,,NPEs in JobClient when mapred.jobtracker.completeuserjobs.maximum is set to zero.,Throwing NPEs is not enough of information for the user. Proper exceptions should be thrown with relevant messages.,Resolved,Incomplete,,Unassigned,Vinod Kumar Vavilapalli,Fri; 20 Feb 2009 13:12:43 +0000,Mon; 21 Jul 2014 20:52:07 +0000,Mon; 21 Jul 2014 20:52:07 +0000,,,,,HADOOP-5247,https://issues.apache.org/jira/browse/MAPREDUCE-193
MAPREDUCE-170,Bug,Major,,JobTracker doesn't need to download job's jar file onto its local filesystem.,,Resolved,Fixed,,Unassigned,Vinod Kumar Vavilapalli,Fri; 20 Feb 2009 13:39:21 +0000,Mon; 21 Jul 2014 20:52:35 +0000,Mon; 21 Jul 2014 20:52:35 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-170
HADOOP-5295,Improvement,Major,,JobTracker can hold the list of lost TaskTrackers instead of removing them completely.,Having the name and possibly the time for which it has been lost and displaying the info via client UI as well as web UI will help in recognizing problematic nodes easily and quickly.,Resolved,Won't Fix,,Unassigned,Vinod Kumar Vavilapalli,Fri; 20 Feb 2009 14:00:51 +0000,Mon; 21 Jul 2014 20:53:06 +0000,Mon; 21 Jul 2014 20:53:06 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5295
MAPREDUCE-472,Improvement,Minor,,Unassigned tasks cannot be killed/failed from the web UI,Even though private actions are enabled; tasks in UNASSIGNED state cannot be killed/failed from the web UI like other tasks as there are no such hyper-links on taskdetails.jsp.,Resolved,Won't Fix,,Unassigned,Vinod Kumar Vavilapalli,Fri; 20 Feb 2009 14:20:11 +0000,Sat; 9 May 2015 01:02:39 +0000,Sat; 9 May 2015 01:00:49 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-472
MAPREDUCE-418,Bug,Minor,,JT not able to find task id while updating status,JT not able to find task ids while updating status in gridmix2. Even though the jobs are completed successfully; JT is displaying following message for most of the tasks:2009-02-20 14:46:29;488 INFO org.apache.hadoop.mapred.JobTracker: Serious problem.  While updating status; cannot find taskid attempt_200902201433_0020_r_000003_1,Resolved,Fixed,,Unassigned,Suman Sehgal,Fri; 20 Feb 2009 15:34:11 +0000,Mon; 21 Jul 2014 20:54:32 +0000,Mon; 21 Jul 2014 20:54:32 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-418
HADOOP-5298,Bug,Major,test,Unit test fails out on trunk org.apache.hadoop.http.TestServletFilter.testServletFilter,From: http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/760/Regressionorg.apache.hadoop.http.TestServletFilter.testServletFilterFailing for the past 1 build (Since #760 )Took 1 min 10 sec.Error Messageurl4=/static/hadoop-logo.jpg expected:8 but was:9,Closed,Fixed,,Tsz Wo Nicholas Sze,Lee Tucker,Fri; 20 Feb 2009 16:38:04 +0000,Thu; 23 Apr 2009 19:18:04 +0000,Mon; 9 Mar 2009 22:15:41 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5298
MAPREDUCE-335,Improvement,Major,,Reducer inputs should be spilled to HDFS rather than local disk.,Currently; both map outputs and reduce inputs are stored on local disks of tasktrackers. (Un) Availability of local disk space for intermediate data is seen as a major factor in job failures. The suggested solution is to store these intermediate data on HDFS (maybe with replication factor of 1). However; the main blocker issue with that solution is that lots of temporary names (proportional to total number of maps); can overwhelm the namenode; especially since the map outputs are typically small (most produce one block output).Also; as we see in many applications; the map outputs can be estimated more accurately; and thus users can plan accordingly; based on available local disk space.However; the reduce input sizes can vary a lot; especially for skewed data (or because of bad partitioning.)So; I suggest that it makes more sense to keep map outputs on local disks; but the reduce inputs (when spilled from reducer memory) should go to HDFS.Adding a configuration variable to indicate the filesystem to be used for reduce-side spills would let us experiment and compare the efficiency of this new scheme.,Open,Unresolved,,Unassigned,Milind Bhandarkar,Fri; 20 Feb 2009 18:14:33 +0000,Sat; 20 Jun 2009 07:51:09 +0000,,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-335
HADOOP-5300,Bug,Major,build,ant javadoc-dev does not work,,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Fri; 20 Feb 2009 19:30:27 +0000,Tue; 24 Aug 2010 20:36:01 +0000,Mon; 23 Feb 2009 20:07:20 +0000,,,,,HADOOP-5312,https://issues.apache.org/jira/browse/HADOOP-5300
HADOOP-5301,Bug,Major,,Hadoop Metrics Log file should be used by only one process,All JVM info are going to chukwa-user-jvm.log so more than one process is writing to this log file.The problem is that each new process will register a new adaptor starting at the beginning of the file.,Resolved,Fixed,CHUKWA-6,Jerome Boulon,Jerome Boulon,Fri; 20 Feb 2009 23:14:34 +0000,Wed; 8 Jul 2009 16:40:45 +0000,Thu; 26 Feb 2009 05:10:48 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5301
HADOOP-5302,Bug,Major,,If a record is too big; the adaptor will stop sending chunks,,Resolved,Fixed,,Jerome Boulon,Jerome Boulon,Fri; 20 Feb 2009 23:27:10 +0000,Wed; 8 Jul 2009 16:40:45 +0000,Fri; 27 Feb 2009 00:26:21 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5302
HADOOP-5303,New Feature,Major,,Oozie; Hadoop Workflow System,This is a proposal for a system specialized in running Hadoop/Pig jobs in a control dependency DAG (Direct Acyclic Graph); a Hadoop workflow application.Attached there is a complete specification and a high level overview presentation.Highlights A Workflow application is DAG that coordinates the following types of actions: Hadoop; Pig; Ssh; Http; Email and sub-workflows. Flow control operations within the workflow applications can be done using decision; fork and join nodes. Cycles in workflows are not supported.Actions and decisions can be parameterized with job properties; actions output (i.e. Hadoop counters; Ssh key/value pairs output) and file information (file exists; file size; etc). Formal parameters are expressed in the workflow definition as {{${VAR}}} variables.A Workflow application is a ZIP file that contains the workflow definition (an XML file); all the necessary files to run all the actions: JAR files for Map/Reduce jobs; shells for streaming Map/Reduce jobs; native libraries; Pig scripts; and other resource files.Before running a workflow job; the corresponding workflow application must be deployed in HWS.Deploying workflow application and running workflow jobs can be done via command line tools; a WS API and a Java API.Monitoring the system and workflow jobs can be done via a web console; command line tools; a WS API and a Java API.When submitting a workflow job; a set of properties resolving all the formal parameters in the workflow definitions must be provided. This set of properties is a Hadoop configuration.Possible states for a workflow jobs are: CREATED; RUNNING; SUSPENDED; SUCCEEDED; KILLED and FAILED.In the case of a action failure in a workflow job; depending on the type of failure; HWS will attempt automatic retries; it will request a manual retry or it will fail the workflow job.HWS can make HTTP callback notifications on action start/end/failure events and workflow end/failure events.In the case of workflow job failure; the workflow job can be resubmitted skipping previously completed actions. Before doing a resubmission the workflow application could be updated with a patch to fix a problem in the workflow application code.,Resolved,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Sun; 22 Feb 2009 03:42:56 +0000,Fri; 13 Aug 2010 07:37:01 +0000,Fri; 13 Aug 2010 07:37:02 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5303
MAPREDUCE-62,Bug,Major,,job.xml should have high replication factor by default,job.xml is set the default replication factor of 3 by the JobClient. The same config file is accessed from the DFS by JT as well as all the TTs. Hence it should be set a high replication factor say 10; just like the job.jar.,Resolved,Not A Problem,,Unassigned,Vinod Kumar Vavilapalli,Mon; 23 Feb 2009 05:52:46 +0000,Sat; 31 Dec 2011 09:54:43 +0000,Sat; 31 Dec 2011 09:54:43 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-62
HADOOP-5305,Bug,Minor,test,Unit test org.apache.hadoop.fs.TestCopyFiles.testMapCount fails on trunk,"org.apache.hadoop.fs.TestCopyFiles.testMapCount fails on trunk quite often with ""Unexpected map count"" error message. See http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/762/testReport/org.apache.hadoop.fs/TestCopyFiles/testMapCount/ for detailed output.",Closed,Fixed,,Tsz Wo Nicholas Sze,Ramya Sunil,Mon; 23 Feb 2009 08:43:43 +0000,Thu; 23 Apr 2009 19:18:04 +0000,Fri; 3 Apr 2009 23:00:16 +0000,,0.20.0;0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5305
HADOOP-5306,Bug,Blocker,,Job History file can have empty string as http port after JobTracker Restart in case of lost TT; which can result in NumberFormatException when JT is restarted 2nd time,"HTTP_PORT=""""  is seen in job history file after JT recovery in case of lost TT. The .recover file of TestJobTrackerRestartWithLostTracker has empty string as HTTP_PORT. If another time JT is restarted and then JT tries to read the history line and tries to createTaskAttempt; it would get NumberFormatException because of Integer.parseInt(httpPort). We somehow need to log a legal value as HTTP_PORT in the history file OR the exception needs to be caught and proper action is to be taken.",Closed,Fixed,,Amar Kamat,Ravi Gummadi,Mon; 23 Feb 2009 08:43:57 +0000,Wed; 8 Jul 2009 16:53:17 +0000,Thu; 26 Feb 2009 11:14:47 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5306
HADOOP-5307,Bug,Major,util,Fix null value handling in StringUtils#arrayToString() and #getStrings(),"StringUtils#arrayToString() converts String array to a String of comma separated elements. If the String array includes null values; these are recovered as ""null"" (literal) from getStrings() method; which eventually causes configuration issues.",Closed,Won't Fix,,Enis Soztutar,Enis Soztutar,Mon; 23 Feb 2009 16:02:21 +0000,Thu; 23 Apr 2009 19:25:04 +0000,Mon; 16 Mar 2009 14:18:26 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5307
HADOOP-5308,Bug,Major,,In case of exception in any FileTailingAdaptor the tailer will exit his main loop ,,Resolved,Fixed,,Jerome Boulon,Jerome Boulon,Mon; 23 Feb 2009 17:06:25 +0000,Wed; 8 Jul 2009 16:40:44 +0000,Fri; 27 Feb 2009 21:28:57 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5308
HADOOP-5309,Bug,Major,, Hourly and Daily rolling should use CHUKWA_CONF_DIR ,,Resolved,Fixed,,Jerome Boulon,Jerome Boulon,Mon; 23 Feb 2009 17:07:19 +0000,Wed; 8 Jul 2009 16:40:44 +0000,Fri; 27 Feb 2009 18:12:25 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5309
HADOOP-5310,Bug,Major,,Init.d script for chukwa-data-processors is not using --config ${CHUKWA_CONF_DIR} ,,Resolved,Fixed,,Jerome Boulon,Jerome Boulon,Mon; 23 Feb 2009 17:09:33 +0000,Wed; 8 Jul 2009 16:40:45 +0000,Thu; 26 Feb 2009 06:00:13 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5310
HADOOP-5311,Bug,Blocker,,Write pipeline recovery fails,A write pipeline recovery fails on the error below:INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 53006; call recoverBlock(blk_1415000632081498137_954380; false; [Lorg.apache.hadoop.hdfs.protocol.DatanodeInfo;@4ec82dc6) from XX: error: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_1415000632081498137_954380 is already commited; storedBlock == null.        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:4487)        at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:473)        at sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)        at java.lang.reflect.Method.invoke(Method.java:597)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:396)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953),Resolved,Fixed,,dhruba borthakur,Hairong Kuang,Mon; 23 Feb 2009 19:42:14 +0000,Wed; 8 Jul 2009 16:43:32 +0000,Fri; 13 Mar 2009 06:24:46 +0000,,0.18.0,,,HADOOP-3998,https://issues.apache.org/jira/browse/HADOOP-5311
HADOOP-5312,Task,Major,build,Not all core javadoc are checked by Hudson,"Since ""ant javadoc"" does not generate all core javadocs; some javadocs (e.g. HDFS javadocs) are not checked by Hudson.",Resolved,Fixed,,Unassigned,Tsz Wo Nicholas Sze,Mon; 23 Feb 2009 20:18:59 +0000,Mon; 21 Jul 2014 20:59:43 +0000,Mon; 21 Jul 2014 20:59:43 +0000,,,,,HADOOP-5300,https://issues.apache.org/jira/browse/HADOOP-5312
HADOOP-5313,Bug,Major,,Variable FileTailingAdaptor adaptor in TerminatorThread should not be static ,The adaptor field in TerminatorThread should not be static. Each TerminatorThread should work on his own private log file via the adaptor.,Resolved,Fixed,,Jerome Boulon,Jerome Boulon,Mon; 23 Feb 2009 21:02:32 +0000,Wed; 8 Jul 2009 16:40:45 +0000,Fri; 27 Feb 2009 00:16:59 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5313
HADOOP-5314,Bug,Major,,needToSave incorrectly calculated in loadFSImage(),FSImage.loadFSImage() incorrectly calculates the value of needToSave; which is always true and results in saving image at startup even if that is not necessary.,Closed,Fixed,,Jakob Homan,Konstantin Shvachko,Mon; 23 Feb 2009 22:50:22 +0000,Wed; 15 Jun 2011 18:31:27 +0000,Fri; 15 May 2009 22:10:48 +0000,,0.19.0,,,HDFS-1981,https://issues.apache.org/jira/browse/HADOOP-5314
HDFS-69,Bug,Minor,,Improve dfsadmin command line help ,"Enhance dfsadmin command line help informing ""A quota of one forces a directory to remain empty""",Closed,Fixed,,Harsh J,Ravi Phulari,Mon; 23 Feb 2009 23:32:06 +0000,Mon; 5 Mar 2012 02:49:21 +0000,Thu; 12 Jan 2012 06:52:39 +0000,,1.0.0,,,,https://issues.apache.org/jira/browse/HDFS-69
HADOOP-5316,Bug,Trivial,,Unable to detect running process from pid file,Linux ps command output produces white space in front of the pid number; if the pid is lesser than 5 digits number.  The current shell script does not parse this correctly for watchdog.  In addition; CHUKWA_PID_DIR needs to be better supported by chukwa's shell script.  In some of the shell scripts; the reference to pid file is in CHUKWA_HOME/var/run; and this should be changed to CHUKWA_PID_DIR.,Closed,Duplicate,NULL,Eric Yang,Eric Yang,Tue; 24 Feb 2009 01:47:28 +0000,Wed; 8 Jul 2009 16:40:44 +0000,Fri; 6 Mar 2009 01:42:52 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5316
HADOOP-5317,Improvement,Major,documentation,Provide documentation for LazyOutput Feature,"HADOOP-4927 introduced support for the ""LazyOutput"" feature. Documentation needs to be updated for this.",Closed,Fixed,,Jothi Padmanabhan,Jothi Padmanabhan,Tue; 24 Feb 2009 04:27:03 +0000,Tue; 24 Aug 2010 20:36:03 +0000,Thu; 5 Mar 2009 19:08:10 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5317
HADOOP-5318,Bug,Major,,Poor IO Performance due to AtomicLong operations,The AtomicLong operations in counting file system statistics can cause high levels of contention with multiple threads. This test demonstrates having multiple threads writing to different sequence files: The results of this benchmark are However; if I comment out the file system statistics increments; the benchmark improves to:,Resolved,Won't Fix,,Todd Lipcon,Ben Maurer,Tue; 24 Feb 2009 04:29:14 +0000,Mon; 30 Sep 2013 07:49:21 +0000,Wed; 9 Sep 2009 07:14:00 +0000,,0.19.0,,,HDFS-5276;HADOOP-6166;HADOOP-6148,https://issues.apache.org/jira/browse/HADOOP-5318
MAPREDUCE-178,Bug,Major,,Inconsistency in handling lost trackers upon jobtracker restart,"If a tasktracker is lost; the jobtracker kills all the tasks that were successful on that tracker and re-executes it somewhere else. In-memory datastructures are all cleared up for the lost tracker. Now if the jobtracker restarts; the new jobtracker has no clue about the trackers that were lost and hence if the lost tracker join back; they will be accepted and all the tasks on those tracker will join back. Following are the issues	If the running task on the lost tracker is killed; its cleanup attempt will be launched. Now the new jobtracker has no idea about this attempt. Also the lost tracker can join back and hence there are 2 attempts that are running with the same id; one which can move the tip to success and other which moves the tip to killed state.	Ideally; the lost tracker should be asked to re-init which wont happen now.",Resolved,Invalid,,Unassigned,Amar Kamat,Tue; 24 Feb 2009 10:54:59 +0000,Wed; 7 Oct 2009 10:27:52 +0000,Wed; 7 Oct 2009 10:27:52 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-178
HADOOP-5320,Bug,Minor,test,TestMapReduceLocal is missing a close() that is causing it to fail while running the test on NFS,The readFile method in this test is not calling a close of the file after it is done reading. This causes some lingering .nfs* files that is preventing the directory from getting deleted properly causing the second program in this test to fail.,Closed,Fixed,HADOOP-4907,Jothi Padmanabhan,Jothi Padmanabhan,Wed; 25 Feb 2009 09:16:34 +0000,Thu; 23 Apr 2009 19:18:04 +0000,Fri; 20 Mar 2009 17:39:23 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5320
HADOOP-5321,Bug,Major,io,BZip2CompressionOutputStream sometimes corrupts data,We are using a BZip2CompressionOutputStream.java version with bugs.See the following 2 issues for details:https://issues.apache.org/bugzilla/show_bug.cgi?id=24798https://issues.apache.org/bugzilla/show_bug.cgi?id=41596,Closed,Duplicate,HADOOP-5326,Zheng Shao,Zheng Shao,Wed; 25 Feb 2009 09:31:13 +0000,Thu; 23 Apr 2009 19:25:04 +0000,Thu; 26 Feb 2009 22:10:44 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5321
HADOOP-5322,Bug,Minor,,comments in JobInProgress related to TaskCommitThread are not valid,There are some comments in JobInProgress referring to TaskCommitThread. Since TaskCommitThread is  no more present; the comments should be deleted/modified.,Closed,Fixed,,Amareshwari Sriramadasu,Amareshwari Sriramadasu,Wed; 25 Feb 2009 10:54:15 +0000,Tue; 24 Aug 2010 20:36:04 +0000,Thu; 26 Mar 2009 23:42:02 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5322
HADOOP-5323,Improvement,Minor,documentation,Trash documentation should describe its directory structure and configurations,"Trash documentation should mention the significance of ""Current"" and ""time-stamp"" directories which get generated inside Trash directory. The documentation should also incorporate modifications done in HADOOP: 4970.",Closed,Fixed,,Weiwei Yang,Suman Sehgal,Wed; 25 Feb 2009 11:04:03 +0000,Thu; 1 Dec 2016 23:11:04 +0000,Thu; 3 Sep 2015 13:41:57 +0000,,0.18.3,newbie,,HADOOP-12374,https://issues.apache.org/jira/browse/HADOOP-5323
MAPREDUCE-34,Bug,Major,,Reduce step hangs while recovering a block from bad datanode,The reduce step hangs infinitely when its trying to recover a block from a bad datanode. The node from which the block is being retrieved is alive and TT and DN are up and running.,Resolved,Not A Problem,,Unassigned,Ramya Sunil,Wed; 25 Feb 2009 11:11:23 +0000,Sat; 31 Dec 2011 09:14:52 +0000,Sat; 31 Dec 2011 09:14:52 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-34
HADOOP-5325,Bug,Blocker,,Jobs are getting  stuck  on JobTracker  restart,"Recovery manager failed to recover the job properly and threw ""2009-02-25 09:24:09;944 INFO org.apache.hadoop.fs.FSInputChecker: Found checksum error: b7168; 7168=org.apache.hadoop.fs.ChecksumException: Checksum error: file filename "".. As part of recovery; one of the attempts got added to the expiry launching tasks list. But. another attempt of the same tip was relaunched with the same attempt id and the job was stuck. Looks like the attempt which got expired was marked as failed and newly launched attempt (with same attempt id) was successful and Jobtracker tried marking a failed tip as successful hence the tip was considered as failed but tip.isComplete returns true.",Closed,Duplicate,NULL,Amar Kamat,Karam Singh,Wed; 25 Feb 2009 11:34:01 +0000,Wed; 8 Jul 2009 16:53:17 +0000,Thu; 26 Feb 2009 04:08:21 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5325
HADOOP-5326,Bug,Major,io,bzip2 codec (CBZip2OutputStream) creates corrupted output file for some inputs,"Bzip2 codec generated corrupted output files in some test executions I performed. This bug is probably related to https://issues.apache.org/bugzilla/show_bug.cgi?id=41596.	In my case; the problem seems to be at the BWT (Burrows-Wheeler Transform) implementation.",Closed,Fixed,HADOOP-5321,Rodrigo Schmidt,Rodrigo Schmidt,Wed; 25 Feb 2009 12:48:11 +0000,Thu; 23 Apr 2009 19:18:04 +0000,Sat; 28 Feb 2009 23:11:49 +0000,,0.19.0;0.19.1;0.19.2;0.20.0;0.21.0,,HADOOP-3646,,https://issues.apache.org/jira/browse/HADOOP-5326
HADOOP-5327,Bug,Blocker,,Job files for a job failing because of  ACLs are not clean from the system directory,Jobs which failed because of ACLs gets added during JT restart recovery,Closed,Fixed,,Amar Kamat,Karam Singh,Wed; 25 Feb 2009 12:58:14 +0000,Wed; 8 Jul 2009 16:53:17 +0000,Thu; 12 Mar 2009 11:31:13 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5327
HADOOP-5328,Bug,Major,,Renaming of Job histroy file is incorrect if Jobtracker is restarted multimple times,After 1st JT restart job history file name was jobfilename.recoverAfter 2and JT restart  job history file name was jobfilename.recover.recoverAfter 3rd JT restart  ob history file name was jobfilename.recover.recover.recover,Closed,Fixed,,Amar Kamat,Karam Singh,Wed; 25 Feb 2009 13:11:58 +0000,Thu; 23 Apr 2009 19:18:04 +0000,Thu; 19 Mar 2009 13:58:13 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5328
HADOOP-5329,Bug,Major,test,TestLocalMRNotification.testMR failed in Hudson,TestLocalMRNotification.testMR failed in Hudson; from build #3911 to the latest; build #3917.,Closed,Cannot Reproduce,,Unassigned,Tsz Wo Nicholas Sze,Wed; 25 Feb 2009 19:07:00 +0000,Fri; 26 Oct 2012 23:47:05 +0000,Sat; 21 Mar 2009 00:12:05 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5329
HADOOP-5330,Bug,Major,,Zombie tasks remain after jobs finish/fail/get killed,"I'm seeing a lot of ""task attempts"" around our hadoop cluster for jobs that are no longer around. The attempts seem to be ""hung""; as they sit there forever. Additionally; they seem to take up map and reduce slots in the cluster unless MapReduce is restarted. This causes real jobs to be unable to utilize the whole cluster.",Resolved,Invalid,,Unassigned,Nathan Marz,Wed; 25 Feb 2009 23:00:17 +0000,Thu; 30 Jul 2009 06:55:56 +0000,Thu; 30 Jul 2009 06:55:56 +0000,,0.19.1,,,,https://issues.apache.org/jira/browse/HADOOP-5330
HADOOP-5331,Improvement,Major,,KFS: Add support for append,KFS has support for file appends.  THis jira is to get the append support for changes included in the KFS+Hadoop glue layer,Closed,Fixed,,Sriram Rao,Sriram Rao,Thu; 26 Feb 2009 02:44:38 +0000,Tue; 24 Aug 2010 20:36:04 +0000,Thu; 26 Mar 2009 08:24:09 +0000,,0.19.1,,,,https://issues.apache.org/jira/browse/HADOOP-5331
HADOOP-5332,Improvement,Blocker,,Make support for file append API configurable,FSNamesystem already has a private dfs.support.append config option.  Make this public in src/hdfs/hdfs-default.xml and set the default to false.,Closed,Fixed,,dhruba borthakur,Nigel Daley,Thu; 26 Feb 2009 04:09:53 +0000,Mon; 3 Aug 2009 12:59:04 +0000,Wed; 11 Mar 2009 16:56:40 +0000,,,,,HADOOP-5224,https://issues.apache.org/jira/browse/HADOOP-5332
HADOOP-5333,Bug,Major,,The libhdfs append API is not coded correctly,The hdfsOpenFile() API does not handle the APPEND bit correctly.,Closed,Fixed,,dhruba borthakur,dhruba borthakur,Thu; 26 Feb 2009 08:59:31 +0000,Mon; 3 Aug 2009 12:59:04 +0000,Wed; 11 Mar 2009 18:59:46 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5333
HDFS-136,Bug,Major,,Not able to run randomwriter/sort on hdfs if all the nodes of same rack are killed.,"Not able to run randomwriter if all the datanodes of any one of the racks are killed. (replication factor : 3)Randomwriter job gets failed and following error message is displayed in log:java.net.ConnectException: Connection refused	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)	at sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:100)	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2398)	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2354)	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1800(DFSClient.java:1744)	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1927)",Resolved,Incomplete,,Unassigned,Suman Sehgal,Thu; 26 Feb 2009 09:19:54 +0000,Mon; 21 Jul 2014 21:06:26 +0000,Mon; 21 Jul 2014 21:06:26 +0000,,,,,HADOOP-5599,https://issues.apache.org/jira/browse/HDFS-136
MAPREDUCE-146,Bug,Major,,Stale job files in mapred system directory,In one scenario; job client uploaded the job split file and job jar onto the mapred system directory but got killed before the actual job got submitted and/or before the job got known to the JT. In such situations; the split and jar files are left as garbage as JT doesn't know of the job yet. On a long running JT; this garbage can accumulate.,Resolved,Not A Problem,,Unassigned,Vinod Kumar Vavilapalli,Thu; 26 Feb 2009 11:09:00 +0000,Mon; 21 Jul 2014 21:07:44 +0000,Mon; 21 Jul 2014 21:07:44 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-146
HADOOP-10867,Bug,Major,,NPE in heartbeat when the configured topology script doesn't exist,,Open,Unresolved,MAPREDUCE-820,Mauro Murari,Vinod Kumar Vavilapalli,Thu; 26 Feb 2009 11:12:53 +0000,Mon; 2 Feb 2015 22:14:18 +0000,,,1.0.3,newbie,,HADOOP-8049,https://issues.apache.org/jira/browse/HADOOP-10867
HADOOP-5337,Bug,Major,,JobTracker greedily schedules tasks without running tasks to join,This issue was observed when JobTracker was restarted 3 times and observed that 4 instances of each reduce task were running. This issue is observed when cluster is not fully occupied.In testcase: Map/reduces capacity is 200/200 slots respectively and Job profile is 11000 maps; 10 reduces and speculative execution is off.  JobTracker was restarted 3 times in small intervals of about 5 mins and after recovery; 40 reduce tasks were running. Task details web page (taskdetails.jsp) was  showing 4 running attempts of each reduce task.,Closed,Fixed,,Amar Kamat,Karam Singh,Thu; 26 Feb 2009 12:06:09 +0000,Wed; 8 Jul 2009 16:53:18 +0000,Fri; 3 Apr 2009 12:02:06 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5337
HADOOP-5338,Bug,Blocker,,Reduce tasks are stuck waiting for map outputs when none are in progress,When JT is restarted several times; a situation is encountered when the reduce tasks are stuck forever waiting for map outputs. However 100%map is complete and none of the map tasks are in progress. The reduce tasks wait infinitely.,Closed,Fixed,,Amar Kamat,Ramya Sunil,Thu; 26 Feb 2009 12:49:37 +0000,Wed; 8 Jul 2009 16:53:17 +0000,Fri; 6 Mar 2009 06:48:15 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5338
HADOOP-5339,Bug,Minor,io,reverse DNS doesnt resolve local loop address 127.0.1.1,if the local IPAddr is loopback; then reverse DNS lookup fails. It may be better to recognise this state and return localhost in such a situation; rather than try to look up 1.1.0.127.in-addr.arpa,Resolved,Duplicate,HADOOP-3426,Unassigned,Steve Loughran,Thu; 26 Feb 2009 13:53:06 +0000,Thu; 29 Apr 2010 08:08:47 +0000,Tue; 5 May 2009 06:04:24 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5339
MAPREDUCE-2812,New Feature,Major,,Combiner that aggregates all the mappers from a machine,From what I can tell; the Combiner just aggregates data from a single map task. It would be useful; especially during map-only jobs; to have a combiner that aggregates data from all the map tasks on a given machine. My use case for this is to vertically partition a set of records which start out in the same files. By doing this in a map-only task; way too many files are created (About 50 files are created per input split). By pumping all the data through a reducer; a lot of unnecessary overhead occurs. With the proposed feature; I would get 50*number of machines files rather than 50*number of input splits files for this use case.,Open,Unresolved,,Unassigned,Nathan Marz,Thu; 26 Feb 2009 19:06:55 +0000,Thu; 11 Aug 2011 18:11:13 +0000,,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-2812
HADOOP-5341,Bug,Major,scripts,hadoop-daemon isn't compatible after HADOOP-4868,The CLI changed for hadoop-daemon.sh in an incompatible way. It now requires the sub-system name in the CLI.,Closed,Fixed,,Sharad Agarwal,Owen O'Malley,Thu; 26 Feb 2009 19:53:04 +0000,Tue; 24 Aug 2010 20:36:05 +0000,Fri; 6 Mar 2009 06:32:39 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5341
HADOOP-5342,Bug,Critical,,DataNodes do not start up because InconsistentFSStateException on just part of the disks in use,"After restarting a cluster (including rebooting) the dfs got corrupted because many DataNodes did not start up; running into the following exception:2009-02-26 22:33:53;774 ERROR org.apache.hadoop.dfs.DataNode: org.apache.hadoop.dfs.InconsistentFSStateException: Directory xxx  is in an inconsistent state: version file in current directory is missing.	at org.apache.hadoop.dfs.Storage$StorageDirectory.analyzeStorage(Storage.java:326)	at org.apache.hadoop.dfs.DataStorage.recoverTransitionRead(DataStorage.java:105)	at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:306)	at org.apache.hadoop.dfs.DataNode.init(DataNode.java:223)	at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:3030)	at org.apache.hadoop.dfs.DataNode.instantiateDataNode(DataNode.java:2985)	at org.apache.hadoop.dfs.DataNode.createDataNode(DataNode.java:2993)	at org.apache.hadoop.dfs.DataNode.main(DataNode.java:3115)This happens when using multiple disks with at least one previously marked as read-only; such that the storage version became out-dated; but after reboot it was mounted read-write; resulting in the DataNode not starting because of out-dated version.This is a big headache. If a DataNode has multiple disks of which at least one has the correct storage version then out-dated versions should not bring down the DataNode.",Open,Unresolved,,Hairong Kuang,Christian Kunz,Thu; 26 Feb 2009 22:43:10 +0000,Mon; 21 Jul 2014 21:12:41 +0000,,,0.18.2,,,,https://issues.apache.org/jira/browse/HADOOP-5342
MAPREDUCE-77,Bug,Critical,,JobConf is deprecated but Job does not support a constructor for Configuration,JobConf has been deprecated but Job does not support any constructor for Configuration which is replacing JobConf.,Resolved,Invalid,,Unassigned,Santhosh Srinivasan,Thu; 26 Feb 2009 23:37:13 +0000,Tue; 11 Aug 2009 06:13:05 +0000,Tue; 11 Aug 2009 06:13:05 +0000,,,,HADOOP-5867,,https://issues.apache.org/jira/browse/MAPREDUCE-77
HADOOP-5344,Bug,Major,documentation,MapReduceBase has been deprecated but there are no references to classes that are replacing it,MapReduceBase has been deprecated but the JavaDoc has no references to classes that are replacements.,Closed,Duplicate,NULL,Unassigned,Santhosh Srinivasan,Thu; 26 Feb 2009 23:38:48 +0000,Thu; 23 Apr 2009 19:25:04 +0000,Fri; 27 Feb 2009 18:35:26 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5344
HADOOP-5345,Bug,Major,documentation,JobID is deprecated but there are no references to classes that are replacing it.,JobID is deprecated but there are no references to classes that are replacing it.,Resolved,Won't Fix,,Unassigned,Santhosh Srinivasan,Thu; 26 Feb 2009 23:43:09 +0000,Mon; 21 Jul 2014 21:13:26 +0000,Mon; 21 Jul 2014 21:13:26 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5345
HADOOP-5346,Task,Major,documentation," ""quota commands"" shown as lower case",http://hadoop.apache.org/core/docs/r0.19.0/hdfs_quota_admin_guide.htmlThe command is shown as:dfsadmin -setquotadfsadmin -setspacequotaBut it should read as in the CLIdfsadmin -setQuotadfsadmin -setSpaceQuotaI'm assuming the other quota commands should also be mixed case.,Closed,Duplicate,HADOOP-4821,Unassigned,Araceli Henley,Fri; 27 Feb 2009 00:43:10 +0000,Thu; 23 Apr 2009 19:25:04 +0000,Fri; 27 Feb 2009 01:29:21 +0000,,0.18.3,,,,https://issues.apache.org/jira/browse/HADOOP-5346
HADOOP-5347,Bug,Minor,,bbp example cannot be run.,FileAlreadyExistsException: Output directory already exists.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Fri; 27 Feb 2009 02:39:50 +0000,Tue; 24 Aug 2010 20:36:06 +0000,Mon; 9 Mar 2009 22:30:22 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5347
HADOOP-5348,New Feature,Major,ipc,Create a ThrowableWritable for serializing exceptions robustly,HADOOP-5201 and other issues would benefit from a stable representation of exceptions; one that can be sent over the network; maybe pushed out to web UIs and which we can be 100% sure that the far end will be able to handle if they have the hadoop-core JAR on their classpath.,Resolved,Won't Fix,,Steve Loughran,Steve Loughran,Fri; 27 Feb 2009 11:30:53 +0000,Mon; 20 Jan 2014 18:53:12 +0000,Mon; 20 Jan 2014 18:53:12 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5348
HADOOP-5349,Bug,Major,,When the size required for a path is -1; LocalDirAllocator.getLocalPathForWrite fails with a DiskCheckerException when the disk it selects is bad.,,Resolved,Fixed,,Vinod Kumar Vavilapalli,Vinod Kumar Vavilapalli,Fri; 27 Feb 2009 12:39:46 +0000,Wed; 8 Jul 2009 16:53:17 +0000,Fri; 8 May 2009 08:04:35 +0000,,0.20.0,,,MAPREDUCE-41,https://issues.apache.org/jira/browse/HADOOP-5349
MAPREDUCE-317,Improvement,Major,,Submitting job information via DFS in Map/Reduce causing consistency and performance issues,"Job submission involves two steps: submitting jobs to the System directory on DFS (done by the client); then submit the job via the JobSubmissionProtocol to JobTracker. This two step process is seen to have some issues:	Since the files need to be read from DFS; slowness in the DFS can cause job initialization to become costly. We faced this as described in HADOOP-5286 and HADOOP-4664.	The two step process could lead to inconsistent information being left around - like in HADOOP-5327 and HADOOP-5335.This JIRA is to explore options to remove the two step process in submitting a job.",Resolved,Incomplete,,Unassigned,Hemanth Yamijala,Fri; 27 Feb 2009 13:59:40 +0000,Mon; 21 Jul 2014 21:14:23 +0000,Mon; 21 Jul 2014 21:14:23 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-317
HADOOP-5351,Bug,Major,,Child.java and Task.java use wrong class names for creating a Log instance,Child.java uses TaskTracker's class name and Task.java uses TaskRunner's class name. This causes confusion during debugging.,Closed,Duplicate,NULL,Unassigned,Vinod Kumar Vavilapalli,Fri; 27 Feb 2009 14:15:35 +0000,Wed; 8 Jul 2009 16:53:18 +0000,Wed; 22 Apr 2009 13:36:09 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5351
MAPREDUCE-85,Bug,Major,,Setting lastProgressReport time in TIP's constructor causes TT to wrongly kill tasks.,,Resolved,Cannot Reproduce,,Unassigned,Vinod Kumar Vavilapalli,Fri; 27 Feb 2009 14:25:20 +0000,Sat; 7 Jul 2012 16:36:09 +0000,Sat; 7 Jul 2012 16:36:09 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-85
HADOOP-5353,Improvement,Minor,fs,add progress callback feature to the slow FileUtil operations with ability to cancel the work,This is something only of relevance of people doing front ends to FS operations; and as they could take the code in FSUtil and add something with this feature; its a blocker to none of them. Current FileUtil.copy can take a long time to move large files around; but there is no progress indicator to GUIs; or a way to cancel the operation mid-way; j interrupting the thread or closing the filesystem.I propose a FileIOProgress interface to the copy ops; one that had a single method to notify listeners of bytes read and written; and the number of files handled. interface FileIOProgress { boolean progress(int files; long bytesRead; long bytesWritten);}The return value would be true to continue the operation; or false to stop the copy and leave the FS in whatever incomplete state it is in currently. it could even be fancier: have  beginFileOperation and endFileOperation callbacks to pass in the name of the current file being worked on; though I don't have a personal need for that.GUIs could show progress bars and cancel buttons; other tools could use the interface to pass any cancellation notice upstream.The FileUtil.copy operations would call this interface (blocking) after every block copy; so the frequency of invocation would depend on block size and network/disk speeds. Which is also why I don't propose having any percentage done indicators; it's too hard to predict percentage of time done for distributed file IO with any degree of accuracy.,Patch Available,Unresolved,,Pranav Prakash,Steve Loughran,Fri; 27 Feb 2009 14:31:06 +0000,Mon; 20 Jun 2016 19:48:12 +0000,,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5353
CHUKWA-5,Bug,Major,Data Collection,AdaptorId should be persistent even after an agent restart,In case of agent restart; an adaptor may not get the same adaptorId. 1)The adaptorId is sent back to the remote application after the Add command.Now; let suppose the agent dies and restarts. Each new adaptor created from the checkpoint will now have another adaptorId. If the remote application tries to stop/shutdown only using the adaptorId received in step (1); the command may be sent to the wrong adaptor.if the adaptorId in for example an MD5 of the command line we don't need to save any additional information inside the checkpoint.This will change the current API from int to String.,Resolved,Fixed,,Ari Rabkin,Jerome Boulon,Fri; 27 Feb 2009 17:20:20 +0000,Sun; 14 Jun 2009 15:40:29 +0000,Fri; 12 Jun 2009 22:12:08 +0000,,,,,CHUKWA-56;CHUKWA-159,https://issues.apache.org/jira/browse/CHUKWA-5
HADOOP-5355,Improvement,Minor,documentation,"Hadoop Quota documentation missing ""user facing"" content",Right now; the HDFS Quota Admin guide is full of good stuff on how to set/unset both space and name quotas.However; there's no documentation on how users should expect the system to behave. What will happen to their running jobs; how will the command line client react; etc?,Open,Unresolved,,Unassigned,Marco Nicosia,Fri; 27 Feb 2009 18:28:06 +0000,Mon; 21 Jul 2014 21:17:48 +0000,,,,newbie,,,https://issues.apache.org/jira/browse/HADOOP-5355
CHUKWA-23,Task,Major,Build and Test Code,Should really clean up chukwa bin scripts,The chukwa/bin directory is overgrown with bin scripts; not all of which are documented or even still in use.  Should really clean this up.,Resolved,Duplicate,NULL,Unassigned,Ari Rabkin,Fri; 27 Feb 2009 19:15:12 +0000,Mon; 10 May 2010 06:07:44 +0000,Mon; 10 May 2010 06:07:44 +0000,,,,,,https://issues.apache.org/jira/browse/CHUKWA-23
CHUKWA-24,Task,Major,Documentation,clean up & improve chukwa docs,There's a lot of obsolete Chukwa documentation floating around; we even have a copy of the CCA paper in /doc.  This should probably go away.,Resolved,Fixed,,Eric Yang,Ari Rabkin,Fri; 27 Feb 2009 19:16:11 +0000,Wed; 29 Apr 2009 05:33:38 +0000,Wed; 29 Apr 2009 05:33:38 +0000,,,,,,https://issues.apache.org/jira/browse/CHUKWA-24
HADOOP-5358,Improvement,Minor,test,Provide scripting functionality to the synthetic load generator,Currently the load generator accepts parameters at start time as to the read and write probabilities to apply against the namenode.  It would be useful to be able to provide it with a script of these values; so that they can be varied over time.  This would allow the namenode to be tested with varying loads over time so its behavior to changing loads can be examined.,Resolved,Fixed,,Jakob Homan,Jakob Homan,Fri; 27 Feb 2009 20:13:58 +0000,Fri; 13 Mar 2009 15:05:18 +0000,Wed; 11 Mar 2009 21:35:31 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5358
HDFS-158,Bug,Major,,HDFS soft lease limit should be configurable per-file,In Hypertable; there is a process called the RangeServer which is responsible for handling updates and queries on portions (ranges) of a database table.  When an update arrives at the RangeServer; it first gets written to a commit log and then it goes into an in-memory table structure.  If at any time; the RangeServer dies; when it comes back up; it replays the commit log to reconstruct the state of the in-memory table structure.  As it stands now; when the RangeServer comes up; it cannot read the commit log file until the soft lease limit has expired.  This causes the RangeServer to hang with all of the ranges that it was managing unaccessible.We would like to be able to configure the soft lease limit per-file so that we can set the limit to 0 for our commit log files.  That would allow Hypertable to recover immediately after a crash.,Resolved,Fixed,,Unassigned,Doug Judd,Fri; 27 Feb 2009 20:44:02 +0000,Thu; 17 Nov 2011 22:22:23 +0000,Thu; 17 Nov 2011 22:22:23 +0000,,,,,HDFS-200,https://issues.apache.org/jira/browse/HDFS-158
HADOOP-5360,Improvement,Minor,,Move from cronjob based watchdog for Chukwa Agent and System metrics to daemon tools startup script,Instead of waiting one minute for cron job based watchdog to check the pid file and determine if the process is running.  Netops request this to be changed to use deamon tools to improve the uptime of chukwa agent and system metrics.,Resolved,Fixed,,Eric Yang,Eric Yang,Fri; 27 Feb 2009 21:55:01 +0000,Wed; 8 Jul 2009 16:40:44 +0000,Fri; 6 Mar 2009 02:06:06 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5360
MAPREDUCE-2813,Bug,Major,,"Tasks freeze with ""No live nodes contain current block""; job takes long time to recover",Running a recent version of trunk on 100 nodes; I occasionally see some tasks freeze at startup and hang the job. These tasks are not speculatively executed either. Here's sample output from one of them: Note how the DFS client fails multiple times to retrieve the block; with a 2 minute wait between each one; without giving up. During this time; the task is not speculated. However; once this task finally failed; a new version of it ran successfully. Getting the input file in question with bin/hadoop fs -get also worked fine.There is no mention of the task attempt in question in the NameNode logs but my guess is that something to do with RPC queues is causing its connection to get lost; and the DFSClient does not recover.,Open,Unresolved,,Unassigned,Matei Zaharia,Fri; 27 Feb 2009 23:34:59 +0000,Thu; 11 Aug 2011 18:11:59 +0000,,,0.21.0,,,,https://issues.apache.org/jira/browse/MAPREDUCE-2813
HDFS-179,Bug,Major,,hadoop fs -test -d should return different codes for difference cases,Currently; hadoop fs -test -d path  returns 0 for the case where path is a directory and -1 for all the other cases such as execution errors; path does not exists; and path is a file.It is better to use different return codes to differentiate these cases.Similar issues also apply to hadoop fs -test -e|z,Resolved,Not A Problem,,Unassigned,Runping Qi,Sat; 28 Feb 2009 00:45:50 +0000,Mon; 21 Jul 2014 21:22:37 +0000,Mon; 21 Jul 2014 21:22:37 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-179
HADOOP-5363,New Feature,Major,,Proxying for multiple HDFS clusters of different versions,A single hdfsproxy server should be able to proxy for multiple HDFS clusters; whose Hadoop versions may be different from each other.,Closed,Fixed,,zhiyong zhang,Kan Zhang,Sat; 28 Feb 2009 02:24:38 +0000,Tue; 24 Aug 2010 20:36:07 +0000,Thu; 26 Mar 2009 01:42:06 +0000,,,,HADOOP-5365,,https://issues.apache.org/jira/browse/HADOOP-5363
HADOOP-5364,Improvement,Major,,Adding SSL certificate expiration warning to hdfsproxy,SSL certificate warning should be provided on both client and proxy server side.,Closed,Fixed,,zhiyong zhang,Kan Zhang,Sat; 28 Feb 2009 02:32:17 +0000,Tue; 24 Aug 2010 20:36:09 +0000,Mon; 4 May 2009 21:39:04 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5364
HADOOP-5365,Improvement,Major,,hdfsprxoy should log every access,Currently; only failed accesses are logged. Need to log successful accesses as well.,Closed,Fixed,,zhiyong zhang,Kan Zhang,Sat; 28 Feb 2009 02:34:19 +0000,Tue; 24 Aug 2010 20:36:10 +0000,Fri; 27 Mar 2009 18:41:18 +0000,,,,HADOOP-5363,,https://issues.apache.org/jira/browse/HADOOP-5365
HADOOP-5366,New Feature,Major,,Support for retrieving files using standard HTTP clients like curl,Currently hdfsproxy only supports HFTP protocol and users have to use HFTP clients. Need to add a simple HTTP GET interface so that users can use standard HTTP clients like curl to retrieve files.,Closed,Fixed,,zhiyong zhang,Kan Zhang,Sat; 28 Feb 2009 02:46:25 +0000,Tue; 24 Aug 2010 20:36:11 +0000,Thu; 26 Mar 2009 18:31:51 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5366
HADOOP-5367,Bug,Critical,,After some jobs have finished; Reducer will run new job's reduce tasks sequentially and not in parallel (mapred.JobTracker: Serious problem.  While updating status; cannot find taskid...),"Hi;After I while; my cluster will only run the reduce tasks sequentially (each reducer running on the same node); the other nodes stay empty. The map phase however will run the jobs on all the nodes; also after such a ""long"" reduce phase has completed. But the reduce phase will then be again executed sequentially. This happens in my cluster after about 160 successfully completed jobs. (Some jobs have reducer set to 0!). As possible solution I have to restart the mapreduce service.I didn't notice this behaviour in version 0.19.0. I can't use version 0.19.0 because of the multipleoutput bug when setting reducers to 0.Anoter site node which might be related. I also tried running the jobs with speculative execution set to on. My cluster would always hold back one reducer and only run it (in multiple instances) after the first of the other 6 reducers had finished; instead of launching all of them at the same time.Below is a short extract from related logfile. It's full of these kind of entries.09/02/28 12:48:07 INFO mapred.JobTracker: Serious problem.  While updating status; cannot find taskid attempt_200902271700_0051_r_000006_109/02/28 12:48:08 INFO mapred.JobTracker: Serious problem.  While updating status; cannot find taskid attempt_200902271700_0041_r_000002_109/02/28 12:48:08 INFO mapred.JobTracker: Serious problem.  While updating status; cannot find taskid attempt_200902271700_0083_r_000006_109/02/28 12:48:08 INFO mapred.JobTracker: Serious problem.  While updating status; cannot find taskid attempt_200902271700_0041_r_000005_109/02/28 12:48:10 INFO mapred.JobTracker: Serious problem.  While updating status; cannot find taskid attempt_200902271700_0105_r_000006_109/02/28 12:48:10 INFO mapred.JobTracker: Serious problem.  While updating status; cannot find taskid attempt_200902271700_0102_r_000006_109/02/28 12:48:12 INFO mapred.JobTracker: Serious problem.  While updating status; cannot find taskid attempt_200902271700_0051_r_000006_109/02/28 12:48:13 INFO mapred.JobTracker: Serious problem.  While updating status; cannot find taskid attempt_200902271700_0041_r_000002_109/02/28 12:48:13 INFO mapred.JobTracker: Serious problem.  While updating status; cannot find taskid attempt_200902271700_0083_r_000006_109/02/28 12:48:13 INFO mapred.JobTracker: Serious problem.  While updating status; cannot find taskid attempt_200902271700_0041_r_000005_1",Resolved,Fixed,,Unassigned,Thibaut,Sat; 28 Feb 2009 13:10:01 +0000,Wed; 8 Jul 2009 16:53:18 +0000,Mon; 30 Mar 2009 06:32:11 +0000,,0.19.1,,,,https://issues.apache.org/jira/browse/HADOOP-5367
HADOOP-5368,Wish,Major,,more user control on customized RecordReader,Currently user can define own InputFormat and RecordReader; but the user has little control on them. An example; we input mutiple files into the mapper and want to handle them in different ways depending on which file this mapper is working.This can be easily done as follows: However; the above code does not work. The reader passed into mapper is wrapped by MapTask; and is either SkippingRecordReader or TrackedRecordReader. We can not cast it back and we can not pass any information through the user defined RecordReader. If the SkippingRecordReader and TrackedRecordReader have a method for getting the raw reader; it will not have this problem.This problem could be resolved by initiating many map-reduce jobs;one job for each file. But this apparently is what we want.Or there exist other solutions? Appreciated for any comments.,Open,Unresolved,,Unassigned,He Yongqiang,Sat; 28 Feb 2009 14:44:30 +0000,Mon; 8 Jun 2009 08:49:05 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-5368
HADOOP-5369,Improvement,Major,,Small tweaks to reduce MapFile index size,"Two minor tweaks can help reduce the memory overhead of the MapFile index a bit:1) Because the index file is a sequence file; it's length is not known. That means the index is built using the standard ""mulitply the size of the buffer on overflow"" with a factor of 3/2. With small keys; the slack in the index can be substantial. This patch has a constant upper bound on the amount of slack allowed.2) In block compressed map files the index file often has entries with the same offset (because the compressed block had more than index interval keys). The entries with identical offsets do not help MapFile do random access any faster. This patch eliminates these types of entries from new map files; and ignores them while reading old map files. This patch greatly helped with memory usage on a compressed hbase table.",Closed,Fixed,,Ben Maurer,Ben Maurer,Sat; 28 Feb 2009 20:45:03 +0000,Tue; 24 Aug 2010 20:36:11 +0000,Mon; 18 May 2009 05:22:39 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5369
HADOOP-5370,Improvement,Minor,,chukwa shouldn't write zero-length sink files,If Chukwa data arrives sporadically; then the collectors will generate a large number of zero-record .done files.  These should be deleted automatically.,Resolved,Fixed,,Ari Rabkin,Ari Rabkin,Sun; 1 Mar 2009 18:11:33 +0000,Wed; 8 Jul 2009 16:40:44 +0000,Tue; 3 Mar 2009 23:19:29 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5370
CHUKWA-25,Improvement,Major,Data Processors,chukwa archiving should group by cluster name,The chukwa archivers ought to have an option for grouping data by cluster name.,Resolved,Fixed,,Ari Rabkin,Ari Rabkin,Sun; 1 Mar 2009 18:12:39 +0000,Mon; 29 Jun 2009 22:15:58 +0000,Mon; 29 Jun 2009 22:15:58 +0000,,0.3.0,,,,https://issues.apache.org/jira/browse/CHUKWA-25
CHUKWA-27,Improvement,Major,,chukwa should not override environmental JAVA_HOME,Right now; JAVA_HOME is hardcoded by the chukwa conf script. This is probably the wrong thing; we should let the user's environment specify this by default.,Resolved,Fixed,,Ari Rabkin,Ari Rabkin,Mon; 2 Mar 2009 00:36:27 +0000,Mon; 30 Mar 2009 23:53:28 +0000,Mon; 30 Mar 2009 23:53:28 +0000,,,,,,https://issues.apache.org/jira/browse/CHUKWA-27
HADOOP-5373,Improvement,Trivial,,chukwa collectors should track lifetime-received chunks,Chukwa collectors should also return a lifetime total of recieved chunks when pinged.,Resolved,Fixed,,Ari Rabkin,Ari Rabkin,Mon; 2 Mar 2009 00:42:46 +0000,Wed; 8 Jul 2009 16:40:44 +0000,Tue; 3 Mar 2009 23:22:34 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5373
HADOOP-5374,Bug,Major,,NPE in JobTracker.getTasksToSave() method,,Closed,Fixed,,Amareshwari Sriramadasu,Vinod Kumar Vavilapalli,Mon; 2 Mar 2009 10:57:58 +0000,Mon; 3 Aug 2009 12:59:04 +0000,Wed; 25 Mar 2009 06:25:14 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5374
MAPREDUCE-102,Bug,Major,,NPE in tracker expiry thread.,I see NullPointerExceptions in Task Expiry thread of the JobTracker. Exception log in JT:,Resolved,Not A Problem,,Unassigned,Vinod Kumar Vavilapalli,Mon; 2 Mar 2009 11:19:19 +0000,Tue; 3 Sep 2013 07:32:23 +0000,Tue; 3 Sep 2013 07:32:23 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-102
HADOOP-5376,Bug,Blocker,,JobInProgress.obtainTaskCleanupTask() throws an ArrayIndexOutOfBoundsException,,Closed,Fixed,,Amareshwari Sriramadasu,Vinod Kumar Vavilapalli,Mon; 2 Mar 2009 11:39:01 +0000,Wed; 8 Jul 2009 16:53:18 +0000,Fri; 6 Mar 2009 11:38:44 +0000,,0.20.0,,,,https://issues.apache.org/jira/browse/HADOOP-5376
HADOOP-5377,Bug,Major,,Inefficient jobtracker history file layout,Storing too many files in a single directory slows things down tremendously and in this case; makes the grid just a bit more difficult to manage.  On our jobtrackers; even with a 45 day purge cycle; we see hundreds of thousands of files in logs/hadoop/history.  The following is an example:pchdm01.ypost.re1: logs/hadoop/history - 1;176;927 files!This is the time(1) output of the `ls | wc -l`real    0m56.042suser    0m28.702ssys     0m1.794sNote that this was the second time I ran this filecount. The first run took more than 4 minutes of real time.===========================================My recommended solution is that the Hadoop team store these files in the following structure:    history/2008/08/19    history/2008/08/20    history/2008/08/21Using this structure gives us 2 important things: consistently good performance and the ability to easily delete or archive old files.  If we expect a Hadoop cluster to process hundreds of thousands of jobs per day; then we may want to break it down byhour like this:    history/2008/08/19/00    history/2008/08/19/01     ...    history/2008/08/19/22    history/2008/08/19/23,Closed,Duplicate,NULL,Unassigned,Nick Rettinghouse,Mon; 2 Mar 2009 17:05:58 +0000,Wed; 8 Jul 2009 16:53:18 +0000,Tue; 3 Mar 2009 04:02:16 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5377
HADOOP-5378,Bug,Major,,Error in displaying job history,"The job history page is broken for some reason. When clicked; the page return the following:HTTP ERROR: 5001RequestURI=/jobhistory.jspPowered by Jetty://When I check the JobTracker log; I saw the following exception:2009-03-02 10:29:00;916 WARN /: /jobhistory.jsp: java.lang.ArrayIndexOutOfBoundsException: 1	at org.apache.hadoop.mapred.jobhistory_jsp$2.compare(jobhistory_jsp.java:109)	at org.apache.hadoop.mapred.jobhistory_jsp$2.compare(jobhistory_jsp.java:93)	at java.util.Arrays.mergeSort(Arrays.java:1270)	at java.util.Arrays.mergeSort(Arrays.java:1281)	at java.util.Arrays.mergeSort(Arrays.java:1281)	at java.util.Arrays.mergeSort(Arrays.java:1281)	at java.util.Arrays.mergeSort(Arrays.java:1281)	at java.util.Arrays.mergeSort(Arrays.java:1281)	at java.util.Arrays.sort(Arrays.java:1210)	at org.apache.hadoop.mapred.jobhistory_jsp._jspService(jobhistory_jsp.java:93)	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)	at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)	at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)	at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)	at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)	at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)	at org.mortbay.http.HttpServer.service(HttpServer.java:954)	at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)	at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)	at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)	at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)	at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)	at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)",Closed,Invalid,,Unassigned,Runping Qi,Mon; 2 Mar 2009 18:37:17 +0000,Wed; 8 Jul 2009 16:53:18 +0000,Tue; 3 Mar 2009 08:32:40 +0000,,0.19.0,,,,https://issues.apache.org/jira/browse/HADOOP-5378
HADOOP-5379,Improvement,Minor,io,Throw exception instead of writing to System.err when there is a CRC error on CBZip2InputStream,From org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.java:,Closed,Fixed,,Rodrigo Schmidt,Rodrigo Schmidt,Mon; 2 Mar 2009 23:38:27 +0000,Thu; 23 Apr 2009 19:18:05 +0000,Thu; 12 Mar 2009 05:54:38 +0000,,0.19.0;0.19.1;0.19.2;0.20.0;0.21.0,,,HADOOP-3646,https://issues.apache.org/jira/browse/HADOOP-5379
MAPREDUCE-2450,Bug,Major,,Calls from running tasks to TaskTracker methods sometimes fail and incur a 60s timeout,"I'm seeing some map tasks in my jobs take 1 minute to commit after they finish the map computation. On the map side; the output looks like this:code2009-03-02 21:30:54;384 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=MAP; sessionId= - already initialized2009-03-02 21:30:54;437 INFO org.apache.hadoop.mapred.MapTask: numReduceTasks: 8002009-03-02 21:30:54;437 INFO org.apache.hadoop.mapred.MapTask: io.sort.mb = 3002009-03-02 21:30:55;493 INFO org.apache.hadoop.mapred.MapTask: data buffer = 239075328/2988441602009-03-02 21:30:55;494 INFO org.apache.hadoop.mapred.MapTask: record buffer = 786432/9830402009-03-02 21:31:00;381 INFO org.apache.hadoop.mapred.MapTask: Starting flush of map output2009-03-02 21:31:07;892 INFO org.apache.hadoop.mapred.MapTask: Finished spill 02009-03-02 21:31:07;951 INFO org.apache.hadoop.mapred.TaskRunner: Task:attempt_200903022127_0001_m_003163_0 is done. And is in the process of commiting2009-03-02 21:32:07;949 INFO org.apache.hadoop.mapred.TaskRunner: Communication exception: java.io.IOException: Call to /127.0.0.1:50311 failed on local exception: java.nio.channels.ClosedChannelException	at org.apache.hadoop.ipc.Client.wrapException(Client.java:765)	at org.apache.hadoop.ipc.Client.call(Client.java:733)	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)	at org.apache.hadoop.mapred.$Proxy0.ping(Unknown Source)	at org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:525)	at java.lang.Thread.run(Thread.java:619)Caused by: java.nio.channels.ClosedChannelException	at java.nio.channels.spi.AbstractSelectableChannel.register(AbstractSelectableChannel.java:167)	at java.nio.channels.SelectableChannel.register(SelectableChannel.java:254)	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:331)	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)	at java.io.FilterInputStream.read(FilterInputStream.java:116)	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:276)	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)	at java.io.DataInputStream.readInt(DataInputStream.java:370)	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)2009-03-02 21:32:07;953 INFO org.apache.hadoop.mapred.TaskRunner: Task 'attempt_200903022127_0001_m_003163_0' done./codeIn the TaskTracker log; it looks like this:code2009-03-02 21:31:08;110 WARN org.apache.hadoop.ipc.Server: IPC Server Responder; call ping(attempt_200903022127_0001_m_003163_0) from 127.0.0.1:56884: output error2009-03-02 21:31:08;111 INFO org.apache.hadoop.ipc.Server: IPC Server handler 10 on 50311 caught: java.nio.channels.ClosedChannelException    at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:126)    at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)    at org.apache.hadoop.ipc.Server.channelWrite(Server.java:1195)    at org.apache.hadoop.ipc.Server.access$1900(Server.java:77)    at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:613)    at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:677)    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:981)/codeNote that the task actually seemed to commit - it didn't get speculatively executed or anything. However; the job wasn't able to continue until this one task was done. Both parties seem to think the channel was closed. How does the channel get closed externally? If closing it from outside is unavoidable; maybe the right thing to do is to set a much lower timeout; because 1 minute delay can be pretty significant for a small job.",Closed,Fixed,,Rajesh Balamohan,Matei Zaharia,Tue; 3 Mar 2009 05:56:48 +0000,Tue; 10 Mar 2015 04:32:32 +0000,Tue; 17 Jan 2012 06:13:30 +0000,,0.23.0;2.0.0-alpha,,,,https://issues.apache.org/jira/browse/MAPREDUCE-2450
MAPREDUCE-290,Improvement,Major,,Extend HADOOP-3293 to MapReduce package also,HADOOP-3293 made changes to FileInputFormat to identify split locations that contribute most to the split. This functionality has to be added to the MapReduce.FileInputFormat too.,Open,Unresolved,,Jothi Padmanabhan,Jothi Padmanabhan,Tue; 3 Mar 2009 11:05:49 +0000,Mon; 21 Jul 2014 21:25:30 +0000,,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-290
HADOOP-5382,Bug,Blocker,,The new map/reduce api doesn't support combiners,Currently; combiners are only called if they are defined using the old deprecated api.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Tue; 3 Mar 2009 18:41:00 +0000,Mon; 26 Dec 2011 06:05:50 +0000,Thu; 19 Mar 2009 00:01:23 +0000,,0.20.0,,,MAPREDUCE-3598,https://issues.apache.org/jira/browse/HADOOP-5382
HADOOP-5383,Bug,Minor,,minor : avoid building error string in verifyReplication(),On NameNode; verifyReplication() is called for every new file created to check if it falls within the configured limits. Currently its implementation always builds the error string though it is almost never needed. This jira fixes it.,Resolved,Fixed,,Raghu Angadi,Raghu Angadi,Tue; 3 Mar 2009 23:34:49 +0000,Wed; 8 Jul 2009 16:43:33 +0000,Wed; 4 Mar 2009 19:25:41 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-5383
HADOOP-5384,Bug,Blocker,test,DataNodeCluster should not create blocks with generationStamp == 1,In DataNodeCluster.main(..); injected blocks are created with generationStamp == 1; which is a reserved value but not a valid generation stamp.  As a consequence; NameNode may die when those blocks are reported.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Wed; 4 Mar 2009 00:49:36 +0000,Thu; 23 Apr 2009 19:18:05 +0000,Wed; 4 Mar 2009 23:56:41 +0000,,,,,HDFS-30;HDFS-279,https://issues.apache.org/jira/browse/HADOOP-5384
CHUKWA-17,New Feature,Minor,,Enhance process information collection ,"We are currently getting the process information from ""top"". And a given process looks like;28288 gmon      18   0 2225m 277m 8876 S    0  7.0   1:38.56 javaHowever; that's still not complete. For it to be truly useful; we would want something like;gmon     28288 28244  0 Nov13 ?        00:01:38 /grid/0/java/jdk/bin/java -Xms1000M -Xmx2000M -DAPP=collector-Dlog4j.configuration=chukwa-log4j.properties -DCHUKWA_HOME=/gri........-DCHUKWA_CONF_DIR=/gri......./../conf -DCHUKWA_LOG_DIR=/grid.........bin/../var/log -classpath:/grid..........We can get those information by using command below;        ps axo pid;user;vsize;size;pcpu;pmem;time;start_time;start;cmd",Resolved,Fixed,,Cheng,Cheng,Wed; 4 Mar 2009 01:22:53 +0000,Fri; 3 Apr 2009 23:31:05 +0000,Thu; 2 Apr 2009 02:23:46 +0000,,,,,,https://issues.apache.org/jira/browse/CHUKWA-17
HADOOP-5386,Bug,Major,,To Probe free ports dynamically for Unit test to replace fixed ports,Currently hdfsproxy unit test uses Cactus in-container test. It uses three fixed ports. one for tomcat start-up/shut-down; another for tomcat http-port and the third for tomcat https-port. If theses ports are already in use; ant build will fail. To fix this; we decided to use a java program to probe the free ports dynamically and update the tomcat conf with these free ports.,Closed,Fixed,,zhiyong zhang,zhiyong zhang,Wed; 4 Mar 2009 02:40:35 +0000,Tue; 24 Aug 2010 20:36:13 +0000,Wed; 11 Mar 2009 23:02:39 +0000,,0.21.0,,,,https://issues.apache.org/jira/browse/HADOOP-5386
HADOOP-5387,New Feature,Minor,,Collect disk read/write operation from iostat,In the demux stage; we should collect read/write operation per second from iostat and populate this information to system metrics.,Resolved,Fixed,,Eric Yang,Eric Yang,Wed; 4 Mar ,,,,,,,,
HADOOP-10000,Bug,Major,,HttpServer log link is inaccessible in secure cluster,"Currently in a secured HDFS cluster; 401 error is returned when clicking the ""NameNode Logs"" link.Looks like the cause of the issue is that the httpServer does not correctly set the security handler and the user realm currently; which causes the httpRequest.getRemoteUser (for the log URL) to return null and later be overwritten to the default web name (e.g.; ""dr.who"") by the filter. In the meanwhile; in a secured cluster the log URL requires the http user to be an administrator. That's why we see the 401 error.",Patch Available,Unresolved,,Jing Zhao,Jing Zhao,Tue; 17 Sep 2013 18:59:05 +0000,Thu; 12 May 2016 18:21:32 +0000,,,3.0.0-alpha1,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10000
HADOOP-10001,Bug,Major,,Document topology with NodeGroup layer configuration in branch-1,NodeGroup layer in NetworkTopology is supported after 1.2.0 release for hadoop better running on virtualization or other additional layer cases. A document settings in hdfs-site.xml and mapred-site.xml to enable this layer for BlockPlacementPolicy and map task scheduling is necessary.A draft doc for user guide is already attached in HADOOP-8468 before. However; We should deliver one in form of Apache Forrest here.,Open,Unresolved,,Junping Du,Junping Du,Thu; 26 Sep 2013 23:03:32 +0000,Thu; 26 Sep 2013 23:08:18 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10001
HADOOP-10002,Bug,Minor,security;util,Tool's config option wouldn't work on secure clusters,"The Tool framework provides a way for clients to run without classpath *-site.xml configs; by letting users pass ""-conf file"" to parse into the app's Configuration object.In a secure cluster config setup; such a runner will not work cause of UserGroupInformation.isSecurityEnabled() check; which is used in Server.java to determine what form of communication to use; will load statically a new Configuration() object to inspect if security is turned on during its initialization; which ignores the application config object and tries to load from classpath and ends up loading non-secure defaults.",Resolved,Duplicate,HADOOP-9021,Unassigned,Harsh J,Fri; 27 Sep 2013 13:58:56 +0000,Mon; 28 Sep 2015 21:06:49 +0000,Sun; 29 Sep 2013 02:17:02 +0000,,2.0.0-alpha,,,,https://issues.apache.org/jira/browse/HADOOP-10002
HADOOP-10003,Bug,Major,fs,HarFileSystem.listLocatedStatus() fails,It looks like HarFileSystem.listLocatedStatus() doesn't work properly because it is inheriting FilterFileSystem's implementation.  This is causing archive unit tests to fail in Hive when using hadoop 2.1.1.If HarFileSystem overrides listLocatedStatus() to use FileSystem's implementation; the Hive unit tests pass.,Closed,Fixed,,Unassigned,Jason Dere,Fri; 27 Sep 2013 17:38:11 +0000,Tue; 30 Jun 2015 07:22:33 +0000,Tue; 1 Oct 2013 23:16:17 +0000,,2.1.1-beta,,,HADOOP-10091,https://issues.apache.org/jira/browse/HADOOP-10003
HADOOP-10004,Bug,Blocker,,[Documentation] hadoop.ssl.enabled knob will no longer be used for MR AM and JobHistoryServer,it is related to MAPREDUCE-5536,Resolved,Invalid,,Omkar Vinit Joshi,Omkar Vinit Joshi,Fri; 27 Sep 2013 20:37:35 +0000,Mon; 7 Oct 2013 18:26:52 +0000,Mon; 7 Oct 2013 18:26:52 +0000,,,,,MAPREDUCE-5536;YARN-1277,https://issues.apache.org/jira/browse/HADOOP-10004
HADOOP-10005,Improvement,Trivial,,No need to check INFO severity level is enabled or not,As a convention in developers; INFO is the default level and INFO logs should be always available. So no need to check it for most of the cases.,Closed,Fixed,,Jackie Chang,Jackie Chang,Fri; 27 Sep 2013 22:30:07 +0000,Mon; 24 Feb 2014 20:57:06 +0000,Wed; 16 Oct 2013 21:11:16 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10005
HADOOP-10006,Bug,Blocker,fs;util,Compilation failure in trunk for o.a.h.fs.swift.util.JSONUtil,The error is like following:...ERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-openstack: Compilation failure: Compilation failure:ERROR /home/jdu/bdc/hadoop-trunk/hadoop-common/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/JSONUtil.java:97;33 type parameters of TT cannot be determined; no unique maximal instance exists for type variable T with upper bounds T;java.lang.ObjectERROR /home/jdu/bdc/hadoop-trunk/hadoop-common/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/JSONUtil.java:115;33 type parameters of TT cannot be determined; no unique maximal instance exists for type variable T with upper bounds T;java.lang.Object,Closed,Fixed,,Junping Du,Junping Du,Sat; 28 Sep 2013 09:58:46 +0000,Wed; 3 Sep 2014 23:33:17 +0000,Sat; 28 Sep 2013 13:35:24 +0000,,,,,HADOOP-8545,https://issues.apache.org/jira/browse/HADOOP-10006
HADOOP-10007,Bug,Major,fs,distcp / mv is not working on ftp,"i'm just trying to backup some files to our ftp-server.hadoop distcp hdfs:///data/ ftp://user:pass@server/data/returns after some minutes with:Task TASKID=""task_201308231529_97700_m_000002"" TASK_TYPE=""MAP"" TASK_STATUS=""FAILED"" FINISH_TIME=""1380217916479"" ERROR=""java\.io\.IOException: Cannot rename parent(source): ftp://x:x@backup2/data/; parent(destination):  ftp://x:x@backup2/data/	at org\.apache\.hadoop\.fs\.ftp\.FTPFileSystem\.rename(FTPFileSystem\.java:557)	at org\.apache\.hadoop\.fs\.ftp\.FTPFileSystem\.rename(FTPFileSystem\.java:522)	at org\.apache\.hadoop\.mapred\.FileOutputCommitter\.moveTaskOutputs(FileOutputCommitter\.java:154)	at org\.apache\.hadoop\.mapred\.FileOutputCommitter\.moveTaskOutputs(FileOutputCommitter\.java:172)	at org\.apache\.hadoop\.mapred\.FileOutputCommitter\.commitTask(FileOutputCommitter\.java:132)	at org\.apache\.hadoop\.mapred\.OutputCommitter\.commitTask(OutputCommitter\.java:221)	at org\.apache\.hadoop\.mapred\.Task\.commit(Task\.java:1000)	at org\.apache\.hadoop\.mapred\.Task\.done(Task\.java:870)	at org\.apache\.hadoop\.mapred\.MapTask\.run(MapTask\.java:329)	at org\.apache\.hadoop\.mapred\.Child$4\.run"" TASK_ATTEMPT_ID="""" .I googled a bit and addedfs.ftp.host = backup2fs.ftp.user.backup2 = userfs.ftp.password.backup2 = passwordto core-site.xml; then I was able to execute:hadoop fs -ls ftp:///data/hadoop fs -rm ftp:///data/test.filebut as soon as I tryhadoop fs -mv file:///data/test.file ftp:///data/test2.filemv: `ftp:///data/test.file': Input/output errorI enabled debug-logging in our ftp-server and got:Sep 27 15:24:33 backup2 ftpd38241: command: LIST /dataSep 27 15:24:33 backup2 ftpd38241: --- 150Sep 27 15:24:33 backup2 ftpd38241: Opening BINARY mode data connection for '/bin/ls'.Sep 27 15:24:33 backup2 ftpd38241: --- 226Sep 27 15:24:33 backup2 ftpd38241: Transfer complete.Sep 27 15:24:33 backup2 ftpd38241: command: CWD ftp:/dataSep 27 15:24:33 backup2 ftpd38241: --- 550Sep 27 15:24:33 backup2 ftpd38241: ftp:/data: No such file or directory.Sep 27 15:24:33 backup2 ftpd38241: command: RNFR test.fileSep 27 15:24:33 backup2 ftpd38241: --- 550looks like the generation of ""CWD"" is buggy; hadoop tries to cd into ""ftp:/data""; but should use ""/data""",Resolved,Cannot Reproduce,,Unassigned,Fabian Zimmermann,Mon; 30 Sep 2013 08:19:34 +0000,Tue; 24 Nov 2015 12:02:04 +0000,Tue; 24 Nov 2015 12:02:04 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10007
HADOOP-10008,Improvement,Minor,build,Hadoop maven protoc plugin to log protoc output at INFO not DEBUG,the protoc plugin only lists the output of protoc at debug level; so if something doesn't compile you need to rebuild at -X. Which works by hand; but not it something like Jenkins is at work. All you get to see then is ERROR protoc compiler error.If the output was logged at INFO; debugging protoc failures would be much easier,Open,Unresolved,,Unassigned,Steve Loughran,Mon; 30 Sep 2013 09:25:03 +0000,Thu; 12 May 2016 18:22:22 +0000,,,2.1.1-beta;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10008
HADOOP-10009,Bug,Major,,Backport HADOOP-7808 to branch-1,In branch-1; SecurityUtil::setTokenService() might throw a NullPointerException; which is fixed in HADOOP-7808.The patch should be backported into branch-1,Resolved,Fixed,,Haohui Mai,Haohui Mai,Mon; 30 Sep 2013 20:31:51 +0000,Fri; 4 Oct 2013 18:55:18 +0000,Fri; 4 Oct 2013 18:55:18 +0000,,1.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-10009
HADOOP-10010,New Feature,Major,,Add expectedFalsePositiveProbability to BloomFilter,It would be nice to see the expected false positive probability of a bloom filter instance to check its quality. This is a simple function but needs access to BloomFilter#bits.,Patch Available,Unresolved,,Unassigned,Xiangrui Meng,Mon; 30 Sep 2013 23:59:32 +0000,Wed; 6 May 2015 03:34:38 +0000,,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10010
HADOOP-10011,Bug,Minor,,NPE if the system can't determine its own name and you go DNS.getDefaultHost(null),"In a test case that I am newly writing; on my infamous ""home machine with broken DNS""; I cant call getByName(null) without seeing a stack trace:Testcase: testNullInterface took 0.014 sec	Caused an ERRORnulljava.lang.NullPointerException	at java.net.NetworkInterface.getByName(NetworkInterface.java:226)	at org.apache.hadoop.net.DNS.getIPs(DNS.java:94)	at org.apache.hadoop.net.DNS.getHosts(DNS.java:141)	at org.apache.hadoop.net.DNS.getDefaultHost(DNS.java:218)	at org.apache.hadoop.net.DNS.getDefaultHost(DNS.java:235)	at org.apache.hadoop.net.TestDNS.testNullInterface(TestDNS.java:62)",Closed,Implemented,,Steve Loughran,Steve Loughran,Fri; 20 Jun 2008 19:04:04 +0000,Thu; 12 May 2016 18:21:47 +0000,Tue; 11 Feb 2014 00:54:30 +0000,,2.1.1-beta;3.0.0-alpha1,,,HADOOP-3426,https://issues.apache.org/jira/browse/HADOOP-10011
HADOOP-10012,Bug,Blocker,ha,Secure Oozie jobs fail with delegation token renewal exception in Namenode HA setup,,Closed,Fixed,,Suresh Srinivas,Arpit Gupta,Tue; 1 Oct 2013 20:02:43 +0000,Tue; 30 Jun 2015 07:11:21 +0000,Wed; 2 Oct 2013 04:22:18 +0000,,2.1.1-beta,,,,https://issues.apache.org/jira/browse/HADOOP-10012
HADOOP-10013,Bug,Major,fs,FileSystem checkPath accepts invalid paths with an authority but no scheme,FileSystem#checkPath will consider paths of the form //junk/path as being valid for the given fs.  The problem is checkPath shorts out if the path contains no scheme - assuming it must be a relative or absolute path for the given fs - whereas the condition should be no scheme and no authority.This causes DistributedFileSystem#getPathName to convert //junk/path into /path; which silently hides the use of invalid paths.,Open,Unresolved,,Unassigned,Daryn Sharp,Thu; 3 Oct 2013 15:40:01 +0000,Thu; 12 May 2016 18:25:07 +0000,,,2.0.0-alpha;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10013
HADOOP-10014,Sub-task,Critical,fs,Symlink resolution is fundamentally broken for multi-layered filesystems,Symlink resolution is performed on a per-filesystem basis.  In a multi-layered filesystem; the symlinks need to be resolved relative to the highest level filesystem in the stack.  Otherwise; fs implementations like viewfs and chroot fs behave incorrectly.  Absolute symlinks may violate the base of the chroot.  Links that should have crossed viewfs mount points are again incorrectly resolved relative to the base filesystem.Symlink resolution has occur above the level of any individual fs to allow a multi-layered fs stack to work correctly; such as via a symlink-aware FilteredFileSystem that wraps any arbitrary fs to ensure links are resolved from the top-down of the stack.,Open,Unresolved,,Unassigned,Daryn Sharp,Thu; 3 Oct 2013 16:50:48 +0000,Sun; 5 Jun 2016 05:51:04 +0000,,,2.0.0-alpha;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10014
HADOOP-10015,Bug,Minor,security,UserGroupInformation prints out excessive ERROR warnings,In UserGroupInformation::doAs(); it prints out a log at ERROR level whenever it catches an exception.However; it prints benign warnings in the following paradigm: For example; FileSystem#exists() follows this paradigm. Distcp uses this paradigm too. The exception is expected therefore there should be no ERROR logs printed in the namenode logs.Currently; the user quickly finds out that the namenode log is quickly filled by benign ERROR logs when he or she runs distcp in secure set up. This behavior confuses the operators.This jira proposes to move the log to DEBUG level.,Closed,Fixed,HDFS-3594;HADOOP-10274,Nicolas Liochon,Haohui Mai,Thu; 3 Oct 2013 18:27:39 +0000,Mon; 17 Jul 2017 18:38:05 +0000,Mon; 24 Mar 2014 18:39:39 +0000,,3.0.0-alpha1,,,HADOOP-10274,https://issues.apache.org/jira/browse/HADOOP-10015
HADOOP-10016,Sub-task,Major,tools/distcp,Distcp should support copy from a secure Hadoop 1 cluster to an insecure Hadoop 2 cluster,Distcp should be able to copy from a secure cluster to an insecure cluster. This functionality is important for operators to migrate data to a new Hadoop installation.,Open,Unresolved,,Haohui Mai,Haohui Mai,Fri; 4 Oct 2013 00:13:34 +0000,Mon; 27 Jul 2015 14:02:06 +0000,,,,,,HDFS-4587,https://issues.apache.org/jira/browse/HADOOP-10016
HADOOP-10017,Sub-task,Major,,Fix NPE in DFSClient#getDelegationToken when doing Distcp from a secured cluster to an insecured cluster,Currently if we run Distcp from a secured cluster and copy data to an insecured cluster; DFSClient#getDelegationToken will throw NPE when processing the NULL token returned by the NN in the insecured cluster. We should be able to handle the NULL token here.,Closed,Fixed,,Haohui Mai,Jing Zhao,Fri; 4 Oct 2013 00:53:56 +0000,Thu; 12 May 2016 18:26:10 +0000,Sun; 6 Oct 2013 06:32:30 +0000,,2.1.1-beta;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10017
HADOOP-10018,Bug,Minor,,TestUserGroupInformation throws NPE when HADOOP_HOME is not set,TestUserGroupInformation throws NPE in System.setProperty() when HADOOP_HOME is not set.,Patch Available,Unresolved,,Haohui Mai,Haohui Mai,Fri; 4 Oct 2013 01:29:34 +0000,Fri; 8 May 2015 16:15:19 +0000,,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10018
HADOOP-10019,Improvement,Major,fs,make symlinks production-ready,This is an umbrella JIRA for all the things we have to do to make symlinks production-ready for Hadoop 2.3.Note that some of these subtasks are scheduled for 2.1.2 / 2.2; but the overall effort is for 2.3.,Open,Unresolved,,Ajith S,Colin P. McCabe,Fri; 4 Oct 2013 01:56:05 +0000,Thu; 10 Nov 2016 20:10:06 +0000,,,2.3.0,,,HDFS-4559;HDFS-4935;HDFS-5294;HDFS-5303;HDFS-5293,https://issues.apache.org/jira/browse/HADOOP-10019
HADOOP-10020,Sub-task,Blocker,fs,disable symlinks temporarily,disable symlinks temporarily until we can make them production-ready in Hadoop 2.3,Closed,Fixed,,Sanjay Radia,Colin P. McCabe,Fri; 4 Oct 2013 02:01:07 +0000,Tue; 30 Jun 2015 07:11:21 +0000,Sun; 6 Oct 2013 23:23:08 +0000,,2.2.0,,,HADOOP-10109,https://issues.apache.org/jira/browse/HADOOP-10020
HADOOP-10021,Sub-task,Major,fs,add distCp support for symlinks,Add support for symlinks to distCp.  We probably want something like rsync; where you can choose to copy symlinks as links; or copy what they refer to.,Open,Unresolved,,Unassigned,Colin P. McCabe,Fri; 4 Oct 2013 02:16:26 +0000,Tue; 28 Jan 2014 23:37:07 +0000,,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10021
HADOOP-10022,Bug,Blocker,,Add support for per project https support,Current configuration hadoop.https.enable turns on https only support for all the daemons in hadoop. This is an umbrella jira to add per project https configuration. For more details; see the detailed proposal - https://issues.apache.org/jira/browse/HADOOP-8581?focusedCommentId=13784332page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13784332The current scope of work is described in - https://issues.apache.org/jira/browse/HADOOP-8581?focusedCommentId=13786567page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13786567,Resolved,Fixed,,Unassigned,Suresh Srinivas,Fri; 4 Oct 2013 23:02:20 +0000,Wed; 16 Oct 2013 20:28:36 +0000,Sun; 6 Oct 2013 19:06:55 +0000,,,,,HDFS-5306;YARN-1277;HDFS-5305,https://issues.apache.org/jira/browse/HADOOP-10022
HDFS-5305,Bug,Major,,Add https support in HDFS,This is the HDFS part of HADOOP-10022. This will serve as the umbrella jira for all the https related cleanup in HDFS.,Closed,Fixed,,Suresh Srinivas,Suresh Srinivas,Sat; 5 Oct 2013 00:04:05 +0000,Mon; 24 Feb 2014 20:56:48 +0000,Tue; 17 Dec 2013 22:50:22 +0000,,2.0.2-alpha,,,HADOOP-10022,https://issues.apache.org/jira/browse/HDFS-5305
HDFS-5308,Improvement,Major,,Replace HttpConfig#getSchemePrefix with implicit schemes in HDFS JSP ,"The JSPs in HDFS calls HttpConfig.getSchemePrefix() to determine which scheme (http / https) to be put into the link. This can be done by putting ""///"" in the JSP instead; thus picking a scheme can no longer rely on HttpConfig.getSchemePrefix() any more.",Closed,Fixed,,Haohui Mai,Haohui Mai,Fri; 4 Oct 2013 20:48:00 +0000,Tue; 30 Jun 2015 07:25:22 +0000,Sat; 5 Oct 2013 20:51:49 +0000,,,,,HADOOP-10025;HADOOP-8581,https://issues.apache.org/jira/browse/HDFS-5308
HADOOP-10025,Improvement,Major,,Replace HttpConfig#getSchemePrefix with implicit scheme in YARN/MR,,Resolved,Won't Fix,,Omkar Vinit Joshi,Haohui Mai,Sat; 5 Oct 2013 02:28:00 +0000,Tue; 19 Nov 2013 21:54:42 +0000,Tue; 19 Nov 2013 21:54:42 +0000,,,,,HDFS-5308,https://issues.apache.org/jira/browse/HADOOP-10025
HADOOP-10026,Task,Major,,Upgrade to Jackson 2,I'd like to see Hadoop upgrade to Jackson 2. This is a change that should be able to be made to each Hadoop project one at a time; so I've attached a patch for the first and will follow up with others if this one is committed.,Patch Available,Unresolved,,Unassigned,Ben McCann,Sat; 5 Oct 2013 21:20:37 +0000,Wed; 6 May 2015 03:34:36 +0000,,,,BB2015-05-TBR,HADOOP-10032,,https://issues.apache.org/jira/browse/HADOOP-10026
HADOOP-10027,Bug,Minor,native,*Compressor_deflateBytesDirect passes instance instead of jclass to GetStaticObjectField,"http://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c?view=markupThis pattern appears in all the native compressors.    // Get members of ZlibCompressor    jobject clazz = (*env)-GetStaticObjectField(env; this;                                                 ZlibCompressor_clazz);The 2nd argument to GetStaticObjectField is supposed to be a jclass; not a jobject. Adding the JVM param -Xcheck:jni will cause ""FATAL ERROR in native method: JNI received a class argument that is not a class"" and a core dump such as the following.(gdb) #0 0x00007f02e4aef8a5 in raise () from /lib64/libc.so.6#1 0x00007f02e4af1085 in abort () from /lib64/libc.so.6#2 0x00007f02e45bd727 in os::abort(bool) () from /opt/jdk1.6.0_31/jre/lib/amd64/server/libjvm.so#3 0x00007f02e43cec63 in jniCheck::validate_class(JavaThread*; _jclass*; bool) () from /opt/jdk1.6.0_31/jre/lib/amd64/server/libjvm.so#4 0x00007f02e43ea669 in checked_jni_GetStaticObjectField () from /opt/jdk1.6.0_31/jre/lib/amd64/server/libjvm.so#5 0x00007f02d38eaf79 in Java_org_apache_hadoop_io_compress_zlib_ZlibCompressor_deflateBytesDirect () from /usr/lib/hadoop/lib/native/libhadoop.so.1.0.0In addition; that clazz object is only used for synchronization. In the case of the native method _deflateBytesDirect; the result is a class wide lock used to access the instance field uncompressed_direct_buf. Perhaps using the instance as the sync point is more appropriate?",Resolved,Fixed,,Hui Zheng,Eric Abbott,Mon; 7 Oct 2013 15:26:56 +0000,Tue; 30 Aug 2016 01:32:53 +0000,Thu; 12 Mar 2015 06:47:42 +0000,,,,,HADOOP-3604,https://issues.apache.org/jira/browse/HADOOP-10027
HADOOP-10028,Bug,Minor,,Malformed ssl-server.xml.example ,The ssl-server.xml.example file has malformed XML leading to DN start error if the example file is reused.,Closed,Fixed,,Haohui Mai,Jing Zhao,Mon; 7 Oct 2013 21:01:47 +0000,Mon; 24 Feb 2014 20:56:47 +0000,Mon; 7 Oct 2013 22:47:16 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10028
HADOOP-10029,Bug,Major,fs,Specifying har file to MR job fails in secure cluster,This is an issue found by Ramya Sunil. See the exception stack trace in the following comment.,Closed,Fixed,,Suresh Srinivas,Suresh Srinivas,Mon; 7 Oct 2013 23:43:11 +0000,Mon; 24 Feb 2014 20:56:47 +0000,Fri; 11 Oct 2013 00:03:17 +0000,,2.0.0-alpha,,,,https://issues.apache.org/jira/browse/HADOOP-10029
HADOOP-10030,Bug,Major,,FsShell -put/copyFromLocal should support Windows local path,In current trunk code; running 'hadoop fs -put c:\windows\path /hdfs/path' will lead to an error put: unexpected URISyntaxException. This is a regression to the 1.0 fs shell commands. FsShell put and copyFromLocal should support Windows local path formats as localSrc argument to the two commands.,Closed,Fixed,,Chuan Liu,Chuan Liu,Tue; 8 Oct 2013 04:27:00 +0000,Thu; 12 May 2016 18:26:36 +0000,Tue; 8 Oct 2013 23:49:34 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10030
HADOOP-10031,Bug,Major,fs,FsShell -get/copyToLocal/moveFromLocal should support Windows local path,In current trunk code; running 'hadoop fs -get /hdfs/path c:\windows\path' will lead to an error put: unexpected URISyntaxException. This is a regression to the 1.0 fs shell commands. FsShell get; copyToLocal; and moveFromLocal should support Windows local path formats as localSrc argument to the two commands.,Closed,Fixed,,Chuan Liu,Chuan Liu,Tue; 8 Oct 2013 04:56:05 +0000,Thu; 12 May 2016 18:26:37 +0000,Thu; 10 Oct 2013 20:36:55 +0000,,2.2.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10031
HADOOP-10032,Task,Major,fs,Backport hadoop-openstack to branch 1,Backport the hadoop-openstack module from trunk to branch-1.This will need a build.xml file to build it; ivy set up to add any extra dependencies and testing. There's one extra FileSystem method in 2.x that we can drop for branch-1.FWIW I've already built and tested hadoop-openstack against branch 1 by editing the .pom file and having that module build against 1.  Before the move from isDir() to isDirectory() it compiled and ran fine,Resolved,Won't Fix,,Unassigned,Steve Loughran,Tue; 8 Oct 2013 09:44:04 +0000,Sun; 8 Feb 2015 17:03:13 +0000,Sun; 8 Feb 2015 17:03:13 +0000,,1.3.0,,HADOOP-10026,,https://issues.apache.org/jira/browse/HADOOP-10032
HADOOP-10033,Task,Major,fs,backport openstack support to branch-2,Backport the hadoop-openstack module to branch-2.This should require little more than testing,Resolved,Implemented,,Unassigned,Steve Loughran,Tue; 8 Oct 2013 09:45:46 +0000,Tue; 28 Jan 2014 23:37:05 +0000,Tue; 8 Oct 2013 18:57:48 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10033
HADOOP-10034,Sub-task,Major,fs,optimize same-filesystem symlinks by doing resolution server-side,We should optimize same-filesystem symlinks by doing resolution server-side rather than client side; as discussed on HADOOP-9780.,Resolved,Duplicate,NULL,Unassigned,Colin P. McCabe,Tue; 8 Oct 2013 22:31:42 +0000,Fri; 18 Oct 2013 06:25:17 +0000,Fri; 18 Oct 2013 06:25:17 +0000,,,,,HADOOP-9780,https://issues.apache.org/jira/browse/HADOOP-10034
HADOOP-10035,Improvement,Major,,Cleanup TestFilterFileSystem,Currently TestFilterFileSystem only checks for FileSystem methods that must be implemented in FilterFileSystem with a list of methods that are exception to this rule. This jira wants to make this check stricter by adding a test for ensuring the methods in exception rule list must not be implemented by the FilterFileSystem.This also cleans up the current class that has methods from exception rule list to interface to avoid having to provide dummy implementation of the methods.,Resolved,Fixed,,Suresh Srinivas,Suresh Srinivas,Wed; 9 Oct 2013 02:04:17 +0000,Tue; 30 Aug 2016 01:32:52 +0000,Mon; 23 Nov 2015 01:33:52 +0000,,2.1.1-beta,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10035
HADOOP-10036,Improvement,Major,ipc,RetryInvocationHandler should recognise that there is no point retrying to auth failures,The RetryInvocationHandler tries to retry connections; so as to handle transient failures. However; auth failures aren't treated as special; so it spins even though the operation will not succeed with the current configuration,Open,Unresolved,HADOOP-12697,Unassigned,Steve Loughran,Wed; 9 Oct 2013 14:21:58 +0000,Fri; 5 Feb 2016 15:35:37 +0000,,,2.3.0;2.6.0,,,YARN-4558,https://issues.apache.org/jira/browse/HADOOP-10036
HADOOP-10037,Bug,Major,fs/s3,s3n read truncated; but doesn't throw exception ,For months now we've been finding that we've been experiencing frequent data truncation issues when reading from S3 using the s3n:// protocol.  I finally was able to gather some debugging output on the issue in a job I ran last night; and so can finally file a bug report.The job I ran last night was on a 16-node cluster (all of them AWS EC2 cc2.8xlarge machines; running Ubuntu 13.04 and Cloudera CDH4.3.0).  The job was a Hadoop streaming job; which reads through a large number (i.e.; ~55;000) of files on S3; each of them approximately 300K bytes in size.All of the files contain 46 columns of data in each record.  But I added in an extra check in my mapper code to count and verify the number of columns in every record - throwing an error and crashing the map task if the column count is wrong.If you look in the attached task logs; you'll see 2 attempts on the same task.  The first one fails due to data truncated (i.e.; my job intentionally fails the map task due to the current record failing the column count check).  The task then gets retried on a different machine and runs to a succesful completion.You can see further evidence of the truncation further down in the task logs; where it displays the count of the records read:  the failed task says 32953 records read; while the successful task says 63133.Any idea what the problem might be here and/or how to work around it?  This issue is a very common occurrence on our clusters.  E.g.; in the job I ran last night before I had gone to bed I had already encountered 8 such failuers; and the job was only 10% complete.  (~25;000 out of ~250;000 tasks.)I realize that it's common for I/O errors to occur - possibly even frequently - in a large Hadoop job.  But I would think that if an I/O failure (like a truncated read) did occur; that something in the underlying infrastructure code (i.e.; either in NativeS3FileSystem or in jets3t) should detect the error and throw an IOException accordingly.  It shouldn't be up to the calling code to detect such failures; IMO.,Closed,Fixed,,Unassigned,David Rosenstrauch,Wed; 9 Oct 2013 16:06:36 +0000,Tue; 30 Jun 2015 07:22:32 +0000,Thu; 19 Mar 2015 23:03:22 +0000,,2.0.0-alpha,,,,https://issues.apache.org/jira/browse/HADOOP-10037
MAPREDUCE-5579,Improvement,Major,,Improve JobTracker web UI,Users will often need to use the JobTracker web UI to debug or tune their jobs in addition to checking the status of their jobs. The current web UI is cumbersome to navigate. The goal is to make the JobTracker web UI easier to navigate and present the data in a cleaner and more intuitive format.,Open,Unresolved,,Unassigned,David Chen,Wed; 9 Oct 2013 23:49:21 +0000,Sat; 12 Oct 2013 01:50:15 +0000,,,1.2.2,,,,https://issues.apache.org/jira/browse/MAPREDUCE-5579
HADOOP-10039,Bug,Major,security,Add Hive to the list of projects using AbstractDelegationTokenSecretManager,org.apache.hadoop.hive.thrift.DelegationTokenSecretManager extends AbstractDelegationTokenSecretManager. This should be captured in the InterfaceAudience annotation of AbstractDelegationTokenSecretManager.,Closed,Fixed,,Haohui Mai,Suresh Srinivas,Thu; 10 Oct 2013 23:41:08 +0000,Mon; 24 Feb 2014 20:57:36 +0000,Fri; 11 Oct 2013 01:29:42 +0000,,2.0.0-alpha,,,,https://issues.apache.org/jira/browse/HADOOP-10039
HADOOP-10040,Bug,Major,,hadoop.cmd in UNIX format and would not run by default on Windows,The hadoop.cmd currently checked in into hadoop-common is in UNIX format; same as most of other src files. However; the hadoop.cmd is meant to be used on Windows only; the fact that it is in UNIX format makes it unrunnable as is on Window platform.An exception shall be made on hadoop.cmd (and other cmd files for what matters) to make sure they are in DOS format; for them to be runnable as is when checked out from source repository.,Closed,Fixed,,Chris Nauroth,Yingda Chen,Fri; 11 Oct 2013 00:25:53 +0000,Wed; 3 Sep 2014 23:33:18 +0000,Mon; 14 Oct 2013 17:52:41 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10040
HADOOP-10041,Bug,Minor,security,UserGroupInformation#spawnAutoRenewalThreadForUserCreds tries to renew even if the kerberos ticket cache is non-renewable,UserGroupInformation#spawnAutoRenewalThreadForUserCreds tries to renew user credentials.  However; it does this even if the kerberos ticket cache in question is non-renewable.This leads to an annoying error message being printed out all the time.,Open,Unresolved,,Unassigned,Colin P. McCabe,Thu; 19 Jul 2012 22:54:41 +0000,Fri; 10 Apr 2015 19:24:11 +0000,,,2.0.1-alpha,,,,https://issues.apache.org/jira/browse/HADOOP-10041
HADOOP-10042,Bug,Major,conf,Heap space error during copy from maptask to reduce task,http://stackoverflow.com/questions/19298357/out-of-memory-error-in-mapreduce-shuffle-phaseI've described the problem on stackoverflow as well. It contains a link to another JIRA: http://hadoop-common.472056.n3.nabble.com/Shuffle-In-Memory-OutOfMemoryError-td433197.htmlMy errors are completely the same: out of memory error when mapred.job.shuffle.input.buffer.percent = 0.7; the program does work when I put it to 0.2; does this mean the original JIRA was not resolved?Does anybody have an idea whether this is a mapreduce issue or is it a misconfiguration from my part?,Resolved,Invalid,,Unassigned,Dieter De Witte,Fri; 11 Oct 2013 11:59:03 +0000,Sat; 12 Oct 2013 07:35:03 +0000,Fri; 11 Oct 2013 15:40:33 +0000,,1.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-10042
HADOOP-10043,Improvement,Major,,Convert org.apache.hadoop.security.token.SecretManager to be an AbstractService,I'm dealing with YARN-1172; a subtask of YARN-1139(ResourceManager HA related task). The sentence as follows is a quoted from YARN-1172's my comment:I've found that it requires org.apache.hadoop.security.token.SecretManager to be an AbstractService;because both AbstractService and org.apache.hadoop.security.token.SecretManager are abstract class and we cannot extend both of them at the same time.,Resolved,Not A Problem,,Tsuyoshi Ozawa,Tsuyoshi Ozawa,Fri; 11 Oct 2013 13:40:09 +0000,Thu; 26 Dec 2013 05:17:11 +0000,Thu; 26 Dec 2013 05:17:11 +0000,,,,YARN-1172,,https://issues.apache.org/jira/browse/HADOOP-10043
HADOOP-10044,Improvement,Minor,,Improve the javadoc of rpc code,,Closed,Fixed,,Sanjay Radia,Sanjay Radia,Fri; 11 Oct 2013 23:39:59 +0000,Thu; 12 May 2016 18:22:00 +0000,Thu; 12 Dec 2013 19:00:06 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10044
HADOOP-10045,Bug,Minor,metrics,QueueMetrics 'maps_killed' and 'reduces_killed' are never incremented in branch-1,In branch-1; maps/reduces_failed metric will be incremented when map/reduce task is failed or killed; otherwise maps/reduces_killed metric won't be incremented then.I found 'maps_killed' and 'reduces_killed' metrics are never incremented not only in QueueMetrics but also in JobTrackerMetrics.,Resolved,Won't Fix,,Akira Ajisaka,Akira Ajisaka,Mon; 14 Oct 2013 18:22:08 +0000,Thu; 28 Jan 2016 05:52:06 +0000,Thu; 28 Jan 2016 05:52:06 +0000,,1.2.1,metrics,,MAPREDUCE-5227,https://issues.apache.org/jira/browse/HADOOP-10045
HADOOP-10046,Improvement,Trivial,,Print a log message when SSL is enabled,It would be nice to have a log message that indicates that SSL is enabled in org.apache.hadoop.http.HttpServer.,Closed,Fixed,,David S. Wang,David S. Wang,Mon; 14 Oct 2013 18:48:41 +0000,Thu; 12 May 2016 18:22:43 +0000,Tue; 15 Oct 2013 18:42:10 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10046
HADOOP-10047,New Feature,Major,io,Add a directbuffer Decompressor API to hadoop,With the Zero-Copy reads in HDFS (HDFS-5260); it becomes important to perform all I/O operations without copying data into byte[] buffers or other buffers which wrap over them.This is a proposal for adding a DirectDecompressor interface to the io.compress; to indicate codecs which want to surface the direct buffer layer upwards.The implementation should work with direct heap/mmap buffers and cannot assume .array() availability.,Closed,Fixed,,Gopal V,Gopal V,Tue; 15 Oct 2013 14:37:55 +0000,Mon; 24 Feb 2014 20:56:43 +0000,Tue; 19 Nov 2013 15:40:04 +0000,,2.3.0,compression,HIVE-6347,HADOOP-10116;HADOOP-8148,https://issues.apache.org/jira/browse/HADOOP-10047
HADOOP-10048,Improvement,Major,,LocalDirAllocator should avoid holding locks while accessing the filesystem,As noted in MAPREDUCE-5584 and HADOOP-7016; LocalDirAllocator can be a bottleneck for multithreaded setups like the ShuffleHandler.  We should consider moving to a lockless design or minimizing the critical sections to a very small amount of time that does not involve I/O operations.,Resolved,Fixed,,Jason Lowe,Jason Lowe,Tue; 15 Oct 2013 16:08:03 +0000,Tue; 30 Aug 2016 01:32:49 +0000,Tue; 7 Jun 2016 16:27:00 +0000,,2.3.0,,,MAPREDUCE-5584;HADOOP-7016,https://issues.apache.org/jira/browse/HADOOP-10048
HADOOP-10049,Bug,Major,bin,2.1.0 beta won't run under cygwin?,"I recently tried out 2.1.0 beta in the hope of benefiting from numerous improvements made re: running on cygwin. I cannot even get a ""bin/hadoop version"" to work; it gets a java.lang.NoClassDefFoundError for the common classes such as org.apache.hadoop.util.VersionInfoI've looked a similiar bug report and have made sure that HADOOP_PREFIX is set; and that HADOOP_HOME is not set. I've looked at the output of ""bin/hadoop classpath"" and it looks correct EXCEPT that 2.1.0 shows the classpath using unix syntax and paths; while my older version of hadoop (1.2.1) shows it using Windows syntax and paths.",Resolved,Won't Fix,,Unassigned,Michael Klybor,Tue; 15 Oct 2013 18:05:38 +0000,Sat; 10 Jan 2015 17:28:26 +0000,Wed; 17 Dec 2014 19:19:20 +0000,,2.1.1-beta,,,HADOOP-11464,https://issues.apache.org/jira/browse/HADOOP-10049
HADOOP-10050,Bug,Minor,documentation,Update single node and cluster install instructions to work with latest bits,A few things i noticed1. changes to yarn.nodemanager.aux-services2. Set the framework to yarn in mapred-site.xml3. Start the history serverAlso noticed no change to the capacity scheduler configs was needed.,Open,Unresolved,YARN-1319,Arpit Gupta,Arpit Gupta,Wed; 16 Oct 2013 00:24:56 +0000,Sat; 7 Jan 2017 01:56:49 +0000,,,2.2.0,,,HADOOP-10139,https://issues.apache.org/jira/browse/HADOOP-10050
HADOOP-10051,Bug,Major,bin,winutil.exe is not included in hadoop bin tarball,"I don't have Windows environment; but one user who tried 2.2.0 releaseon Windows reported that released tar ball doesn't contain""winutil.exe"" and cannot run any commands. I confirmed that winutil.exe is not included in 2.2.0 bin tarball surely.",Open,Unresolved,,Unassigned,Tsuyoshi Ozawa,Wed; 16 Oct 2013 07:23:30 +0000,Mon; 10 Oct 2016 09:21:42 +0000,,,2.2.0;2.4.0;2.5.0,,,HADOOP-11003;HADOOP-10397,https://issues.apache.org/jira/browse/HADOOP-10051
HADOOP-10052,Sub-task,Major,fs,Temporarily disable client-side symlink resolution,As a follow-on to the JIRA that disabled creation of symlinks on the server-side; we should also disable client-side resolution so old clients talking to a new server behave properly.,Closed,Fixed,,Andrew Wang,Andrew Wang,Wed; 16 Oct 2013 17:26:45 +0000,Mon; 24 Feb 2014 20:58:28 +0000,Mon; 21 Oct 2013 18:08:54 +0000,,2.2.0,,,HADOOP-10162,https://issues.apache.org/jira/browse/HADOOP-10052
HADOOP-10053,Improvement,Trivial,fs,fix typo and add clarification to ViewFs javadoc,When viewFs is used to hide the details of federated namespace; HA nn; etc.; the best HDFS url format becomes fuzzy. It's not directly used by user. But internally it's being referred not only at run time as FileSystem Cache key; but also used in other cluster setup in hdfs-site.xml.This ticket is to improve the javadoc to make cluster admin to be aware of the fact that the hdfs urls have to unique within company. Otherwise; the read/write operations among clusters will be erroneous. In details; in ViewFs javadoc; replace nnXXXXX with nnXXXX.example.com.Hope it helps.,Open,Unresolved,,Unassigned,Paul Han,Thu; 17 Oct 2013 00:43:53 +0000,Thu; 17 Oct 2013 00:43:53 +0000,,,2.0.5-alpha,,,,https://issues.apache.org/jira/browse/HADOOP-10053
HADOOP-10054,Improvement,Minor,fs,ViewFsFileStatus.toString() is broken,"ViewFsFileStatus.toString is broken. Following code snippet : produces the output: Note that ""path=null"" is not correct.",Reopened,Unresolved,,Hanisha Koneru,Paul Han,Thu; 17 Oct 2013 00:53:52 +0000,Wed; 3 Jan 2018 02:00:23 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10054
HADOOP-10055,Bug,Trivial,documentation,"FileSystemShell.apt.vm doc has typo ""numRepicas"" ","HDFS-5139 added ""numRepicas"" to FileSystemShell.apt.vm; should be ""numReplicas"".",Closed,Fixed,,Akira Ajisaka,Eli Collins,Thu; 17 Oct 2013 04:36:32 +0000,Wed; 3 Sep 2014 23:33:19 +0000,Thu; 17 Oct 2013 18:18:14 +0000,,2.2.0,newbie,,HDFS-5139,https://issues.apache.org/jira/browse/HADOOP-10055
MAPREDUCE-5590,Bug,Major,,MiniMRClientClusterFactory in Hadoop 1.x forces local mode,Noticed the following in the 1.x branches: The 2.x branches allow you to set what hadoop configuration to use (and which DFS cluster to connect to). For instance; in the Sqoop project we are using the MiniDFSCluster in conjunction with the MiniMRCluster.,Open,Unresolved,,Unassigned,Abraham Elmahrek,Thu; 17 Oct 2013 20:15:15 +0000,Thu; 17 Oct 2013 20:43:40 +0000,,,1.0.4,,,,https://issues.apache.org/jira/browse/MAPREDUCE-5590
HADOOP-10057,Improvement,Major,,Add ability in Hadoop servers (Namenode; JobTracker; Datanode ) to support multiple QOP  (Authentication ; Privacy) simultaneously,Add ability in Hadoop servers (Namenode; JobTracker Datanode ) to support multiple QOP  (Authentication ; Privacy) simlutaneouslyHadoop Servers currently support only one QOP(quality of protection)for the whole cluster.We want Hadoop servers to support multiple QOP  at the same time. The logic used to determine the QOP should be pluggable.This will enable hadoop servers to communicate with different types of clients with different QOP.A sample usecase:Let each Hadoop server support two QOP .1. Authentication2. Privacy (Privacy includes Authentication) .The Hadoop servers and internal clients require to do Authentication only without incurring cost of encryption. External clients use Privacy. An ip-whitelist logic to determine the QOP is provided and used as the default QOP resolution logic.,Resolved,Won't Do,,Benoy Antony,Benoy Antony,Thu; 17 Oct 2013 21:47:12 +0000,Mon; 16 Oct 2017 19:45:57 +0000,Mon; 16 Oct 2017 19:45:57 +0000,,1.2.1,,,HADOOP-10211;HADOOP-9709,https://issues.apache.org/jira/browse/HADOOP-10057
HADOOP-10058,Bug,Minor,metrics,TestMetricsSystemImpl#testInitFirstVerifyStopInvokedImmediately fails on trunk,"Here is the output when I executed ""mvn test -Dtest=TestMetricsSystemImpl"". I found the test doesn't always fail. The test sometimes success.",Closed,Fixed,HADOOP-9990;HADOOP-10061,Chen He,Akira Ajisaka,Thu; 17 Oct 2013 22:13:40 +0000,Thu; 12 May 2016 18:26:04 +0000,Thu; 5 Dec 2013 17:49:01 +0000,,2.2.0;3.0.0-alpha1,java7,,HADOOP-10062,https://issues.apache.org/jira/browse/HADOOP-10058
HADOOP-10059,Bug,Minor,metrics,RPC authentication and authorization metrics overflow to negative values on busy clusters,The RPC metrics for authorization and authentication successes can easily overflow to negative values on a busy cluster that has been up for a long time.  We should consider providing 64-bit values for these counters.,Closed,Fixed,,Tsuyoshi Ozawa,Jason Lowe,Fri; 18 Oct 2013 13:58:39 +0000,Mon; 1 Dec 2014 03:08:24 +0000,Mon; 18 Aug 2014 16:36:09 +0000,,0.23.9;2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10059
HADOOP-10060,Task,Major,bin,Unable to run JDK_1.7.1_40 commands in Cygwin2.829 (32 bit,"Actions Performed 1. As advised by Hadoop exported JAVA_HOME=/cygwin/jdk1.2.7_40/bin in hadoop-env.sh and tried to run java commands with error  	jar  /cygwin/hadoop-1.2.1/hadoop-examples-1.2.1.jar              -bash: jar: command not found 2. However when echoed  I get right pointed path 	echo $JAVA_HOME               /cygwin/jdk1.7.0_40/bin 3. Tried including export path in .bash_profile; and .profile; with same error.Unable to move forward with out JDK calls. Anticipate the same with Hadoop related commands.",Resolved,Cannot Reproduce,,Unassigned,Anand Murali,Sun; 20 Oct 2013 09:16:35 +0000,Fri; 14 Nov 2014 11:31:27 +0000,Fri; 14 Nov 2014 11:31:27 +0000,,1.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-10060
HADOOP-10061,Bug,Minor,metrics,TestMetricsSystemImpl#testInitFirstVerifyStopInvokedImmediately failed,"TestMetricsSystemImpl#testInitFirstVerifyStopInvokedImmediately failed with ""Wanted at most 2 times but was 3""",Resolved,Duplicate,HADOOP-10058,Unassigned,Colin P. McCabe,Mon; 21 Oct 2013 17:09:57 +0000,Tue; 28 Jan 2014 23:37:03 +0000,Mon; 21 Oct 2013 18:19:30 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10061
HADOOP-10062,Bug,Major,metrics,race condition in MetricsSystemImpl#publishMetricsNow that causes incorrect results,"TestMetricsSystemInpl#testMultiThreadedPublish failed with ""Metrics not collected""",Closed,Fixed,HADOOP-9990;HADOOP-10165,Sangjin Lee,Shinichi Yamashita,Mon; 21 Oct 2013 18:39:02 +0000,Thu; 12 May 2016 18:27:57 +0000,Thu; 5 Feb 2015 03:01:35 +0000,,3.0.0-alpha1,,,HADOOP-10058,https://issues.apache.org/jira/browse/HADOOP-10062
HADOOP-10063,Bug,Major,util,RunJar ClassLoader cached the class can't release,At the end of method main()  The ClassLoader cached the class can't release.Because the development time class code will often changes; so will be very inconvenient.Rewrite as follows:,Open,Unresolved,,Unassigned,Joe Au,Tue; 22 Oct 2013 09:52:12 +0000,Mon; 28 Oct 2013 08:06:37 +0000,,,1.0.3;2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10063
HADOOP-10064,Improvement,Major,build,Upgrade to maven antrun plugin version 1.7,v1.6 does not respect 'mvn -q'. I have been building with 1.7 on my dev machine and haven't encountered any problems so far.,Closed,Fixed,,Arpit Agarwal,Arpit Agarwal,Tue; 22 Oct 2013 22:39:39 +0000,Thu; 12 May 2016 18:21:41 +0000,Thu; 24 Oct 2013 02:41:09 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10064
HADOOP-10065,Bug,Minor,documentation,Fix namenode format documentation,Current namenode format dochttp://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CommandsManual.html#namenodeDoes not list the various options format can be called with and their use.,Resolved,Duplicate,HDFS-6781,Unassigned,Arpit Gupta,Wed; 23 Oct 2013 14:56:27 +0000,Thu; 21 Aug 2014 05:28:43 +0000,Thu; 21 Aug 2014 05:28:41 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10065
HADOOP-10066,Bug,Major,build,Cleanup ant dependencies of maven build,The Maven build seems to pull in multiple versions of some ant plugins. Filing a Jira to address this. (As pointed out by Jonathan Eagles on HADOOP-10064),Open,Unresolved,,Unassigned,Arpit Agarwal,Thu; 24 Oct 2013 02:16:37 +0000,Thu; 12 May 2016 18:22:37 +0000,,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10066
HADOOP-10067,Improvement,Minor,,Missing POM dependency on jsr305,Compiling for Fedora revels a missing declaration for javax.annotation.Nullable.  This is the result of a missing explicit dependency on jsr305.,Closed,Fixed,,Robert Rati,Robert Rati,Thu; 24 Oct 2013 18:07:53 +0000,Mon; 24 Feb 2014 20:58:13 +0000,Thu; 14 Nov 2013 09:51:17 +0000,,,,,HADOOP-9991;HADOOP-10101,https://issues.apache.org/jira/browse/HADOOP-10067
HADOOP-10068,Improvement,Trivial,,Improve log4j regex in testFindContainingJar,Improved the regular expression in TestClassUtil:testFindContainingJar to work in in both Fedora and non-Fedora environments,Resolved,Fixed,,Robert Rati,Robert Rati,Thu; 24 Oct 2013 18:52:02 +0000,Tue; 30 Aug 2016 01:32:48 +0000,Mon; 23 Nov 2015 02:31:18 +0000,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10068
HADOOP-10069,Bug,Minor,scripts,Not need to start secondary namenode upon NN HA?,the wiki mentioned that with namenode HA mode; it is not necessary and an error to start a secondary namenode. While currently sbin/start-dfs.sh still launch the secondary namenode even nothing related is configured in hdfs-site.xml. Should this be fixed? or people just don't use it for launch NN HA?,Resolved,Duplicate,NULL,Unassigned,Raymond Liu,Fri; 25 Oct 2013 01:24:41 +0000,Fri; 25 Oct 2013 01:27:18 +0000,Fri; 25 Oct 2013 01:27:18 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10069
HADOOP-10070,Bug,Major,security,RPC client doesn't use per-connection conf to determine server's expected Kerberos principal name,"Currently; RPC client caches the Configuration object that was passed in to its constructor and uses that same conf for every connection it sets up thereafter. This can cause problems when security is enabled if the Configuration object provided when the first RPC connection was made does not contain all possible entries for all server principals that will later be used by subsequent connections. When this happens; it will result in later RPC connections incorrectly failing with the error ""Failed to specify server's Kerberos principal name"" even though the principal name was specified in the Configuration object provided on later RPC connection attempts.I believe this means that we've inadvertently reintroduced HADOOP-6907.",Closed,Fixed,,Aaron T. Myers,Aaron T. Myers,Fri; 25 Oct 2013 01:46:47 +0000,Thu; 4 Sep 2014 01:16:46 +0000,Tue; 4 Mar 2014 18:54:16 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10070
HDFS-5428,Bug,Major,snapshots,under construction files deletion after snapshot+checkpoint+nn restart leads nn safemode,1. allow snapshots under dir /foo2. create a file /foo/test/bar and start writing to it3. create a snapshot s1 under /foo after block is allocated and some data has been written to it4. Delete the directory /foo/test5. wait till checkpoint or do saveNameSpace6. restart NN.NN enters to safemode.Analysis:Snapshot nodes loaded from fsimage are always complete and all blocks will be in COMPLETE state. So when the Datanode reports RBW blocks those will not be updated in blocksmap.Some of the FINALIZED blocks will be marked as corrupt due to length mismatch.,Closed,Fixed,,Jing Zhao,Vinayakumar B,Fri; 25 Oct 2013 11:59:56 +0000,Mon; 24 Feb 2014 20:58:30 +0000,Tue; 19 Nov 2013 07:40:18 +0000,,2.2.0,,,HDFS-5443;HDFS-5283,https://issues.apache.org/jira/browse/HDFS-5428
HADOOP-10072,Bug,Trivial,nfs;test,TestNfsExports#testMultiMatchers fails due to non-deterministic timing around cache expiry check.,TestNfsExports#testMultiMatchers has been failing sporadically in my environment.  The final step of this test is to check that a cache entry has expired.  It looks like the timing is too tight around this check.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Fri; 25 Oct 2013 20:33:54 +0000,Thu; 12 May 2016 18:24:31 +0000,Sat; 26 Oct 2013 04:40:35 +0000,,2.2.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10072
HADOOP-10073,Bug,Major,,Branding issue on download site,"The download page headers 123 should identify the software as ""Apache Hadoop"" and ""Apache Chukwa"" rather than just ""Hadoop"" or ""Chukwa""1  http://www.apache.org/dist/hadoop/chukwa/2 http://www.apache.org/dist/hadoop/common/3 http://www.apache.org/dist/hadoop/core/",Open,Unresolved,,Unassigned,Sebb,Sat; 26 Oct 2013 09:58:15 +0000,Sun; 3 Aug 2014 22:27:28 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10073
HADOOP-10074,Bug,Minor,,Typos in the Cluster Setup webpage (hadoop.apaghe.org/docs/current),In the http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html; There is a operating the hadoop cluster section existed.In the Hadoop Startup content;$HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start datanodeShould be modified as:$HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs start datanode(hadoop-daemon.sh is for master; hadoop-daemons. Is for slave when every slave address is defined at slave file)Also; Run a script to start NodeManagers on all slaves has same typo.So; it should be modified as$HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR start nodemanagerHadoop shutdown section also needs to be modified for above two scripts.,Open,Unresolved,,Unassigned,Yoonmin Nam,Sun; 27 Oct 2013 08:05:59 +0000,Sun; 27 Oct 2013 08:05:59 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10074
HADOOP-10075,Improvement,Critical,,Update jetty dependency to version 9,Jetty6 is no longer maintained.  Update the dependency to jetty9.,Resolved,Fixed,HADOOP-9650;HADOOP-9244;HDFS-6267,Robert Kanter,Robert Rati,Mon; 28 Oct 2013 14:20:30 +0000,Fri; 3 Mar 2017 18:46:43 +0000,Sat; 29 Oct 2016 00:36:58 +0000,,2.2.0;2.6.0,,,HADOOP-11995;HADOOP-9244,https://issues.apache.org/jira/browse/HADOOP-10075
HADOOP-10076,Improvement,Major,,Update tomcat/jasper dependency to version 7,Tomcat 6 is no longer maintained.  Update to use tomcat/jasper 7.,Open,Unresolved,,Unassigned,Robert Rati,Mon; 28 Oct 2013 14:30:46 +0000,Mon; 15 Feb 2016 12:20:34 +0000,,,2.2.0,,,HADOOP-9244;HADOOP-12809,https://issues.apache.org/jira/browse/HADOOP-10076
HADOOP-10077,Bug,Major,,o.a.h.s.Groups should refresh in the background,org.apache.hadoop.security.Groups maintains a cache of mappings between user names and sets of associated group names.  Periodically; the entries in this cache expire and must be refetched from the operating system.Currently; this is done in the context of whatever thread happens to try to access the group mapping information right after the time period expires.  However; this is problematic; since that thread may be holding the FSNamesystem lock.  This means that if the GroupMappingServiceProvider is slow; the whole NameNode may grind to a halt until it finishes.  This can generate periodic load spikes or even NameNode failovers.Instead; we should allow the refreshing of the group mappings to be done asynchronously in a background thread pool.,Resolved,Duplicate,HADOOP-11238,Colin P. McCabe,Colin P. McCabe,Wed; 30 Oct 2013 23:37:31 +0000,Thu; 4 Dec 2014 21:35:20 +0000,Thu; 4 Dec 2014 21:35:20 +0000,,2.3.0,,,HADOOP-11238;HADOOP-10079,https://issues.apache.org/jira/browse/HADOOP-10077
HADOOP-10078,Bug,Minor,security,KerberosAuthenticator always does SPNEGO,"HADOOP-8883 made this change to KerberosAuthenticator to fix OOZIE-1010.  However; as Andrey Klochkov pointed out recently; this inadvertently made the if statement always false because it turns out that the JDK excludes some headers; including the ""Authorization"" one that we're checking (see discussion here).  This means that it was always either calling doSpnegoSequence(token); or getFallBackAuthenticator().authenticate(url; token);; which is actually the old behavior that existed before HADOOP-8855 changed it in the first place.In any case; I tried removing the ""Authorization"" check and Oozie still works with and without Kerberos; the NPE reported in OOZIE-1010 has since been properly fixed due as a side effect for a similar issue in OOZIE-1368.",Closed,Fixed,,Robert Kanter,Robert Kanter,Thu; 31 Oct 2013 21:10:05 +0000,Mon; 27 Jul 2015 20:19:21 +0000,Wed; 13 Nov 2013 21:15:54 +0000,,2.0.3-alpha,,,HADOOP-11467;OOZIE-800;OOZIE-1010;HADOOP-8883;OOZIE-1368;HADOOP-8855,https://issues.apache.org/jira/browse/HADOOP-10078
HADOOP-10079,Improvement,Major,,log a warning message if group resolution takes too long.,We should log a warning message if group resolution takes too long.,Closed,Fixed,,Colin P. McCabe,Colin P. McCabe,Thu; 31 Oct 2013 23:51:54 +0000,Mon; 24 Feb 2014 20:56:57 +0000,Sat; 2 Nov 2013 00:52:09 +0000,,2.2.0,,,HADOOP-10077,https://issues.apache.org/jira/browse/HADOOP-10079
HADOOP-10080,Bug,Minor,conf,Variables in configuration properties are substituted in wrong order of precedence,"API doc of Configuration states:Value strings are first processed for variable expansion. The available properties are:1.  Other properties defined in this Configuration; and; if a name is undefined here;2.  Properties in System.getProperties().The current implementation of variable expansion; however; gives precedence to system properties. If a system property is defined it's value is used no matter whether a configuration property of same name exists.This may cause inconsistent behaviour; e.g.; if in the API doc example ""basedir"" is set as system property (-Dbasedir=/mnt/tmp) the results of two intuitively equivalent calls differ:",Open,Unresolved,,Unassigned,Sebastian Nagel,Fri; 1 Nov 2013 15:44:59 +0000,Thu; 7 Nov 2013 16:48:43 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10080
HADOOP-10081,Bug,Critical,ipc,Client.setupIOStreams can leak socket resources on exception or error,The setupIOStreams method in org.apache.hadoop.ipc.Client can leak socket resources if an exception is thrown before the inStream and outStream local variables are assigned to this.in and this.out; respectively.,Closed,Fixed,,Tsuyoshi Ozawa,Jason Lowe,Mon; 4 Nov 2013 22:41:39 +0000,Wed; 3 Sep 2014 23:43:10 +0000,Thu; 5 Dec 2013 16:15:07 +0000,,0.23.9;2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10081
HADOOP-10082,Test,Minor,,TestHDFSCLI fails on Windows due to difference in line endings,This is similar issue with HADOOP-8657. git appended /r by default on Windows at checkout. When TestHDFSCLI runs; the verification will fail on file size for files like data15bytes; data30bytes; etc.,Open,Unresolved,,Unassigned,Chuan Liu,Mon; 4 Nov 2013 23:50:15 +0000,Thu; 12 May 2016 18:22:23 +0000,,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10082
HADOOP-10083,Improvement,Minor,build,Fix commons logging warning in Hadoop build,A clean build spews multiple instances of the following warning on my OS X dev machine. I couldn't find another bug mentioning these so filing one to get these cleaned up.,Resolved,Not A Problem,,Unassigned,Arpit Agarwal,Tue; 5 Nov 2013 04:53:19 +0000,Thu; 12 May 2016 18:22:05 +0000,Fri; 2 May 2014 22:14:09 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10083
HIVE-5747,Improvement,Minor,HCatalog,Hcat alter table add parttition: add skip header/row feature,"Creating hcatalog table using creating tables and alter table add partition is most used approach.However at times the incoming files can come with header row/column names.In such cases it would be good feature to be able skip header/rows.Suggestions below:hcat ""alter table rawevents add partition (ds='20100819') location 'hdfs://data/rawevents/20100819/data' -skip header""hcat ""alter table rawevents add partition (ds='20100819') location 'hdfs://data/rawevents/20100819/data' -skip n""hcat ""alter table rawevents add partition (ds='20100819') location 'hdfs://data/rawevents/20100819/data'"" -DskipRow=1 can choose with bounded array (rows) for selecting rows for tablehcat ""alter table rawevents add partition (ds='20100819') location 'hdfs://data/rawevents/20100819/data' -rows2:""  // from first row till allhcat ""alter table rawevents add partition (ds='20100819') location 'hdfs://data/rawevents/20100819/data' -rows2:100""  // from first row till 100 rowsCorrect place for this feature in hive or hcat?or with -D can be handled in hcat?ThanksRekha",Open,Unresolved,,Unassigned,Rekha Joshi,Tue; 5 Nov 2013 11:37:47 +0000,Tue; 5 Nov 2013 12:33:36 +0000,,,0.10.0,,,,https://issues.apache.org/jira/browse/HIVE-5747
HADOOP-10085,Bug,Blocker,,CompositeService should allow adding services while being inited,We can add services to a CompositeService. However; if we do that while initing the CompositeService; it leads to a ConcurrentModificationException.It would be nice to allow adding services even during the init of CompositeService.,Closed,Fixed,,Steve Loughran,Karthik Kambatla,Wed; 6 Nov 2013 16:36:40 +0000,Mon; 3 Nov 2014 18:33:35 +0000,Sun; 2 Feb 2014 20:44:03 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10085
HADOOP-10086,Improvement,Minor,documentation,User document for authentication in secure cluster,"There are no independent section for basic security features such as authentication and group mapping in the user documentation; though there are sections for ""Service Level Authorization"" and ""HTTP Authentication"".Creating independent section for authentication and moving contents about secure cluster currently residing in ""Cluster Setup"" section could be good starting point.",Closed,Fixed,,Masatake Iwasaki,Masatake Iwasaki,Thu; 7 Nov 2013 01:52:58 +0000,Mon; 24 Feb 2014 20:57:49 +0000,Mon; 27 Jan 2014 18:41:39 +0000,,,documentaion;security,,HDFS-5273;HADOOP-9621,https://issues.apache.org/jira/browse/HADOOP-10086
HADOOP-10087,Bug,Major,security,UserGroupInformation.getGroupNames() fails to return primary group first when JniBasedUnixGroupsMappingWithFallback is used,When JniBasedUnixGroupsMappingWithFallback is used as the group mapping resolution provider; UserGroupInformation.getGroupNames() fails to return the primary group first in the list as documented.,Closed,Fixed,,Colin P. McCabe,Yu Gao,Fri; 8 Nov 2013 19:37:00 +0000,Tue; 11 Mar 2014 03:22:13 +0000,Wed; 11 Dec 2013 18:58:39 +0000,,2.1.0-beta;2.2.0,security,,HADOOP-10401,https://issues.apache.org/jira/browse/HADOOP-10087
HADOOP-10088,Bug,Major,build,copy-nativedistlibs.sh needs to quote snappy lib dir,copy-nativedistlibs.sh needs to quote snappy lib dir. Currently this fails for directories with 'spaces' or 'windows path seperartor',Closed,Fixed,,Raja Aluri,Raja Aluri,Fri; 8 Nov 2013 19:53:28 +0000,Thu; 12 May 2016 18:25:00 +0000,Sat; 9 Nov 2013 07:38:36 +0000,,2.2.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10088
HADOOP-10089,New Feature,Major,,How to build eclipse plugin for hadoop 2.2 or what is the alternative to program hadoop 2.2?,I build the eclipse plugin for hadoop 1.2.1 but how to build it for 2.2 or is there any alternative for it.,Open,Unresolved,,Unassigned,Asif,Sat; 9 Nov 2013 19:01:36 +0000,Mon; 28 Sep 2015 19:00:02 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10089
HADOOP-10090,Bug,Major,metrics,Jobtracker metrics not updated properly after execution of a mapreduce job,After executing a wordcount mapreduce sample job; jobtracker metrics are not updated properly. Often times the response from the jobtracker has higher number of job_completed than job_submitted (for example 8 jobs completed and 7 jobs submitted). Issue reported by Toma Paunovic.,Closed,Fixed,,Ivan Mitic,Ivan Mitic,Mon; 11 Nov 2013 14:48:32 +0000,Mon; 24 Feb 2014 20:58:23 +0000,Thu; 26 Dec 2013 19:55:52 +0000,,1.2.1,,,HADOOP-8050;YARN-1043,https://issues.apache.org/jira/browse/HADOOP-10090
HADOOP-10091,Bug,Blocker,fs,Job with a har archive as input fails on 0.23,Attempting to run a MapReduce job with a har as input fails.  Sample stacktrace to follow.  We need to backport the fix for HADOOP-10003 to branch-0.23.,Resolved,Fixed,,Jason Lowe,Jason Lowe,Mon; 11 Nov 2013 22:30:38 +0000,Thu; 14 Nov 2013 11:33:47 +0000,Wed; 13 Nov 2013 23:23:44 +0000,,0.23.10,,,HADOOP-10003,https://issues.apache.org/jira/browse/HADOOP-10091
HADOOP-10092,Bug,Major,,hadoop-2.2.0-src build fails owing to wrong maven-site-plugin version,"I downloaded hadoop-2.2.0-src.tar.gz ; in an attempt to build the native libraries 64-bit;  but building according to the instructions in BUILDING.txt with $  mvn package -Pdist;native;docs;src -DskipTests -Dtarfails because of this issue:  https://cwiki.apache.org/confluence/display/MAVEN/AetherClassNotFound(ie. the build fails with: ""java.lang.NoClassDefFoundError: org/sonatype/aether/graph/DependencyFilterCaused by: java.lang.ClassNotFoundException: org.sonatype.aether.graph.DependencyFilter""because the wrong version of maven-site-plugin is specified in the top level hadoop pom.xml; @ line 211 :      plugin        artifactIdmaven-site-plugin/artifactId        version3.0/version     ...Changing this to:        version3.3/versionfixes the problem .",Resolved,Duplicate,HADOOP-10273,Unassigned,Jason Vas Dias,Tue; 12 Nov 2013 18:06:09 +0000,Thu; 14 Aug 2014 20:15:04 +0000,Thu; 14 Aug 2014 20:15:04 +0000,,2.2.0,build;maven;newbie;patch,,,https://issues.apache.org/jira/browse/HADOOP-10092
HADOOP-10093,Bug,Major,conf,hadoop-env.cmd sets HADOOP_CLIENT_OPTS with a max heap size that is too small.,HADOOP-9211 increased the default max heap size set by hadoop-env.sh to 512m.  The same change needs to be applied to hadoop-env.cmd for Windows.,Closed,Fixed,,shanyu zhao,shanyu zhao,Tue; 12 Nov 2013 22:24:18 +0000,Wed; 3 Sep 2014 23:33:19 +0000,Wed; 13 Nov 2013 01:18:40 +0000,,2.2.0,,,HADOOP-9211,https://issues.apache.org/jira/browse/HADOOP-10093
HADOOP-10094,Bug,Trivial,util,NPE in GenericOptionsParser#preProcessForWindows(),main() in java guarantees that args is not null; but on some uses of Tool interface from java; people seem to pass around null as args; causing a NPE in GenericOptionsParser. Although; passing null is not recommended; we can do a trivial fix.,Closed,Fixed,,Enis Soztutar,Enis Soztutar,Wed; 13 Nov 2013 00:14:12 +0000,Thu; 12 May 2016 18:27:50 +0000,Thu; 14 Nov 2013 17:52:05 +0000,,1-win;2.2.0;3.0.0-alpha1,,,HADOOP-9660,https://issues.apache.org/jira/browse/HADOOP-10094
HADOOP-10095,Improvement,Minor,io,Performance improvement in CodecPool,CodecPool shows up when profiling HBase with a mixed workload (it says we spend 1% of the time there).It could be a profiler side effect; but on the other hand we can save some 'Map#contains'.,Closed,Fixed,,Nicolas Liochon,Nicolas Liochon,Wed; 13 Nov 2013 11:35:58 +0000,Thu; 12 May 2016 18:27:43 +0000,Wed; 13 Nov 2013 22:52:54 +0000,,2.2.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10095
HADOOP-10096,Bug,Minor,,Missing dependency on commons-collections,There's a missing dependency on commons-collections,Resolved,Duplicate,MAPREDUCE-5431,Unassigned,Robert Rati,Wed; 13 Nov 2013 19:36:41 +0000,Wed; 13 Nov 2013 20:01:05 +0000,Wed; 13 Nov 2013 20:01:05 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10096
HADOOP-10097,Improvement,Minor,,Extend JenkinsHash package interface to allow increased code sharing,I copied some code from org.apache.hadoop.util.hash.JenkinsHash and added it to org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes and modified it slightly because the interface was not quite right for use CuckooSetBytes. I propose modifying org.apache.hadoop.util.hash.JenkinsHash to provide an additional interface function:public int hash(byte[] key; int start; int nbytes; int initval)This would return a hash value for the sequence of bytes beginning at start and ending at start + nbytes (exclusive).The existing interface function in org.apache.hadoop.util.hash.JenkinsHashpublic int hash(byte[] key; int nbytes; int initval)would then be modified to call this new function. The original hash() function does not take a start parameter; and always assumes the key in byte[] key starts at position 0. This will expand the use cases for the JenkinsHash package. At that point; the Hive CuckooSetBytes class can be modified so that it can reference the JenkinsHash package of Hadoop and use it directly; rather than using a copied and modified version of the code locally.Existing users of hash(byte[] key; int nbytes; int initval) will then have to pay an extra function call. If the performance ramifications of this worry anyone; please comment. Alternatives would be to copy the new version of hash() in entirety into JenkinsHash; or simply not do this JIRA.,Open,Unresolved,,Eric Hanson,Eric Hanson,Wed; 13 Nov 2013 19:41:19 +0000,Wed; 13 Nov 2013 20:07:22 +0000,,,,,,HIVE-5583;HIVE-5583,https://issues.apache.org/jira/browse/HADOOP-10097
MAPREDUCE-5624,Improvement,Major,build,move grizzly-test and junit dependencies to test scope,stop the the grizzly dependences  Junit getting into everything downstream by moving them to test scope,Resolved,Fixed,,Ted Yu,Steve Loughran,Wed; 13 Nov 2013 20:34:10 +0000,Fri; 15 Nov 2013 12:39:17 +0000,Fri; 15 Nov 2013 12:39:17 +0000,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-5624
HADOOP-10099,Improvement,Minor,ipc,Reduce chance for RPC denial of service,A RPC server may accept an unlimited number of connections unless indirectly bounded by a blocking operation in the RPC handler threads.  The NN's namespace locking happens to cause this blocking; but other RPC servers such as yarn's generate async events which allow unbridled connection acceptance.,Open,Unresolved,,Unassigned,Daryn Sharp,Wed; 13 Nov 2013 21:19:28 +0000,Thu; 12 May 2016 18:27:21 +0000,,,2.0.0-alpha;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10099
HADOOP-10100,Bug,Major,,MiniKDC shouldn't use apacheds-all artifact,The MiniKDC currently depends on the apacheds-all artifact: However; this artifact includes; inside of itself; a lot of other packages; including antlr; ehcache; apache commons; and mina (you can see a full list of the packages in the jar here).  This can be problematic if other projects (e.g. Oozie) try to use MiniKDC and have a different version of one of those dependencies (in my case; ehcache).  Because the packages are included inside the apacheds-all jar; we can't override their version.  Instead; we should remove apacheds-all and use dependencies that only include org.apache.directory.* packages; the other necessary dependencies should be included normally.,Closed,Fixed,,Robert Kanter,Robert Kanter,Thu; 14 Nov 2013 00:06:56 +0000,Mon; 24 Feb 2014 20:58:28 +0000,Thu; 14 Nov 2013 23:53:38 +0000,,2.3.0,,,HADOOP-9991,https://issues.apache.org/jira/browse/HADOOP-10100
HADOOP-10101,Improvement,Major,,Update guava dependency to the latest version,The existing guava version is 11.0.2 which is quite old. This issue tries to update the version to as latest version as possible.,Resolved,Fixed,HADOOP-11319;HDFS-7040;HDFS-9793,Tsuyoshi Ozawa,Rakesh R,Thu; 14 Nov 2013 14:43:54 +0000,Tue; 24 Oct 2017 11:00:02 +0000,Sat; 25 Mar 2017 01:17:48 +0000,,3.0.0-alpha2,BB2015-05-TBR,HADOOP-14187,HADOOP-11319;HADOOP-14957;BOOKKEEPER-708;HADOOP-11602;HADOOP-11616;HADOOP-14238;HADOOP-12064;HBASE-9667;HADOOP-11032;HADOOP-10067;HADOOP-11612;HADOOP-14380;HADOOP-14891;HADOOP-11286;HADOOP-11470;JENA-842;YARN-3029;HADOOP-14382,https://issues.apache.org/jira/browse/HADOOP-10101
HADOOP-10102,Sub-task,Minor,build,update commons IO from 2.1 to 2.4,commons IO is at v2.4; release notes: http://commons.apache.org/proper/commons-io/upgradeto2_4.htmlOne change is marked as source  semantically incompatible with 2.2: IO-318,Closed,Fixed,,Akira Ajisaka,Steve Loughran,Fri; 15 Nov 2013 12:33:49 +0000,Mon; 24 Feb 2014 20:57:25 +0000,Wed; 4 Dec 2013 10:54:20 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10102
HADOOP-10103,Sub-task,Minor,build,update commons-lang to 2.6,update commons-lang from 2.5 to 2.6,Closed,Fixed,,Akira Ajisaka,Steve Loughran,Fri; 15 Nov 2013 12:35:49 +0000,Mon; 24 Feb 2014 20:57:43 +0000,Thu; 21 Nov 2013 13:40:16 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10103
HADOOP-10104,Sub-task,Minor,build,Update jackson to 1.9.13,Jackson is now at 1.9.13; apparently; hadoop 2.2 at 1.8.8.jackson isn't used that much in the code so risk from an update should be low,Closed,Fixed,,Akira Ajisaka,Steve Loughran,Fri; 15 Nov 2013 12:38:44 +0000,Sat; 27 Sep 2014 20:20:30 +0000,Wed; 9 Apr 2014 11:11:41 +0000,,2.2.0;2.3.0;2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10104
HADOOP-10105,Improvement,Blocker,build,remove httpclient dependency,httpclient is now end-of-life and is no longer being developed.  Now that we have a dependency on httpcore; we should phase out our use of the old discontinued httpclient library in Hadoop.  This will allow us to reduce CLASSPATH bloat and get updated code.,Resolved,Fixed,HADOOP-11053,Akira Ajisaka,Colin P. McCabe,Fri; 15 Nov 2013 18:51:45 +0000,Thu; 27 Apr 2017 08:08:26 +0000,Thu; 27 Apr 2017 08:00:49 +0000,,2.8.0,,HDFS-10623,HADOOP-9991;MAPREDUCE-6264;YARN-3217;HADOOP-12767;HADOOP-14359,https://issues.apache.org/jira/browse/HADOOP-10105
HADOOP-10106,Bug,Minor,,Incorrect thread name in RPC log messages,INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: readAndProcess from client 10.115.201.46 threw exception org.apache.hadoop.ipc.RpcServerException: Unknown out of band call #-2147483647This is thrown by a reader thread; so the message should be likeINFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8020: readAndProcess from client 10.115.201.46 threw exception org.apache.hadoop.ipc.RpcServerException: Unknown out of band call #-2147483647Another example is Responder.processResponse; which can also be called by handler thread. When that happend; the thread name should be the handler thread; not the responder thread.,Closed,Fixed,,Ming Ma,Ming Ma,Sat; 16 Nov 2013 00:41:24 +0000,Mon; 24 Feb 2014 20:58:37 +0000,Mon; 16 Dec 2013 22:14:35 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10106
HADOOP-10107,Sub-task,Major,ipc,Server.getNumOpenConnections may throw NPE,"Found this in build #5440Caused by: java.lang.NullPointerException	at org.apache.hadoop.ipc.Server.getNumOpenConnections(Server.java:2434)	at org.apache.hadoop.ipc.metrics.RpcMetrics.numOpenConnections(RpcMetrics.java:74)",Closed,Fixed,HDFS-5528,Kihwal Lee,Tsz Wo Nicholas Sze,Sat; 16 Nov 2013 01:47:19 +0000,Mon; 24 Feb 2014 20:58:04 +0000,Tue; 19 Nov 2013 07:46:24 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10107
HADOOP-10108,Improvement,Major,security,Add support for kerberos delegation to hadoop-auth,Most services that need to perform Hadoop operations on behalf of an end-user make use of the built-in ability to configure trusted services and use Hadoop-specific delegation tokens. However; some web-applications need delegated access to both Hadoop and other kerberos-authenticated services. It'd be useful for these applications to user kerberos delegation when using hadoop-auth's SPNEGO libraries.,Patch Available,Unresolved,,Joey Echeverria,Joey Echeverria,Sun; 17 Nov 2013 21:13:06 +0000,Thu; 12 May 2016 18:21:59 +0000,,,2.2.0;3.0.0-alpha1,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10108
HADOOP-10109,Sub-task,Major,test,Fix test failure in TestOfflineEditsViewer introduced by HADOOP-10052,Fix test failure in TestOfflineEditsViewer introduced by HADOOP-10052,Closed,Fixed,,Colin P. McCabe,Colin P. McCabe,Mon; 18 Nov 2013 19:08:52 +0000,Mon; 24 Feb 2014 20:58:30 +0000,Mon; 18 Nov 2013 19:47:51 +0000,,2.3.0,,,HADOOP-10020,https://issues.apache.org/jira/browse/HADOOP-10109
HADOOP-10110,Bug,Blocker,build,hadoop-auth has a build break due to missing dependency,We have a build break in hadoop-auth if build with maven cache cleaned. The error looks like the follows. The problem exists on both Windows and Linux. If you have old jetty jars in your maven cache; you won't see the error.,Closed,Fixed,HADOOP-10117;HADOOP-10170,Chuan Liu,Chuan Liu,Mon; 18 Nov 2013 20:38:17 +0000,Fri; 9 Jan 2015 16:40:26 +0000,Tue; 21 Jan 2014 05:56:39 +0000,,2.0.6-alpha;2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10110
HADOOP-10111,Improvement,Major,,Allow DU to be initialized with an initial value,"When a DU object is created; the du command runs right away. If the target directory contains a huge number of files and directories; its constructor may not return for many seconds.  It will be nice if it can be told to delay the initial scan and use a specified initial ""used"" value.",Closed,Fixed,,Kihwal Lee,Kihwal Lee,Mon; 18 Nov 2013 22:32:13 +0000,Wed; 3 Sep 2014 23:33:19 +0000,Thu; 21 Nov 2013 16:15:29 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10111
HADOOP-10112,Bug,Major,tools,har file listing  doesn't work with wild card,"test@test001 root$ hdfs dfs -ls har:///tmp/filename.har/*-ls: Can not create a Path from an empty stringUsage: hadoop fs generic options -ls -d -h -R &lt;path ...It works without ""*"".",Closed,Fixed,,Brandon Li,Brandon Li,Tue; 19 Nov 2013 01:02:11 +0000,Mon; 24 Feb 2014 20:57:10 +0000,Thu; 6 Feb 2014 23:11:11 +0000,,0.23.10;2.2.0,,,HADOOP-9981,https://issues.apache.org/jira/browse/HADOOP-10112
HADOOP-10113,Bug,Major,,There are some threads which will be dead silently when uncaught exception/error occurs,"Related to HDFS-5500; I found there are some threads be dead silently when uncaught exception/error occured.For example; following threads are I mentioned.	refreshUsed in DU	reloader in ReloadingX509TrustManager	t in UserGroupInformation#spawnAutoRenewalThreadForUserCreds	errThread in Shell#runCommand	sinkThread in MetricsSinkAdapter	blockScannerThread in DataBlockScanner	emptier in NameNode#startTrashEmptier (when we use TrashPolicyDefault)There are some critical threads if we can't notice the dead (e.g DU). I think we should handle those exception/error; and monitor the liveness or log that.",Open,Unresolved,,Unassigned,Kousuke Saruta,Tue; 19 Nov 2013 01:58:19 +0000,Thu; 12 May 2016 18:23:03 +0000,,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10113
HADOOP-10114,Bug,Major,,jenkins tried to test hadoop-hdfs without building the native code in hadoop-common,There was a jenkins build where libhadoop.so was not created; even though we were testing hadoop-hdfs.  The log of the build can be found at https://builds.apache.org/job/PreCommit-HDFS-Build/5470/consoleFull.We ought to fix test-patch.sh so that this scenario doesn't happen again.,Resolved,Invalid,,Haohui Mai,Haohui Mai,Tue; 19 Nov 2013 04:26:34 +0000,Fri; 28 Feb 2014 21:43:25 +0000,Fri; 28 Feb 2014 21:43:25 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10114
HADOOP-10115,Improvement,Major,build,Exclude duplicate jars in hadoop package under different component's lib,In the hadoop package distribution there are more than 90% of the jars are duplicated in multiple places.For Ex:almost all jars in share/hadoop/hdfs/lib are already there in share/hadoop/common/libSame case for all other lib in share directory.Anyway for all the daemon processes all directories are added to classpath.So to reduce the package distribution size and the classpath overhead; remove the duplicate jars from the distribution.,Resolved,Fixed,HADOOP-12363;HADOOP-11680,Vinayakumar B,Vinayakumar B,Tue; 19 Nov 2013 06:20:50 +0000,Thu; 12 May 2016 18:22:42 +0000,Tue; 10 Mar 2015 04:44:30 +0000,,2.2.0;3.0.0-alpha1,common;hdfs;mapreduce;nfs;yarn,,,https://issues.apache.org/jira/browse/HADOOP-10115
HADOOP-10116,Bug,Major,,"fix ""inconsistent synchronization"" warnings in ZlibCompressor",Fix findbugs warnings in ZlibCompressor.  I believe these were introduced by HADOOP-10047.,Resolved,Duplicate,HADOOP-10681,Unassigned,Colin P. McCabe,Tue; 19 Nov 2013 20:21:09 +0000,Thu; 16 Apr 2015 19:04:03 +0000,Thu; 16 Apr 2015 19:04:03 +0000,,,,,HADOOP-10047,https://issues.apache.org/jira/browse/HADOOP-10116
HADOOP-10117,Bug,Major,,Unable to compile source code from stable 2.2.0 release,I am trying to compile the source code but I am getting the following error.ERROR D:\hadoop-src\hadoop-2.2.0-src\hadoop-common-project\hadoop-auth\src\test\java\org\apache\hadoop\security\authentication\client\AuthenticatorTestCase.java:86;13 cannot access org.mortbay.component.AbstractLifeCycleclass file for org.mortbay.component.AbstractLifeCycle not found    server = new Server(0);ERROR D:\hadoop-src\hadoop-2.2.0-src\hadoop-common-project\hadoop-auth\src\test\java\org\apache\hadoop\security\authentication\client\AuthenticatorTestCase.java:96;29 cannot access org.mortbay.component.LifeCycleclass file for org.mortbay.component.LifeCycle not found    server.getConnectors()0.setHost(host);ERROR D:\hadoop-src\hadoop-2.2.0-src\hadoop-common-project\hadoop-auth\src\test\java\org\apache\hadoop\security\authentication\client\AuthenticatorTestCase.java:98;10 cannot find symbolsymbol  : method start()location: class org.mortbay.jetty.ServerERROR D:\hadoop-src\hadoop-2.2.0-src\hadoop-common-project\hadoop-auth\src\test\java\org\apache\hadoop\security\authentication\client\AuthenticatorTestCase.java:104;12 cannot find symbolsymbol  : method stop()location: class org.mortbay.jetty.ServerLooks like the build is broken.Please fix and let me know as to when I can download the stable version.,Resolved,Duplicate,HADOOP-10110,Unassigned,Prasad Ramalingam,Tue; 19 Nov 2013 21:44:57 +0000,Wed; 19 Feb 2014 22:51:59 +0000,Tue; 19 Nov 2013 22:22:49 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10117
HADOOP-10118,Bug,Major,fs,"FsShell never interpret ""--""","We cannot use ""--"" option to skip args following that.CommandFormat#parse is implemented as follows. But; FsShell is called through ToolRunner and ToolRunner use GenericOptionParser. GenericOptionParser use GnuParser; which discard ""--"" when parsing args.",Open,Unresolved,,Unassigned,Kousuke Saruta,Tue; 19 Nov 2013 22:00:21 +0000,Thu; 12 May 2016 18:24:17 +0000,,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10118
HADOOP-10119,Bug,Minor,documentation,Document hadoop archive -p option,Now hadoop archive -p (relative parent path) option is required but the option is not documented.See http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CommandsManual.html#archive .,Resolved,Duplicate,MAPREDUCE-5943,Unassigned,Akira Ajisaka,Wed; 20 Nov 2013 05:02:39 +0000,Tue; 19 Aug 2014 07:50:06 +0000,Tue; 19 Aug 2014 07:50:06 +0000,,2.2.0,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10119
HADOOP-10120,New Feature,Major,metrics,Additional sliding window metrics,"For HDFS-5350 we'd like to report the last few fsimage transfer times as a health metric. This would mean (for example) a sliding window of the last 10 transfer times; when it was last updated; the total count. It'd be nice to have a metrics class that did this.It'd also be interesting to have some kind of time-based sliding window for statistics like counts and averages. This would let us answer questions like ""how many RPCs happened in the last 10s? minute? 5 minutes? 10 minutes?"". Commutative metrics like counts and averages are easy to aggregate in this fashion.",Open,Unresolved,,Andrew Wang,Andrew Wang,Wed; 20 Nov 2013 21:57:50 +0000,Thu; 21 Nov 2013 19:14:21 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10120
HADOOP-10121,Bug,Trivial,documentation,Fix javadoc spelling for HadoopArchives#writeTopLevelDirs,There's a misspelling at HadoopArchives.java. It should be fixed as follows:,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Thu; 21 Nov 2013 08:34:03 +0000,Mon; 1 Dec 2014 03:07:24 +0000,Thu; 14 Aug 2014 20:27:37 +0000,,2.2.0,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10121
YARN-1442,New Feature,Minor,,change yarn minicluster base directory via system property,"The yarn minicluster used for testing uses the ""target"" directory by default. We use gradle for building our projects and we would like to see it using a different directory. This patch makes it possible to use a different directory by setting the yarn.minicluster.directory system property.",Open,Unresolved,,Unassigned,Andr√© Kelpe,Thu; 21 Nov 2013 14:37:51 +0000,Mon; 25 Aug 2014 22:12:53 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/YARN-1442
YARN-1479,Improvement,Major,,Invalid NaN values in Hadoop REST API JSON response,"I've been occasionally coming across instances where Hadoop's Cluster Applications REST API (http://hadoop.apache.org/docs/r0.23.6/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html#Cluster_Applications_API) has returned JSON that PHP's json_decode function failed to parse.  I've tracked the syntax error down to the presence of the unquoted word NaN appearing as a value in the JSON.  For example:""progress"":NaN;NaN is not part of the JSON spec; so its presence renders the whole JSON string invalid.  Hadoop needs to return something other than NaN in this case  perhaps an empty string or the quoted string ""NaN"".",Closed,Fixed,,Chen He,Kendall Thrapp,Thu; 16 May 2013 20:29:29 +0000,Wed; 3 Sep 2014 20:35:40 +0000,Wed; 19 Feb 2014 17:56:12 +0000,,0.23.6;2.0.4-alpha,,,,https://issues.apache.org/jira/browse/YARN-1479
MAPREDUCE-5646,Improvement,Major,,Option to shuffle splits of equal size,Mapreduce split calculation has the following base logic (via JobClient and the major InputFormat implementations ): enumerate input files in natural (aka linear) order. create one split for each 'block-size' of each input. Apart from rack-awareness; combining and so on; the input file order remains in its natural order. sort the splits by size using a stable sort based on splitsize.When data from multiple storage services are used in a single hadoop job; we get better I/O utilization if the list of splits does round-robin or random-access across the services. The particular scenario arises in Azure HDInsight where jobs can easily read from many storage accounts and each storage account has hard limits on throughtput.  Concurrent access to the accounts is substantially better than Two common scenarios can cause non-ideal access pattern: 1. many/all input files are the same size 2. files have different sizes; but many/all input files have sizeblocksize. In the second scenario; for each file will have one or more splits with size exactly equal to block size so it basically degenerates to the first scenario.There are various ways to solve the problem but the simplest is to alter the mapreduce JobClient to sort splits by size and randomize the order of splits with equal size. This keeps the old behavior effectively unchanged while also fixing both common problematic scenarios.Some rare scenarios will still suffer bad access patterns due. For example if two storage accounts are used and the files from one storage account are all smaller than from the other then problems can arise. Addressing these scenarios would be further work; perhaps by completely randomizing the split order. These problematic scenarios are considered rare and not requiring immediate attention.If further algorithms for split ordering are necessary; the implementation in JobClient will change to being interface-based (eg interface splitOrderer) with various standard implementations.  At this time there is only the need for two implementations and so simple Boolean flag and if/then logic is used.,Resolved,Won't Fix,,Mike Liddell,Mike Liddell,Fri; 22 Nov 2013 22:34:10 +0000,Tue; 10 Mar 2015 04:30:40 +0000,Thu; 23 Jan 2014 19:45:51 +0000,,1-win,,,,https://issues.apache.org/jira/browse/MAPREDUCE-5646
HADOOP-10125,Bug,Major,ipc,no need to process RPC request if the client connection has been dropped,If the client has dropped the connection before the RPC is processed; RPC server doesn't need to process the RPC call. We have encountered issues where bad applications can bring down the NN. https://issues.apache.org/jira/i#browse/Hadoop-9640 tries to address that. When this occurs; NN's RPC queues are filled up with client requests and DN requests; sometimes we want to stop the flooding by stopping the bad applications and/or DNs. Some RPC processing like DatanodeProtocol::blockReport could take couple hundred milliseconds. So it is worthwhile to have NN skip the RPC calls if DNs have been stopped.,Closed,Fixed,HADOOP-11019,Ming Ma,Ming Ma,Sat; 23 Nov 2013 00:38:39 +0000,Thu; 12 May 2016 18:26:54 +0000,Wed; 15 Jan 2014 21:54:15 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10125
HADOOP-10126,Bug,Minor,util,"LightWeightGSet log message is confusing : ""2.0% max memory = 2.0 GB""","Following message log message from LightWeightGSet is confusing. ; where 2GB is max JVM memory; but log message confuses like 2% of max memory is 2GB. It can be better like this""2.0% of max memory 2.0 GB = 40.9 MB""",Closed,Fixed,,Vinayakumar B,Vinayakumar B,Mon; 25 Nov 2013 05:27:29 +0000,Mon; 24 Feb 2014 20:58:27 +0000,Mon; 25 Nov 2013 19:46:43 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10126
HADOOP-10127,Bug,Major,ipc,Add ipc.client.connect.retry.interval to control the frequency of connection retries,Currently; ipc.Client client attempts to connect to the server every 1 second. It would be nice to make this configurable to be able to connect more/less frequently. Changing the number of retries alone is not granular enough.,Closed,Fixed,,Karthik Kambatla,Karthik Kambatla,Tue; 26 Nov 2013 01:10:50 +0000,Mon; 3 Nov 2014 18:33:24 +0000,Tue; 3 Dec 2013 22:47:28 +0000,,2.2.0,,YARN-1460,,https://issues.apache.org/jira/browse/HADOOP-10127
HADOOP-10128,Bug,Major,,Please delete old releases from mirroring system,To reduce the load on the ASF mirrors; projects are required to delete old releases.Please can you remove all non-current releases?i.e. anything except0.23.91.2.12.2.0Thanks.,Open,Unresolved,,Unassigned,Sebb,Tue; 26 Nov 2013 17:43:13 +0000,Thu; 16 Nov 2017 14:48:09 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10128
HADOOP-10129,Bug,Critical,tools/distcp,Distcp may succeed when it fails,Distcp uses IOUtils.cleanup to close its output streams w/o previously attempting to close the streams.  IOUtils.cleanup will swallow close or implicit flush on close exceptions.  As a result; distcp may silently skip files when a partial file listing is generated; and/or appear to succeed when individual copies fail.,Closed,Fixed,,Daryn Sharp,Daryn Sharp,Tue; 26 Nov 2013 18:59:40 +0000,Thu; 12 May 2016 18:21:46 +0000,Thu; 5 Dec 2013 15:55:30 +0000,,0.23.0;2.0.0-alpha;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10129
HADOOP-10130,Bug,Minor,,RawLocalFS::LocalFSFileInputStream.pread does not track FS::Statistics,RawLocalFS::LocalFSFileInputStream.pread does not track FS::Statistics,Closed,Fixed,,Binglin Chang,Binglin Chang,Wed; 27 Nov 2013 09:12:42 +0000,Mon; 24 Feb 2014 20:58:14 +0000,Mon; 2 Dec 2013 17:30:19 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10130
HADOOP-10131,Bug,Major,,NetWorkTopology#countNumOfAvailableNodes() is returning wrong value if excluded nodes passed are not part of the cluster tree,"I got ""File /hdfs_COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation."" in the following case1. 2 DNs cluster;2. One of the datanodes was not responding from last 10 min; but about to detect as dead at NN.3. Tried to write one file; for the block NN allocated both DNs.4. Client While creating the pipeline took some time to detect one node failure.5. Before client detects pipeline failure; NN side dead node was removed from cluster map.6. Now; client has abandoned previous block and asked for new block with dead node in excluded list and got above exception even though one more node was available live.When I dig this more; found that;NetWorkTopology#countNumOfAvailableNodes() is not giving correct count when the excludeNodes passed from client are not part of the cluster map.Adding to this one more case where count is wrong.1. If there is no node present for the normalized scope in cluster.",Closed,Fixed,,Vinayakumar B,Vinayakumar B,Tue; 20 Aug 2013 15:21:56 +0000,Thu; 12 May 2016 18:27:57 +0000,Mon; 22 Sep 2014 06:02:11 +0000,,2.0.5-alpha;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10131
HADOOP-10132,Improvement,Minor,,RPC#stopProxy() should log the class of proxy when IllegalArgumentException is encountered,When investigating HBASE-10029; Tsz Wo Nicholas Sze made the suggestion of logging the class of proxy when IllegalArgumentException is thrown.,Closed,Fixed,,Ted Yu,Ted Yu,Wed; 27 Nov 2013 17:32:47 +0000,Wed; 3 Sep 2014 23:33:18 +0000,Wed; 27 Nov 2013 21:00:40 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10132
HADOOP-10133,Bug,Major,fs,winutils detection on windows-cygwin fails,java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278) at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)at org.apache.hadoop.util.Shell.clinit(Shell.java:293),Resolved,Won't Fix,,Unassigned,Franjo Markovic,Wed; 27 Nov 2013 20:31:20 +0000,Sat; 10 Jan 2015 17:28:30 +0000,Tue; 4 Nov 2014 15:58:02 +0000,,2.2.0,,,HADOOP-11464,https://issues.apache.org/jira/browse/HADOOP-10133
HADOOP-10134,Bug,Minor,,[JDK8] Fix Javadoc errors caused by incorrect or illegal tags in doc comments ,Javadoc is more strict by default in JDK8 and will error out on malformed or illegal tags found in doc comments. Although tagged as JDK8 all of the required changes are generic Javadoc cleanups.,Closed,Fixed,,Andrew Purtell,Andrew Purtell,Wed; 27 Nov 2013 21:12:42 +0000,Tue; 30 Aug 2016 01:32:45 +0000,Tue; 9 Dec 2014 11:16:00 +0000,,2.3.0;3.0.0-alpha1,,,HADOOP-11090;HDFS-5578;MAPREDUCE-5657;YARN-1453,https://issues.apache.org/jira/browse/HADOOP-10134
HADOOP-10135,Bug,Major,fs,writes to swift fs over partition size leave temp files and empty output file,"The OpenStack/swift filesystem produces incorrect output when the written objects exceed the configured partition size. After job completion; the expected files in the swift container have length == 0 and a collection of temporary files remain with names that appear to be URLs.This can be replicated with teragen against the minicluster using the following command line:bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-SNAPSHOT.jar teragen 100000 swift://mycontainer.myservice/teradataWhere core-site.xml contains:  property    namefs.swift.impl/name    valueorg.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem/value  /property  property    namefs.swift.partsize/name    value1024/value  /property  property    namefs.swift.service.myservice.auth.url/name    valuehttps://auth.api.rackspacecloud.com/v2.0/tokens/value  /property  property    namefs.swift.service.myservice.username/name    value[your-cloud-username]/value  /property  property    namefs.swift.service.myservice.region/name    valueDFW/value  /property  property    namefs.swift.service.myservice.apikey/name    value[your-api-key]/value  /property  property    namefs.swift.service.myservice.public/name    valuetrue/value  /propertyContainer ""mycontainer"" should have a collection of objects with names starting with ""teradata/part-m-00000"".  Instead; that file is empty and there is a collection of objects with names like ""swift://mycontainer.myservice/teradata/_temporary/0/_temporary/attempt_local415043862_0001_m_000000_0/part-m-00000/000010""",Closed,Fixed,,David Dobbins,David Dobbins,Wed; 27 Nov 2013 21:18:13 +0000,Thu; 12 May 2016 18:21:54 +0000,Mon; 2 Dec 2013 11:12:21 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10135
HADOOP-10136,Improvement,Major,,Custom JMX server to avoid random port usage by default JMX Server,If any of the java process want to enable the JMX MBean server  then following VM arguments needs to be passed. But the issue here is this will use one more random port other than 14005 while starting JMX. This can be a problem if that random port is used for some other service.So support a custom JMX Server through which random port can be avoided.,Patch Available,Unresolved,,Vinayakumar B,Vinayakumar B,Thu; 28 Nov 2013 13:35:39 +0000,Wed; 6 May 2015 03:32:52 +0000,,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10136
YARN-1464,Test,Major,,TestRMNMSecretKeys#testNMUpdation fails with NullPointerException,Here is the stack trace:,Resolved,Duplicate,YARN-1463,Unassigned,Ted Yu,Fri; 29 Nov 2013 15:31:22 +0000,Tue; 3 Dec 2013 07:17:44 +0000,Tue; 3 Dec 2013 07:17:44 +0000,,,,,,https://issues.apache.org/jira/browse/YARN-1464
HADOOP-10138,Improvement,Minor,,Support custom record separator with streaming,We store XML documents in sequence files as values.  The values may contain newlines.  It is useful to have hadoop-streaming output a zero-byte instead of a newline to delimit key-value boundaries.  The mapping script can then find the key-value pairs unambiguously by looking for zero-bytes.I find this really useful so I can use a ruby script for quick adhoc queries.I have a patch with unit test that I will attach.,Open,Unresolved,,Unassigned,Christopher Auston,Fri; 29 Nov 2013 18:13:16 +0000,Fri; 29 Nov 2013 18:15:44 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10138
HADOOP-10139,Improvement,Major,documentation,Update and improve the Single Cluster Setup document,"The document should be understandable to a newcomer because the first place he will go is ""setup a single node"".",Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Mon; 2 Dec 2013 18:54:30 +0000,Thu; 4 Sep 2014 01:16:47 +0000,Thu; 30 Jan 2014 19:31:21 +0000,,2.2.0,,,HADOOP-10050,https://issues.apache.org/jira/browse/HADOOP-10139
HADOOP-10140,Improvement,Minor,scripts,Specification of HADOOP_CONF_DIR via the environment in hadoop_config.cmd,It would be nice if HADOOP_CONF_DIR could be set in the environment like YARN_CONF_DIR. This could be done in lib-exec/hadoop.cmd by setting HADOOP_CONF_DIR conditionally.Using the Windows command shell; the modification would be as follows:if not defined HADOOP_CONF_DIR (set HADOOP_CONF_DIR=%HADOOP_HOME%\etc\hadoop)This would allow the Hadoop configuration to be placed in ProgramData more easily.,Closed,Fixed,HADOOP-11508,Kiran Kumar M R,Ian Jackson,Mon; 2 Dec 2013 22:55:41 +0000,Fri; 10 Apr 2015 20:04:45 +0000,Thu; 12 Feb 2015 00:41:32 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10140
HADOOP-10141,Bug,Major,security,Create an API to separate encryption key storage from applications,As with the filesystem API; we need to provide a generic mechanism to support multiple key storage mechanisms that are potentially from third parties. An additional requirement for long term data lakes is to keep multiple versions of each key so that keys can be rolled periodically without requiring the entire data set to be re-written. Rolling keys provides containment in the event of keys being leaked.Toward that end; I propose an API that is configured using a list of URLs of KeyProviders. The implementation will look for implementations using the ServiceLoader interface and thus support third party libraries.Two providers will be included in this patch. One using the credentials cache in MapReduce jobs and the other using Java KeyStores from either HDFS or local file system.,Closed,Fixed,HADOOP-10528;HADOOP-9825,Owen O'Malley,Owen O'Malley,Tue; 3 Dec 2013 00:11:30 +0000,Mon; 1 Dec 2014 03:10:54 +0000,Fri; 20 Dec 2013 00:27:46 +0000,,,,,HADOOP-9534;HADOOP-10607;HDFS-6134,https://issues.apache.org/jira/browse/HADOOP-10141
HADOOP-10142,Bug,Major,,"Avoid groups lookup for unprivileged users such as ""dr.who""",Reduce the logs generated by ShellBasedUnixGroupsMapping.For ex: Using WebHdfs from windows generates following log for each request,Closed,Fixed,,Vinayakumar B,Vinayakumar B,Tue; 3 Dec 2013 06:33:32 +0000,Mon; 24 Feb 2014 20:57:36 +0000,Sat; 7 Dec 2013 00:32:18 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10142
HADOOP-10143,Improvement,Major,io,replace WritableFactories's hashmap with ConcurrentHashMap,"We observed a lock contend hotspot from a HBase cluster:""IPC Reader 9 on port 12600"" daemon prio=10 tid=0x00007f85b8aceed0 nid=0x4be8 waiting for monitor entry 0x00007f8501c57000   java.lang.Thread.State: BLOCKED (on object monitor)        at org.apache.hadoop.io.WritableFactories.getFactory(WritableFactories.java:44)	locked 0x00000007fd1328a8 (a java.lang.Class for org.apache.hadoop.io.WritableFactories)        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:680)        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)        at org.apache.hadoop.hbase.client.MultiAction.readFields(MultiAction.java:116)        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:126)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processData(SecureServer.java:618)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processOneRpc(SecureServer.java:596)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.saslReadAndProcess(SecureServer.java:362)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.readAndProcess(SecureServer.java:492)        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:770)        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:561)	locked 0x000000043da3fea0 (a org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader)        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:536)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)        at java.lang.Thread.run(Thread.java:662)""IPC Reader 7 on port 12600"" daemon prio=10 tid=0x00007f85b8a99df0 nid=0x4be6 waiting for monitor entry 0x00007f8501cd9000   java.lang.Thread.State: BLOCKED (on object monitor)        at org.apache.hadoop.io.WritableFactories.getFactory(WritableFactories.java:44)	locked 0x00000007fd1328a8 (a java.lang.Class for org.apache.hadoop.io.WritableFactories)        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:680)        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)        at org.apache.hadoop.hbase.client.MultiAction.readFields(MultiAction.java:116)        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:126)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processData(SecureServer.java:618)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processOneRpc(SecureServer.java:596)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.saslReadAndProcess(SecureServer.java:362)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.readAndProcess(SecureServer.java:492)        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:770)        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:561)	locked 0x000000043da232e8 (a org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader)        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:536)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)        at java.lang.Thread.run(Thread.java:662)""IPC Reader 5 on port 12600"" daemon prio=10 tid=0x00007f85b8a64d40 nid=0x4be2 runnable 0x00007f8501d5b000   java.lang.Thread.State: RUNNABLE        at org.apache.hadoop.io.WritableFactories.getFactory(WritableFactories.java:44)	locked 0x00000007fd1328a8 (a java.lang.Class for org.apache.hadoop.io.WritableFactories)        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:680)        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)        at org.apache.hadoop.hbase.client.Action.readFields(Action.java:103)        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)        at org.apache.hadoop.hbase.client.MultiAction.readFields(MultiAction.java:116)        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:126)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processData(SecureServer.java:618)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processOneRpc(SecureServer.java:596)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.saslReadAndProcess(SecureServer.java:362)        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.readAndProcess(SecureServer.java:492)        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:770)        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:561)	locked 0x000000043da27300 (a org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader)        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:536)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)        at java.lang.Thread.run(Thread.java:662)All those three threads just wanted to get/read the factory; so to me;  it looks like a perfect use case for ConcurrentHashMap here.",Closed,Fixed,,Liang Xie,Liang Xie,Wed; 4 Dec 2013 07:58:06 +0000,Wed; 3 Sep 2014 23:33:18 +0000,Tue; 21 Jan 2014 05:17:07 +0000,,2.0.0-alpha;2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10143
HADOOP-10144,Bug,Major,native,Error on build in windows 7 box,"Here is log file:INFOINFO  exec-maven-plugin:1.2:exec (compile-ms-native-dll) @ hadoop-common -Building the projects in this solution one at a time. To enable parallel build;please add the ""/m"" switch.Build started 12/4/2013 8:59:35 PM.Project ""C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.sln"" on node 1 (default targets).ValidateSolutionConfiguration:  Building solution configuration ""Release|Win32"".Project ""C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.sln"" (1) is building ""C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj"" (2) on node 1 (default targets).InitializeBuildStatus:  Touching ""..\..\..\target\native\Release\native.tlog\unsuccessfulbuild"".ClCompile:  All outputs are up-to-date.Link:  C:\Program Files\Microsoft Visual Studio 12.0\VC\bin\x86_amd64\link.exe /ERRORREPORT:QUEUE /OUT:""C:\hdfs\hadoop-common-project\hadoop-common\target/bin/hadoop.dll"" /INCREMENTAL:NO /NOLOGO /LIBPATH:..\..\..\target\bin Ws2_32.lib libwinutils.lib kernel32.lib user32.lib gdi32.lib winspool.lib comdlg32.lib advapi32.lib shell32.lib ole32.lib oleaut32.lib uuid.lib odbc32.lib odbccp32.lib /MANIFEST /MANIFESTUAC:""level='asInvoker' uiAccess='false'"" /manifest:embed /DEBUG /PDB:""C:\hdfs\hadoop-common-project\hadoop-common\target/bin/hadoop.pdb"" /SUBSYSTEM:WINDOWS /OPT:REF /OPT:ICF /LTCG /TLBID:1 /DYNAMICBASE /NXCOMPAT /IMPLIB:""C:\hdfs\hadoop-common-project\hadoop-common\target/bin/hadoop.lib"" /MACHINE:X64 /DLL ..\..\..\target\native\Release\lz4.obj   ..\..\..\target\native\Release\Lz4Compressor.obj  ..\..\..\target\native\Release\Lz4Decompressor.obj  ..\..\..\target\native\Release\file_descriptor.obj   ..\..\..\target\native\Release\NativeIO.obj  ..\..\..\target\native\Release\JniBasedUnixGroupsMappingWin.obj  ..\..\..\target\native\Release\bulk_crc32.obj  ..\..\..\target\native\Release\NativeCodeLoader.obj  ..\..\..\target\native\Release\NativeCrc32.obj     Creating library C:\hdfs\hadoop-common-project\hadoop-common\target/bin/hadoop.lib and object C:\hdfs\hadoop-common-project\hadoop-common\target/bin/hadoop.expNativeIO.obj : error LNK2001: unresolved external symbol FindFileOwnerAndPermissionByHandle [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]NativeIO.obj : error LNK2001: unresolved external symbol SymbolicLinkCheck [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]NativeIO.obj : error LNK2001: unresolved external symbol ChangeFileModeByMask [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]NativeIO.obj : error LNK2001: unresolved external symbol GetAccntNameFromSid [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]NativeIO.obj : error LNK2001: unresolved external symbol JunctionPointCheck [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]NativeIO.obj : error LNK2001: unresolved external symbol CheckAccessForCurrentUser C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxprojJniBasedUnixGroupsMappingWin.obj : error LNK2001: unresolved external symbol NetApiBufferFree [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]JniBasedUnixGroupsMappingWin.obj : error LNK2001: unresolved external symbol GetLocalGroupsForUser [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]NativeCodeLoader.obj : error LNK2001: unresolved external symbol GetLibraryNameC:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxprojC:\hdfs\hadoop-common-project\hadoop-common\target/bin/hadoop.dll : fatal errorLNK1120: 9 unresolved externals [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]Done Building Project ""C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj"" (default targets)  FAILED.Done Building Project ""C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.sln"" (default targets)  FAILED.Build FAILED.""C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.sln"" (default target) (1) -""C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj"" (default target) (2) -(Link target) -  NativeIO.obj : error LNK2001: unresolved external symbol FindFileOwnerAndPermissionByHandle [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]  NativeIO.obj : error LNK2001: unresolved external symbol SymbolicLinkCheck [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]  NativeIO.obj : error LNK2001: unresolved external symbol ChangeFileModeByMaskC:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj  NativeIO.obj : error LNK2001: unresolved external symbol GetAccntNameFromSid [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]  NativeIO.obj : error LNK2001: unresolved external symbol JunctionPointCheck [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]  NativeIO.obj : error LNK2001: unresolved external symbol CheckAccessForCurrentUser [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]  JniBasedUnixGroupsMappingWin.obj : error LNK2001: unresolved external symbol NetApiBufferFree [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]  JniBasedUnixGroupsMappingWin.obj : error LNK2001: unresolved external symbol GetLocalGroupsForUser [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]  NativeCodeLoader.obj : error LNK2001: unresolved external symbol GetLibraryName C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj  C:\hdfs\hadoop-common-project\hadoop-common\target/bin/hadoop.dll : fatal error LNK1120: 9 unresolved externals [C:\hdfs\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]    0 Warning(s)    10 Error(s)Time Elapsed 00:00:00.47INFO ------------------------------------------------------------------------INFO Reactor Summary:INFOINFO Apache Hadoop Main ................................ SUCCESS 1.159sINFO Apache Hadoop Project POM ......................... SUCCESS 1.160sINFO Apache Hadoop Annotations ......................... SUCCESS 1.815sINFO Apache Hadoop Assemblies .......................... SUCCESS 0.165sINFO Apache Hadoop Project Dist POM .................... SUCCESS 1.902sINFO Apache Hadoop Maven Plugins ....................... SUCCESS 2.528sINFO Apache Hadoop Auth ................................ SUCCESS 1.971sINFO Apache Hadoop Auth Examples ....................... SUCCESS 1.740sINFO Apache Hadoop Common .............................. FAILURE 6.208sINFO Apache Hadoop NFS ................................. SKIPPED...",Resolved,Fixed,HADOOP-9922,Unassigned,Maer,Wed; 4 Dec 2013 16:20:21 +0000,Fri; 30 Dec 2016 17:50:16 +0000,Thu; 5 Dec 2013 09:23:06 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10144
HADOOP-10145,Bug,Major,conf,Reduce task stuck on 0.16666667%,All of sudden; one of the Hadoop jobs is stuck; basically the reduce takes forever to complete(we have waited for 30 hours; usually it takes an hour to complete).in tasktracker logs i see tons of following messages; however at times; resubmitting the same job works fine. 2013-12-04 00:00:00;381 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201310070546_159167_r_000041_0 0.16666667% reduce  copy (1 of 2 at 0.01 MB/s) 2013-12-04 00:00:00;750 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201310070546_159167_r_000048_0 0.16666667% reduce  copy (1 of 2 at 0.01 MB/s) 2013-12-04 00:00:01;729 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201310070546_159262_r_000046_0 0.16666667% reduce  copy (1 of 2 at 0.03 MB/s) 2013-12-04 00:00:01;918 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201310070546_159262_r_000055_0 0.16666667% reduce  copy (1 of 2 at 0.03 MB/s) 2013-12-04 00:00:01;919 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201310070546_159262_r_000021_0 0.16666667% reduce  copy (1 of 2 at 0.03 MB/s) 2013-12-04 00:00:01;922 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201310070546_159262_r_000031_0 0.16666667% reduce  copy (1 of 2 at 0.03 MB/s) 2013-12-04 00:00:01;940 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201310070546_159262_r_000057_0 0.16666667% reduce  copy (1 of 2 at 0.03 MB/s) 2013-12-04 00:00:02;443 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201310070546_159167_r_000047_0 0.16666667% reduce  copy (1 of 2 at 0.01 MB/s) there are no other resonable clues in log for me to get a direction on; what am i looking for. with my setup; upgrading to new version is not an option.please help!,Resolved,Invalid,,Unassigned,vikash kumar,Wed; 4 Dec 2013 17:26:22 +0000,Wed; 4 Dec 2013 18:27:59 +0000,Wed; 4 Dec 2013 18:27:59 +0000,,0.20.2,,,,https://issues.apache.org/jira/browse/HADOOP-10145
HADOOP-10146,Bug,Critical,util,Workaround JDK7 Process fd close bug,JDK7's Process output streams have an async fd-close race bug.  This manifests as commands run via o.a.h.u.Shell causing threads to hang; OOM; or cause other bizarre behavior.  The NM is likely to encounter the bug under heavy load.Specifically; ProcessBuilder's UNIXProcess starts a thread to reap the process and drain stdout/stderr to avoid a lingering zombie process.  A race occurs if the thread using the stream closes it; the underlying fd is recycled/reopened; while the reaper is draining it.  ProcessPipeInputStream.drainInputStream's will OOM allocating an array if in.available() returns a huge number; or may wreak havoc by incorrectly draining the fd.,Closed,Fixed,,Daryn Sharp,Daryn Sharp,Thu; 5 Dec 2013 17:25:30 +0000,Thu; 12 May 2016 18:26:27 +0000,Thu; 16 Jan 2014 18:58:28 +0000,,0.23.0;2.0.0-alpha;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10146
HADOOP-10147,Bug,Minor,build,Upgrade to commons-logging 1.1.3 to avoid potential deadlock in MiniDFSCluster,There is a deadlock in commons-logging 1.1.1 (see LOGGING-119) that can manifest itself while running MiniDFSCluster JUnit tests.This deadlock has been fixed in commons-logging 1.1.2.  The latest version available is commons-logging 1.1.3; and Hadoop should upgrade to that in order to address this deadlock.,Closed,Fixed,HDFS-5678,Steve Loughran,Eric Sirianni,Fri; 6 Dec 2013 20:25:07 +0000,Mon; 24 Feb 2014 20:56:43 +0000,Thu; 2 Jan 2014 13:42:22 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10147
HADOOP-10148,Sub-task,Minor,ipc,backport hadoop-10107 to branch-0.23,"Found this in build #5440Caused by: java.lang.NullPointerException	at org.apache.hadoop.ipc.Server.getNumOpenConnections(Server.java:2434)	at org.apache.hadoop.ipc.metrics.RpcMetrics.numOpenConnections(RpcMetrics.java:74)",Closed,Fixed,,Chen He,Chen He,Fri; 6 Dec 2013 21:48:37 +0000,Fri; 27 Jun 2014 13:57:09 +0000,Mon; 9 Dec 2013 19:13:11 +0000,,0.23.10,,,,https://issues.apache.org/jira/browse/HADOOP-10148
HADOOP-10149,Bug,Major,,Create ByteBuffer-based cipher API,As part of HDFS-5143; Yi Liu included a ByteBuffer-based API for encryption and decryption. Especially; because of the zero-copy work this seems like an important piece of work. This API should be discussed independently instead of just as part of HDFS-5143.,Open,Unresolved,,Owen O'Malley,Owen O'Malley,Sat; 7 Dec 2013 00:40:48 +0000,Tue; 8 Jul 2014 06:30:42 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10149
HADOOP-10150,New Feature,Major,security,Hadoop cryptographic file system,"There is an increasing need for securing data when Hadoop customers use various upper layer applications; such as Map-Reduce; Hive; Pig; HBase and so on.HADOOP CFS (HADOOP Cryptographic File System) is used to secure data; based on HADOOP  FilterFileSystem  decorating DFS or other file systems; and transparent to upper layer applications. It s configurable; scalable and fast.High level requirements:1.	Transparent to and no modification required for upper layer applications.2.	 Seek ;  PositionedReadable  are supported for input stream of CFS if the wrapped file system supports them.3.	Very high performance for encryption and decryption; they will not become bottleneck.4.	Can decorate HDFS and all other file systems in Hadoop; and will not modify existing structure of file system; such as namenode and datanode structure if the wrapped file system is HDFS.5.	Admin can configure encryption policies; such as which directory will be encrypted.6.	A robust key management framework.7.	Support Pread and append operations if the wrapped file system supports them.",Closed,Fixed,,Yi Liu,Yi Liu,Thu; 29 Aug 2013 07:54:41 +0000,Thu; 12 May 2016 18:24:44 +0000,Wed; 20 Aug 2014 18:58:02 +0000,,3.0.0-alpha1,rhino,,HADOOP-9534;HDFS-6134,https://issues.apache.org/jira/browse/HADOOP-10150
HADOOP-10151,Task,Major,security,Implement a Buffer-Based Chiper InputStream and OutputStream,Cipher InputStream and OuputStream are buffer-based; and the buffer is used to cache the encrypted data or result.   Cipher InputStream is used to read encrypted data; and the result is plain text . Cipher OutputStream is used to write plain data and result is encrypted data.,Resolved,Won't Fix,,Yi Liu,Yi Liu,Mon; 9 Dec 2013 12:17:41 +0000,Thu; 12 May 2016 18:22:14 +0000,Wed; 14 May 2014 05:12:16 +0000,,3.0.0-alpha1,rhino,,,https://issues.apache.org/jira/browse/HADOOP-10151
HADOOP-10152,Task,Major,security,Distributed file cipher InputStream and OutputStream which provide 1:1 mapping of plain text data and cipher data.,To be easily seek and positioned read distributed file; the length of encrypted file should be the same as the length of plain file; and the positions have 1:1 mapping.  So in this JIRA we defines distributed file cipher InputStream(FSDecryptorStream) and OutputStream(FSEncryptorStream). The distributed file cipher InputStream is seekable and positonedReadable.  This JIRA is different from HADOOP-10151; the file may be read and written many times and on multiple nodes.,Resolved,Duplicate,NULL,Yi Liu,Yi Liu,Mon; 9 Dec 2013 12:19:41 +0000,Thu; 12 May 2016 18:22:15 +0000,Wed; 14 May 2014 05:20:29 +0000,,3.0.0-alpha1,rhino,,,https://issues.apache.org/jira/browse/HADOOP-10152
HADOOP-10153,Task,Major,security,Define Crypto policy interfaces and provide its default implementation.,The JIRA defines crypto policy interface; developers/users can implement their own crypto policy to decide how files/directories are encrypted. This JIRA also includes a default implementation.,Resolved,Won't Fix,,Yi Liu,Yi Liu,Mon; 9 Dec 2013 12:22:45 +0000,Thu; 12 May 2016 18:22:15 +0000,Wed; 14 May 2014 05:20:54 +0000,,3.0.0-alpha1,rhino,,,https://issues.apache.org/jira/browse/HADOOP-10153
HADOOP-10154,Task,Major,security,Provide cryptographic filesystem implementation and it's data IO.,The JIRA includes Cryptographic filesystem data  InputStream which extends FSDataInputStream and OutputStream which extends FSDataOutputStream.  Implantation of Cryptographic file system is also included in this JIRA.,Resolved,Won't Fix,,Yi Liu,Yi Liu,Mon; 9 Dec 2013 12:25:28 +0000,Thu; 12 May 2016 18:22:18 +0000,Wed; 14 May 2014 05:11:31 +0000,,3.0.0-alpha1,rhino,,,https://issues.apache.org/jira/browse/HADOOP-10154
HADOOP-10155,Task,Major,security,Hadoop-crypto which includes native cipher implementation. ,Native cipher is used to improve performance; when using OpenSSL and with AES-NI enabled; Native cipher is 20x faster than Java cipher; for example CBC/CTR mode.,Resolved,Won't Fix,,Yi Liu,Yi Liu,Mon; 9 Dec 2013 12:27:09 +0000,Thu; 12 May 2016 18:22:17 +0000,Wed; 14 May 2014 00:53:30 +0000,,3.0.0-alpha1,rhino,,,https://issues.apache.org/jira/browse/HADOOP-10155
HADOOP-10156,Task,Major,security,Define Buffer-based Encryptor/Decryptor interfaces and provide implementation for AES CTR.,Define encryptor and decryptor interfaces; and they are buffer-based to improve performance.  We use direct buffer to avoid bytes copy between JAVA and native if there is JNI call.  In this JIRA; AES CTR mode encryption and decryption are implemented.,Resolved,Duplicate,NULL,Yi Liu,Yi Liu,Tue; 10 Dec 2013 08:38:06 +0000,Thu; 12 May 2016 18:23:30 +0000,Wed; 14 May 2014 05:19:54 +0000,,3.0.0-alpha1,rhino,,,https://issues.apache.org/jira/browse/HADOOP-10156
HADOOP-10157,Bug,Minor,ipc,move doRead method from IPC Listener class to IPC Reader class,Current doRead method belongs to Listener class. Semantically it is better to move doRead method from Listener class to Reader class.,Open,Unresolved,,Unassigned,Ming Ma,Tue; 10 Dec 2013 14:55:34 +0000,Tue; 10 Dec 2013 14:55:34 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10157
HADOOP-10158,Bug,Critical,,SPNEGO should work with multiple interfaces/SPNs.,This is the list of internal servlets added by namenode. Name  Auth  Need to be accessible by end users  StartupProgressServlet  none  no  GetDelegationTokenServlet  internal SPNEGO  yes  RenewDelegationTokenServlet  internal SPNEGO  yes   CancelDelegationTokenServlet  internal SPNEGO  yes   FsckServlet  internal SPNEGO  yes   GetImageServlet  internal SPNEGO  no   ListPathsServlet  token in query  yes   FileDataServlet  token in query  yes   FileChecksumServlets  token in query  yes  ContentSummaryServlet  token in query  yes GetDelegationTokenServlet; RenewDelegationTokenServlet; CancelDelegationTokenServlet and FsckServlet are accessed by end users; but hard-coded to use the internal SPNEGO filter.If a name node HTTP server binds to multiple external IP addresses; the internal SPNEGO service principal name may not work with an address to which end users are connecting.  The current SPNEGO implementation in Hadoop is limited to use a single service principal per filter.If the underlying hadoop kerberos authentication handler cannot easily be modified; we can at least create a separate auth filter for the end-user facing servlets so that their service principals can be independently configured. If not defined; it should fall back to the current behavior.,Closed,Fixed,,Daryn Sharp,Kihwal Lee,Wed; 4 Dec 2013 16:59:57 +0000,Sun; 6 Nov 2016 06:28:29 +0000,Thu; 8 May 2014 18:28:56 +0000,,2.2.0,,,HADOOP-10702;HADOOP-10307,https://issues.apache.org/jira/browse/HADOOP-10158
HADOOP-10159,Bug,Major,build;documentation,Maven artifacts have incorrect javadoc jar,"Several of the  generated Maven artifacts have a (labeled as) javadoc jar that contain things besides javadocs.I checked the following; likely other artifacts are also impacted:	hadoop-client (all the 2.x contain shaded deps and no docs)	hadoop-common (all the 2.x and 0.23.3+ have both javadocs and shaded deps)	hadoop-hdfs (same as hadoop-common; plus classes)	hadoop-streaming (all 2.x and 0.23.3+ have compiled classes and javadocs)",Open,Unresolved,,Unassigned,Sean Busbey,Tue; 10 Dec 2013 21:57:01 +0000,Tue; 10 Dec 2013 21:57:01 +0000,,,0.23.3;2.0.0-alpha;2.0.1-alpha;2.0.2-alpha;0.23.4;2.0.3-alpha;0.23.5;0.23.6;0.23.7;2.1.0-beta;2.0.4-alpha;0.23.8;0.23.9;0.23.10;2.1.1-beta;2.0.6-alpha;2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10159
HDFS-5649,Bug,Major,nfs,Unregister NFS and Mount service when NFS gateway is shutting down,The services should be unregistered if the gateway is asked to shutdown gracefully.,Closed,Fixed,,Brandon Li,Brandon Li,Tue; 10 Dec 2013 23:00:55 +0000,Thu; 12 May 2016 18:13:33 +0000,Wed; 8 Jan 2014 00:05:44 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HDFS-5649
HADOOP-10161,Improvement,Major,metrics,Add a method to change the default value of dmax in hadoop.properties,The property of dmax in ganglia is a configurable time to rotate metrics. Therefore; no more value of the metric will be emit to the gmond; after 'dmax' seconds; then gmond will destroy the metric in memory. In Hadoop metrics framework; the default value of 'dmax' is 0. It means the gmond will never destroy the metric although the metric is disappeared. The gmetad daemon also does not delete the rrdtool file forever. We need to add a method to configure the default value of dmax for all metrics in hadoop.properties.,Patch Available,Unresolved,,Unassigned,Yang He,Wed; 11 Dec 2013 03:06:36 +0000,Wed; 6 May 2015 03:32:56 +0000,,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10161
HADOOP-10162,Bug,Major,,Fix symlink-related test failures in TestFileContextResolveAfs and TestStat in branch-2,We need to backport HADOOP-10052  to branch-2 as I found that the TestFileContextResolveAfs is failing after HADOOP-10020 went in.Also the test TestStat is failing for the same reason. It needs to be fixed as well.,Closed,Fixed,,Mit Desai,Mit Desai,Wed; 11 Dec 2013 23:38:32 +0000,Mon; 24 Feb 2014 20:58:13 +0000,Thu; 12 Dec 2013 18:59:02 +0000,,2.3.0,,,HADOOP-10052,https://issues.apache.org/jira/browse/HADOOP-10162
HADOOP-10163,Improvement,Major,,Attachment Id for last tested patch should be passed to test-patch.sh,In HBASE-10044; attempt was made to filter attachments according to known file extensions.However; that change alone wouldn't work because when non-patch is attached; QA bot doesn't provide attachment Id for last tested patch.This results in the modified test-patch.sh to seek backward and launch duplicate test run for last tested patch.If attachment Id for last tested patch is provided; test-patch.sh can decide whether there is need to run test.,Resolved,Later,,Unassigned,Ted Yu,Thu; 12 Dec 2013 18:26:34 +0000,Thu; 2 Jan 2014 23:36:31 +0000,Thu; 2 Jan 2014 23:36:31 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10163
HADOOP-10164,Improvement,Major,,Allow UGI to login with a known Subject,For storm I would love to let Hadoop initialize based off of credentials that were already populated in a Subject.  This is not currently possible because logging in a user always creates a new blank Subject.  This is to allow a user to be logged in based off a pre-existing subject through a new method.,Closed,Fixed,,Robert Joseph Evans,Robert Joseph Evans,Fri; 13 Dec 2013 16:44:25 +0000,Mon; 29 Aug 2016 19:30:43 +0000,Wed; 18 Dec 2013 21:34:51 +0000,,,,,HADOOP-13558,https://issues.apache.org/jira/browse/HADOOP-10164
HADOOP-10165,Bug,Minor,,TestMetricsSystemImpl#testMultiThreadedPublish occasionally fails,From https://builds.apache.org/job/Hadoop-Common-trunk/982/testReport/junit/org.apache.hadoop.metrics2.impl/TestMetricsSystemImpl/testMultiThreadedPublish/ :,Resolved,Duplicate,HADOOP-10062,Unassigned,Ted Yu,Sun; 15 Dec 2013 15:07:21 +0000,Mon; 16 Dec 2013 02:42:45 +0000,Mon; 16 Dec 2013 02:42:45 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10165
HADOOP-10166,Bug,Major,,I am not able to run mvn package -Pdist;native-win -DskipTests -Dtar,"When i try to build hadoop common i am getting below error.Loading source files for package org.apache.hadoop.security.authentication.examples...Constructing Javadoc information...Standard Doclet version 1.7.0_17Building tree for all the packages and classes...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\org\apache\hadoop\security\authentication\examples\RequestLoggerFilter.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\org\apache\hadoop\security\authentication\examples\WhoClient.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\org\apache\hadoop\security\authentication\examples\WhoServlet.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\org\apache\hadoop\security\authentication\examples\package-frame.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\org\apache\hadoop\security\authentication\examples\package-summary.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\org\apache\hadoop\security\authentication\examples\package-tree.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\constant-values.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\serialized-form.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\org\apache\hadoop\security\authentication\examples\class-use\WhoServlet.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\org\apache\hadoop\security\authentication\examples\class-use\WhoClient.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\org\apache\hadoop\security\authentication\examples\class-use\RequestLoggerFilter.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\org\apache\hadoop\security\authentication\examples\package-use.html...Building index for all the packages and classes...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\overview-tree.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\index-all.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\deprecated-list.html...Building index for all classes...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\allclasses-frame.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\allclasses-noframe.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\index.html...Generating C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\help-doc.html...INFO Building jar: C:\hdfs\common\hadoop-common-project\hadoop-auth-examples\target\hadoop-auth-examples-3.0.0-SNAPSHOT-javadoc.jarINFOINFO ------------------------------------------------------------------------INFO Building Apache Hadoop Common 3.0.0-SNAPSHOTINFO ------------------------------------------------------------------------INFOINFO  maven-enforcer-plugin:1.3.1:enforce (enforce-os) @ hadoop-common INFOINFO  maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-common INFO Executing tasksmain:INFO Executed tasksINFOINFO  hadoop-maven-plugins:3.0.0-SNAPSHOT:protoc (compile-protoc) @ hadoop-common INFOINFO  hadoop-maven-plugins:3.0.0-SNAPSHOT:version-info (version-info) @ hadoop-common WARNING svn; info failed: java.io.IOException: Cannot run program ""svn"": CreateProcess error=2; The system cannot find the file specifiedWARNING git; branch failed: java.io.IOException: Cannot run program ""git"": CreateProcess error=2; The system cannot find the file specifiedINFO SCM: NONEINFO Computed MD5: 9d8075203e1bd5184e801b2d7354da4aINFOINFO  maven-resources-plugin:2.2:resources (default-resources) @ hadoop-common INFO Using default encoding to copy filtered resources.INFOINFO  maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-common INFO Compiling 12 source files to C:\hdfs\common\hadoop-common-project\hadoop-common\target\classesINFOINFO  native-maven-plugin:1.0-alpha-7:javah (default) @ hadoop-common INFO cmd.exe /X /C ""C:\Java\jdk1.7.0_17\bin\javah -d C:\hdfs\common\hadoop-common-project\hadoop-common\target\native\javah -classpath C:\hdfs\common\hadoop-common-project\hadoop-common\target\classes;C:\hdfs\common\hadoop-common-project\hadoop-annotations\target\hadoop-annotations-3.0.0-SNAPSHOT.jar;C:\Java\jdk1.7.0_17\jre\..\lib\tools.jar;C:\Users\Pushpa\.m2\repository\com\google\guava\guava\11.0.2\guava-11.0.2.jar;C:\Users\Pushpa\.m2\repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;C:\Users\Pushpa\.m2\repository\org\apache\commons\commons-math3\3.1.1\commons-math3-3.1.1.jar;C:\Users\Pushpa\.m2\repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;C:\Users\Pushpa\.m2\repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;C:\Users\Pushpa\.m2\repository\commons-codec\commons-codec\1.4\commons-codec-1.4.jar;C:\Users\Pushpa\.m2\repository\commons-io\commons-io\2.4\commons-io-2.4.jar;C:\Users\Pushpa\.m2\repository\commons-net\commons-net\3.1\commons-net-3.1.jar;C:\Users\Pushpa\.m2\repository\commons-collections\commons-collections\3.2.1\commons-collections-3.2.1.jar;C:\Users\Pushpa\.m2\repository\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;C:\Users\Pushpa\.m2\repository\org\mortbay\jetty\jetty\6.1.26\jetty-6.1.26.jar;C:\Users\Pushpa\.m2\repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;C:\Users\Pushpa\.m2\repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;C:\Users\Pushpa\.m2\repository\com\sun\jersey\jersey-json\1.9\jersey-json-1.9.jar;C:\Users\Pushpa\.m2\repository\org\codehaus\jettison\jettison\1.1\jettison-1.1.jar;C:\Users\Pushpa\.m2\repository\stax\stax-api\1.0.1\stax-api-1.0.1.jar;C:\Users\Pushpa\.m2\repository\com\sun\xml\bind\jaxb-impl\2.2.3-1\jaxb-impl-2.2.3-1.jar;C:\Users\Pushpa\.m2\repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;C:\Users\Pushpa\.m2\repository\javax\activation\activation\1.1\activation-1.1.jar;C:\Users\Pushpa\.m2\repository\org\codehaus\jackson\jackson-jaxrs\1.8.8\jackson-jaxrs-1.8.8.jar;C:\Users\Pushpa\.m2\repository\org\codehaus\jackson\jackson-xc\1.8.8\jackson-xc-1.8.8.jar;C:\Users\Pushpa\.m2\repository\com\sun\jersey\jersey-server\1.9\jersey-server-1.9.jar;C:\Users\Pushpa\.m2\repository\asm\asm\3.2\asm-3.2.jar;C:\Users\Pushpa\.m2\repository\commons-logging\commons-logging\1.1.1\commons-logging-1.1.1.jar;C:\Users\Pushpa\.m2\repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;C:\Users\Pushpa\.m2\repository\net\java\dev\jets3t\jets3t\0.9.0\jets3t-0.9.0.jar;C:\Users\Pushpa\.m2\repository\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;C:\Users\Pushpa\.m2\repository\org\apache\httpcomponents\httpcore\4.2.5\httpcore-4.2.5.jar;C:\Users\Pushpa\.m2\repository\com\jamesmurty\utils\java-xmlbuilder\0.4\java-xmlbuilder-0.4.jar;C:\Users\Pushpa\.m2\repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;C:\Users\Pushpa\.m2\repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;C:\Users\Pushpa\.m2\repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;C:\Users\Pushpa\.m2\repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;C:\Users\Pushpa\.m2\repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;C:\Users\Pushpa\.m2\repository\org\slf4j\slf4j-api\1.7.5\slf4j-api-1.7.5.jar;C:\Users\Pushpa\.m2\repository\org\codehaus\jackson\jackson-core-asl\1.8.8\jackson-core-asl-1.8.8.jar;C:\Users\Pushpa\.m2\repository\org\codehaus\jackson\jackson-mapper-asl\1.8.8\jackson-mapper-asl-1.8.8.jar;C:\Users\Pushpa\.m2\repository\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;C:\Users\Pushpa\.m2\repository\com\thoughtworks\paranamer\paranamer\2.3\paranamer-2.3.jar;C:\Users\Pushpa\.m2\repository\org\xerial\snappy\snappy-java\1.0.4.1\snappy-java-1.0.4.1.jar;C:\Users\Pushpa\.m2\repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;C:\hdfs\common\hadoop-common-project\hadoop-auth\target\hadoop-auth-3.0.0-SNAPSHOT.jar;C:\Users\Pushpa\.m2\repository\com\jcraft\jsch\0.1.42\jsch-0.1.42.jar;C:\Users\Pushpa\.m2\repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;C:\Users\Pushpa\.m2\repository\org\apache\zookeeper\zookeeper\3.4.5\zookeeper-3.4.5.jar;C:\Users\Pushpa\.m2\repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;C:\Users\Pushpa\.m2\repository\org\tukaani\xz\1.0\xz-1.0.jar org.apache.hadoop.io.compress.zlib.ZlibCompressor org.apache.hadoop.io.compress.zlib.ZlibDecompressor org.apache.hadoop.security.JniBasedUnixGroupsMapping org.apache.hadoop.io.nativeio.NativeIO org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping org.apache.hadoop.io.compress.snappy.SnappyCompressor org.apache.hadoop.io.compress.snappy.SnappyDecompressor org.apache.hadoop.io.compress.lz4.Lz4Compressor org.apache.hadoop.io.compress.lz4.Lz4Decompressor org.apache.hadoop.util.NativeCrc32""INFOINFO  exec-maven-plugin:1.2:exec (compile-ms-winutils) @ hadoop-common INFO ------------------------------------------------------------------------INFO Reactor Summary:INFOINFO Apache Hadoop Main ................................ SUCCESS 1.641sINFO Apache Hadoop Project POM ......................... SUCCESS 1.520sINFO Apache Hadoop Annotations ......................... SUCCESS 3.854sINFO Apache Hadoop Assemblies .......................... SUCCESS 0.258sINFO Apache Hadoop Project Dist POM .................... SUCCESS 2.229sINFO Apache Hadoop Maven Plugins ....................... SUCCESS 3.851sINFO Apache Hadoop MiniKDC ............................. SUCCESS 3.013sINFO Apache Hadoop Auth ................................ SUCCESS 4.154sINFO Apache Hadoop Auth Examples ....................... SUCCESS 2.457sINFO Apache Hadoop Common .............................. FAILURE 9.363sINFO Apache Hadoop NFS ................................. SKIPPEDINFO Apache Hadoop Common Project ...................... SKIPPEDINFO Apache Hadoop HDFS ................................ SKIPPEDINFO Apache Hadoop HttpFS .............................. SKIPPEDINFO Apache Hadoop HDFS BookKeeper Journal ............. SKIPPEDINFO Apache Hadoop HDFS-NFS ............................ SKIPPEDINFO Apache Hadoop HDFS Project ........................ SKIPPEDINFO hadoop-yarn ....................................... SKIPPEDINFO hadoop-yarn-api ................................... SKIPPEDINFO hadoop-yarn-common ................................ SKIPPEDINFO hadoop-yarn-server ................................ SKIPPEDINFO hadoop-yarn-server-common ......................... SKIPPEDINFO hadoop-yarn-server-nodemanager .................... SKIPPEDINFO hadoop-yarn-server-web-proxy ...................... SKIPPEDINFO hadoop-yarn-server-resourcemanager ................ SKIPPEDINFO hadoop-yarn-server-tests .......................... SKIPPEDINFO hadoop-yarn-client ................................ SKIPPEDINFO hadoop-yarn-applications .......................... SKIPPEDINFO hadoop-yarn-applications-distributedshell ......... SKIPPEDINFO hadoop-mapreduce-client ........................... SKIPPEDINFO hadoop-mapreduce-client-core ...................... SKIPPEDINFO hadoop-yarn-applications-unmanaged-am-launcher .... SKIPPEDINFO hadoop-yarn-site .................................. SKIPPEDINFO hadoop-yarn-project ............................... SKIPPEDINFO hadoop-mapreduce-client-common .................... SKIPPEDINFO hadoop-mapreduce-client-shuffle ................... SKIPPEDINFO hadoop-mapreduce-client-app ....................... SKIPPEDINFO hadoop-mapreduce-client-hs ........................ SKIPPEDINFO hadoop-mapreduce-client-jobclient ................. SKIPPEDINFO hadoop-mapreduce-client-hs-plugins ................ SKIPPEDINFO Apache Hadoop MapReduce Examples .................. SKIPPEDINFO hadoop-mapreduce .................................. SKIPPEDINFO Apache Hadoop MapReduce Streaming ................. SKIPPEDINFO Apache Hadoop Distributed Copy .................... SKIPPEDINFO Apache Hadoop Archives ............................ SKIPPEDINFO Apache Hadoop Rumen ............................... SKIPPEDINFO Apache Hadoop Gridmix ............................. SKIPPEDINFO Apache Hadoop Data Join ........................... SKIPPEDINFO Apache Hadoop Extras .............................. SKIPPEDINFO Apache Hadoop Pipes ............................... SKIPPEDINFO Apache Hadoop OpenStack support ................... SKIPPEDINFO Apache Hadoop Client .............................. SKIPPEDINFO Apache Hadoop Mini-Cluster ........................ SKIPPEDINFO Apache Hadoop Scheduler Load Simulator ............ SKIPPEDINFO Apache Hadoop Tools Dist .......................... SKIPPEDINFO Apache Hadoop Tools ............................... SKIPPEDINFO Apache Hadoop Distribution ........................ SKIPPEDINFO ------------------------------------------------------------------------INFO BUILD FAILUREINFO ------------------------------------------------------------------------INFO Total time: 35.162sINFO Finished at: Sun Dec 15 23:45:56 IST 2013INFO Final Memory: 51M/491MINFO ------------------------------------------------------------------------ERROR Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2:exec (compile-ms-winutils) on project hadoop-common: Command execution failed.Cannot run program ""msbuild"" (in directory ""C:\hdfs\common\hadoop-common-project\hadoop-common""): CreateProcess error=2; The system cannot find the file specified - Help 1ERRORERROR To see the full stack trace of the errors; re-run Maven with the -e switch.ERROR Re-run Maven using the -X switch to enable full debug logging.ERRORERROR For more information about the errors and possible solutions; please read the following articles:ERROR Help 1 http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionExceptionERRORERROR After correcting the problems; you can resume the build with the commandERROR   mvn goals -rf :hadoop-commonC:\hdfs\commonmvn package -Pdist;native-win -DskipTests -Dtar",Resolved,Invalid,,Unassigned,Nalini Ranjan,Sun; 15 Dec 2013 18:23:14 +0000,Thu; 27 Mar 2014 16:51:07 +0000,Thu; 27 Mar 2014 16:51:07 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10166
HADOOP-10167,Improvement,Major,build,Mark hadoop-common source as UTF-8 in Maven pom files / refactoring,"While looking at BIGTOP-831; turned out that the way Bigtop calls maven build / site:site generation causes the errors like this:ERROR Exit code: 1 - /home/user/jenkins/workspace/BigTop-RPM/label/centos-6-x86_64-HAD-1-buildbot/bigtop-repo/build/hadoop/rpm/BUILD/hadoop-2.0.2-alpha-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/source/JvmMetricsInfo.java:31: error: unmappable character for encoding ANSI_X3.4-1968ERROR JvmMetrics(""JVM related metrics etc.""); // record info??Making the whole hadoop-common to use UTF-8 fixes that and seems in general good thing to me.Attaching first version of patch for review.Original issue was observed on openjdk 7 (x86-64).",Closed,Fixed,,Mikhail Antonov,Mikhail Antonov,Sun; 15 Dec 2013 23:32:13 +0000,Wed; 3 Sep 2014 22:27:02 +0000,Thu; 23 Jan 2014 22:35:44 +0000,,2.0.6-alpha,build,BIGTOP-831,,https://issues.apache.org/jira/browse/HADOOP-10167
HADOOP-10168,Bug,Major,,fix javadoc of ReflectionUtils.copy ,In the javadoc of ReflectionUtils.copy; the return value is not documented; the arguments are named incorrectly.,Closed,Fixed,,Thejas M Nair,Thejas M Nair,Tue; 17 Dec 2013 03:18:14 +0000,Mon; 24 Feb 2014 20:56:59 +0000,Tue; 17 Dec 2013 18:25:15 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10168
HADOOP-10169,Improvement,Minor,metrics,remove the unnecessary  synchronized in JvmMetrics class,When i looked into a HBase JvmMetric impl; just found this synchronized seems not essential.,Closed,Fixed,,Liang Xie,Liang Xie,Tue; 17 Dec 2013 05:37:44 +0000,Thu; 12 May 2016 18:27:56 +0000,Fri; 20 Dec 2013 22:09:01 +0000,,2.2.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10169
HADOOP-10170,Bug,Major,build,Unable to compile source code from stable 2.2.0 release.,I have downloaded src ((hadoop-2.2.0-src.tar.gz )) from http://www.carfab.com/apachesoftware/hadoop/common/stable/ While maven build I am getting following errorERROR] C:\hdfs\hadoop-common-project\hadoop-auth\src\test\java\org\apache\hadoop\security\authentication\client\AuthenticatorTestCase.java:88;11 error: cannot access AbstractLifeCycleERROR class file for org.mortbay.component.AbstractLifeCycle not foundERROR C:\hdfs\hadoop-common-project\hadoop-auth\src\test\java\org\apache\hadoop\security\authentication\client\AuthenticatorTestCase.java:96;29 error: cannot access LifeCycleERROR class file for org.mortbay.component.LifeCycle not foundERROR C:\hdfs\hadoop-common-project\hadoop-auth\src\test\java\org\apache\hadoop\security\authentication\client\AuthenticatorTestCase.javaUnable to compile source code from stable 2.2.0 release ( There is a Jira Hadoop-10117  which says it's fixed but couldn't get the stable version ),Resolved,Duplicate,HADOOP-10110,Unassigned,Hadoop Developer,Tue; 17 Dec 2013 06:29:38 +0000,Fri; 20 Dec 2013 21:36:11 +0000,Fri; 20 Dec 2013 21:36:11 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10170
HADOOP-10171,Bug,Major,,TestRPC fails intermittently on jkd7,Branch-2 runs JDK7 which has a random test order. So we get an error in TestRPC (testStopsAllThreads) failing on the AssertEquals.,Closed,Fixed,,Mit Desai,Mit Desai,Tue; 17 Dec 2013 15:53:41 +0000,Wed; 3 Sep 2014 23:33:17 +0000,Wed; 18 Dec 2013 17:13:02 +0000,,2.2.0,java7,,,https://issues.apache.org/jira/browse/HADOOP-10171
HADOOP-10172,Improvement,Critical,ipc,Cache SASL server factories,Performance for SASL server creation is atrocious.  Sasl.createSaslServer does not cache the provider resolution for the factories.  Factory resolution and server instantiation has 3 major contention points.  During bursts of connections; one reader accepting a connection stalls other readers accepting connections; in turn stalling all existing connections handled by those readers.I benched 5 threads at 187 instances/s - total; not per thread.  With this and another change; I've boosted it to 33K instances/s.,Closed,Fixed,,Daryn Sharp,Daryn Sharp,Tue; 17 Dec 2013 18:51:06 +0000,Thu; 12 May 2016 18:27:37 +0000,Thu; 19 Dec 2013 18:28:33 +0000,,0.23.0;2.0.0-alpha;3.0.0-alpha1,,,HADOOP-10173,https://issues.apache.org/jira/browse/HADOOP-10172
HADOOP-10173,Improvement,Critical,ipc,Remove UGI from DIGEST-MD5 SASL server creation,Instantiation of SASL server instances within the readers threads is performed within a UGI.getCurrentUser().doAs.  getCurrentUser is synchronized; and doAs also degrades performance.  GSSAPI (kerberos) requires instantiation within a doAs; but DIGEST-MD5 (token) does not.,Closed,Fixed,,Daryn Sharp,Daryn Sharp,Tue; 17 Dec 2013 18:57:35 +0000,Thu; 12 May 2016 18:27:37 +0000,Thu; 2 Jan 2014 15:01:03 +0000,,0.23.0;2.3.0;3.0.0-alpha1,,,HADOOP-10172;HADOOP-10174,https://issues.apache.org/jira/browse/HADOOP-10173
HADOOP-10174,Improvement,Major,ipc,Run RPC server within the Subject that instantiated it,RPC servers would not require as many doAs blocks if the server threads run within the access control context that instantiates the server.,Open,Unresolved,,Unassigned,Daryn Sharp,Tue; 17 Dec 2013 19:03:29 +0000,Thu; 12 May 2016 18:27:38 +0000,,,0.23.0;2.0.0-alpha;3.0.0-alpha1,,,HADOOP-10173,https://issues.apache.org/jira/browse/HADOOP-10174
HADOOP-10175,Bug,Major,fs,Har files system authority should preserve userinfo,When Har file system parse the URI to get the authority at initialization; the userinfo is not preserved. This may lead to failures if the underlying file system relies on the userinfo to work properly. E.g. har://file-user:passwd@localhost:80/test.har will be parsed to har://file-localhost:80/test.har; where user:passwd is lost in the processing.,Closed,Fixed,,Chuan Liu,Chuan Liu,Thu; 19 Dec 2013 02:32:53 +0000,Thu; 12 May 2016 18:21:39 +0000,Mon; 23 Dec 2013 18:43:17 +0000,,2.2.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10175
HADOOP-10176,Bug,Minor,fs,swiftfs doesn't correctly handle object names starting with slash,"When objects are created in swift prefixed by a slash; swiftfs does not correctly expose the implied directory structure.  For example; given a container with the following objects:/foo/foo/1/foo/2teradatateradata/part-mteradata/part-m-00000teradata/part-m-00001A GET request against that container will return the list above.  A 'hadoop fs -ls swift://container.service/' will return the following:drwxrwxrwx   -          0 2013-12-19 15:49 /foodrwxrwxrwx   -          0 2013-12-19 15:06 /foo/1drwxrwxrwx   -          0 2013-12-19 15:09 /foo/2drwxrwxrwx   -          0 2013-12-04 04:11 /teradataAdditionally; if an object named 'foo' is also created; where a GET will return:/foo/foo/1/foo/2foorcfileteradatateradata/part-mteradata/part-m-00000teradata/part-m-00001then 'hadoop fs -ls swift://container.service/' will return the following:drwxrwxrwx   -          0 2013-12-19 15:49 /foodrwxrwxrwx   -          0 2013-12-19 15:06 /foo/1drwxrwxrwx   -          0 2013-12-19 15:09 /foo/2drwxrwxrwx   -          0 2013-12-19 19:24 /foodrwxrwxrwx   -          0 2013-12-04 04:11 /teradatawhich appears to have a duplicate object ""/foo"".",Open,Unresolved,,Unassigned,David Dobbins,Thu; 19 Dec 2013 19:24:02 +0000,Fri; 3 Jan 2014 14:48:51 +0000,,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10176
HADOOP-10177,Bug,Major,security,Create CLI tools for managing keys via the KeyProvider API,The KeyProvider API provides access to keys; but we need CLI tools to provide the ability to create and delete keys. I'd think it would look something like:,Closed,Fixed,HADOOP-10528,Larry McCay,Owen O'Malley,Fri; 20 Dec 2013 19:35:43 +0000,Mon; 1 Dec 2014 03:07:39 +0000,Thu; 16 Jan 2014 17:49:20 +0000,,,,,HADOOP-9902,https://issues.apache.org/jira/browse/HADOOP-10177
HADOOP-10178,Bug,Major,conf,"Configuration deprecation always emit ""deprecated"" warnings when a new key is used","Even if you use any new configuration properties; you still find ""deprecated"" warnings in your logs. E.g.:13/12/14 01:00:51 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead; use mapreduce.input.fileinputformat.input.dir.recursive",Closed,Fixed,HADOOP-9344,shanyu zhao,shanyu zhao,Sat; 21 Dec 2013 02:29:15 +0000,Wed; 3 Sep 2014 23:33:19 +0000,Fri; 10 Jan 2014 20:20:10 +0000,,2.2.0,,,HADOOP-8167,https://issues.apache.org/jira/browse/HADOOP-10178
HADOOP-10179,Task,Major,,Hey;,,Resolved,Invalid,,Unassigned,Ravi Hemnani,Mon; 23 Dec 2013 14:56:55 +0000,Mon; 23 Dec 2013 19:43:52 +0000,Mon; 23 Dec 2013 19:43:52 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10179
HADOOP-10180,Task,Trivial,,Getting some error while increasing the hadoop cluster size. ,"We have a 5-node hadoop cluster and we are trying to increase the size of the cluster. We have added 2 new disks to each of the 5-boxes and we followed all the steps of putting the disk to the hadoop cluster. Everything works fine except when we restart a datanode; there are errors multiple times in the log file. Following is the error which appears in the log files; 2013-12-23 14:32:19;406 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(172.16.200.128:50010; storageID=DS-1937554000-172.16.200.128-50010-1376068931321; infoPort=50075; ipcPort=50020):DataXceiverorg.apache.hadoop.hdfs.server.datanode.BlockAlreadyExistsException: Block blk_-8997395530627676954_276834 is valid; and cannot be written to.	at org.apache.hadoop.hdfs.server.datanode.FSDataset.writeToBlock(FSDataset.java:1428)	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.init(BlockReceiver.java:114)	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:302)	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:112)	at java.lang.Thread.run(Thread.java:724)",Resolved,Invalid,,Unassigned,Ravi Hemnani,Mon; 23 Dec 2013 15:06:27 +0000,Mon; 23 Dec 2013 19:46:34 +0000,Mon; 23 Dec 2013 19:46:34 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10180
HADOOP-10181,Bug,Minor,metrics,GangliaContext does not work with multicast ganglia setup,The GangliaContext class which is used to send Hadoop metrics to Ganglia uses a DatagramSocket to send these metrics.  This works fine for Ganglia multicast setups that are all on the same VLAN.  However; when working with multiple VLANs; a packet sent via DatagramSocket to a multicast address will end up with a TTL of 1.  Multicast TTL indicates the number of network hops for which a particular multicast packet is valid.  The packets sent by GangliaContext do not make it to ganglia aggregrators on the same multicast group; but in different VLANs.To fix; we'd need a configuration property that specifies that multicast is to be used; and another that allows setting of the multicast packet TTL.  With these set; we could then use MulticastSocket setTimeToLive() instead of just plain ol' DatagramSocket.,Closed,Fixed,,Andrew Johnson,Andrew Otto,Tue; 24 Dec 2013 18:43:48 +0000,Fri; 10 Apr 2015 20:04:26 +0000,Mon; 2 Feb 2015 19:25:20 +0000,,2.6.0,ganglia;hadoop;metrics;multicast,,,https://issues.apache.org/jira/browse/HADOOP-10181
HADOOP-10182,Bug,Major,ha,ZKFailoverController should ensure that parent ZNodes are pre-created during startup,Currently; a manual ZK format step is necessary before ZKFailoverController can join leader election. This generally adds overhead on what is needed to configure an HA cluster. My proposal is to always ensure that parent znodes exist during startup of the ZKFailoverController. If folks do not want to make this a default; we can put it under a config.,Open,Unresolved,,Ivan Mitic,Ivan Mitic,Thu; 26 Dec 2013 20:10:58 +0000,Sat; 7 Jan 2017 01:57:00 +0000,,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10182
HADOOP-10183,Improvement,Major,security,Allow use of UPN style principals in keytab files,Hadoop currently only allows SPN style (E.g. hdfs/node.fqdn@REALM) principals in keytab files in a cluster configured with Kerberos security. This cause the burden of creating multiple principals and keytabs for each node of the cluster. Active Directory allows the use of single principal across multiple hosts if the SPNs for different hosts have been setup correctly on the principal. With this scheme we have the server side using keytab file with UPN style (E.g. hdfs@REALM) principal for a given service for all the nodes of the cluster. The client side will request service tickets with SPN and it's own TGT and Active Directory will grant service tickets with the correct secret. This will simplify the use of principals and keytab files for Active Directory users with one principal for each service across all the nodes of the cluster. I have a patch to allow the use of UPN style principals in Hadoop. The patch will not affect the use of SPN style principals. I couldn't figure out a way to write test cases against MiniKDC so I have included the Oracle/Sun sample Sasl server and client code along with the configuration I used to confirm this scheme works.,Patch Available,Unresolved,,Mubashir Kazia,Mubashir Kazia,Thu; 26 Dec 2013 20:44:35 +0000,Thu; 2 Jun 2016 15:58:42 +0000,,,2.2.0,BB2015-05-TBR,,ZOOKEEPER-2433,https://issues.apache.org/jira/browse/HADOOP-10183
HADOOP-10184,New Feature,Major,fs;security,Hadoop Common changes required to support HDFS ACLs.,This is an umbrella issue for tracking all Hadoop Common changes required to support the HDFS ACLs project.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Thu; 26 Dec 2013 22:58:22 +0000,Tue; 15 Jul 2014 23:20:50 +0000,Wed; 19 Mar 2014 22:27:53 +0000,,HDFS ACLs (HDFS-4685),,,HADOOP-10213;HADOOP-10241;HADOOP-10277;HADOOP-10270;HADOOP-10344;HADOOP-10352;HADOOP-10361;HADOOP-10422;HADOOP-10845,https://issues.apache.org/jira/browse/HADOOP-10184
HADOOP-10185,Sub-task,Major,,FileSystem API for ACLs.,Add new methods to FileSystem for manipulating ACLs.,Resolved,Fixed,,Chris Nauroth,Chris Nauroth,Tue; 3 Dec 2013 19:35:51 +0000,Thu; 26 Dec 2013 23:01:08 +0000,Tue; 10 Dec 2013 17:59:06 +0000,,HDFS ACLs (HDFS-4685),,,HDFS-5638;HADOOP-10186,https://issues.apache.org/jira/browse/HADOOP-10185
HADOOP-10186,Sub-task,Major,security,Remove AclReadFlag and AclWriteFlag in FileSystem API,AclReadFlag and AclWriteFlag intended to capture various options used in getfacl and setfacl. These options determine whether the tool should traverse the filesystem recursively; follow the symlink; etc.; but they are not part of the core ACLs abstractions.The client program has more information and more flexibility to implement these options. This jira proposes to remove these flags to simplify the APIs.,Resolved,Fixed,,Haohui Mai,Haohui Mai,Tue; 10 Dec 2013 23:54:37 +0000,Fri; 27 Dec 2013 20:58:49 +0000,Wed; 11 Dec 2013 20:57:12 +0000,,HDFS ACLs (HDFS-4685),,,HADOOP-10185;HDFS-5599;HDFS-5607;HDFS-5611;HADOOP-10192,https://issues.apache.org/jira/browse/HADOOP-10186
HADOOP-10187,Sub-task,Major,tools,FsShell CLI: add getfacl and setfacl with minimal support for getting and setting ACLs.,Implement and test FsShell CLI commands for getfacl and setfacl.,Resolved,Fixed,HADOOP-10188;HADOOP-10189;HADOOP-10190,Vinayakumar B,Chris Nauroth,Tue; 3 Dec 2013 19:37:58 +0000,Thu; 10 Jul 2014 17:05:05 +0000,Fri; 27 Dec 2013 20:47:51 +0000,,HDFS ACLs (HDFS-4685),,,HDFS-5702;HADOOP-10213;HADOOP-10277,https://issues.apache.org/jira/browse/HADOOP-10187
HADOOP-10188,Sub-task,Major,tools,FsShell CLI: add setfacl flags for ACL entry modification and removal.,Implement and test setfacl support for flags that allow partial modification of an ACL and modification of specific ACL entries.,Resolved,Duplicate,HADOOP-10187,Vinayakumar B,Chris Nauroth,Tue; 3 Dec 2013 19:38:13 +0000,Fri; 27 Dec 2013 23:34:46 +0000,Fri; 27 Dec 2013 23:34:46 +0000,,HDFS ACLs (HDFS-4685),,,,https://issues.apache.org/jira/browse/HADOOP-10188
HADOOP-10189,Sub-task,Major,tools,FsShell CLI: add setfacl flag for removal of default ACL entries.,Implement and test setfacl support for removal of just the default entries in an ACL.,Resolved,Duplicate,HADOOP-10187,Vinayakumar B,Chris Nauroth,Tue; 3 Dec 2013 19:38:28 +0000,Fri; 27 Dec 2013 23:35:05 +0000,Fri; 27 Dec 2013 23:35:05 +0000,,HDFS ACLs (HDFS-4685),,,,https://issues.apache.org/jira/browse/HADOOP-10189
HADOOP-10190,Sub-task,Major,tools,FsShell CLI: add support for recursive flag in ACL commands.,Implement and test handling of recursive flag for getfacl and setfacl.,Resolved,Duplicate,HADOOP-10187,Vinayakumar B,Chris Nauroth,Tue; 3 Dec 2013 19:38:43 +0000,Fri; 27 Dec 2013 23:35:21 +0000,Fri; 27 Dec 2013 23:35:21 +0000,,HDFS ACLs (HDFS-4685),,,,https://issues.apache.org/jira/browse/HADOOP-10190
HADOOP-10191,Bug,Blocker,viewfs,Missing executable permission on viewfs internal dirs,"ViewFileSystem allows 1) unconditional listing of internal directories (mount points) and 2) and changing work directories.1) requires read permission2) requires executable permissionHowever; the hardcoded PERMISSION_RRR == 444 for FileStatus representing an internal dir does not have executable bit set.This confuses YARN localizer for public resources on viewfs because it requires executable permission for ""other"" on all of the ancestor directories of the resource.",Closed,Fixed,,Gera Shegalov,Gera Shegalov,Fri; 27 Dec 2013 00:59:36 +0000,Thu; 4 Sep 2014 01:16:48 +0000,Sat; 22 Mar 2014 05:22:02 +0000,,,,YARN-1542,HADOOP-7257,https://issues.apache.org/jira/browse/HADOOP-10191
HADOOP-10192,Sub-task,Trivial,documentation;fs,FileSystem#getAclStatus has incorrect JavaDocs.,When this API was changed in HADOOP-10186; it stopped returning a RemoteIterator and now returns a single AclStatus.  The JavaDoc needs to be updated accordingly.,Resolved,Fixed,,Chris Nauroth,Chris Nauroth,Fri; 27 Dec 2013 20:22:29 +0000,Fri; 27 Dec 2013 21:00:49 +0000,Fri; 27 Dec 2013 21:00:49 +0000,,HDFS ACLs (HDFS-4685),,,HADOOP-10186,https://issues.apache.org/jira/browse/HADOOP-10192
HADOOP-10193,Bug,Minor,security,hadoop-auth's PseudoAuthenticationHandler can consume getInputStream,I'm trying to use the AuthenticationFilter in front of Apache Solr.  The issue I'm running into is that the PseudoAuthenticationHandler calls ServletRequest.getParameter which affects future calls to ServletRequest.getInputStream.  I.e. from the javadoc: Solr calls getInputStream after the filter and errors result.,Closed,Fixed,,Gregory Chanan,Gregory Chanan,Sat; 28 Dec 2013 01:14:21 +0000,Fri; 7 Mar 2014 02:25:52 +0000,Mon; 6 Jan 2014 18:20:22 +0000,,,,,HIVE-6576,https://issues.apache.org/jira/browse/HADOOP-10193
HADOOP-10194,Bug,Major,native,CLONE - setnetgrent in native code is not portable,HADOOP-6864 uses the setnetgrent function in a way which is not compatible with BSD APIs; where the call returns void rather than int. This prevents the native libs from building on OSX; for example.,Resolved,Duplicate,HADOOP-10699,Unassigned,carltone,Mon; 30 Dec 2013 05:58:02 +0000,Fri; 13 Feb 2015 20:32:01 +0000,Fri; 13 Feb 2015 20:32:01 +0000,,0.22.0;0.23.0,,,HADOOP-7824;HADOOP-6864,https://issues.apache.org/jira/browse/HADOOP-10194
HADOOP-10195,Bug,Major,fs,swiftfs object list stops at 10000 objects,listing objects in a container in swift is limited to 10000 objects per request. swiftfs only makes one request and is therefore limited to the first 10000 objects in the container; ignoring any remaining objects,Patch Available,Unresolved,,David Dobbins,David Dobbins,Mon; 30 Dec 2013 12:31:59 +0000,Wed; 6 May 2015 03:26:28 +0000,,,2.3.0;2.6.0,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10195
HADOOP-10196,Bug,Major,io,Bzip2Codec Compress cannot work,"Bzip2Codec Uncompress cannot work.1. Compress Sample file:hadoop@localhost ~$ cat StreamCompressor.javaimport org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.compress.CompressionOutputStream;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.util.ReflectionUtils;public class StreamCompressor {public static void main(String[] args) throws Exception{ String codecClassname = args[0]; Class? codecClass = Class.forName(codecClassname); Configuration conf = new Configuration(); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass; conf); CompressionOutputStream out = codec.createOutputStream(System.out); IOUtils.copyBytes(System.in; out; 4096; false); out.finish(); }}2. Uncompress Sample file:hadoop@localhost ~$ cat StreamUncompressor.javaimport org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.compress.CompressionOutputStream;import org.apache.hadoop.io.compress.CompressionInputStream;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.util.ReflectionUtils;public class StreamUncompressor {public static void main(String[] args) throws Exception{ String codecClassname = args[0]; Class? codecClass = Class.forName(codecClassname); Configuration conf = new Configuration(); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass; conf); CompressionInputStream in = codec.createInputStream(System.in); IOUtils.copyBytes(in; System.out; 4096; false); in.close(); }}2. How to compile/run1) javac -classpath /usr/lib/gphd/hadoop/hadoop-common-2.0.5-alpha-gphd-2.1.1.0.jar StreamCompressor.java2) javac -classpath /usr/lib/gphd/hadoop/hadoop-common-2.0.5-alpha-gphd-2.1.1.0.jar StreamUncompressor.java3) jar -cvf Stream.jar StreamCompressor.class StreamUncompressor.class4) rm -rf /tmp/my.txt.bz2 &amp; echo abc  /tmp/my.txt &amp; bzip2 /tmp/my.txt &amp; cat /tmp/my.txt.bz2 | hadoop jar ./Stream.jar StreamUncompressor org.apache.hadoop.io.compress.BZip2Codec5) echo ""text"" | hadoop jar ./Stream.jar StreamCompressor org.apache.hadoop.io.compress.BZip2Codec | bzcat3. Test ResultFrom test; hadoop doesn't support native bzip2 and java bzip2.1) hadoop support bzip2 uncompress.rm -rf /tmp/my.txt.bz2 &amp; echo abc  /tmp/my.txt &amp; bzip2 /tmp/my.txt &amp; cat /tmp/my.txt.bz2 | hadoop jar ./Stream.jar StreamUncompressor org.apache.hadoop.io.compress.BZip2Codec13/12/17 03:58:20 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native; will use pure-Java versionabc &lt; expect2) bzip2 compress cannot work as following:a) hadoop@localhost hadoop$ echo ""text"" | hadoop jar ./Stream.jar StreamCompressor org.apache.hadoop.io.compress.BZip2Codec13/12/17 04:00:59 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native; will use pure-Java versionBZ &lt;&lt; not expectb) hadoop@localhost hadoop$ echo ""text"" | hadoop jar ./Stream.jar StreamCompressor org.apache.hadoop.io.compress.BZip2Codec | bzcat13/12/17 04:01:31 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native; will use pure-Java versionbzcat: Compressed file ends unexpectedly;perhaps it is corrupted? Possible reason follows.bzcat: Invalid argumentInput file = (stdin); output file = (stdout)It is possible that the compressed file(s) have become corrupted.You can use the -tvv option to test integrity of such files.You can use the `bzip2recover' program to attempt to recoverdata from undamaged sections of corrupted files.",Open,Unresolved,,Unassigned,Guo Ruijing,Tue; 31 Dec 2013 06:07:13 +0000,Tue; 31 Dec 2013 08:31:10 +0000,,,2.0.2-alpha;2.0.5-alpha;2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10196
HADOOP-10197,Improvement,Major,build,Disable additional m2eclipse plugin execution,M2Eclipse complains on when importing the maven modules into Eclipse.We should add more filter in the org.eclipse.m2e.lifecycle-mapping plugin.,Patch Available,Unresolved,,Unassigned,Eric Charles,Thu; 2 Jan 2014 13:39:53 +0000,Tue; 28 Jun 2016 13:55:18 +0000,,,3.0.0-alpha1,,,HADOOP-10574,https://issues.apache.org/jira/browse/HADOOP-10197
HADOOP-10198,Improvement,Minor,native,DomainSocket: add support for socketpair,Add support for DomainSocket#socketpair.  This function uses the POSIX function of the same name to create two UNIX domain sockets which are connected to each other.  This will be useful for HDFS-5182.,Closed,Fixed,,Colin P. McCabe,Colin P. McCabe,Thu; 2 Jan 2014 19:05:57 +0000,Mon; 24 Feb 2014 20:56:51 +0000,Fri; 3 Jan 2014 19:06:33 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10198
HADOOP-10199,Bug,Blocker,,Precommit Admin build is not running because no previous successful build is available,It seems at some point the builds started failing for an unknown reason and eventually the last successful was rolled off. At that point the precommit builds started failing because they pull an artifact from the last successful build.,Resolved,Fixed,,Brock Noland,Brock Noland,Thu; 2 Jan 2014 19:22:36 +0000,Fri; 3 Jan 2014 14:47:15 +0000,Thu; 2 Jan 2014 19:50:08 +0000,,,,,HADOOP-10200,https://issues.apache.org/jira/browse/HADOOP-10199
HADOOP-10200,Bug,Major,,Fix precommit script patch_tested.txt fallback option,"HADOOP-10199 created a ""fallback"" option when there is successful artifact. However that fallback option used the jenkins lastBuild build indicator. It appears that does not mean the last completed build; but strictly the last build; which in this context is the current build. The current build is running so it doesn't have any artifacts.",Resolved,Fixed,,Brock Noland,Brock Noland,Thu; 2 Jan 2014 21:17:19 +0000,Fri; 3 Jan 2014 14:47:15 +0000,Fri; 3 Jan 2014 04:03:36 +0000,,,,,HADOOP-10199,https://issues.apache.org/jira/browse/HADOOP-10200
HADOOP-10201,Sub-task,Major,security,Add Listing Support to Key Management APIs,Extend the key management APIs from HADOOP-10141 to include the ability to list the available keys.,Closed,Fixed,,Larry McCay,Larry McCay,Thu; 2 Jan 2014 23:35:21 +0000,Fri; 12 Feb 2016 02:07:47 +0000,Mon; 6 Jan 2014 23:43:48 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10201
HADOOP-10202,Task,Minor,,OK_JAVADOC_WARNINGS is out of date; leading to negative javadoc warning count,From https://builds.apache.org/job/PreCommit-HDFS-Build/5813//testReport/ : OK_JAVADOC_WARNINGS should be updated.,Resolved,Cannot Reproduce,,Unassigned,Ted Yu,Fri; 3 Jan 2014 00:07:59 +0000,Thu; 28 Sep 2017 17:16:16 +0000,Thu; 28 Sep 2017 17:16:16 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10202
HADOOP-10203,Bug,Major,fs/s3,Connection leak in Jets3tNativeFileSystemStore#retrieveMetadata ,Jets3tNativeFileSystemStore#retrieveMetadata  is leaking connections. This affects any client that tries to read many small files very quickly (e.g. distcp from s3 to hdfs with small files blocks due to connection pool starvation). This is not a problem for larger files because when the GC runs any connection that's out of scope will be released in #finalize().We are seeing the following log messages as a symptom of this problem:,Closed,Fixed,,Andrei Savu,Andrei Savu,Fri; 3 Jan 2014 23:16:12 +0000,Mon; 24 Feb 2014 20:58:36 +0000,Mon; 27 Jan 2014 16:22:51 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10203
HADOOP-10204,Bug,Minor,fs,ThrottledInputStream should #close() the underlying stream ,While working on HADOOP-10203 I've noticed that ThrottledInputStream (DistCP V2) does not override #close(). This can also leak connection.,Resolved,Invalid,,Andrei Savu,Andrei Savu,Fri; 3 Jan 2014 23:34:42 +0000,Mon; 6 Jan 2014 19:22:42 +0000,Mon; 6 Jan 2014 19:22:42 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10204
HADOOP-10205,Wish,Minor,,progress bar for Hadoop FS CLI,It could be really nice to have a progress bar when you use certain commands like move or copy.I think that it should be exposed as a WebPage\JSON like map reduce jobs; in order to be able to monitor the command's progress from a few computers at once.,Open,Unresolved,,Unassigned,bl3nder,Mon; 6 Jan 2014 11:50:42 +0000,Mon; 6 Jan 2014 11:50:42 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10205
HADOOP-10206,Improvement,Major,,Port WASB HBase support to Hadoop 2.0,A series of changes for HBase support on Hadoop 2.0. These changes include support for page blobs; fixes to allows HBase logging to block blobs; and support for hsync and hflush methods in the Syncable interface.,Resolved,Duplicate,HADOOP-10809,Unassigned,Dexter Bradshaw,Mon; 6 Jan 2014 17:46:35 +0000,Fri; 13 Feb 2015 21:19:10 +0000,Fri; 13 Feb 2015 21:19:09 +0000,,2.2.0,patch,HADOOP-8645,,https://issues.apache.org/jira/browse/HADOOP-10206
HADOOP-10207,Test,Minor,,TestUserGroupInformation#testLogin is flaky,This test depends on the execution order of tests. If TestUserGroupInformation#testGetServerSideGroups() runs first; TestUserGroupInformation#testLogin will fail.,Closed,Fixed,,Jimmy Xiang,Jimmy Xiang,Mon; 6 Jan 2014 22:12:56 +0000,Wed; 3 Sep 2014 23:33:21 +0000,Wed; 8 Jan 2014 23:28:35 +0000,,,,,HDFS-5220,https://issues.apache.org/jira/browse/HADOOP-10207
HADOOP-10208,Improvement,Trivial,,Remove duplicate initialization in StringUtils.getStringCollection,The values is initialized twice.,Closed,Fixed,,Benoy Antony,Benoy Antony,Tue; 7 Jan 2014 00:31:58 +0000,Mon; 24 Feb 2014 20:56:44 +0000,Wed; 8 Jan 2014 18:43:41 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10208
HADOOP-10209,Bug,Major,auto-failover,Fix findbugs in ActiveStandbyElector,findBugs points out some multithreaded correctness warnings in ActiveStandbyElector. We should verify and fix those.,Resolved,Duplicate,HADOOP-10214,Unassigned,Karthik Kambatla,Tue; 7 Jan 2014 02:11:16 +0000,Thu; 14 Apr 2016 04:54:36 +0000,Thu; 9 Jan 2014 06:27:13 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10209
MAPREDUCE-5713,Bug,Trivial,documentation,InputFormat and JobConf JavaDoc Fixes,"https://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/mapred/InputFormat.htmlInstead of ""record boundaries are to respected""Should be ""record boundaries are to be respected""https://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/mapred/JobConf.htmlInstead of ""some parameters interact subtly rest of the framework""Should be ""some parameters interact subtly with the rest of the framework""",Closed,Fixed,,Chen He,Ben Robie,Tue; 7 Jan 2014 19:17:05 +0000,Wed; 3 Sep 2014 20:33:53 +0000,Thu; 13 Mar 2014 16:51:20 +0000,,1.2.1;2.2.0,,,,https://issues.apache.org/jira/browse/MAPREDUCE-5713
HADOOP-10211,Improvement,Major,security,Enable RPC protocol to negotiate SASL-QOP values between clients and servers,SASL allows different types of protection are referred to as the quality of protection (qop). It is negotiated between the client and server during the authentication phase of the SASL exchange. Currently hadoop allows specifying a single QOP value  via hadoop.rpc.protection. The enhancement enables a user to specify multiple QOP values -  authentication; integrity; privacy as a comma separated list via hadoop.rpc.protectionThe client and server can have different set of values for  hadoop.rpc.protection and they will negotiate to determine the QOP to be used for communication.,Closed,Fixed,,Benoy Antony,Benoy Antony,Tue; 7 Jan 2014 20:33:35 +0000,Mon; 11 May 2015 17:27:46 +0000,Wed; 5 Mar 2014 22:34:01 +0000,,2.2.0,,,HADOOP-10057,https://issues.apache.org/jira/browse/HADOOP-10211
HADOOP-10212,Bug,Major,documentation,Incorrect compile command in Native Library document,The following old command still exists in Native Library document. Now maven is used instead of ant.,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Wed; 8 Jan 2014 06:36:36 +0000,Wed; 3 Sep 2014 23:33:21 +0000,Mon; 27 Jan 2014 20:41:30 +0000,,2.2.0,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10212
HADOOP-10213,Bug,Major,tools,Fix bugs parsing ACL spec in FsShell setfacl.,When calling setfacl -x to remove ACL entries; it does not make sense for the entries in the ACL spec to contain permissions.  The permissions should be unspecified; and the CLI should return an error if the user attempts to provide permissions.,Resolved,Fixed,,Vinayakumar B,Chris Nauroth,Wed; 8 Jan 2014 23:06:27 +0000,Tue; 21 Jan 2014 04:14:02 +0000,Mon; 20 Jan 2014 18:03:05 +0000,,HDFS ACLs (HDFS-4685),,,HADOOP-10187;HADOOP-10184,https://issues.apache.org/jira/browse/HADOOP-10213
HADOOP-10214,Bug,Major,ha,Fix multithreaded correctness warnings in ActiveStandbyElector,When i worked at HADOOP-9420; found the unrelated findbugs warning:https://builds.apache.org/job/PreCommit-HADOOP-Build/3408//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html,Closed,Fixed,HADOOP-10209,Liang Xie,Liang Xie,Thu; 9 Jan 2014 04:10:55 +0000,Thu; 12 May 2016 18:23:20 +0000,Thu; 9 Jan 2014 06:38:25 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10214
HDFS-5842,Bug,Major,security,Cannot create hftp filesystem when using a proxy user ugi and a doAs on a secure cluster,Noticed this while debugging issues in another application. We saw an error when trying to do a FileSystem.get using an hftp file system on a secure cluster using a proxy user ugi.This is a small snippet used The same code worked for hdfs and webhdfs but not for hftp when the ugi used was UserGroupInformation.createProxyUser,Closed,Fixed,,Jing Zhao,Arpit Gupta,Thu; 9 Jan 2014 05:06:39 +0000,Mon; 24 Feb 2014 20:57:11 +0000,Wed; 29 Jan 2014 22:14:24 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HDFS-5842
HADOOP-10216,Improvement,Major,fs,Unnecessary disk check triggered when socket operation has problem.,"When BlockReceiver transfer data fails; it can be found SocketOutputStream translates the exception as IOException with the message ""The stream is closed"":2014-01-06 11:48:04;716 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver.run():java.io.IOException: The stream is closed        at org.apache.hadoop.net.SocketOutputStream.write        at java.io.BufferedOutputStream.flushBuffer        at java.io.BufferedOutputStream.flush        at java.io.DataOutputStream.flush        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run        at java.lang.Thread.runWhich makes the checkDiskError method of DataNode called and triggers the disk scan.Can we make the modifications like below in checkDiskError to avoiding this unneccessary disk scan operations?:",Resolved,Invalid,,Unassigned,MaoYuan Xian,Thu; 9 Jan 2014 07:33:48 +0000,Thu; 9 Jan 2014 07:36:28 +0000,Thu; 9 Jan 2014 07:36:28 +0000,,1.1.2,,,,https://issues.apache.org/jira/browse/HADOOP-10216
HADOOP-10217,Bug,Major,conf,Unable to run 'hadoop' commands; after installing on Cygwin,Did following1. export JAVA_HOME=/cygdrive/e/JDK2. export HADOOP_INSTALL=/cygdrive/e/hadoop-2.2.03. export PATH=:$PATH:$HADOOP_INSTALL/bin:$HADOOP_INSTALL/sbin:$HADOOP_INSTALL/etc:$HADOOP_INSTALL/share:$HADOOP_INSTALL/lib:$HADOOP_INSTALL/libexec$hadoop versionError: Could not find or load main class org.apache.hadoop.util.VersionInfo.Cannot run anymore commands. I am unable to detect what path problems is causing this error,Resolved,Invalid,,Unassigned,Anand Murali,Fri; 10 Jan 2014 10:12:07 +0000,Sat; 10 Jan 2015 17:28:34 +0000,Fri; 10 Jan 2014 19:30:21 +0000,,2.2.0,test,,HADOOP-11464,https://issues.apache.org/jira/browse/HADOOP-10217
HADOOP-10218,Bug,Major,fs/s3,Using brace glob pattern in S3N URL causes exception due to Path created with empty string,When using a brace glob pattern inside a S3 URL; an exception is thrown because a Path is constructed with the empty string. The simplest reproduction case I've found is: It does not seem to make a difference whether any file exists that match the pattern. The problem only seems to affect buckets with public read access. The private buckets tried seem to work fine. When running through a Hadoop step; the following backtrace was produced: Furthermore; interestingly; the following works: but this fails:,Open,Unresolved,,Unassigned,Bj√∂rn Ramberg,Fri; 10 Jan 2014 11:32:17 +0000,Fri; 16 Jun 2017 10:31:41 +0000,,,1.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-10218
HADOOP-10219,Bug,Major,ipc,ipc.Client.setupIOstreams() needs to check for ClientCache.stopClient requested shutdowns ,When ClientCache.stopClient() is called to stop the IPC client; if the clientis blocked spinning due to a connectivity problem; it does not exit until the policy has timed out -so the stopClient() operation can hang for an extended period of time.This can surface in the shutdown hook of FileSystem.cache.closeAll(),Patch Available,Unresolved,,Kihwal Lee,Steve Loughran,Fri; 10 Jan 2014 15:22:31 +0000,Tue; 22 Mar 2016 11:15:54 +0000,,,2.2.0;2.6.0,,,HADOOP-12464;HADOOP-12950,https://issues.apache.org/jira/browse/HADOOP-10219
HADOOP-10220,Sub-task,Major,fs,Add ACL indicator bit to FsPermission.,This patch will take a previously unused bit in the 16-bit FsPermission and use it to indicate the presence of an ACL on the file or directory.  The CLI will use this to display an indicator in the permission string if an ACL is present.  The NameNode will use it in ACL modification APIs; permission checks; and display of an indicator in the directory browser web UI.,Resolved,Fixed,,Chris Nauroth,Chris Nauroth,Fri; 10 Jan 2014 17:22:13 +0000,Fri; 14 Nov 2014 17:55:10 +0000,Sat; 11 Jan 2014 00:38:42 +0000,,HDFS ACLs (HDFS-4685),,,HADOOP-10354;HDFS-5621;HDFS-7384,https://issues.apache.org/jira/browse/HADOOP-10220
HADOOP-10221,Improvement,Major,security,Add a plugin to specify SaslProperties for RPC protocol based on connection properties,Add a plugin to specify SaslProperties for RPC protocol based on connection properties.HADOOP-10211 enables client and server to specify and support multiple QOP.  Some connections needs to be restricted to a specific set of QOP based on connection properties.Eg. connections from client from a specific subnet needs to be encrypted (QOP=privacy),Closed,Fixed,,Benoy Antony,Benoy Antony,Fri; 10 Jan 2014 21:31:40 +0000,Thu; 4 Sep 2014 01:16:48 +0000,Wed; 19 Mar 2014 20:20:04 +0000,,2.2.0,,,HDFS-5910;HADOOP-10451;HADOOP-10547,https://issues.apache.org/jira/browse/HADOOP-10221
HADOOP-10222,Test,Minor,test,TestSSLHttpServer creates ssl-server.xml in hadoop-common test classes without cleaning up,"TestSSLHttpServer creates ssl-server.xml hadoop-common test-classes without cleaning up. Other tests could pick up the ssl-server configuration file and get the wrong keystore and truststore.Got the following exception while running org.apache.hadoop.hdfs.TestNameNodeHttpServer and the cause is TestNameNodeHttpServer is picking up the ssl-server.xml file written by TestSSLHttpServerjava.io.IOException: Keystore was tampered with; or password was incorrect	at com.ibm.crypto.provider.JavaKeyStore.engineLoad(Unknown Source)	at java.security.KeyStore.load(KeyStore.java:414)	at org.mortbay.jetty.security.SslSocketConnector.createFactory(SslSocketConnector.java:246)	at org.mortbay.jetty.security.SslSocketConnector.newServerSocket(SslSocketConnector.java:476)	at org.mortbay.jetty.bio.SocketConnector.open(SocketConnector.java:73)	at org.mortbay.jetty.AbstractConnector.doStart(AbstractConnector.java:283)	at org.mortbay.jetty.bio.SocketConnector.doStart(SocketConnector.java:147)	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)	at org.mortbay.jetty.Server.doStart(Server.java:235)	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)	at org.apache.hadoop.http.HttpServer.start(HttpServer.java:692)	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:158)	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:626)	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:488)	at org.apache.hadoop.hdfs.server.namenode.NameNode.init(NameNode.java:684)	at org.apache.hadoop.hdfs.server.namenode.NameNode.init(NameNode.java:669)	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1254)	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:893)	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:784)	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:642)	at org.apache.hadoop.hdfs.MiniDFSCluster.init(MiniDFSCluster.java:334)	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:316)	at org.apache.hadoop.hdfs.TestNameNodeHttpServer.testSslConfiguration(TestNameNodeHttpServer.java:38)",Open,Unresolved,,Unassigned,Jinghui Wang,Fri; 10 Jan 2014 22:22:06 +0000,Mon; 28 Sep 2015 18:59:34 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10222
HADOOP-10223,Bug,Minor,,MiniKdc#main() should close the FileReader it creates,FileReader is used to read MiniKDC properties.This FileReader should be closed after reading.,Closed,Fixed,,Ted Yu,Ted Yu,Fri; 10 Jan 2014 23:49:43 +0000,Mon; 24 Feb 2014 20:57:44 +0000,Sun; 12 Jan 2014 23:27:32 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10223
HADOOP-10224,Bug,Major,security,JavaKeyStoreProvider has to protect against corrupting underlying store,Java keystores get corrupted at times. A key management operation that writes the store to disk could cause a corruption and all protected data would then be unaccessible.,Closed,Fixed,HADOOP-10869,Arun Suresh,Larry McCay,Sat; 11 Jan 2014 00:02:38 +0000,Mon; 1 Dec 2014 03:08:25 +0000,Sat; 9 Aug 2014 00:01:15 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10224
HADOOP-10225,Improvement,Major,build;documentation,Publish Maven javadoc and sources artifacts with Hadoop releases.,Right now Maven javadoc and sources artifacts do not accompany Hadoop releases within Maven central. This means that one needs to checkout source code to DEBUG aspects of the codebase... this is not user friendly.The build script(s) should be amended to accommodate publication of javadoc and sources artifacts alongside pom and jar artifacts. Some history on this conversation can be seen belowhttp://s.apache.org/7qR,Open,Unresolved,,Lewis John McGibbney,Lewis John McGibbney,Sun; 12 Jan 2014 14:05:14 +0000,Tue; 14 Nov 2017 19:24:26 +0000,,,3.0.0-alpha1,hadoop;javadoc;maven;sources,,HADOOP-6635,https://issues.apache.org/jira/browse/HADOOP-10225
HADOOP-10226,Bug,Critical,bin,Help! My Hadoop doesn't work!,I have installed hadoop but it it is failing please help!!,Resolved,Invalid,,Unassigned,Steve Loughran,Sun; 12 Jan 2014 21:09:13 +0000,Sun; 12 Jan 2014 21:09:55 +0000,Sun; 12 Jan 2014 21:09:55 +0000,,1.0.3,,,,https://issues.apache.org/jira/browse/HADOOP-10226
HADOOP-10227,Bug,Minor,ipc,IPC shutdown hangs if remote connection not reachable,If the remote HDFS namenode isn't reachable; and an attempt to shut down the JVM is made; the process will keep running,Resolved,Duplicate,NULL,Unassigned,Steve Loughran,Sun; 12 Jan 2014 21:19:37 +0000,Sun; 12 Jan 2014 21:26:15 +0000,Sun; 12 Jan 2014 21:26:15 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10227
HADOOP-10228,Improvement,Minor,fs,FsPermission#fromShort() should cache FsAction.values(),FsPermission#fromShort() calls FsAction.values() every time; which causes unnecessary performance penalty.,Closed,Fixed,,Haohui Mai,Haohui Mai,Mon; 13 Jan 2014 23:43:50 +0000,Wed; 3 Sep 2014 23:33:18 +0000,Tue; 14 Jan 2014 20:37:29 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10228
HADOOP-10229,Bug,Minor,,DaemonFactory should not extend Daemon,The static nested class org.apache.hadoop.util.Daemon.DaemonFactory unnecessarily extends its nesting class Daemon; though a thread factory is not required to be a thread.,Open,Unresolved,,Unassigned,Hiroshi Ikeda,Tue; 14 Jan 2014 02:38:46 +0000,Fri; 11 Apr 2014 10:46:55 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10229
HADOOP-10230,Bug,Trivial,,GSetByHashMap breaks contract of GSet,The contract of GSet says it is ensured to throw NullPointerException if a given argument is null for many methods; but GSetByHashMap doesn't. I think just writing non-null preconditions for GSet are required.,Patch Available,Unresolved,,Andres Perez,Hiroshi Ikeda,Tue; 14 Jan 2014 05:14:18 +0000,Mon; 16 May 2016 03:20:59 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10230
HADOOP-10231,Improvement,Minor,documentation,Add some components in Native Libraries document,The documented components in Native Libraries are only zlib and gzip.Now Native Libraries includes some other components such as other compression formats (lz4; snappy); libhdfs and fuse module. These components should be documented.,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Tue; 14 Jan 2014 06:28:54 +0000,Mon; 1 Dec 2014 03:09:55 +0000,Thu; 14 Aug 2014 21:40:55 +0000,,2.2.0,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10231
HADOOP-10232,Bug,Major,,More options to HttpServer Builder,o.a.h.h.HttpServer can can be instanciated and configured:1. Via classical constructor2. Via static build methodThose 2 methods don't populate the same way the (deprecated) hostname and port; nor the jetty Connector.This gives issue when using hbase on hadoop3 (HBASE-6581),Resolved,Won't Fix,,Unassigned,Eric Charles,Mon; 13 Jan 2014 14:47:44 +0000,Thu; 12 May 2016 18:25:06 +0000,Thu; 16 Jan 2014 04:55:11 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10232
HADOOP-10233,Bug,Critical,ipc,RPC lacks output flow control,"The RPC layer has input flow control via the callq; however it lacks any output flow control.  A handler will try to directly send the response.  If the full response is not sent then it is queued for the background responder thread.  The RPC layer may end up queuing so many buffers that it ""locks"" up in GC.",Resolved,Duplicate,HADOOP-8942,Unassigned,Daryn Sharp,Tue; 14 Jan 2014 22:58:11 +0000,Thu; 12 May 2016 18:25:43 +0000,Wed; 29 Jan 2014 20:12:34 +0000,,2.0.0-alpha;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10233
HADOOP-10234,Bug,Major,scripts,hadoop.cmd jar does not propagate exit code.,"Running ""hadoop.cmd jar"" does not always propagate the exit code to the caller.  In interactive use; it works fine.  However; in some usages (notably Hive); it gets called through Shell#getRunScriptCommand; which needs to do an intermediate ""cmd /c"" to execute the script.  In that case; the last exit code is getting dropped; so Hive can't detect job failures.",Closed,Fixed,,Chris Nauroth,Chris Nauroth,Tue; 14 Jan 2014 23:44:02 +0000,Mon; 24 Feb 2014 20:57:34 +0000,Wed; 15 Jan 2014 05:49:45 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10234
HADOOP-10235,Bug,Major,build,Hadoop tarball has 2 versions of stax-api JARs,They are:stax-api-1.0-2.jarstax-api-1.0.2.jarMaven cannot resolve them property because they are published under different groupIds; because of that for maven are different things.we need to exclude one of them explicitly,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Wed; 15 Jan 2014 01:13:55 +0000,Thu; 12 May 2016 18:27:12 +0000,Fri; 17 Jan 2014 19:31:22 +0000,,2.3.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10235
HADOOP-10236,Bug,Trivial,,Fix typo in o.a.h.ipc.Client#checkResponse,There's a typo in o.a.h.ipc.Client.java.  It should be fixed as follows:,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Wed; 15 Jan 2014 01:32:40 +0000,Mon; 24 Feb 2014 20:58:15 +0000,Wed; 15 Jan 2014 18:33:13 +0000,,2.2.0,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10236
HADOOP-10237,Bug,Major,security,JavaKeyStoreProvider needs to set keystore permissions properly,In order protect access to the created keystores permissions should initially be set to 700 by the JavaKeyStoreProvider. Subsequent permission changes can then be done using FS.,Closed,Fixed,,Larry McCay,Larry McCay,Thu; 16 Jan 2014 18:45:03 +0000,Mon; 1 Dec 2014 03:09:03 +0000,Fri; 28 Mar 2014 15:58:00 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10237
HADOOP-10238,Bug,Major,security,Decouple the Creation of Key metadata from the creation of a key version,Currently; KeyProvider createKey establishes a new key metadata and an initial version of the key. This should be separated such that a key is created and an initial version is then realized through the KeyProvider rollNewVersion method.,Open,Unresolved,,Larry McCay,Larry McCay,Thu; 16 Jan 2014 18:50:07 +0000,Thu; 16 Jan 2014 18:50:07 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10238
HADOOP-10239,Task,Major,documentation,Add Spark as a related project on the Hadoop page,,Resolved,Fixed,,Unassigned,Reynold Xin,Fri; 17 Jan 2014 06:57:20 +0000,Thu; 23 Jan 2014 19:49:41 +0000,Thu; 23 Jan 2014 19:36:23 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10239
HADOOP-10240,Bug,Trivial,documentation,Windows build instructions incorrectly state requirement of protoc 2.4.1 instead of 2.5.0,When we upgraded to protoc 2.5.0; we updated BUILDING.txt to state the new requirement.  However; there is another section of the document for Windows builds; and that section still lists version 2.4.1.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Fri; 17 Jan 2014 19:57:15 +0000,Wed; 3 Sep 2014 23:33:16 +0000,Fri; 17 Jan 2014 23:33:00 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10240
HADOOP-10241,Bug,Major,fs,Clean up output of FsShell getfacl.,"This patch will clean up a few formatting issues in the output of the getfacl command:	Currently; the file name is output as the full URI with ""hdfs"" scheme.  We'll change this to print just the path portion for consistency with other shell commands like ls.	Print a blank line after printing an ACL.  Linux getfacl does this.  It's particularly helpful with getfacl -R; so that you get a visual indicator in between each returned ACL.	The '+' indicator appended to the permissions when the ACL bit is on throws off the nicely aligned table formatting of ls.",Resolved,Fixed,,Chris Nauroth,Chris Nauroth,Sat; 18 Jan 2014 17:34:04 +0000,Tue; 25 Feb 2014 00:26:59 +0000,Tue; 21 Jan 2014 21:21:05 +0000,,HDFS ACLs (HDFS-4685),,,HDFS-5702;HADOOP-10247;HADOOP-10184;HADOOP-10361,https://issues.apache.org/jira/browse/HADOOP-10241
MAPREDUCE-6145,Bug,Trivial,,No space in an error output message,"nzhdusr@nhga0007 ~$ hadoop job -history ~/1Exception in thread ""main"" java.io.IOException: Not able to initialize History viewer	at org.apache.hadoop.mapred.HistoryViewer.init(HistoryViewer.java:95)	at org.apache.hadoop.mapred.JobClient.viewHistory(JobClient.java:1917)	at org.apache.hadoop.mapred.JobClient.run(JobClient.java:1866)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)	at org.apache.hadoop.mapred.JobClient.main(JobClient.java:2123)Caused by: java.io.IOException: History directory /home/nzhdusr/1/_logs/historydoes not exist",Resolved,Not A Problem,,Dasha Boudnik,Anton Balashov,Mon; 20 Jan 2014 05:59:01 +0000,Tue; 17 Mar 2015 09:58:25 +0000,Tue; 17 Mar 2015 09:58:25 +0000,,1.3.0,,,,https://issues.apache.org/jira/browse/MAPREDUCE-6145
HDFS-5800,Bug,Trivial,hdfs-client,Typo: soft-limit for hard-limit in DFSClient,"In DFSClient#renewLease; there is a log message as follows. This log message includes ""soft-limit"" but; considering the context; I think it's typo for ""hard-limit"".",Closed,Fixed,,Kousuke Saruta,Kousuke Saruta,Mon; 20 Jan 2014 06:31:15 +0000,Mon; 24 Feb 2014 20:58:28 +0000,Mon; 20 Jan 2014 14:10:48 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-5800
HADOOP-10244,Bug,Major,security,TestKeyShell improperly tests the results of a Delete,The TestKeyShell.testKeySuccessfulKeyLifecycle test is supposed to ensure that the deleted key is no longer in the results of a subsequent delete command. Mistakenly; it is testing that it is STILL there.The delete command is actually working but the stdout capture should be reset instead of flushed. Therefore; the test is picking up the existence of the key name from the deletion message in the previous command.,Closed,Fixed,,Larry McCay,Larry McCay,Mon; 20 Jan 2014 20:22:47 +0000,Mon; 1 Dec 2014 03:10:31 +0000,Tue; 4 Feb 2014 00:12:15 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10244
HADOOP-10245,Bug,Major,bin;scripts,"Hadoop command line always appends ""-Xmx"" option twice","The Hadoop command line scripts (hadoop.sh or hadoop.cmd) will call java with ""-Xmx"" options twice. The impact is that any user defined HADOOP_HEAP_SIZE env variable will take no effect because it is overwritten by the second ""-Xmx"" option.For example; here is the java cmd generated for command ""hadoop fs -ls /""; Notice that there are two ""-Xmx"" options: ""-Xmx1000m"" and ""-Xmx512m"" in the command line:java -Xmx1000m  -Dhadoop.log.dir=C:\tmp\logs -Dhadoop.log.file=hadoop.log -Dhadoop.root.logger=INFO;console;DRFA -Xmx512m  -Dhadoop.security.logger=INFO;RFAS -classpath XXX org.apache.hadoop.fs.FsShell -ls /Here is the root cause:The call flow is: hadoop.sh calls hadoop_config.sh; which in turn calls hadoop-env.sh. In hadoop.sh; the command line is generated by the following pseudo code:java $JAVA_HEAP_MAX $HADOOP_CLIENT_OPTS -classpath ...In hadoop-config.sh; $JAVA_HEAP_MAX is initialized as ""-Xmx1000m"" if user didn't set $HADOOP_HEAP_SIZE env variable.In hadoop-env.sh; $HADOOP_CLIENT_OPTS is set as this:export HADOOP_CLIENT_OPTS=""-Xmx512m $HADOOP_CLIENT_OPTS""To fix this problem; we should remove the ""-Xmx512m"" from HADOOP_CLIENT_OPTS. If we really want to change the memory settings we need to use $HADOOP_HEAP_SIZE env variable.",Resolved,Not A Problem,HADOOP-9902,shanyu zhao,shanyu zhao,Mon; 20 Jan 2014 23:30:19 +0000,Sun; 8 Feb 2015 17:42:34 +0000,Sun; 8 Feb 2015 17:42:34 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10245
HADOOP-10246,Improvement,Minor,fs,define FS permissions model with tests,It's interesting that HDFS mkdirs(dir; permission) uses the umask; but setPermissions() does notThe permissions model; including umask logic should be defined and have tests implemented by those filesystems that support permissions-based security,Open,Unresolved,,Unassigned,Steve Loughran,Tue; 21 Jan 2014 15:47:06 +0000,Thu; 3 Jul 2014 11:52:05 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10246
HADOOP-10247,Bug,Minor,fs,FsShell -ls -R output misaligned if child path has longer string representation than parent.,When running a recursive ls; like hdfs dfs -ls -R /; the column formatting becomes misaligned if there is a child path with a longer string representation (such as caused by a longer username) compared to its parent.,Open,Unresolved,,Unassigned,Chris Nauroth,Tue; 21 Jan 2014 19:29:56 +0000,Wed; 22 Jan 2014 22:51:36 +0000,,,2.2.0,,,HADOOP-10241,https://issues.apache.org/jira/browse/HADOOP-10247
HADOOP-10248,Improvement,Major,,Property name should be included in the exception where property value is null,I saw the following when trying to determine startup failure: Property name should be included in the following exception:,Closed,Fixed,,Akira Ajisaka,Ted Yu,Tue; 21 Jan 2014 19:35:27 +0000,Wed; 3 Sep 2014 23:33:20 +0000,Fri; 24 Jan 2014 06:30:46 +0000,,2.2.0,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10248
HADOOP-10249,Bug,Major,,LdapGroupsMapping should trim ldap password read from file,org.apache.hadoop.security.LdapGroupsMapping allows specifying ldap connection password in a file using property keyhadoop.security.group.mapping.ldap.bind.password.fileThe code in LdapGroupsMapping  that reads the content of the password file does not trim the password value. This causes ldap connection failure as the password in the password file ends up having a trailing newline.Most of the text editors and echo adds a new line at the end of file.So; LdapGroupsMapping should trim the password read from the file.,Closed,Fixed,,Dilli Arumugam,Dilli Arumugam,Tue; 21 Jan 2014 20:14:51 +0000,Thu; 10 Apr 2014 13:11:46 +0000,Thu; 13 Feb 2014 23:48:15 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10249
HADOOP-10250,Bug,Major,,VersionUtil returns wrong value when comparing two versions,"VersionUtil.compareVersions(""1.0.0-beta-1""; ""1.0.0"") returns 7 instead of negative number; which is wrong; because 1.0.0-beta-1 older than 1.0.0.",Closed,Fixed,,Yongjun Zhang,Yongjun Zhang,Tue; 21 Jan 2014 21:58:22 +0000,Tue; 25 Mar 2014 21:50:20 +0000,Mon; 27 Jan 2014 21:39:22 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10250
HADOOP-10251,Bug,Critical,ha,Both NameNodes could be in STANDBY State if SNN network is unstable,Following corner scenario happened in one of our cluster.1. NN1 was Active and NN2 was Standby2. NN2 machine's network was slow 3. NN1 got shutdown.4. NN2 ZKFC got the notification and trying to check for old active for fencing. (This took little more time; again due to slow network)5. In between; NN1 got restarted by our automatic monitoring; and ZKFC made it Active.6. Now NN2 ZKFC got Old Active as NN1 and it did graceful fencing of NN1 to STANBY.7. Before writing ActiveBreadCrumb to ZK; NN2 ZKFC got session timeout and got shutdown before making NN2 Active.Now cluster having both NameNodes as STANDBY.NN1 ZKFC still thinks that its nameNode is in Active state. NN2 ZKFC waiting for election.,Closed,Fixed,,Vinayakumar B,Vinayakumar B,Wed; 22 Jan 2014 12:05:23 +0000,Fri; 12 Jun 2015 08:04:01 +0000,Wed; 23 Apr 2014 19:05:54 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10251
HADOOP-10252,Bug,Major,,HttpServer can't start if hostname is not specified,HADOOP-8362 added a checking to make sure configuration values are not null. By default; we don't specify the hostname for the HttpServer. So we could not start info server due to,Closed,Fixed,,Jimmy Xiang,Jimmy Xiang,Wed; 22 Jan 2014 16:43:27 +0000,Mon; 24 Feb 2014 20:57:35 +0000,Wed; 22 Jan 2014 18:13:48 +0000,,2.0.2-alpha,,,HBASE-10336,https://issues.apache.org/jira/browse/HADOOP-10252
HADOOP-10253,Bug,Major,,Remove deprecated methods in HttpServer,There are a lot of deprecated methods in HttpServer. They are not used anymore. They should be removed.,Resolved,Won't Fix,,Unassigned,Haohui Mai,Wed; 22 Jan 2014 19:12:09 +0000,Tue; 28 Jan 2014 07:32:09 +0000,Tue; 28 Jan 2014 07:32:09 +0000,,,,,HADOOP-10254;HBASE-10336,https://issues.apache.org/jira/browse/HADOOP-10253
HADOOP-10254,Bug,Major,,HttpServer doesn't load listeners,With HADOOP-10252; we fixed the IAE issue. However; the server isn't starting properly since listeners are not loaded.,Resolved,Won't Fix,,Jimmy Xiang,Jimmy Xiang,Wed; 22 Jan 2014 20:00:49 +0000,Thu; 23 Jan 2014 05:03:34 +0000,Wed; 22 Jan 2014 22:18:20 +0000,,2.2.0,,,HADOOP-10253;HBASE-10336,https://issues.apache.org/jira/browse/HADOOP-10254
HADOOP-10255,Bug,Blocker,,Rename HttpServer to HttpServer2 to retain older HttpServer in branch-2 for compatibility,As suggested in HADOOP-10253; HBase needs a temporary copy of HttpServer from branch-2.2 to make sure it works across multiple 2.x releases.This patch renames the current HttpServer into HttpServer2; and bring  the HttpServer in branch-2.2 into the repository.,Closed,Fixed,,Haohui Mai,Haohui Mai,Wed; 22 Jan 2014 22:31:02 +0000,Thu; 21 Aug 2014 23:08:17 +0000,Tue; 28 Jan 2014 07:52:43 +0000,,2.3.0,,,HBASE-10336;HADOOP-10995,https://issues.apache.org/jira/browse/HADOOP-10255
HADOOP-10256,Bug,Major,,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Resolved,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:09:28 +0000,Thu; 1 Jan 2015 22:37:51 +0000,Thu; 1 Jan 2015 22:37:51 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10256
HADOOP-10257,Bug,Major,,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Resolved,Duplicate,HADOOP-10256;HADOOP-10258;HADOOP-10268;HADOOP-10263;HADOOP-10265;HADOOP-10266;HADOOP-10267;HADOOP-10259;HADOOP-10260;HADOOP-10261;HADOOP-10262;HADOOP-10264,Unassigned,david lee,Thu; 23 Jan 2014 03:10:01 +0000,Thu; 1 Jan 2015 22:36:38 +0000,Thu; 1 Jan 2015 22:36:38 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10257
HADOOP-10258,Bug,Major,,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Resolved,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:10:24 +0000,Thu; 1 Jan 2015 22:38:36 +0000,Thu; 1 Jan 2015 22:38:36 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10258
HADOOP-10259,Bug,Major,build,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Closed,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:10:58 +0000,Tue; 30 Jun 2015 07:22:32 +0000,Thu; 1 Jan 2015 22:38:50 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10259
HADOOP-10260,Bug,Major,build,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Closed,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:12:02 +0000,Tue; 30 Jun 2015 07:22:35 +0000,Thu; 1 Jan 2015 22:39:05 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10260
HADOOP-10261,Bug,Major,build,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Closed,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:12:22 +0000,Tue; 30 Jun 2015 07:22:38 +0000,Thu; 1 Jan 2015 22:39:18 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10261
HADOOP-10262,Bug,Major,build,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Closed,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:12:32 +0000,Tue; 30 Jun 2015 07:22:33 +0000,Thu; 1 Jan 2015 22:39:32 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10262
HADOOP-10263,Bug,Critical,build,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Closed,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:12:40 +0000,Tue; 30 Jun 2015 07:22:37 +0000,Thu; 1 Jan 2015 22:36:57 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10263
HADOOP-10264,Bug,Minor,build,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Closed,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:12:46 +0000,Tue; 30 Jun 2015 07:22:33 +0000,Thu; 1 Jan 2015 22:40:05 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10264
HADOOP-10265,Bug,Critical,build,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Closed,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:12:50 +0000,Tue; 30 Jun 2015 07:22:38 +0000,Thu; 1 Jan 2015 22:37:17 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10265
HADOOP-10266,Bug,Critical,build,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Closed,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:13:08 +0000,Tue; 30 Jun 2015 07:22:35 +0000,Thu; 1 Jan 2015 22:37:34 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10266
HADOOP-10267,Bug,Critical,build,hadoop2.2 building error,while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Closed,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:13:25 +0000,Tue; 30 Jun 2015 07:22:35 +0000,Thu; 1 Jan 2015 22:35:32 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10267
HADOOP-10268,Bug,Major,build,hadoop2.2 building error,hadoop2.2centos6.3 x64jdk-7u51-linux-x64protobuf-2.5.0apache-maven-3.1.1while running: mvn clean install -DskipTestsi got these errorsERROR Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:ERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:32;48 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33;48 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;4 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:55;33 OutputFormat is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;4 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:59;35 XMLSerializer is internal proprietary API and may be removed in a future releaseERROR /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:93;0 error: error while writing FsDatasetImpl: No space left on deviceERROR - Help 1i google for a long time; and have no idea how to resole it. can someone help me? thank you very much,Resolved,Duplicate,HADOOP-10257,Unassigned,david lee,Thu; 23 Jan 2014 03:14:36 +0000,Thu; 1 Jan 2015 22:39:47 +0000,Thu; 1 Jan 2015 22:39:47 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10268
HADOOP-10269,Bug,Major,security,SaslException is completely ignored,"In ""org/apache/hadoop/security/SaslOutputStream.java""; there is the following code pattern: On line 181; the exception thrown by disposeSasl(); which can be from SaslServer.dispose() or SaslClient.dispose(); is ignored completely without even logging it. Maybe at least log it?Ding",Resolved,Not A Problem,,Unassigned,Ding Yuan,Thu; 23 Jan 2014 11:37:34 +0000,Tue; 4 Mar 2014 22:26:44 +0000,Tue; 4 Mar 2014 22:26:44 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10269
HADOOP-10270,Bug,Minor,fs,getfacl does not display effective permissions of masked entries.,The mask entry of an ACL can be changed to restrict permissions that would be otherwise granted via named user and group entries.  In these cases; the typical implementation of getfacl also displays the effective permissions after applying the mask.,Resolved,Fixed,,Chris Nauroth,Chris Nauroth,Thu; 23 Jan 2014 19:07:21 +0000,Tue; 25 Feb 2014 00:30:41 +0000,Fri; 31 Jan 2014 18:59:59 +0000,,HDFS ACLs (HDFS-4685),,,HADOOP-10184;HADOOP-10361,https://issues.apache.org/jira/browse/HADOOP-10270
HDFS-5825,Improvement,Minor,,Use FileUtils.copyFile() to implement DFSTestUtils.copyFile(),DFSTestUtils.copyFile() is implemented by copying data through FileInputStream / FileOutputStream. Apache Common IO provides FileUtils.copyFile(). It uses FileChannel which is more efficient.This jira proposes to implement DFSTestUtils.copyFile() using FileUtils.copyFile().,Closed,Fixed,,Haohui Mai,Haohui Mai,Thu; 23 Jan 2014 22:18:07 +0000,Mon; 24 Feb 2014 20:57:47 +0000,Mon; 27 Jan 2014 19:04:48 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-5825
HADOOP-10272,Bug,Major,fs,"Hadoop 2 ""-copyFromLocal"" fail when source is a folder and there are spaces in the path","Repro steps:with folder structure like: /ab/c d/ef.txthadoop command (hadoop fs -copyFromLocal /ab/ /) or (hadoop fs -copyFromLocal ""/ab/c d/"" /) fail with error:copyFromLocal: File file:/ab/c%20d/ef.txt does not existHowever command (hadoop fs -copyFromLocal ""/ab/c d/ef.txt"" /) success.Seems like hadoop treat file and directory differently when ""copyFromLocal"".This only happens in Hadoop 2 and causing 2 Hive unit test failures (external_table_with_space_in_location_path.q and load_hdfs_file_with_space_in_the_name.q).",Resolved,Duplicate,HDFS-4329,Chuan Liu,Shuaishuai Nie,Fri; 24 Jan 2014 01:19:37 +0000,Thu; 12 May 2016 18:27:11 +0000,Tue; 28 Jan 2014 19:02:01 +0000,,2.2.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10272
HADOOP-10273,Bug,Major,build,Fix 'mvn site','mvn site' fails with Looks related to https://cwiki.apache.org/confluence/display/MAVEN/AetherClassNotFoundBumping the maven-site-plugin version should fix it.,Closed,Fixed,HADOOP-10324,Arpit Agarwal,Arpit Agarwal,Fri; 24 Jan 2014 02:20:44 +0000,Thu; 12 May 2016 18:27:12 +0000,Wed; 5 Feb 2014 06:09:49 +0000,,2.2.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10273
HADOOP-10274,Improvement,Minor,security,Lower the logging level from ERROR to WARN for UGI.doAs method,"Recently we got the error msg ""Request is a replay (34) - PROCESS_TGS"" while we are using the HBase client API to put data into HBase-0.94.16 with krb5-1.6.1 enabled. The related msg as follows... Finally; we found that the HBase would doing the retry (5 * 10 times) and recovery this 'request is a replay (34)' issue; but based on the HBase user viewpoint; the error msg at first line may be frightful; as we were afraid there was any data loss occurring at the first sight... So I'd like to suggest to change the logging level from 'ERROR' to 'WARN' for o.a.hadoop.security.UserGroupInformation#doAs(PrivilegedExceptionActionT) method Due to this method already throws _checked exception_s which can be handled by the client code; so the error may not really be an error if client  code can handle it...such as this case.For more details pls refer to HBASE-10379",Closed,Fixed,HADOOP-10015,takeshi.miao,takeshi.miao,Fri; 24 Jan 2014 02:54:32 +0000,Wed; 3 Sep 2014 23:33:20 +0000,Tue; 28 Jan 2014 05:03:21 +0000,,1.0.4,,,HADOOP-10015;HBASE-10379,https://issues.apache.org/jira/browse/HADOOP-10274
HADOOP-10275,Bug,Minor,,Serialization should remove its type parameter,org.apache.hadoop.io.serializer.Serialization is defined as: but the type parameter T is semantically invalid; and type mismatchings in the code are suppressed by explicit cast and annotations.This interface should be defined as follows:,Open,Unresolved,,Unassigned,Hiroshi Ikeda,Fri; 24 Jan 2014 08:53:56 +0000,Fri; 24 Jan 2014 08:58:29 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10275
HADOOP-10276,Bug,Major,,RawLocalFs#getFileLinkStatus does not fill in the link owner and mode by default,RawLocalFs#getFileLinkStatus does not actually get the owner and mode of the symlink; but instead uses the owner and mode of the symlink target.  If the target can't be found; it fills in bogus values (the empty string and FsPermission.getDefault) for these.Symlinks have an owner distinct from the owner of the target they point to; and getFileLinkStatus ought to expose this.In some operating systems; symlinks can have a permission other than 0777.  We ought to expose this in RawLocalFilesystem and other places; although we don't necessarily have to support this behavior in HDFS.,Open,Unresolved,,Unassigned,Colin P. McCabe,Fri; 24 Jan 2014 14:54:18 +0000,Fri; 24 Jan 2014 14:59:23 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10276
HADOOP-10277,Bug,Major,fs,setfacl -x fails to parse ACL spec if trying to remove the mask entry.,You should be able to use setfacl -x to remove the mask entry (if also removing all other extended ACL entries).  Right now; this causes a failure to parse the ACL spec due to a bug in AclEntry#parseAclSpec.,Resolved,Fixed,,Vinayakumar B,Chris Nauroth,Fri; 24 Jan 2014 21:41:46 +0000,Mon; 27 Jan 2014 18:06:24 +0000,Mon; 27 Jan 2014 18:06:24 +0000,,HDFS ACLs (HDFS-4685),,,HADOOP-10187;HADOOP-10184,https://issues.apache.org/jira/browse/HADOOP-10277
HADOOP-10278,Sub-task,Major,ipc,Refactor to make CallQueue pluggable,"Refactor CallQueue into an interface; base; and default implementation that matches today's behavior	Make the call queue impl configurable; keyed on port so that we minimize coupling",Closed,Fixed,,Chris Li,Chris Li,Fri; 24 Jan 2014 22:09:02 +0000,Thu; 4 Sep 2014 01:16:46 +0000,Fri; 21 Feb 2014 20:57:31 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10278
HADOOP-10279,Sub-task,Major,,Create multiplexer; a requirement for the fair queue,"The Multiplexer helps the FairCallQueue decide which of its internal sub-queues to read from during a poll() or take(). It controls the penalty of being in a lower queue. Without the mux; the FairCallQueue would have issues with starvation of low-priority requests.The WeightedRoundRobinMultiplexer is an implementation which uses a weighted round robin approach to muxing the sub-queues. It is configured with an integer list pattern.For example: 10; 5; 5; 2 means:	Read queue 0 10 times	Read queue 1 5 times	Read queue 2 5 times	Read queue 3 2 times	Repeat",Closed,Fixed,,Chris Li,Chris Li,Fri; 24 Jan 2014 23:18:03 +0000,Wed; 3 Sep 2014 20:36:31 +0000,Fri; 20 Jun 2014 05:55:38 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10279
HADOOP-10280,Sub-task,Major,,Make Schedulables return a configurable identity of user or group,In order to intelligently schedule incoming calls; we need to know what identity it falls under.We do this by defining the Schedulable interface; which has one method; getIdentity(IdentityType idType)The scheduler can then query a Schedulable object for its identity; depending on what idType is. For example:Call 1: Made by user=Alice; group=adminsCall 2: Made by user=Bob; group=adminsCall 3: Made by user=Carlos; group=usersCall 4: Made by user=Alice; group=adminsDepending on what the identity is; we would treat these requests differently. If we query on Username; we can bucket these 4 requests into 3 sets for Alice; Bob; and Carlos. If we query on Groupname; we can bucket these 4 requests into 2 sets for admins and users.In this initial version; idType can be username or primary group. In future versions; it could be jobID; request class (read or write); or some explicit QoS field. These are user-defined; and will be reloaded on callqueue refresh.,Closed,Fixed,,Chris Li,Chris Li,Fri; 24 Jan 2014 23:19:40 +0000,Thu; 4 Sep 2014 01:16:48 +0000,Tue; 25 Mar 2014 21:54:25 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10280
HADOOP-10281,Sub-task,Major,,Create a scheduler; which assigns schedulables a priority level,"The Scheduler decides which sub-queue to assign a given Call. It implements a single method getPriorityLevel(Schedulable call) which returns an integer corresponding to the subqueue the FairCallQueue should place the call in.The HistoryRpcScheduler is one such implementation which uses the username of each call and determines what % of calls in recent history were made by this user.It is configured with a historyLength (how many calls to track) and a list of integer thresholds which determine the boundaries between priority levels.For instance; if the scheduler has a historyLength of 8; and priority thresholds of 4;2;1; and saw calls made by these users in order:Alice; Bob; Alice; Alice; Bob; Jerry; Alice; Alice	Another call by Alice would be placed in queue 3; since she has already made = 4 calls	Another call by Bob would be placed in queue 2; since he has = 2 but less than 4 calls	A call by Carlos would be placed in queue 0; since he has no calls in the historyAlso; some versions of this patch include the concept of a 'service user'; which is a user that is always scheduled high-priority. Currently this seems redundant and will probably be removed in later patches; since its not too useful.----------------As of now; the current scheduler is the DecayRpcScheduler; which only keeps track of the number of each type of call and decays these counts periodically.",Closed,Fixed,,Chris Li,Chris Li,Fri; 24 Jan 2014 23:20:13 +0000,Mon; 1 Dec 2014 03:07:40 +0000,Wed; 13 Aug 2014 00:55:53 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10281
HADOOP-10282,Sub-task,Major,,Create a FairCallQueue: a multi-level call queue which schedules incoming calls and multiplexes outgoing calls,The FairCallQueue ensures quality of service by altering the order of RPC calls internally. It consists of three parts:1. a Scheduler (`HistoryRpcScheduler` is provided) which provides a priority number from 0 to N (0 being highest priority)2. a Multi-level queue (residing in `FairCallQueue`) which provides a way to keep calls in priority order internally3. a Multiplexer (`WeightedRoundRobinMultiplexer` is provided) which provides logic to control which queue to take fromCurrently the Mux and Scheduler are not pluggable; but they probably should be (up for discussion).This is how it is used:// Production1. Call is created and given to the CallQueueManager2. CallQueueManager requests a `put(T call)` into the `FairCallQueue` which implements `BlockingQueue`3. `FairCallQueue` asks its scheduler for a scheduling decision; which is an integer e.g. 124. `FairCallQueue` inserts Call into the 12th queue: `queues.get(12).put(call)`// Consumption1. CallQueueManager requests `take()` or `poll()` on FairCallQueue2. `FairCallQueue` asks its multiplexer for which queue to draw from; which will also be an integer e.g. 23. `FairCallQueue` draws from this queue if it has an available call (or tries other queues if it is empty)Additional information is available in the linked JIRAs regarding the Scheduler and Multiplexer's roles.,Closed,Fixed,,Chris Li,Chris Li,Fri; 24 Jan 2014 23:21:14 +0000,Fri; 20 May 2016 21:57:01 +0000,Fri; 22 Aug 2014 22:17:09 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10282
HADOOP-10283,Sub-task,Minor,,Make Scheduler and Multiplexer swappable,Currently the FairCallQueue uses the DecayRpcScheduler  RoundRobinMultiplexer; this task is to allow the user to configure the scheduler and mux in config settings,Resolved,Not A Problem,,Chris Li,Chris Li,Fri; 24 Jan 2014 23:22:28 +0000,Thu; 6 Nov 2014 21:51:46 +0000,Thu; 6 Nov 2014 21:51:46 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10283
HADOOP-10284,Sub-task,Major,,Add metrics to the HistoryRpcScheduler,,Resolved,Not A Problem,,Chris Li,Chris Li,Fri; 24 Jan 2014 23:22:41 +0000,Thu; 6 Nov 2014 21:52:26 +0000,Thu; 6 Nov 2014 21:52:26 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10284
HADOOP-10285,Sub-task,Major,,Admin interface to swap callqueue at runtime,We wish to swap the active call queue during runtime in order to do performance tuning without restarting the namenode.This patch adds the ability to refresh the call queue on the namenode; through dfsadmin -refreshCallQueue,Closed,Fixed,,Chris Li,Chris Li,Fri; 24 Jan 2014 23:23:16 +0000,Thu; 4 Sep 2014 01:16:47 +0000,Fri; 28 Feb 2014 20:48:52 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10285
HADOOP-10286,Sub-task,Minor,,Allow RPCCallBenchmark to benchmark calls by different users,,Open,Unresolved,,Chris Li,Chris Li,Fri; 24 Jan 2014 23:25:59 +0000,Fri; 22 Aug 2014 23:31:24 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10286
HADOOP-10287,Bug,Major,fs,FSOutputSummer should support any checksum size,HADOOP-9114 only fixes if checksum size is 0; but doesn't handle the generic case.FSOutputSummer should work with any checksum size (between 0 and 8 since Checksum.getValue() returns a long),Patch Available,Unresolved,,Unassigned,Laurent Goujon,Sat; 25 Jan 2014 06:36:15 +0000,Thu; 12 May 2016 21:16:50 +0000,,,2.2.0;3.0.0-alpha1,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10287
HADOOP-10288,Bug,Major,util,Explicit reference to Log4JLogger breaks non-log4j users,In HttpRequestLog; we make an explicit reference to the Log4JLogger class for an instanceof check. If the log4j implementation isn't actually on the classpath; the instanceof check throws NoClassDefFoundError instead of returning false. This means that dependent projects that don't use log4j can no longer embed HttpServer  typically this is an issue when they use MiniDFSCluster as part of their testing.,Closed,Fixed,,Todd Lipcon,Todd Lipcon,Sat; 25 Jan 2014 19:51:52 +0000,Mon; 24 Feb 2014 20:58:04 +0000,Mon; 27 Jan 2014 22:15:11 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10288
HADOOP-10289,Bug,Major,util,o.a.h.u.ReflectionUtils.printThreadInfo() causes deadlock in TestHttpServer,This bug is a followup on HADOOP-9964ReflectionUtils.printThreadInfo is now a synchronized method. This change creates sometimes deadlock situation in TestHttpServer if one servlet thread calling this method is waiting on client to consume output.In TestHttpServer; several tests connect to the http server only to check the status code but without reading the full inputstream. Depending on HttpURLConnection; the deadlock scenario may be triggered or not.Note that in the original ticket; it is not explained why synchronized fixed the issue. According to the attached stacktrace; test was blocked on HttpServer.stop(); waiting on worker threads to stop; which didn't happen because those threads were waiting for their output to be consumed; so the original issue looks very similar to what I'm experiencing.My proposed fix is to remove synchronized (as it seems to make the issue worse) but configure HttpServer.stop() to forcibly kill threads after a configurable period of time,Patch Available,Unresolved,,Unassigned,Laurent Goujon,Sat; 25 Jan 2014 20:05:04 +0000,Thu; 12 May 2016 21:06:11 +0000,,,2.3.0;3.0.0-alpha1,,,HADOOP-9964,https://issues.apache.org/jira/browse/HADOOP-10289
HADOOP-10290,Bug,Major,build,Surefire steals focus on MacOS,When running tests on MacOS X; surefire plugin keeps stealing focus from current application.This can be avoided by adding   to the surefire commandline,Reopened,Unresolved,HADOOP-11855,Unassigned,Laurent Goujon,Sun; 26 Jan 2014 03:18:41 +0000,Tue; 21 Apr 2015 19:42:01 +0000,,,,,,SPARK-2602,https://issues.apache.org/jira/browse/HADOOP-10290
HADOOP-10291,Bug,Major,,TestSecurityUtil#testSocketAddrWithIP fails,testSocketAddrWithIP fails with Assertion Error,Closed,Fixed,,Mit Desai,Mit Desai,Mon; 27 Jan 2014 17:04:03 +0000,Thu; 12 May 2016 18:27:23 +0000,Wed; 29 Jan 2014 04:44:35 +0000,,2.2.0;3.0.0-alpha1,java7,,,https://issues.apache.org/jira/browse/HADOOP-10291
HADOOP-10292,Bug,Major,,Restore HttpServer from branch-2.2 in branch-2,This jira is a follow-up jira of HADOOP-10255. It brings in the HttpServer in branch-2.2 directly into branch-2 to restore the compatibility of HBase.,Closed,Fixed,,Haohui Mai,Haohui Mai,Mon; 27 Jan 2014 21:38:52 +0000,Mon; 24 Feb 2014 20:58:15 +0000,Tue; 28 Jan 2014 07:57:48 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10292
HADOOP-10293,Bug,Major,fs,Though symlink is disabled by default;  related code interprets path to be link incorrectly,"File path ...xyz/abc`/tfile is interpreted as a link; due to the existence of backtick in the file path. ""abc`"" is a directory name here.There are two issues here; 1. When symlink is disabled; the code that interprets symlink should be disabled too. This is the issue to resolve in this jira.2. When symlink is enabled; using of backtick ` as delimiter to interpret whether a path is link need to be revisited; will file a different JIRA.",Resolved,Fixed,,Yongjun Zhang,Yongjun Zhang,Mon; 27 Jan 2014 22:19:06 +0000,Sat; 1 Feb 2014 01:04:51 +0000,Sat; 1 Feb 2014 01:04:38 +0000,,2.3.0,,,HADOOP-10294,https://issues.apache.org/jira/browse/HADOOP-10293
HADOOP-10294,Sub-task,Major,fs,"Using backtick ""`"" as delimiter for parsing file path disallows ""`"" in file path name","This is the second issue reported in bug HADOOP-10293.When symlink code is enabled; it uses backtick ""`"" as delimiter when interpreting a path and to tell whether it's a link or not. This disallows ""`"" to appear in file pathname that's not a link.",Open,Unresolved,,Unassigned,Yongjun Zhang,Mon; 27 Jan 2014 22:25:08 +0000,Sat; 1 Feb 2014 01:22:59 +0000,,,2.3.0,,,HADOOP-10293,https://issues.apache.org/jira/browse/HADOOP-10294
HADOOP-10295,Improvement,Major,tools/distcp,Allow distcp to automatically identify the checksum type of source files and use it for the target,"Currently while doing distcp; users can use ""-Ddfs.checksum.type"" to specify the checksum type in the target FS. This works fine if all the source files are using the same checksum type. If files in the source cluster have mixed types of checksum; users have to either use ""-skipcrccheck"" or have checksum mismatching exception. Thus we may need to consider adding a new option to distcp so that it can automatically identify the original checksum type of each source file and use the same checksum type in the target FS.",Closed,Fixed,,Jing Zhao,Jing Zhao,Mon; 27 Jan 2014 22:43:52 +0000,Thu; 4 Sep 2014 01:16:46 +0000,Fri; 31 Jan 2014 00:01:18 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10295
HADOOP-10296,Bug,Minor,,Incorrect null check in SwiftRestClient#buildException(),The null check should be for availableContentRange,Resolved,Fixed,HADOOP-10331,Kanaka Kumar Avvaru,Ted Yu,Tue; 28 Jan 2014 00:12:47 +0000,Tue; 30 Aug 2016 01:32:44 +0000,Thu; 1 Oct 2015 16:37:35 +0000,,2.3.0,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10296
HADOOP-10297,Improvement,Major,,FileChecksum should provide getChecksumOpt method,o.a.h.f.FileSystem has several methods which accepts directly or indirectly a ChecksumOpt parameter to configure checksum options; but there's no generic way of querying checksum options used for a given file.MD5MD5CRC32FileChecksum used by DFSClient has a getChecksumOpt() but since not just DistributedFileSystem is accepting a ChecksumOpt argument; but any FileSystem subclass (although only DistributedFileSystem implements a specific behaviour); I suggest to make getChecksumOpt an abstract method of FileChecksum. This could be used by tools like DistCp to replicate checksum options for example.,Resolved,Duplicate,NULL,Unassigned,Laurent Goujon,Tue; 28 Jan 2014 05:08:10 +0000,Fri; 31 Jan 2014 00:06:10 +0000,Fri; 31 Jan 2014 00:06:10 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10297
HADOOP-10298,Improvement,Major,,Clean up HttpServer2,after HADOOP-10255 HttpServer2 is an internal class that is only used by HDFS and YARN. Therefore HttpServer2 can be cleaned up; and the deprecated methods can be removed.,Open,Unresolved,,Unassigned,Haohui Mai,Tue; 28 Jan 2014 07:34:00 +0000,Tue; 28 Jan 2014 07:34:00 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10298
HDFS-5844,Bug,Minor,documentation,Fix broken link in WebHDFS.apt.vm,There is one broken link in WebHDFS.apt.vm. should be,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Tue; 28 Jan 2014 07:40:26 +0000,Wed; 3 Sep 2014 23:34:06 +0000,Wed; 29 Jan 2014 05:01:25 +0000,,2.2.0,newbie,,HDFS-5297,https://issues.apache.org/jira/browse/HDFS-5844
HADOOP-10300,Sub-task,Major,ipc,Allowed deferred sending of call responses,RPC handlers currently do not return until the RPC call completes and response is sent; or a partially sent response has been queued for the responder.  It would be useful for a proxy method to notify the handler to not yet the send the call's response.An potential use case is a namespace handler in the NN might want to return before the edit log is synced so it can service more requests and allow increased batching of edits per sync.  Background syncing could later trigger the sending of the call response to the client.,Resolved,Fixed,,Daryn Sharp,Daryn Sharp,Tue; 28 Jan 2014 19:29:29 +0000,Wed; 2 Nov 2016 05:25:12 +0000,Wed; 2 Nov 2016 05:25:12 +0000,,2.0.0-alpha;3.0.0-alpha1,BB2015-05-TBR,,HADOOP-12483,https://issues.apache.org/jira/browse/HADOOP-10300
HADOOP-10301,Bug,Blocker,security,AuthenticationFilter should return Forbidden for failed authentication,The hadoop-auth AuthenticationFilter returns a 401 Unauthorized without a WWW-Authenticate headers.  The is illegal per the HTTP RPC and causes a NPE in the HttpUrlConnection.This is half of a fix that affects webhdfs.  See HDFS-4564.,Closed,Fixed,HADOOP-9311,Daryn Sharp,Daryn Sharp,Tue; 28 Jan 2014 21:28:02 +0000,Thu; 12 May 2016 18:22:26 +0000,Fri; 28 Mar 2014 21:32:35 +0000,,0.23.0;2.0.0-alpha;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10301
HADOOP-10302,Sub-task,Major,,Allow CallQueue impls to be swapped at runtime (part 1: internals) Depends on: subtask1,We wish to swap the active call queue during runtime in order to do performance tuning without restarting the namenode.This patch adds only the internals necessary to swap. Part 2 will add a user interface so that it can be used.,Resolved,Not A Problem,,Chris Li,Chris Li,Tue; 28 Jan 2014 22:07:59 +0000,Thu; 6 Nov 2014 23:46:49 +0000,Sat; 15 Feb 2014 03:58:32 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10302
HADOOP-10303,Improvement,Minor,,multi-supergroup supports,Most operating system supports multiple groups of administrators. For Hadoop; it only supports a single supergroup.This Jira requires to enhance open source hadoop to support multiple group of administrators.  for example;we have data administrators to manage HDFS (supergroup A)and application administrators(supergroup B) to manage Mapreduce.,Open,Unresolved,,Unassigned,Jiqiu,Wed; 29 Jan 2014 05:42:00 +0000,Sat; 7 Jan 2017 01:56:55 +0000,,,1.2.1;2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10303
HADOOP-10304,Bug,Minor,,Configuration should not expose its instance in constructors,org.apache.hadoop.conf.Configuration exposes a reference of its instance in constructors via its class variable REGISTRY; which means incomplete instances are accessible. For example addDefaultResource() may access incomplete instances (especially for subclasses of Configuration).Actually; static methods in Configuration are not needed to access its instances; and it is enough that each instance checks modification of class variables. This is also useful to avoid deadlock between locking instances and locking the class object; which may be happened when you will resolve race conditions yet existing in Configuration.,Open,Unresolved,,Unassigned,Hiroshi Ikeda,Wed; 29 Jan 2014 06:01:38 +0000,Wed; 29 Jan 2014 06:01:38 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10304
HADOOP-10305,Bug,Major,metrics,"Add ""rpc.metrics.quantile.enable"" and ""rpc.metrics.percentiles.intervals"" to core-default.xml","rpc.metrics.quantile.enable and ""rpc.metrics.percentiles.intervals"" were added in HADOOP-9420; but these two parameters are not written in core-default.xml.",Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Wed; 29 Jan 2014 06:18:29 +0000,Mon; 24 Feb 2014 20:58:00 +0000,Thu; 30 Jan 2014 01:08:19 +0000,,,,,HADOOP-9420,https://issues.apache.org/jira/browse/HADOOP-10305
HADOOP-10306,Bug,Trivial,,Unnecessary weak reference map to cache classes in Configuration,"In Configuration.getClassByNameOrNull(): Change ""new WeaHashMapString; ...()"" to ""new HashMapString; ..."" or something. Otherwise; even while the class is actively used; this may drop its class cache.",Open,Unresolved,,Unassigned,Hiroshi Ikeda,Wed; 29 Jan 2014 06:22:09 +0000,Thu; 3 Dec 2015 22:20:57 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10306
HADOOP-10307,Improvement,Major,security,Support multiple Authentication mechanisms for HTTP,Currently it is possible to specify a custom Authentication Handler  for HTTP authentication.  We have a requirement to support multiple mechanisms  to authenticate HTTP access.,Resolved,Won't Do,,Benoy Antony,Benoy Antony,Wed; 29 Jan 2014 19:41:24 +0000,Mon; 16 Oct 2017 19:49:41 +0000,Mon; 16 Oct 2017 19:49:41 +0000,,2.2.0,BB2015-05-TBR,,HADOOP-10158;HDFS-5716;HADOOP-10709,https://issues.apache.org/jira/browse/HADOOP-10307
HADOOP-10308,Improvement,Major,conf,Remove from core-default.xml unsupported 'classic' and add 'yarn-tez' as value for mapreduce.framework.name property,Classic mr-v1 is no more supported in trunk.On the other hand; we will soon have yarn-tez implementation of mapreduce (tez layer allowing to have a single AM for all map-reduce jobs).core-default.xml must reflect this.,Open,Unresolved,,Unassigned,Eric Charles,Wed; 29 Jan 2014 19:45:19 +0000,Mon; 14 Apr 2014 08:38:26 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10308
HADOOP-10309,Improvement,Minor,fs/s3,S3 block filesystem should more aggressively delete temporary files,The S3 FileSystem reading implementation downloads block files into a configurable temporary directory. deleteOnExit() is called on these files; so they are deleted when the JVM exits.However; JVM reuse can lead to JVMs that stick around for a very long time. This can cause these temporary files to build up indefinitely and; in the worst case; fill up the local directory.After a block file has been read; there is no reason to keep it around. It should be deleted.Writing to the S3 FileSystem already has this behavior; after a temporary block file is written and uploaded to S3; it is deleted immediately; there is no need to wait for the JVM to exit.,Patch Available,Unresolved,,Unassigned,Joe Kelley,Wed; 29 Jan 2014 22:15:22 +0000,Wed; 6 May 2015 03:26:34 +0000,,,2.6.0,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10309
HADOOP-10310,Bug,Blocker,security,SaslRpcServer should be initialized even when no secret manager present,HADOOP-8783 made a change which caused the SaslRpcServer not to be initialized if there is no secret manager present. This works fine for most Hadoop daemons because they need a secret manager to do their business; but JournalNodes do not. The result of this is that JournalNodes are broken and will not handle RPCs in a Kerberos-enabled environment; since the SaslRpcServer will not be initialized.,Closed,Fixed,,Aaron T. Myers,Aaron T. Myers,Wed; 29 Jan 2014 23:47:18 +0000,Mon; 24 Feb 2014 20:57:29 +0000,Thu; 30 Jan 2014 15:57:05 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10310
HADOOP-10311,Bug,Blocker,,Cleanup vendor names from the code base,,Closed,Fixed,,Alejandro Abdelnur,Suresh Srinivas,Thu; 30 Jan 2014 04:18:07 +0000,Mon; 24 Feb 2014 20:58:02 +0000,Fri; 31 Jan 2014 20:13:04 +0000,,2.3.0,,,HDFS-5852,https://issues.apache.org/jira/browse/HADOOP-10311
HADOOP-10312,Bug,Minor,util,Shell.ExitCodeException to have more useful toString,Shell's ExitCodeException doesn't include the exit code in the toString value; so isn't that useful in diagnosing container start failures in YARN,Closed,Fixed,,Steve Loughran,Steve Loughran,Thu; 30 Jan 2014 14:27:43 +0000,Fri; 15 Aug 2014 05:39:35 +0000,Thu; 3 Jul 2014 11:28:15 +0000,,2.4.0,,,YARN-1438;YARN-522,https://issues.apache.org/jira/browse/HADOOP-10312
HADOOP-10313,Bug,Major,build,Script and jenkins job to produce Hadoop release artifacts,As discussed in the dev mailing lists; we should have a jenkins job to build the release artifacts.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Thu; 30 Jan 2014 19:29:30 +0000,Mon; 24 Feb 2014 20:57:53 +0000,Fri; 31 Jan 2014 03:57:58 +0000,,2.3.0,,,HADOOP-8914,https://issues.apache.org/jira/browse/HADOOP-10313
HADOOP-10314,Bug,Major,,The ls command help still shows outdated 0.16 format.,The description of output format is vastly outdated. It was changed after version 0.16.,Closed,Fixed,,Rushabh S Shah,Kihwal Lee,Thu; 30 Jan 2014 22:00:35 +0000,Thu; 4 Sep 2014 01:16:48 +0000,Mon; 3 Mar 2014 16:43:35 +0000,,2.2.0,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10314
HADOOP-10315,Bug,Major,,Log the original exception when getGroups() fail in UGI.,In UserGroupInformation; getGroupNames() swallows the original exception. There have been many occasions that more information on the original exception could have helped.,Patch Available,Unresolved,HADOOP-12840,Ted Yu,Kihwal Lee,Thu; 30 Jan 2014 22:20:35 +0000,Mon; 11 Sep 2017 05:21:38 +0000,,,0.23.10;2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10315
HADOOP-10316,Bug,Minor,,HadoopArchives#HArchiveInputFormat#getSplits() should check reader against null before calling close(),Around line 267: reader should be checked against null,Resolved,Invalid,,Unassigned,Ted Yu,Thu; 30 Jan 2014 23:54:32 +0000,Thu; 31 Mar 2016 23:14:04 +0000,Thu; 31 Mar 2016 23:14:04 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10316
HADOOP-10317,Bug,Major,,Rename branch-2.3 release version from 2.4.0-SNAPSHOT to 2.3.0-SNAPSHOT,Right now the pom.xml's refer to 2.4 rather than 2.3 in branch-2.3. We need to update them.,Closed,Fixed,,Andrew Wang,Andrew Wang,Fri; 31 Jan 2014 01:14:06 +0000,Mon; 24 Feb 2014 20:58:37 +0000,Fri; 31 Jan 2014 02:10:58 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10317
HADOOP-10318,Bug,Minor,,Incorrect reference to nodeFile in RumenToSLSConverter error message,jsonFile on the last line should be nodeFile,Resolved,Fixed,,Wei Yan,Ted Yu,Fri; 31 Jan 2014 05:30:53 +0000,Tue; 30 Aug 2016 01:32:41 +0000,Thu; 3 Sep 2015 13:51:46 +0000,,,BB2015-05-TBR;newbie,,,https://issues.apache.org/jira/browse/HADOOP-10318
HADOOP-10319,Bug,Major,fs,Unable to run Hadoop (2.2.0) commands on Cygwin (2.831) on Windows XP 3,Did Following on starting Shell1. ssh localhost2. cd /cygdrive/e/hadoop-2.2.03. export JAVA_HOME=/cygdrive/e/JDK4. export HADOOP_INSTALL=/cygdrive/e/hadoop-2.2.o5. export PATH=$PATH:$JAVA_HOME:$HADOOP_INSTALL/bin:$HADOOP_INSTALL/sbinI have installed JDK1.7.0_51At $hadoop version (throws) Error: Could not find or load main class org.apache.hadoop.util.VersioninfoSimilar errors are thrown for fs and jar. I have not made any changes to any environment variables or scripts.I am new to Hadoop-2.2.0 and following text Hadoop- The Definitive Guide by Tom white where it has been suggested that Cygwin can be used with Windows and Hadoop 2.Advise appreciated. ThanksAnand,Closed,Invalid,,Unassigned,Anand Murali,Fri; 31 Jan 2014 11:11:28 +0000,Tue; 30 Jun 2015 07:11:21 +0000,Sat; 1 Feb 2014 12:58:58 +0000,,2.2.0,patch,,HADOOP-11464,https://issues.apache.org/jira/browse/HADOOP-10319
HADOOP-10320,Bug,Trivial,documentation,Javadoc in InterfaceStability.java lacks final </ul>,,Closed,Fixed,,Ren√© Nyffenegger,Ren√© Nyffenegger,Fri; 31 Jan 2014 17:57:49 +0000,Tue; 3 Mar 2015 02:10:14 +0000,Fri; 31 Jan 2014 20:19:12 +0000,,2.2.0,,HADOOP-11449,,https://issues.apache.org/jira/browse/HADOOP-10320
HADOOP-10321,Bug,Major,,TestCompositeService should cover all enumerations of adding a service to a parent service,HADOOP-10085 fixes some synchronization issues in CompositeService#addService(). The tests should cover all cases.,Resolved,Fixed,,Ray Chiang,Karthik Kambatla,Fri; 31 Jan 2014 18:06:30 +0000,Tue; 30 Aug 2016 01:32:40 +0000,Tue; 1 Mar 2016 03:02:54 +0000,,2.3.0,BB2015-05-RFC;supportability;test,,,https://issues.apache.org/jira/browse/HADOOP-10321
HADOOP-10322,Improvement,Major,security,Add ability to read principal names from a keytab,It will be useful to have an ability to enumerate the principals stored in a keytab.,Closed,Fixed,,Benoy Antony,Benoy Antony,Sat; 1 Feb 2014 00:30:11 +0000,Wed; 3 Sep 2014 20:36:29 +0000,Mon; 28 Apr 2014 13:55:43 +0000,,2.2.0,,,HADOOP-10711,https://issues.apache.org/jira/browse/HADOOP-10322
HADOOP-10323,Improvement,Major,tools/distcp,Allow users to do a dryrun of distcp,This jira plans to add a dryrun option in distcp which will make distcp go through all the steps except the real data copying. In this way; users can quickly understand potential issues.,Open,Unresolved,,Jing Zhao,Jing Zhao,Tue; 4 Feb 2014 02:05:22 +0000,Tue; 4 Feb 2014 02:52:26 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10323
HADOOP-10324,Bug,Major,,Bump the version of maven-site-plugin to 3.3,The current version of maven-site-plugin (which is 3.0) is incompatible with Maven 3.1. As a result; users of Maven 3.1 are unable to run mvn site to generate the documentation.The newer version of maven-site-plugin has fixed this issue. See https://cwiki.apache.org/confluence/display/MAVEN/AetherClassNotFound for more details.,Resolved,Duplicate,HADOOP-10273,Haohui Mai,Haohui Mai,Tue; 4 Feb 2014 19:38:22 +0000,Wed; 5 Feb 2014 01:32:56 +0000,Wed; 5 Feb 2014 01:32:56 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10324
HADOOP-10325,Bug,Major,,improve jenkins javadoc warnings from test-patch.sh,Currently test-patch.sh uses OK_JAVADOC_WARNINGS to know how many warnings trunk is expected to have.  However; this is a fragile and difficult to use system; since different build slaves may generate different numbers of warnings (based on compiler revision; etc.).  Also; programmers must remember to update OK_JAVADOC_WARNINGS; which they don't always.  Finally; there is no easy way to find what the new javadoc warnings are in the huge pile of warnings.We should change this to work the same way the javac warnings code does: to simply build with and without the patch and do a diff.  The diff should be saved for easy perusal.  We also should not complain about warnings being removed.,Resolved,Fixed,,Colin P. McCabe,Colin P. McCabe,Tue; 4 Feb 2014 22:23:33 +0000,Thu; 12 May 2016 18:26:40 +0000,Thu; 6 Feb 2014 00:13:41 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10325
HADOOP-10326,Bug,Major,security,M/R jobs can not access S3 if Kerberos is enabled,With Kerberos enabled; any job that is taking as input or output s3 files fails.It can be easily reproduced with wordcount shipped in hadoop-examples.jar and a public S3 file: returns:,Closed,Fixed,,bc Wong,Manuel DE FERRAN,Fri; 10 Aug 2012 13:49:57 +0000,Thu; 10 Apr 2014 13:11:59 +0000,Tue; 11 Feb 2014 02:49:34 +0000,,2.2.0,s3,,,https://issues.apache.org/jira/browse/HADOOP-10326
HADOOP-10327,Bug,Blocker,native,Trunk windows build broken after HDFS-5746,Hadoop build broken with Native code errors in windows.,Closed,Fixed,,Vinayakumar B,Vinayakumar B,Tue; 4 Feb 2014 07:16:30 +0000,Thu; 12 May 2016 18:27:18 +0000,Thu; 6 Feb 2014 18:43:25 +0000,,2.4.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10327
HADOOP-10328,Bug,Major,tools,loadGenerator exit code is not reliable,LoadGenerator exit code is determined using the following logic At the end of the run we just return the exitCode. So essentially if you are arguments are correct you will always get 0 back.,Closed,Fixed,,Haohui Mai,Arpit Gupta,Thu; 6 Feb 2014 20:33:48 +0000,Fri; 22 Sep 2017 20:14:20 +0000,Thu; 20 Feb 2014 19:09:15 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10328
HADOOP-10329,Bug,Major,conf,Fully qualified URIs are inconsistant and sometimes break in hadoop conf files,When specifying paths in the *-site.xml files; some are required to be fully qualified; while others (specifically hadoop.tmp.dir) break when a fully qualified uri is used.Example:If I set hadoop.tmp.dir in core-site to file:///something it'll create a file: directory in my $PWD.  Other places; like the datanode; or the nodemanager; will complain if I don't use fully qualified uris,Patch Available,Unresolved,,Mohammad Kamrul Islam,Travis Thompson,Thu; 6 Feb 2014 21:26:02 +0000,Wed; 6 May 2015 03:27:08 +0000,,,2.2.0,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10329
HADOOP-10330,Bug,Major,test,TestFrameDecoder fails if it cannot bind port 12345,TestFrameDecoder fails if port 12345 is in use.,Closed,Fixed,,Arpit Agarwal,Arpit Agarwal,Fri; 7 Feb 2014 00:38:12 +0000,Thu; 12 May 2016 18:22:31 +0000,Fri; 7 Feb 2014 01:15:10 +0000,,2.2.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10330
HADOOP-10331,Bug,Minor,,SwiftRestClient#buildException() references wrong length,availableContentRange should be checked instead.,Resolved,Duplicate,HADOOP-10296,Unassigned,Ted Yu,Sun; 9 Feb 2014 16:37:19 +0000,Sun; 5 Oct 2014 15:19:00 +0000,Sun; 5 Oct 2014 15:19:00 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10331
HADOOP-10332,Bug,Major,,HttpServer's jetty audit log always logs 200 OK,HttpServer inserts the audit logger handler  before the actual servlet handlers; so the default 200 is always logged even if the operation fails.,Closed,Fixed,,Jonathan Eagles,Daryn Sharp,Mon; 10 Feb 2014 16:46:55 +0000,Thu; 12 May 2016 18:24:26 +0000,Thu; 13 Mar 2014 16:15:08 +0000,,0.23.0;2.0.0-alpha;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10332
HADOOP-10333,Improvement,Trivial,,Fix grammatical error in overview.html document,The file trunk/hadoop-common-project/hadoop-common/src/main/java/overview.html contains a typo. I will (try to) create a patch for this.,Closed,Fixed,,Ren√© Nyffenegger,Ren√© Nyffenegger,Mon; 10 Feb 2014 17:43:01 +0000,Thu; 10 Apr 2014 13:12:04 +0000,Mon; 10 Feb 2014 19:36:21 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10333
HDFS-6959,New Feature,Minor,,Make the HDFS home directory location customizable.,"The path is currently hardcoded:public Path getHomeDirectory() {    return makeQualified(new Path(""/user/"" + dfs.ugi.getShortUserName()));  }It would be nice to have that as a customizable value.  Thank you",Closed,Fixed,HDFS-593;HDFS-4634;HDFS-6857,Yongjun Zhang,Kevin Odell,Mon; 10 Feb 2014 17:53:33 +0000,Wed; 17 Dec 2014 19:47:41 +0000,Thu; 4 Sep 2014 02:36:21 +0000,,2.2.0,supportability,,HADOOP-10944,https://issues.apache.org/jira/browse/HDFS-6959
HADOOP-10335,Improvement,Major,,An ip whilelist based implementation to resolve Sasl properties per connection,As noted in HADOOP-10221; it is sometimes required for a Hadoop Server to communicate with some client over encrypted channel and with some other clients over unencrypted channel. Hadoop-10221 introduced an interface SaslPropertiesResolver  and the changes required to plugin and use SaslPropertiesResolver  to identify the SaslProperties to be used for a connection. In this jira; an ip-whitelist based implementation of SaslPropertiesResolver  is attempted.,Closed,Fixed,,Benoy Antony,Benoy Antony,Mon; 10 Feb 2014 20:46:28 +0000,Mon; 1 Dec 2014 03:10:30 +0000,Sun; 17 Aug 2014 19:06:35 +0000,,,,,HADOOP-10565,https://issues.apache.org/jira/browse/HADOOP-10335
HADOOP-10336,Bug,Critical,,Yarn WebUIs are accessible with Http protocol when yarn.http.policy is set to HTTPS_ONLY,"yarn.http.policy is set to ""HTTPS_ONLY"".The RM/ NM web UI is expected to be accessed using https://RM:RM_HTTPS_PORT and https://NM:NM_HTTPS_PORTCurrently; RM and NM UIs are accessible at http://RM:RM_HTTPS_PORT and http://NM:NM_HTTPS_PORT",Resolved,Duplicate,YARN-1553,Unassigned,Yesha Vora,Mon; 10 Feb 2014 21:45:58 +0000,Fri; 14 Feb 2014 23:01:47 +0000,Fri; 14 Feb 2014 23:01:37 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10336
HADOOP-10337,Bug,Major,metrics,ConcurrentModificationException from MetricsDynamicMBeanBase.createMBeanInfo(),This stack trace came from our HBase 0.94.3 production env:2014-02-11;17:34:46;562 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute tbl.micloud_gallery_albumsharetag_v2.region.96a6d0bc9f0153e0e1ec0318b39ecc45.next_histogram_99th_percentile of hadoop:service=RegionServer;name=RegionServerDynamicStatistics threw an exceptionjavax.management.RuntimeMBeanException: java.util.ConcurrentModificationException        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:856)        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:869)        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:670)        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)        at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:315)        at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:293)        at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:193)        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)        at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1057)        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)        at org.mortbay.jetty.Server.handle(Server.java:326)        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)Caused by: java.util.ConcurrentModificationException        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)        at java.util.HashMap$ValueIterator.next(HashMap.java:822)        at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.createMBeanInfo(MetricsDynamicMBeanBase.java:87)        at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.updateMbeanInfoIfMetricsListChanged(MetricsDynamicMBeanBase.java:78)        at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.getAttribute(MetricsDynamicMBeanBase.java:138)        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)        ... 27 more,Closed,Fixed,,Liang Xie,Liang Xie,Wed; 12 Feb 2014 02:04:35 +0000,Thu; 12 May 2016 18:25:26 +0000,Tue; 11 Mar 2014 04:44:37 +0000,,2.2.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10337
HADOOP-10338,Bug,Major,,Cannot get the FileStatus of the root inode from the new Globber,"We can no longer get the correct FileStatus of the root inode ""/"" from the Globber.",Closed,Fixed,,Colin P. McCabe,Andrew Wang,Wed; 5 Feb 2014 00:42:30 +0000,Thu; 10 Apr 2014 13:12:04 +0000,Wed; 12 Feb 2014 02:17:56 +0000,,2.3.0,,,HADOOP-10344,https://issues.apache.org/jira/browse/HADOOP-10338
HADOOP-10339,Improvement,Major,,Set io.sort.mb to 10 for distcp jobs,For distcp we don't emit anything; it's a map-only job with no output (just side-effects).In that case we can set io.sort.mb to 10.This way we don't waste the default memory in a buffer that is not needed.,Resolved,Not A Problem,,Unassigned,Siqi Li,Wed; 12 Feb 2014 19:42:01 +0000,Wed; 12 Feb 2014 22:55:45 +0000,Wed; 12 Feb 2014 22:55:45 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10339
MAPREDUCE-5756,Bug,Major,,CombineFileInputFormat.getSplits() including directories in its results,"Trying to track down HIVE-6401; where we see some ""is not a file"" errors because getSplits() is giving us directories.  I believe the culprit is FileInputFormat.listStatus(): Which seems to be allowing directories to be added to the results if recursive is false.  Is this meant to return directories? If not; I think it should look like this:",Closed,Fixed,,Jason Dere,Jason Dere,Wed; 12 Feb 2014 19:55:22 +0000,Tue; 16 Dec 2014 03:02:33 +0000,Mon; 21 Jul 2014 21:30:39 +0000,,,,HIVE-6401,,https://issues.apache.org/jira/browse/MAPREDUCE-5756
HADOOP-10341,Improvement,Major,ha;io;ipc,Improvements on error handling code,Possibly some improvements should be made in a few error handling logics. See comment for detail.,Open,Unresolved,,Unassigned,Ding Yuan,Wed; 12 Feb 2014 22:23:43 +0000,Wed; 12 Feb 2014 22:28:33 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10341
HADOOP-10342,Bug,Major,security,Extend UserGroupInformation to return a UGI given a preauthenticated kerberos Subject,We need the ability to use a Subject that was created inside an embedding application through a kerberos authentication. For example; an application that uses JAAS to authenticate to a KDC should be able to provide the resulting Subject and get a UGI instance to call doAs on.Example:,Closed,Fixed,,Larry McCay,Larry McCay,Thu; 13 Feb 2014 01:12:39 +0000,Fri; 15 Aug 2014 05:39:47 +0000,Fri; 14 Feb 2014 21:25:56 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10342
HADOOP-10343,Improvement,Minor,,Change info to debug log in LossyRetryInvocationHandler,in LossyRetryInvocationHandler we print logs at info level when we drop responses. This causes lot of noise on the console.,Closed,Fixed,,Arpit Gupta,Arpit Gupta,Thu; 13 Feb 2014 19:05:48 +0000,Thu; 10 Apr 2014 13:11:36 +0000,Thu; 13 Feb 2014 22:03:44 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10343
HADOOP-10344,Bug,Trivial,test,Fix TestAclCommands after merging HADOOP-10338 patch.,TestAclCommands was coded on the HDFS-4685 branch to stub just enough behavior to satisfy the ls command's usage of the Globber.  HADOOP-10338 recently changed the Globber; and now the stubbing isn't quite sufficient; so the tests fail.,Resolved,Fixed,,Chris Nauroth,Chris Nauroth,Thu; 13 Feb 2014 19:45:17 +0000,Thu; 13 Feb 2014 20:49:41 +0000,Thu; 13 Feb 2014 20:49:41 +0000,,HDFS ACLs (HDFS-4685),,,HADOOP-10338;HADOOP-10184,https://issues.apache.org/jira/browse/HADOOP-10344
HADOOP-10345,Improvement,Minor,security,Sanitize the the inputs (groups and hosts) for the proxyuser configuration,Currently there are no input cleansing done on  hadoop.proxyuser.user-name.groups  and hadoop.proxyuser.user-name.hosts .It will be an improvement to trim each value; remove duplicate and empty values during init/refresh.,Closed,Fixed,,Benoy Antony,Benoy Antony,Thu; 13 Feb 2014 20:22:20 +0000,Fri; 15 Aug 2014 05:39:49 +0000,Mon; 31 Mar 2014 20:55:57 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10345
HADOOP-10346,Bug,Blocker,security,Deadlock while logging tokens,Ran into a deadlock between two threads that were both wielding Tokens.  One was trying to log a token while the other was trying to set the token service on a different token.,Closed,Fixed,,Jason Lowe,Jason Lowe,Fri; 14 Feb 2014 18:29:20 +0000,Thu; 10 Apr 2014 13:12:03 +0000,Tue; 18 Feb 2014 15:30:46 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10346
HADOOP-10347,Bug,Critical,build,branch-2 fails to compile,I get the following error compiling branch-2.,Resolved,Not A Problem,,Unassigned,Arpit Agarwal,Fri; 14 Feb 2014 22:55:22 +0000,Wed; 5 Mar 2014 02:21:58 +0000,Fri; 14 Feb 2014 23:25:59 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10347
HADOOP-10348,Improvement,Major,,Deprecate hadoop.ssl.configuration in branch-2; and remove it in trunk,As discussed inhttps://issues.apache.org/jira/browse/HADOOP-8581?focusedCommentId=13786567page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13786567The configuration hadoop.ssl.enabled should be deprecated. We need to mark them as deprecated in CommonConfigurationKeysPublic,Closed,Fixed,,Haohui Mai,Haohui Mai,Fri; 14 Feb 2014 23:05:58 +0000,Thu; 4 Sep 2014 01:16:47 +0000,Thu; 20 Feb 2014 18:48:19 +0000,,,,,HADOOP-10995,https://issues.apache.org/jira/browse/HADOOP-10348
HADOOP-10349,Bug,Major,security,TaskUmbilicalProtocol always uses TOKEN authentication even when configured as SIMPLE.,Since job tokens are always created. HADOOP-9698 introduced the change that will make TaskUmbilicalProtocol always uses TOKEN authentication. Wondering what the benefit is for having the extra TOKEN authentication.,Open,Unresolved,,Unassigned,Jinghui Wang,Sat; 15 Feb 2014 01:29:59 +0000,Sat; 15 Feb 2014 01:29:59 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10349
HADOOP-10350,Bug,Major,,BUILDING.txt should mention openssl dependency required for hadoop-pipes,BUILDING.txt should mention openssl dependency required for hadoop-pipes,Closed,Fixed,,Vinayakumar B,Vinayakumar B,Mon; 17 Feb 2014 15:30:06 +0000,Fri; 15 Aug 2014 05:39:29 +0000,Thu; 10 Apr 2014 19:36:11 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10350
HADOOP-10351,Test,Major,fs/swift;test,Unit test TestSwiftFileSystemLsOperations#testListEmptyRoot and testListNonEmptyRoot failure.,TestSwiftFileSystemLsOperations#testListEmptyRoot and testLisNontEmptyRoot fails because the unit test TestFSMainOperationsSwift creates the testing directory test.build.dir through its parent class. But during the parent classes tearDown; only the test.build.dir/test directory is deleted leaving the test.build.dir in the container. However; tests TestSwiftFileSystemLsOperations#testListEmptyRoot and testListEmptyRoot do not expect the directory to exists in the container thus causing the failure.  TestSwiftFileSystemLsOperations.testListEmptyRoot:126-Assert.assertEquals:472-Assert.assertEquals:128-Assert.failNotEquals:647-Assert.fail:93 Non-empty root/00 SwiftFileStatus{ path=swift://container1.service/home; isDirectory=true; length=0; blocksize=33554432; modification_time=1392850893440} expected:0 but was:1  TestSwiftFileSystemLsOperations.testListNonEmptyRoot:137-Assert.assertEquals:472-Assert.assertEquals:128-Assert.failNotEquals:647-Assert.fail:93 Wrong #of root children/00 SwiftFileStatus{ path=swift://container1.service/home; isDirectory=true; length=0; blocksize=33554432; modification_time=1392850893440}01 SwiftFileStatus{ path=swift://patchtest.softlayer/test; isDirectory=true; length=0; blocksize=33554432; modification_time=1392851462990} expected:1 but was:2,Patch Available,Unresolved,,Unassigned,Jinghui Wang,Thu; 20 Feb 2014 17:58:52 +0000,Tue; 3 Nov 2015 22:58:25 +0000,,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10351
HADOOP-10352,Bug,Major,fs,Recursive setfacl erroneously attempts to apply default ACL to files.,When calling setfacl -R with an ACL spec containing default ACL entries; the command can fail if there is a mix of directories and files underneath the specified path.  It attempts to set the default ACL entries on the files; but only directories can have a default ACL.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Thu; 20 Feb 2014 18:09:20 +0000,Thu; 12 May 2016 18:22:52 +0000,Fri; 21 Feb 2014 06:30:52 +0000,,3.0.0-alpha1,,,HADOOP-10184,https://issues.apache.org/jira/browse/HADOOP-10352
HADOOP-10353,Bug,Major,fs,FsUrlStreamHandlerFactory is not thread safe,The FsUrlStreamHandlerFactory class uses a plain HashMap for caching.When the number of inserted values exceeds the the map's load threshold; it triggers a rehash. During this time; a different thread that performs a get operation on a previously inserted key can obtain a null value instead of the actual value associated with that key.The result is a NPE potentially being thrown when calling FsUrlStreamHandlerFactory#createURLStreamHandler(String protocol) concurrently.,Closed,Fixed,,Tudor Scurtu,Tudor Scurtu,Thu; 20 Feb 2014 18:41:33 +0000,Thu; 4 Sep 2014 01:16:46 +0000,Thu; 27 Feb 2014 19:17:50 +0000,,2.3.0,race-condition,,,https://issues.apache.org/jira/browse/HADOOP-10353
HADOOP-10354,Bug,Major,fs,TestWebHDFS fails after merge of HDFS-4685 to trunk,After merging HDFS-4685 to trunk; some dev environments are experiencing a failure to parse a permission string in TestWebHDFS.  The problem appears to occur only in environments with security extensions enabled on the local file system; such as Smack or ACLs.,Closed,Fixed,,Chris Nauroth,Yongjun Zhang,Thu; 20 Feb 2014 23:31:21 +0000,Thu; 12 May 2016 18:24:18 +0000,Fri; 21 Feb 2014 18:42:37 +0000,,3.0.0-alpha1,,,HADOOP-10220;HDFS-5923,https://issues.apache.org/jira/browse/HADOOP-10354
HADOOP-10355,Bug,Major,,TestLoadGenerator#testLoadGenerator fails,From https://builds.apache.org/job/PreCommit-HDFS-Build/6194//testReport/,Closed,Fixed,HDFS-5993,Haohui Mai,Akira Ajisaka,Fri; 21 Feb 2014 00:01:17 +0000,Thu; 10 Apr 2014 13:11:45 +0000,Fri; 21 Feb 2014 06:19:46 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10355
HADOOP-10356,Bug,Trivial,bin,Corrections in winutils/chmod.c,There are two small things in winutils/chmod.c:  pathName should be pointer to a constant WSTR  the declartion      LPWSTR pathName = NULL;  seems to be wrong.        LPCWSTR pathName = NULL;   should b used instead.  ------------------------------------------------   I believe the fragment      switch (c)      {      case NULL:   to be wrong as pointers are not permitted as   case values.  ------------------------------------------------,Resolved,Fixed,,Ren√© Nyffenegger,Ren√© Nyffenegger,Fri; 21 Feb 2014 06:44:14 +0000,Tue; 30 Aug 2016 01:32:36 +0000,Fri; 8 May 2015 18:43:59 +0000,,,BB2015-05-RFC,,,https://issues.apache.org/jira/browse/HADOOP-10356
HADOOP-10357,Bug,Major,security,Memory Leak in UserGroupInformation.doAs for JDBC Connection to Hive,When using UGI.doAs in order to make a connection there appears to be a memory leak involving the UGI that is used for the doAs and the UGI held by TUGIAssumingTransport.When using this approach to establishing a JDBC connection in an environment that will serve many users and requests client side eventually runs out of memory.,Open,Unresolved,,Unassigned,Larry McCay,Fri; 21 Feb 2014 15:53:29 +0000,Thu; 27 Mar 2014 15:01:12 +0000,,,1.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10357
HADOOP-10358,Improvement,Minor,native,libhadoop doesn't compile on Mac OS X,The native component of hadoop-common (libhadoop.so on linux; libhadoop.dylib on mac) fails to compile on Mac OS X. The problem is in  hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.c at lines 76-78:exec /Users/ilyam/src/github/apache/hadoop-common/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.c:77:26: error: invalid operands to binary expression ('void' and 'int')exec  if(setnetgrent(cgroup) == 1) {exec      ~~~~~~~~~~~~~~~~~~~ ^  ~There are two problems in the code:1) The #ifndef guard only checks for _FreeBSD_ but should check for either one of _FreeBSD_ or _APPLE_. This is because Mac OS X inherits its syscalls from FreeBSD rather than Linux; and thus the setnetgrent() syscall returns void.2) setnetgrentCalledFlag = 1 is set outside the #ifndef guard; but the syscall is only invoked inside the guard. This means that on FreeBSD; endnetgrent() can be called in the cleanup code without a corresponding setnetgrent() invocation.I have a patch that fixes both issues (will attach in a bit). With this patch; I'm able to compile libhadoop.dylib on Mac OS X; which in turn lets me install native snappy; lzo; etc compressor libraries on my client. That lets me run commands like 'hadoop fs -text somefile.lzo' from the macbook rather than having to ssh to a linux box; etc.Note that this patch only fixes the native build of hadoop-common-project. Some other components of hadoop still fail to build their native components; but libhadoop.dylib is enough for the client.,Resolved,Duplicate,HADOOP-7147;HADOOP-10699,Unassigned,Ilya Maykov,Fri; 21 Feb 2014 21:50:49 +0000,Wed; 18 Jun 2014 19:48:45 +0000,Wed; 18 Jun 2014 19:48:45 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10358
HADOOP-10359,Improvement,Minor,native,Native bzip2 compression support is broken on non-Linux systems,While testing the patch for HADOOP-9648; I noticed that the bzip2 native compressor/decompressor support wasn't working properly. I dug around a bit and got native bzip2 support to work on my macbook. Will attach a patch in a bit. (This probably needs to be tested on FreeBSD / Windows / Linux; but I don't have the time to set up the necessary VMs to do it. I assume the build bot will test Linux).,Patch Available,Unresolved,,Unassigned,Ilya Maykov,Sat; 22 Feb 2014 02:12:22 +0000,Wed; 6 May 2015 03:33:11 +0000,,,2.2.0,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10359
HADOOP-10360,Improvement,Minor,,Use 2 network adapter In hdfs read and write,,Closed,Won't Fix,,Unassigned,guodongdong,Mon; 24 Feb 2014 02:07:23 +0000,Thu; 10 Apr 2014 13:11:35 +0000,Tue; 1 Apr 2014 16:01:17 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10360
HADOOP-10361,Bug,Minor,fs,Correct alignment in CLI output for ACLs.,This patch will correct a few lingering issues in the alignment of the text output from the CLI related to ACLs.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Tue; 25 Feb 2014 00:26:32 +0000,Thu; 12 May 2016 18:25:36 +0000,Tue; 25 Feb 2014 05:55:14 +0000,,3.0.0-alpha1,,,HDFS-5923;HADOOP-10241;HADOOP-10270;HADOOP-10184,https://issues.apache.org/jira/browse/HADOOP-10361
HADOOP-10362,Bug,Minor,,Closing of Reader in HadoopArchives#HArchiveInputFormat#getSplits() should check against null,If Reader ctor throws exception; the close() method would be called on null object.,Resolved,Duplicate,HADOOP-11529,Unassigned,Ted Yu,Tue; 25 Feb 2014 04:06:44 +0000,Mon; 23 Nov 2015 02:50:16 +0000,Mon; 23 Nov 2015 02:50:16 +0000,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10362
HADOOP-10363,Bug,Minor,,Closing of SequenceFile.Reader / SequenceFile.Writer in DistCh should check against null,Here is related code:  If ctor of Reader / Writer throws exception; the close() would be called on null object.,Resolved,Not A Problem,,Unassigned,Ted Yu,Tue; 25 Feb 2014 04:11:57 +0000,Fri; 8 May 2015 22:31:58 +0000,Fri; 8 May 2015 22:31:44 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10363
HADOOP-10364,Bug,Major,conf,JsonGenerator in Configuration#dumpConfiguration() is not closed,dumpGenerator is not closed in Configuration#dumpConfiguration()Looking at the source code of org.codehaus.jackson.impl.WriterBasedGenerator#close(); there is more than flushing the buffer.,Open,Unresolved,,Unassigned,Ted Yu,Tue; 25 Feb 2014 05:36:52 +0000,Thu; 28 Sep 2017 17:15:34 +0000,,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10364
HADOOP-10365,Bug,Minor,util,BufferedOutputStream in FileUtil#unpackEntries() should be closed in finally block,outputStream should be closed in finally block.,Closed,Fixed,,Kiran Kumar M R,Ted Yu,Tue; 25 Feb 2014 05:42:32 +0000,Fri; 6 Jan 2017 07:40:18 +0000,Tue; 1 Sep 2015 17:07:37 +0000,,,BB2015-05-RFC,,,https://issues.apache.org/jira/browse/HADOOP-10365
HADOOP-10366,Improvement,Minor,documentation,Add whitespaces between the classes for values in core-default.xml to fit better in browser,"The io.serialization property in core-default.xml has a very long value in a single line; as below which not only break the code style (a very long line) but also not fit well in browser. Due to this single very long line; the ""description"" column can not show in browser by default",Resolved,Fixed,,Kanaka Kumar Avvaru,Chengwei Yang,Tue; 25 Feb 2014 06:40:54 +0000,Tue; 30 Aug 2016 01:32:33 +0000,Thu; 21 May 2015 08:55:58 +0000,,3.0.0-alpha1,documentation;newbie,,,https://issues.apache.org/jira/browse/HADOOP-10366
HADOOP-10367,Bug,Major,build,Hadoop 2.2 Building error,"mvn package -X -Pdist;native;docs;src -DskipTests -DtarERROR Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (make) on project hadoop-hdfs: An Ant BuildException has occured: exec returned: 1 - Help 1org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (make) on project hadoop-hdfs: An Ant BuildException has occured: exec returned: 1	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:217)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)	at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)	at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)	at java.lang.reflect.Method.invoke(Method.java:597)	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)Caused by: org.apache.maven.plugin.MojoExecutionException: An Ant BuildException has occured: exec returned: 1	at org.apache.maven.plugin.antrun.AntRunMojo.execute(AntRunMojo.java:283)	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)	... 19 moreCaused by: /home/nshi/workspace/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/target/antrun/build-main.xml:5: exec returned: 1	at org.apache.tools.ant.taskdefs.ExecTask.runExecute(ExecTask.java:650)	at org.apache.tools.ant.taskdefs.ExecTask.runExec(ExecTask.java:676)	at org.apache.tools.ant.taskdefs.ExecTask.execute(ExecTask.java:502)	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)	at java.lang.reflect.Method.invoke(Method.java:597)	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)	at org.apache.tools.ant.Task.perform(Task.java:348)	at org.apache.tools.ant.Target.execute(Target.java:390)	at org.apache.tools.ant.Target.performTasks(Target.java:411)	at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1397)	at org.apache.tools.ant.Project.executeTarget(Project.java:1366)	at org.apache.maven.plugin.antrun.AntRunMojo.execute(AntRunMojo.java:270)	... 21 moreERROR ERROR ERROR For more information about the errors and possible solutions; please read the following articles:ERROR Help 1 http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionExceptionERROR ERROR After correcting the problems; you can resume the build with the commandERROR   mvn goals -rf :hadoop-hdfs",Resolved,Invalid,,Unassigned,Shining,Tue; 25 Feb 2014 19:25:24 +0000,Sat; 14 Mar 2015 01:04:59 +0000,Sat; 14 Mar 2015 01:04:59 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10367
HADOOP-10368,Bug,Minor,util,InputStream is not closed in VersionInfo ctor,is should be closed at the end of the method.,Closed,Fixed,,Tsuyoshi Ozawa,Ted Yu,Tue; 25 Feb 2014 21:55:09 +0000,Thu; 10 Apr 2014 13:12:09 +0000,Wed; 26 Feb 2014 20:00:49 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10368
HADOOP-10369,Bug,Minor,,"hadoop fs -ls prints ""Found 1 items"" for each entry when globbing",This behavior is new to 2.0. In 1.X it would not print this at all: We can workaround this today by filtering with grep first; but I don't think this is the sort of thing that should be printed to stdout in the first place. Seems like it would be more appropriate to output to stderr.,Resolved,Duplicate,HADOOP-8691,Unassigned,Chris Li,Wed; 26 Feb 2014 01:39:48 +0000,Wed; 26 Feb 2014 14:52:40 +0000,Wed; 26 Feb 2014 14:52:40 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10369
HADOOP-10370,Bug,Minor,documentation,[Doc] not use deprecated properties in *-default.xml,There are some mismatch between DeprecatedrPoperties.html and the four *-default.xml; it's better if we can fix them to avoid the new coming get confused.,Open,Unresolved,,Unassigned,Chengwei Yang,Wed; 26 Feb 2014 02:04:21 +0000,Thu; 12 May 2016 18:26:26 +0000,,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10370
HADOOP-10371,Task,Minor,contrib/eclipse-plugin,The eclipse-plugin cannot work  in my environment,I compile the hadoop  plugin for eclipse 4.2.2 and 4.3.0;but both of them cannot work well;the error is like this:Cannot connect to the Map/Reduce location: hxCall to localhost/127.0.0.1:9001 failed on connection exception: java.net.ConnectException:  : Retrying connect to server: localhost/127.0.0.1:9001. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10; sleepTime=1 SECONDS)  26; 2014 10:53:42   org.apache.hadoop.ipc.Client$Connection handleConnectionFailure : Retrying connect to server: localhost/127.0.0.1:9001. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10; sleepTime=1 SECONDS)  26; 2014 10:53:43   org.apache.hadoop.ipc.Client$Connection handleConnectionFailure : Retrying connect to server: localhost/127.0.0.1:9001. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10; sleepTime=1 SECONDS)  26; 2014 10:53:44   org.apache.hadoop.ipc.Client$Connection handleConnectionFailure : Retrying connect to server: localhost/127.0.0.1:9001. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10; sleepTime=1 SECONDS)  26; 2014 10:53:45   org.apache.hadoop.ipc.Client$Connection handleConnectionFailure : Retrying connect to server: localhost/127.0.0.1:9001. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10; sleepTime=1 SECONDS)I don't know what happens;the version of  eclipse  is not right?,Resolved,Fixed,,Unassigned,huangxing,Wed; 26 Feb 2014 15:16:17 +0000,Thu; 27 Feb 2014 13:44:29 +0000,Thu; 27 Feb 2014 13:44:29 +0000,,1.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-10371
HADOOP-10372,Bug,Minor,fs/s3,Deprecate S3 implementation.,We encourage users to use the S3N implementation. We should consider deprecating the older S3 implementation to avoid confusion down the road.,Resolved,Duplicate,HADOOP-12709,Unassigned,Amandeep Khurana,Wed; 26 Feb 2014 20:26:07 +0000,Tue; 19 Jan 2016 23:24:27 +0000,Tue; 19 Jan 2016 23:24:27 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10372
HADOOP-10373,Improvement,Major,fs/s3,create tools/hadoop-amazon for aws/EMR support,"After HADOOP-9565 adds a marker interface for blobstores; move s3  s3n into their own hadoop-amazon library	keeps the S3 dependencies out of the standard hadoop client dependency graph.	lets people switch this for alternative implementations.feature #2 would let you swap over to another s3n impl (e.g. amazon's) without rebuilding everything",Closed,Fixed,,Steve Loughran,Steve Loughran,Thu; 27 Feb 2014 09:40:41 +0000,Wed; 24 Dec 2014 02:27:16 +0000,Thu; 11 Sep 2014 18:47:06 +0000,,2.3.0,,,HADOOP-11444,https://issues.apache.org/jira/browse/HADOOP-10373
HADOOP-10374,Improvement,Major,,InterfaceAudience annotations should have RetentionPolicy.RUNTIME,There are valid use cases for accessing the InterfaceAudience annotations programatically. In HBase; we are writing a unit test to check whether every class in the client packages are annotated with one of the annotations. Plus; we are also thinking about a golden file to containing all public method sigs; so that we can ensure public facing API-compatibility from a unit test. Related: HBASE-8546; HBASE-10462; HBASE-8275,Closed,Fixed,,Enis Soztutar,Enis Soztutar,Thu; 27 Feb 2014 21:45:33 +0000,Thu; 4 Sep 2014 01:16:46 +0000,Thu; 27 Feb 2014 23:34:08 +0000,,,,,HBASE-10462;HADOOP-10383,https://issues.apache.org/jira/browse/HADOOP-10374
HADOOP-10375,Bug,Minor,fs,Local FS doesn't raise an error on mkdir() over a file,if you mkdir() on a path where there is already a file; the operation doesnot fail. Instead the operation returns 0.This is at odds with the behaviour of HDFS. HADOOP-6229 add the check for the parent dir not being a file; but something similar is needed for the destination dir itself,Resolved,Duplicate,HADOOP-9361,Steve Loughran,Steve Loughran,Fri; 28 Feb 2014 15:54:24 +0000,Sat; 13 Aug 2016 17:12:08 +0000,Sat; 13 Aug 2016 17:12:08 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10375
HADOOP-10376,Improvement,Minor,,Refactor refresh*Protocols into a single generic refreshConfigProtocol,See https://issues.apache.org/jira/browse/HADOOP-10285There are starting to be too many refresh*Protocols We can refactor them to use a single protocol with a variable payload to choose what to do.Thereafter; we can return an indication of success or failure.,Closed,Fixed,,Chris Li,Chris Li,Fri; 28 Feb 2014 19:37:48 +0000,Mon; 21 Mar 2016 15:39:03 +0000,Thu; 12 Jun 2014 01:43:11 +0000,,,,,HDFS-7008;HDFS-10187,https://issues.apache.org/jira/browse/HADOOP-10376
HADOOP-10377,Test,Minor,,End to End test case for refreshing call queue,See https://issues.apache.org/jira/browse/HADOOP-10285Once we have the capability of returning errors from the generic refresh config protocol; we can have an integration test of the features.,Open,Unresolved,,Chris Li,Chris Li,Fri; 28 Feb 2014 19:40:39 +0000,Fri; 28 Feb 2014 19:41:33 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10377
HADOOP-10378,Bug,Major,,Typo in help printed by hdfs dfs -help,"There is a typo in the description of the following commandhdfs dfs -help ""REMAINING_QUATA"" should be ""REMAINING_QUOTA""",Closed,Fixed,,Mit Desai,Mit Desai,Mon; 3 Mar 2014 17:01:37 +0000,Thu; 12 May 2016 18:24:16 +0000,Mon; 3 Mar 2014 22:13:39 +0000,,0.23.9;2.4.0;3.0.0-alpha1,,,HADOOP-12245,https://issues.apache.org/jira/browse/HADOOP-10378
HADOOP-10379,Improvement,Major,,Protect authentication cookies with the HttpOnly and Secure flags,Browser vendors have adopted proposals to enhance the security of HTTP cookies. For example; the server can mark a cookie as Secure so that it will not be transfer via plain-text HTTP protocol; and the server can mark a cookie as HttpOnly to prohibit the JavaScript to access that cookie.This jira proposes to adopt these flags in Hadoop to protect the HTTP cookie used for authentication purposes.,Closed,Fixed,,Haohui Mai,Haohui Mai,Mon; 3 Mar 2014 22:36:15 +0000,Tue; 17 Jun 2014 03:52:10 +0000,Wed; 5 Mar 2014 01:57:22 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10379
HADOOP-10380,Improvement,Major,build,Cleanup javac warnings,There are many javac warnings when compiling Hadoop.See https://builds.apache.org/job/PreCommit-HADOOP-Build/3621/artifact/trunk/patchprocess/filteredTrunkJavacWarnings.txt,Open,Unresolved,,Unassigned,Akira Ajisaka,Tue; 4 Mar 2014 02:05:40 +0000,Thu; 12 May 2016 18:24:11 +0000,,,3.0.0-alpha1,,,HDFS-6090;MAPREDUCE-5800,https://issues.apache.org/jira/browse/HADOOP-10380
HADOOP-10381,Improvement,Major,conf,Support unit suffix in the configuration values ,"BackgroundCurrently; most configuration values implicitly indicate the unit; for example:	dfs.namenode.delegation.key.update-interval; the unit is milliseconds;	dfs.ha.log-roll.period; the unit is seconds	dfs.datanode.du.reserved; the unit is bytesThis causes a few inconveniences1. When modify a configuration file; one need to refer to the document to find out the expected unit; convert from the human readable unit to the expect unit. Otherwise; wrong value may be configured.2. While reviewing the configuration file; it'll take some time to figure out the human readable size of a large number.  For example; 134217728 is actually 128MBProposalFor any space or time related configurations; it should allow include the unit suffix; for example:	space related units: 64kb; 128mb; 1gb	time related units: 1000ms; 1min; 60sec; 1hourIf no unit suffix is given; the default unit for the configuration item is used.",Resolved,Duplicate,HADOOP-7910;HADOOP-8608,Unassigned,Mingjiang Shi,Tue; 4 Mar 2014 08:07:13 +0000,Wed; 5 Mar 2014 09:42:01 +0000,Wed; 5 Mar 2014 09:42:01 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10381
HADOOP-10382,Bug,Major,documentation,Add Apache Tez to the Hadoop homepage as a related project,Add Apache Tez to the Hadoop homepage as a related project,Resolved,Fixed,,Arun C Murthy,Arun C Murthy,Tue; 4 Mar 2014 16:31:20 +0000,Thu; 10 Apr 2014 13:55:09 +0000,Thu; 10 Apr 2014 13:55:09 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10382
HADOOP-10383,Improvement,Major,,InterfaceStability annotations should have RetentionPolicy.RUNTIME,Same as in HADOOP-10374. Forgot to make the change there.,Closed,Fixed,,Enis Soztutar,Enis Soztutar,Tue; 4 Mar 2014 22:45:33 +0000,Wed; 3 Sep 2014 20:39:01 +0000,Tue; 18 Mar 2014 01:12:56 +0000,,,,,HADOOP-10374;HBASE-10462,https://issues.apache.org/jira/browse/HADOOP-10383
HADOOP-10384,Improvement,Minor,,CLONE - Upgrade servlet-api dependency from version 2.5 to 3.0.,Please update the servlet-api jar from 2.5 to javax.servlet 3.0 via Maven:dependency        groupIdjavax.servlet/groupId        artifactIdjavax.servlet-api/artifactId        version3.0.1/version        scopeprovided/scope/dependencyI am running a 2.0.3 dev-cluster and can confirm compatibility. I have removed the servlet-api-2.5.jar file and replaced it with javax.servlet-3.0.jar file. I am using javax.servlet-3.0 because it implements methods that I use for a filter; namely the HttpServletResponse.getStatus() method.I believe it is a gain to have this dependency as it allows more functionality and has so far proven to be backwards compatible.,Resolved,Duplicate,HADOOP-9244,Plamen Jeliazkov,Steve Loughran,Wed; 5 Mar 2014 11:48:10 +0000,Thu; 12 May 2016 18:24:47 +0000,Wed; 5 Mar 2014 11:54:26 +0000,,2.0.3-alpha,,,,https://issues.apache.org/jira/browse/HADOOP-10384
HADOOP-10385,Improvement,Minor,fs,'-ls /dir/*' prints nothing if there are empty subdirectories,For example; empty subdirectories /test/1 and /test/2 are listed by hdfs dfs -ls '/test*' but are not listed by hdfs dfs -ls '/test/*'. This behavior looks confusing.,Open,Unresolved,,Unassigned,Akira Ajisaka,Wed; 5 Mar 2014 19:01:44 +0000,Sat; 7 Jan 2017 01:56:51 +0000,,,2.3.0,,,HADOOP-8691,https://issues.apache.org/jira/browse/HADOOP-10385
HADOOP-10386,Improvement,Minor,ha,Log proxy hostname in various exceptions being thrown in a HA setup,In a HA setup any time we see an exception such as safemode or namenode in standby etc we dont know which namenode it came from. The user has to go to the logs of the namenode and determine which one was active and/or standby around the same time.I think it would help with debugging if any such exceptions could include the namenode hostname so the user could know exactly which namenode served the request.,Closed,Fixed,,Haohui Mai,Arpit Gupta,Wed; 11 Dec 2013 04:23:25 +0000,Thu; 10 Apr 2014 13:11:07 +0000,Wed; 5 Mar 2014 23:19:37 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10386
HADOOP-10387,Bug,Minor,conf;test,Misspelling of threshold in log4j.properties for tests in hadoop-common-project,"log4j.properties file for test contains misspelling ""log4j.threshhold"".We should use ""log4j.threshold"" correctly.",Resolved,Fixed,,Brahma Reddy Battula,Kenji Kikushima,Thu; 6 Mar 2014 02:19:54 +0000,Tue; 30 Aug 2016 01:32:32 +0000,Thu; 7 May 2015 07:56:15 +0000,,2.7.0,,,HADOOP-11922;HDFS-8325;MAPREDUCE-6356;YARN-3577;HADOOP-11854,https://issues.apache.org/jira/browse/HADOOP-10387
HADOOP-10388,New Feature,Major,,Pure native hadoop client,A pure native hadoop client has following use case/advantages:1.  writing Yarn applications using c++2.  direct access to HDFS; without extra proxy overhead; comparing to web/nfs interface.3.  wrap native library to support more languages; e.g. python4.  lightweight; small footprint compare to several hundred MB of JDK and hadoop library with various dependencies.,Open,Unresolved,,Colin P. McCabe,Binglin Chang,Thu; 6 Mar 2014 04:08:47 +0000,Fri; 8 Sep 2017 02:14:50 +0000,,,HADOOP-10388,,,HDFS-6994,https://issues.apache.org/jira/browse/HADOOP-10388
HADOOP-10389,Sub-task,Major,,Native RPCv9 client,,Resolved,Fixed,,Colin P. McCabe,Binglin Chang,Thu; 6 Mar 2014 04:14:55 +0000,Thu; 5 Mar 2015 02:03:19 +0000,Thu; 1 May 2014 02:10:53 +0000,,HADOOP-10388,,,HADOOP-11671,https://issues.apache.org/jira/browse/HADOOP-10389
HADOOP-10390,Bug,Major,test,DFSCIOTest looks for the wrong version of libhdfs,Run benckmark DFSCIOTest failed at libhdfs.so.1hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.3.0-tests.jar DFSCIOTest -write -nrFiles 1 -fileSize 100DFSCIOTest.0.0.114/03/06 02:52:55 INFO fs.DFSCIOTest: nrFiles = 114/03/06 02:52:55 INFO fs.DFSCIOTest: fileSize (MB) = 10014/03/06 02:52:55 INFO fs.DFSCIOTest: bufferSize = 100000014/03/06 02:52:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFile /hadoop/hadoop-smoke/libhdfs/libhdfs.so.1 does not existcan get libhdfs.so.0.0.0 under ./lib/nativeroot@namenode hadoop-smoke# find ./ -name libhdfs*./lib/native/libhdfs.so./lib/native/libhdfs.so.0.0.0./lib/native/libhdfs.a,Patch Available,Unresolved,MAPREDUCE-5781,Binglin Chang,Wenwu Peng,Thu; 6 Mar 2014 07:56:09 +0000,Wed; 6 May 2015 03:32:47 +0000,,,2.0.0-alpha,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10390
HADOOP-10391,Bug,Blocker,security,HADOOP-10211 change for comma-separated list of QOP values broke backwards-compatibility with existing configs.,"HADOOP-10211 changed parsing of QOP values to support a comma-separated list.  This change accidentally broke backwards-compatibility with existing configs.  Previously; an unrecognized value caused it to default to authentication.  Now; an unrecognized value causes IllegalArgumentException.  Some deployments had been using a value of ""none"" in insecure clusters; so the change would break those existing deployments.",Resolved,Won't Fix,,Benoy Antony,Chris Nauroth,Thu; 6 Mar 2014 23:00:01 +0000,Fri; 21 Mar 2014 00:10:32 +0000,Fri; 7 Mar 2014 22:15:20 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10391
HADOOP-10392,Sub-task,Minor,fs,Use FileSystem#makeQualified(Path) instead of Path#makeQualified(FileSystem),There're some methods calling Path.makeQualified(FileSystem); which causes javac warning.,Resolved,Fixed,,Akira Ajisaka,Akira Ajisaka,Sat; 8 Mar 2014 00:51:14 +0000,Thu; 17 Aug 2017 04:41:18 +0000,Fri; 11 Aug 2017 16:26:59 +0000,,2.3.0,BB2015-05-TBR;newbie,,,https://issues.apache.org/jira/browse/HADOOP-10392
HADOOP-10393,Sub-task,Minor,security,Fix hadoop-auth javac warnings,There are quite a few generic warnings and other javac warnings in hadoop-auth.  All of them are minor.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Sat; 8 Mar 2014 00:51:20 +0000,Thu; 10 Apr 2014 13:11:18 +0000,Sat; 8 Mar 2014 02:20:39 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10393
HADOOP-10394,Bug,Major,test,TestAuthenticationFilter is flaky,We have seen this assert cause occasional failures on Ubuntu. The expected fudge is up to 100ms; we have seen up to ~110ms in practice.,Closed,Fixed,,Arpit Agarwal,Arpit Agarwal,Sat; 8 Mar 2014 01:13:46 +0000,Thu; 4 Sep 2014 01:16:48 +0000,Tue; 11 Mar 2014 00:04:47 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10394
HADOOP-10395,Bug,Minor,test,TestCallQueueManager is flaky,TestCallQueueManager#testSwapUnderContention fails occasionally on a test VM with this assert. Although it appears unlikely; it is possible for the queue to be intermittently empty while the putters and getters are running. The assert can be removed or coded differently.,Closed,Fixed,,Arpit Agarwal,Arpit Agarwal,Sat; 8 Mar 2014 16:50:50 +0000,Thu; 4 Sep 2014 01:16:48 +0000,Tue; 11 Mar 2014 00:00:53 +0000,,2.4.0,,,HADOOP-10473,https://issues.apache.org/jira/browse/HADOOP-10395
HADOOP-10396,Bug,Minor,documentation,The 2.X official document is in sorry state.,Hello; I am new to Hadoop 2.X line and I found most of the document does poorly; hard to read; and misleading to a point it's doing more harm than good.Example; most documents in http://hadoop.apache.org/docs/r2.2.0/ identify itself as 'Apache Hadoop 2.1.1-beta' instead of 'Apache Hadoop 2.2 release'The first section building hadoop from source tells nothing about what package required to be built (jdk?g++?)That's just tip of iceberg; that second page 'single cluster setup'; that second section 'Mapreduce Tarball' says: 'You should be able to obtain the MapReduce tarball from the release. If not; you should be able to create a tarball from the source.'What it should do it put a like to hadoop download release link. let alone all that confusing test in section 3 'Setting up the environment' which did no explaination on what does all that do.Lots of documents in release are ridden problems like this. Review is needed for those document or at least a warning about it may be outdated.,Open,Unresolved,,Unassigned,doomleika,Sun; 9 Mar 2014 23:43:58 +0000,Mon; 10 Mar 2014 01:27:44 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10396
HADOOP-10397,Bug,Major,,No 64-bit native lib in Hadoop releases,Recently; I had a chance to talk to a Hadoop user; who complained there's no 64-bit native lib in Hadaoop releases; and it was user unfriendly to make them download all the dependenies to build 64-bit themselves.Hence I checked the recent two releases; 2.2 and 2.3; whose native lib are both ELF 32-bit LSB shared objects. I'm not aware of the reason why we don't release 64-bit; but I'd like to open the ticket to tackle this issue given we didn't before.,Open,Unresolved,,Unassigned,Zhijie Shen,Mon; 10 Mar 2014 02:11:34 +0000,Fri; 2 Jan 2015 00:27:55 +0000,,,,,,HADOOP-10051;HADOOP-9971;HADOOP-3979,https://issues.apache.org/jira/browse/HADOOP-10397
HADOOP-10398,Bug,Major,security,KerberosAuthenticator failed to fall back to PseudoAuthenticator after HADOOP-10078,The problem of the code above is that HTTP_OK does not implies authentication completed.  We should check if the token can be extracted successfully.This problem was reported by Bowen Zhang in this comment earlier.,Resolved,Invalid,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Mon; 10 Mar 2014 19:44:25 +0000,Thu; 20 Nov 2014 18:47:44 +0000,Sat; 22 Mar 2014 00:52:06 +0000,,,,,HADOOP-10417;HADOOP-10416;OOZIE-800,https://issues.apache.org/jira/browse/HADOOP-10398
HADOOP-10399,Sub-task,Major,fs,FileContext API for ACLs.,Add new methods to AbstractFileSystem and FileContext for manipulating ACLs.,Closed,Fixed,,Vinayakumar B,Chris Nauroth,Mon; 10 Mar 2014 22:02:59 +0000,Thu; 12 May 2016 18:21:23 +0000,Mon; 10 Mar 2014 23:42:04 +0000,,2.4.0;3.0.0-alpha1,,,HDFS-5638,https://issues.apache.org/jira/browse/HADOOP-10399
HADOOP-10400,New Feature,Major,fs;fs/s3,Incorporate new S3A FileSystem implementation,"The s3native filesystem has a number of limitations (some of which were recently fixed by HADOOP-9454). This patch adds an s3a filesystem which uses the aws-sdk instead of the jets3t library. There are a number of improvements over s3native including:	Parallel copy (rename) support (dramatically speeds up commits on large files)	AWS S3 explorer compatible empty directories files ""xyz/"" instead of ""xyz_$folder$"" (reduces littering)	Ignores s3native created _$folder$ files created by s3native and other S3 browsing utilities	Supports multiple output buffer dirs to even out IO when uploading files	Supports IAM role-based authentication	Allows setting a default canned ACL for uploads (public; private; etc.)	Better error recovery handling	Should handle input seeks without having to download the whole file (used for splits a lot)This code is a copy of https://github.com/Aloisius/hadoop-s3a with patches to various pom files to get it to build against trunk. I've been using 0.0.1 in production with CDH 4 for several months and CDH 5 for a few days. The version here is 0.0.2 which changes around some keys to hopefully bring the key name style more inline with the rest of hadoop 2.x.Tunable parameters:    fs.s3a.access.key - Your AWS access key ID (omit for role authentication)    fs.s3a.secret.key - Your AWS secret key (omit for role authentication)    fs.s3a.connection.maximum - Controls how many parallel connections HttpClient spawns (default: 15)    fs.s3a.connection.ssl.enabled - Enables or disables SSL connections to S3 (default: true)    fs.s3a.attempts.maximum - How many times we should retry commands on transient errors (default: 10)    fs.s3a.connection.timeout - Socket connect timeout (default: 5000)    fs.s3a.paging.maximum - How many keys to request from S3 when doing directory listings at a time (default: 5000)    fs.s3a.multipart.size - How big (in bytes) to split a upload or copy operation up into (default: 104857600)    fs.s3a.multipart.threshold - Until a file is this large (in bytes); use non-parallel upload (default: 2147483647)    fs.s3a.acl.default - Set a canned ACL on newly created/copied objects (private | public-read | public-read-write | authenticated-read | log-delivery-write | bucket-owner-read | bucket-owner-full-control)    fs.s3a.multipart.purge - True if you want to purge existing multipart uploads that may not have been completed/aborted correctly (default: false)    fs.s3a.multipart.purge.age - Minimum age in seconds of multipart uploads to purge (default: 86400)    fs.s3a.buffer.dir - Comma separated list of directories that will be used to buffer file writes out of (default: uses ${hadoop.tmp.dir}/s3a )Caveats:Hadoop uses a standard output committer which uploads files as filename.COPYING before renaming them. This can cause unnecessary performance issues with S3 because it does not have a rename operation and S3 already verifies uploads against an md5 that the driver sets on the upload request. While this FileSystem should be significantly faster than the built-in s3native driver because of parallel copy support; you may want to consider setting a null output committer on our jobs to further improve performance.Because S3 requires the file length and MD5 to be known before a file is uploaded; all output is buffered out to a temporary file first similar to the s3native driver.Due to the lack of native rename() for S3; renaming extremely large files or directories make take a while. Unfortunately; there is no way to notify hadoop that progress is still being made for rename operations; so your job may time out unless you increase the task timeout.This driver will fully ignore _$folder$ files. This was necessary so that it could interoperate with repositories that have had the s3native driver used on them; but means that it won't recognize empty directories that s3native has been used on.Statistics for the filesystem may be calculated differently than the s3native filesystem. When uploading a file; we do not count writing the temporary file on the local filesystem towards the local filesystem's written bytes count. When renaming files; we do not count the S3-S3 copy as read or write operations. Unlike the s3native driver; we only count bytes written when we start the upload (as opposed to the write calls to the temporary local file). The driver also counts read  write ops; but they are done mostly to keep from timing out on large s3 operations.The AWS SDK unfortunately passes the multipart threshold as an int which meansfs.s3a.multipart.threshold can not be greater than 2^31-1 (2147483647).This is currently implemented as a FileSystem and not a AbstractFileSystem.",Closed,Fixed,HADOOP-13277,Jordan Mendelson,Jordan Mendelson,Tue; 11 Mar 2014 01:57:53 +0000,Sun; 6 Aug 2017 18:12:32 +0000,Mon; 15 Sep 2014 15:32:24 +0000,,2.4.0,,HADOOP-10676;HADOOP-10677;HADOOP-10675,HADOOP-9680;HADOOP-9454;HADOOP-9384;HADOOP-13402;HADOOP-11571,https://issues.apache.org/jira/browse/HADOOP-10400
HADOOP-10401,Bug,Major,util,ShellBasedUnixGroupsMapping#getGroups does not always return primary group first,ShellBasedUnixGroupsMapping#getGroups does not always return the primary group first.  It should do this so that clients who expect it don't get the wrong result.  We should also document that the primary group is returned first in the API.  Note that JniBasedUnixGroupsMapping does return the primary group first.,Closed,Fixed,,Akira Ajisaka,Colin P. McCabe,Tue; 11 Mar 2014 03:21:54 +0000,Fri; 15 Aug 2014 05:39:38 +0000,Wed; 14 May 2014 21:07:23 +0000,,2.4.0,,,HADOOP-10087,https://issues.apache.org/jira/browse/HADOOP-10401
HADOOP-10402,Bug,Major,,Configuration.getValByRegex does not substitute for variables,"When using Configuration.getValByRegex(...); variables are not resolved.  For example: If you then try to do something like Configuration.getValByRegex(foo.*); it will return a Map containing ""foo3=${bar}"" instead of ""foo3=woot""",Closed,Fixed,,Robert Kanter,Robert Kanter,Wed; 12 Mar 2014 00:53:32 +0000,Mon; 1 Dec 2014 03:10:02 +0000,Sun; 10 Aug 2014 22:30:30 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10402
HADOOP-10403,Bug,Major,conf;fs;util,Hadoop2.2 is not fully transfer hostname to IP address on WEB UI,I could not get access when I was trying to click links which are include hostname in the URL.For example: My master ip address is 192.168.1.110 and hostname is hostname1http://192.168.1.110:50070/ works finebut http://hostname1:50070/ does not work. My slave servers also have the same issues. some links only display http://hostname:port.  and some links used IP address replace of hostname.I can ping hostname1; hostname2 and hostname3 and also ssh slave servers without password from master server. ********************************************************************cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.110 hostname1192.168.1.111 hostname2192.168.1.112 hostname3*********************************************************************Please help me to check this issue.  I don't know whether I am missing some configuration.  Your hands will be appreciated.,Resolved,Done,,Unassigned,hawin,Wed; 12 Mar 2014 00:58:40 +0000,Wed; 2 Apr 2014 19:35:06 +0000,Wed; 2 Apr 2014 19:35:06 +0000,,2.2.0,patch,,,https://issues.apache.org/jira/browse/HADOOP-10403
HADOOP-10404,Bug,Minor,,Some accesses to DomainSocketWatcher#closed are not protected by lock,There're two places; NotificationHandler.handle() and kick(); where access to closed is without holding lock.,Closed,Fixed,HDFS-7212,Colin P. McCabe,Ted Yu,Wed; 12 Mar 2014 04:21:50 +0000,Fri; 20 Feb 2015 21:13:01 +0000,Mon; 6 Oct 2014 21:41:39 +0000,,,,,HADOOP-11604,https://issues.apache.org/jira/browse/HADOOP-10404
HADOOP-10405,New Feature,Major,, CLOVER coverage analysis for Hadoop-Commoon tests,Part of the HCFS initiative involves defining test coverage goals for the FileSystem interface.  To do this we first need to evaluate existing coverage.We can't directly measure test coverage using static analysis due to the dynamic way classes are generally invoked in hadoop.However Clover and Corbertura which integrates very well with jenkins can be used to give a profile of how much of a classes methods are executed by unit tests.,Open,Unresolved,,Unassigned,jay vyas,Wed; 12 Mar 2014 16:02:28 +0000,Sat; 19 Jul 2014 19:07:03 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10405
HADOOP-10406,Bug,Major,ipc,TestIPC.testIpcWithReaderQueuing may fail,The test may fail with AssertionError.  The value server.getNumOpenConnections() could be larger than maxAccept; see comments for more details.,Resolved,Fixed,,Xiao Chen,Tsz Wo Nicholas Sze,Wed; 12 Mar 2014 21:08:27 +0000,Tue; 30 Aug 2016 01:32:31 +0000,Mon; 30 Nov 2015 09:19:08 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10406
HADOOP-10407,Sub-task,Minor,ipc,Fix the javac warnings in the ipc package.,Fix the javac warnings in the org.apache.hadoop.ipc package.  Most of them are generic warnings.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Wed; 12 Mar 2014 21:26:15 +0000,Thu; 10 Apr 2014 13:12:02 +0000,Fri; 14 Mar 2014 21:23:58 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10407
HADOOP-10408,Bug,Major,,TestMetricsSystemImpl fails occasionally,TestMetricsSystemImpl#testMultiThreadedPublish fails occasionally due to dropped events. Exception details in comment below.Running the test in a loop on an Ubuntu VM results in a failure after ~20 iterations; however it repros almost daily in our automated test runs.,Open,Unresolved,,Unassigned,Arpit Agarwal,Thu; 13 Mar 2014 19:33:58 +0000,Thu; 13 Mar 2014 22:28:07 +0000,,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10408
HADOOP-10409,Improvement,Major,io,Bzip2 error message isn't clear,If you compile hadoop without bzip2-devel installed (on RHEL); bzip2 doesn't get compiled into libhadoop; as is expected.  This is not documented however and the error message thrown from hadoop checknative -a is not helpful. You can see that it wasn't compiled in here: After installing bzip2-devel and recompiling:  The error message thrown should hint that perhaps libhadoop wasn't compiled with the bzip2 headers installed.  It would also be nice if compile time dependencies were documented somewhere...,Resolved,Won't Fix,,Mohammad Kamrul Islam,Travis Thompson,Fri; 14 Mar 2014 23:28:27 +0000,Thu; 19 Feb 2015 19:11:01 +0000,Fri; 4 Apr 2014 00:55:30 +0000,,2.3.0,,,HADOOP-8642;HADOOP-10452,https://issues.apache.org/jira/browse/HADOOP-10409
HADOOP-10410,New Feature,Major,native,Support ioprio_set in NativeIO,It would be better to HBase application if HDFS layer provide a fine-grained IO request priority. Most of modern kernel should support ioprio_set system call now.,Patch Available,Unresolved,,Liang Xie,Liang Xie,Tue; 18 Mar 2014 07:09:08 +0000,Thu; 12 May 2016 18:22:25 +0000,,,2.4.0;3.0.0-alpha1,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10410
HADOOP-10411,Bug,Minor,,TestCacheDirectives.testExceedsCapacity fails occasionally,See this link; error message:,Resolved,Duplicate,HDFS-6257,Unassigned,Binglin Chang,Tue; 18 Mar 2014 07:11:14 +0000,Wed; 14 May 2014 01:03:29 +0000,Wed; 14 May 2014 01:03:29 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10411
HADOOP-10412,Bug,Major,ipc,First call from Client fails after Server restart,"This seems to happen only for ProtobufRpc based services. Could not reproduce using simple WritableRpc.Steps to reproduce :Consider the case of namenode HA failover. nn1 and nn2 are both namenodes; nn1 is 'active' and nn2 is 'standby'1) Bring down nn1 process. Now nn2 is active2) Bring nn1 process back up. Now nn1 is standby and nn2 is active.3) Manually issue failover using command :$ hdfs haadmin -failover nn2 nn1It is observed that the first call always fails with the Following exception :Operation failed: Failed to become active. Couldn't make NameNode at centos62-2/192.168.2.202:8020 activejava.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: ""centos62-2/192.168.2.202""; destination host is: ""centos62-2"":8020;	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)	at org.apache.hadoop.ipc.Client.call(Client.java:1351)	at org.apache.hadoop.ipc.Client.call(Client.java:1300)	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)	at com.sun.proxy.$Proxy8.transitionToActive(Unknown Source)	at org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB.transitionToActive(HAServiceProtocolClientSideTranslatorPB.java:100)	at org.apache.hadoop.ha.HAServiceProtocolHelper.transitionToActive(HAServiceProtocolHelper.java:48)	at org.apache.hadoop.ha.ZKFailoverController.becomeActive(ZKFailoverController.java:373)	at org.apache.hadoop.ha.ZKFailoverController.access$900(ZKFailoverController.java:59)	at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.becomeActive(ZKFailoverController.java:818)	at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:803)	at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:415)	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:596)	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:495)Caused by: java.io.EOFException	at java.io.DataInputStream.readInt(DataInputStream.java:392)	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:995)	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:891)	at org.apache.hadoop.ha.ZKFailoverController.doGracefulFailover(ZKFailoverController.java:673)	at org.apache.hadoop.ha.ZKFailoverController.access$400(ZKFailoverController.java:59)	at org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:592)	at org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:589)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:415)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)	at org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:589)	at org.apache.hadoop.ha.ZKFCRpcServer.gracefulFailover(ZKFCRpcServer.java:94)	at org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB.gracefulFailover(ZKFCProtocolServerSideTranslatorPB.java:61)	at org.apache.hadoop.ha.proto.ZKFCProtocolProtos$ZKFCProtocolService$2.callBlockingMethod(ZKFCProtocolProtos.java:1548)	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2048)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:415)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2042)The calls succeeds if I issue the same command subsequently",Open,Unresolved,,Unassigned,Arun Suresh,Tue; 18 Mar 2014 21:10:41 +0000,Mon; 23 Jun 2014 12:11:51 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10412
HADOOP-10413,Improvement,Major,,Log statements must include pid and tid information,Log statements do not include process IDs and thread IDs which makes debugging hard when the output of multiple requests is interleaved. It's even worse when looking at the output of test runs because the logs from multiple daemons are interleaved in the same file.Log4j does not provide a builtin mechanism for this; so we'd likely have to write some extra code. One possible solution is to initialize the IDs in the MDC and extract via ConversionPattern as described here.,Open,Unresolved,,Unassigned,Arpit Agarwal,Tue; 18 Mar 2014 21:32:23 +0000,Thu; 12 May 2016 18:23:35 +0000,,,2.3.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10413
HADOOP-10414,Bug,Major,conf,Incorrect property name for RefreshUserMappingProtocol in hadoop-policy.xml,In HDFS-1096 and MAPREDUCE-1836; the name of the ACL property for the RefreshUserMappingsProtocol service changed from security.refresh.usertogroups.mappings.protocol.acl to security.refresh.user.mappings.protocol.acl; but the example in hadoop-policy.xml was not updated. The example should be fixed to avoid confusion.,Closed,Fixed,,Joey Echeverria,Joey Echeverria,Thu; 20 Mar 2014 11:52:49 +0000,Tue; 30 Jun 2015 07:11:20 +0000,Tue; 1 Apr 2014 16:22:42 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10414
HDFS-6131,Bug,Major,documentation,Move HDFSHighAvailabilityWithNFS.apt.vm and HDFSHighAvailabilityWithQJM.apt.vm from Yarn to HDFS,Currently in branch-2; the document HDFSHighAvailabilityWithNFS.apt.vm and HDFSHighAvailabilityWithQJM.apt.vm are still in the Yarn project. We should move them to HDFS just like in trunk.,Closed,Fixed,,Jing Zhao,Jing Zhao,Thu; 20 Mar 2014 18:49:58 +0000,Thu; 10 Apr 2014 13:11:30 +0000,Thu; 20 Mar 2014 21:05:38 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HDFS-6131
HADOOP-10416,Bug,Minor,security,For pseudo authentication; what to do if there is an expired token?,"PseudoAuthenticationHandler currently only gets username from the ""user.name"" parameter.  If there is an expired auth token in the request; the token is ignored (without returning any error back to the client).  Further; if anonymous is enabled; the client will be authenticated as anonymous.The above behavior seems non-desirable since the client does not want to be authenticated as anonymous.",Patch Available,Unresolved,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Sat; 22 Mar 2014 00:44:29 +0000,Wed; 6 May 2015 03:33:50 +0000,,,,BB2015-05-TBR,,HADOOP-10398;OOZIE-800,https://issues.apache.org/jira/browse/HADOOP-10416
HADOOP-10417,Bug,Major,security,There is no token for anonymous authentication,According to Alejandro Abdelnur; if ANONYMOUS is enabled; then there is a token (cookie) and the response is 200.  However; it never sets cookie when the token is ANONYMOUS in the code below.,Open,Unresolved,,Unassigned,Tsz Wo Nicholas Sze,Sat; 22 Mar 2014 00:50:25 +0000,Thu; 27 Mar 2014 16:33:50 +0000,,,,,,HADOOP-10398,https://issues.apache.org/jira/browse/HADOOP-10417
HADOOP-10418,Bug,Major,security,SaslRpcClient should not assume that remote principals are in the default_realm,In SaslRpcClient#getServerPrincipal; when constructing the KerberosPrincipal to compare to the configured value; we just assume that the remote principal is in the default realm configured in /etc/krb5.conf. This will not always be the case; however. Instead; we should use the configured domain_realm mapping to determine the realm of the remote principal.,Closed,Fixed,,Aaron T. Myers,Aaron T. Myers,Sat; 22 Mar 2014 01:29:40 +0000,Tue; 30 Jun 2015 07:11:21 +0000,Mon; 24 Mar 2014 00:03:08 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10418
HADOOP-10419,Bug,Minor,fs,BufferedFSInputStream NPEs on getPos() on a closed stream,if you call getPos on a ChecksumFileSystem after a close() you get an NPE.While throwing an exception in this states is legitimate (HDFS does; RawLocal does not); it should be an IOException,Closed,Fixed,,Steve Loughran,Steve Loughran,Sun; 23 Mar 2014 20:36:06 +0000,Thu; 12 May 2016 18:26:05 +0000,Thu; 3 Jul 2014 13:04:16 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10419
HADOOP-10420,Improvement,Major,fs;fs/swift;tools,Add support to Swift-FS to support tempAuth,Currently; hadoop-openstack Swift FS supports keystone authentication. The attached patch adds support for tempAuth. Users will be able to configure which authentication to use.,Patch Available,Unresolved,,Jim VanOosten,Jinghui Wang,Mon; 24 Mar 2014 17:17:39 +0000,Wed; 4 Nov 2015 03:13:27 +0000,,,2.3.0,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10420
HADOOP-10421,Test,Major,security;test,Enable Kerberos profiled UTs to run with IBM JAVA,KerberosTestUtils in hadoop-auth does not support IBM JAVA; which has different Krb5LoginModule configuration options.,Resolved,Duplicate,HADOOP-10774,Jinghui Wang,Jinghui Wang,Mon; 24 Mar 2014 17:54:05 +0000,Sun; 1 Mar 2015 07:23:58 +0000,Sun; 1 Mar 2015 07:23:58 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10421
HADOOP-10422,Bug,Minor,ipc,Remove redundant logging of RPC retry attempts.,RetryUtils logs each retry attempt at both info level and debug level.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Mon; 24 Mar 2014 20:34:15 +0000,Thu; 4 Sep 2014 01:16:46 +0000,Mon; 24 Mar 2014 23:37:27 +0000,,2.3.0,,,HADOOP-10184,https://issues.apache.org/jira/browse/HADOOP-10422
HADOOP-10423,Improvement,Minor,documentation,Clarify compatibility policy document for combination of new client and old server.,As discussed on the dev mailing lists and MAPREDUCE-4052; we need to update the text of the compatibility policy to discuss a new client combined with an old server.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Mon; 24 Mar 2014 20:59:54 +0000,Thu; 4 Sep 2014 01:16:46 +0000,Mon; 24 Mar 2014 23:28:26 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10423
MAPREDUCE-5810,Bug,Major,contrib/streaming,TestStreamingTaskLog#testStreamingTaskLogWithHadoopCmd is failing,"testStreamingTaskLogWithHadoopCmd(org.apache.hadoop.streaming.TestStreamingTaskLog)  Time elapsed: 44.069 sec  &lt; FAILURE!java.lang.AssertionError: environment set for child is wrong	at org.junit.Assert.fail(Assert.java:93)	at org.junit.Assert.assertTrue(Assert.java:43)	at org.apache.hadoop.streaming.TestStreamingTaskLog.runStreamJobAndValidateEnv(TestStreamingTaskLog.java:157)	at org.apache.hadoop.streaming.TestStreamingTaskLog.testStreamingTaskLogWithHadoopCmd(TestStreamingTaskLog.java:107)Results :Failed tests:   TestStreamingTaskLog.testStreamingTaskLogWithHadoopCmd:107-runStreamJobAndValidateEnv:157 environment set for child is wrong",Closed,Fixed,HADOOP-10438,Akira Ajisaka,Mit Desai,Mon; 24 Mar 2014 21:22:13 +0000,Thu; 12 May 2016 18:22:50 +0000,Fri; 28 Mar 2014 22:22:49 +0000,,2.4.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/MAPREDUCE-5810
HADOOP-10425,Bug,Critical,fs,Incompatible behavior of LocalFileSystem:getContentSummary,Unlike in Hadoop1; FilterFileSystem overrides getContentSummary; which causes content summary to be called on rawLocalFileSystem in Local mode.This impacts the computations of Stats in Hive with getting back FileSizes that include the size of the crc files.,Closed,Fixed,,Tsz Wo Nicholas Sze,Brandon Li,Mon; 24 Mar 2014 23:42:19 +0000,Tue; 22 Apr 2014 20:58:15 +0000,Tue; 25 Mar 2014 02:07:16 +0000,,2.3.0,,,MAPREDUCE-5853;HADOOP-8014,https://issues.apache.org/jira/browse/HADOOP-10425
HADOOP-10426,Sub-task,Minor,fs,CreateOpts.getOpt(..) should declare with generic type argument,Similar to CreateOpts.setOpt(..); the CreateOpts.getOpt(..) should also declare with a generic type parameter T extends CreateOpts.  Then; all the casting from CreateOpts to its subclasses can be avoided.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Tue; 25 Mar 2014 01:18:50 +0000,Fri; 15 Aug 2014 05:39:49 +0000,Tue; 25 Mar 2014 18:11:47 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10426
HADOOP-10427,Improvement,Major,security,KeyProvider implementations should be thread safe,The KeyProvider API should be thread-safe so it can be used safely in server apps.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 25 Mar 2014 05:25:48 +0000,Thu; 12 May 2016 18:26:58 +0000,Wed; 9 Apr 2014 19:43:52 +0000,,3.0.0-alpha1,,HADOOP-10430;HADOOP-10433,,https://issues.apache.org/jira/browse/HADOOP-10427
HADOOP-10428,Improvement,Major,security,JavaKeyStoreProvider should accept keystore password via configuration falling back to ENV VAR,Currently the password for the JavaKeyStoreProvider must be set in an ENV VAR.Allowing the password to be set via configuration enables applications to interactively ask for the password before initializing the JavaKeyStoreProvider.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 25 Mar 2014 05:34:05 +0000,Thu; 12 May 2016 18:26:57 +0000,Thu; 10 Apr 2014 05:09:32 +0000,,3.0.0-alpha1,,HADOOP-10430;HADOOP-10433,HADOOP-12597,https://issues.apache.org/jira/browse/HADOOP-10428
HADOOP-10429,Improvement,Major,security,KeyStores should have methods to generate the materials themselves; KeyShell should use them,Currently; the KeyProvider API expects the caller to provide the key materials. And; the KeyShell generates key materials.For security reasons; KeyProvider implementations may want to generate and hide (from the user generating the key) the key materials.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 25 Mar 2014 05:40:17 +0000,Thu; 12 May 2016 18:26:58 +0000,Wed; 9 Apr 2014 19:46:55 +0000,,3.0.0-alpha1,,HADOOP-10430;HADOOP-10433,,https://issues.apache.org/jira/browse/HADOOP-10429
HADOOP-10430,Improvement,Major,security,KeyProvider Metadata should have an optional description; there should be a method to retrieve the metadata from all keys,Being able to attach an optional description (and show it when displaying metadata) will enable giving some context on the keys.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 25 Mar 2014 05:51:27 +0000,Thu; 12 May 2016 18:26:58 +0000,Wed; 23 Apr 2014 17:44:57 +0000,,3.0.0-alpha1,,HADOOP-10431;HADOOP-10433;HADOOP-10427;HADOOP-10428;HADOOP-10429,,https://issues.apache.org/jira/browse/HADOOP-10430
HADOOP-10431,Improvement,Major,security,Change visibility of KeyStore.Options getter methods to public,Making Options getter methods public will enable KeyProvider implementations to use those classes.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 25 Mar 2014 05:57:28 +0000,Thu; 12 May 2016 18:26:58 +0000,Sat; 12 Apr 2014 18:05:23 +0000,,3.0.0-alpha1,,HADOOP-10433;HADOOP-10430,,https://issues.apache.org/jira/browse/HADOOP-10431
HADOOP-10432,Improvement,Major,security,Refactor SSLFactory to expose static method to determine HostnameVerifier,The SSFactory.getHostnameVerifier() method is private and takes a configuration to fetch a hardcoded property. Having a public method to resolve a verifier based on the provided value will enable getting a verifier based on the verifier constant (DEFAULT; DEFAULT_AND_LOCALHOST; STRICT; STRICT_IE6; ALLOW_ALL).,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 25 Mar 2014 06:08:09 +0000,Thu; 12 May 2016 18:26:59 +0000,Wed; 9 Apr 2014 19:40:50 +0000,,3.0.0-alpha1,,HADOOP-10433,,https://issues.apache.org/jira/browse/HADOOP-10432
HADOOP-10433,Improvement,Major,security,Key Management Server based on KeyProvider API,"(from HDFS-6134 proposal)Hadoop KMS is the gateway; for Hadoop and Hadoop clients; to the underlying KMS. It provides an interface that works with existing Hadoop security components (authenticatication; confidentiality).Hadoop KMS will be implemented leveraging the work being done in HADOOP-10141 and HADOOP-10177.Hadoop KMS will provide an additional implementation of the Hadoop KeyProvider class. This implementation will be a client-server implementation.The client-server protocol will be secure:	Kerberos HTTP SPNEGO (authentication)	HTTPS for transport (confidentiality and integrity)	Hadoop ACLs (authorization)The Hadoop KMS implementation will not provide additional ACL to access encrypted files. For sophisticated access control requirements; HDFS ACLs (HDFS-4685) should be used.Basic key administration will be supported by the Hadoop KMS via the; already available; Hadoop KeyShell command line toolThere are minor changes that must be done in Hadoop KeyProvider functionality:The KeyProvider contract; and the existing implementations; must be thread-safeKeyProvider API should have an API to generate the key material internallyJavaKeyStoreProvider should use; if present; a password provided via configurationKeyProvider Option and Metadata should include a label (for easier cross-referencing)To avoid overloading the underlying KeyProvider implementation; the Hadoop KMS will cache keys using a TTL policy.Scalability and High Availability of the Hadoop KMS can achieved by running multiple instances behind a VIP/Load-Balancer. For High Availability; the underlying KeyProvider implementation used by the Hadoop KMS must be High Available.",Closed,Fixed,HADOOP-10528,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 25 Mar 2014 06:15:04 +0000,Thu; 12 May 2016 18:26:59 +0000,Mon; 5 May 2014 21:43:42 +0000,,3.0.0-alpha1,,HADOOP-10534;HADOOP-10427;HADOOP-10428;HADOOP-10429;HADOOP-10430;HADOOP-10431;HADOOP-10432,,https://issues.apache.org/jira/browse/HADOOP-10433
HADOOP-10434,Improvement,Minor,fs,"Is it possible to use ""df"" to calculate the dfs usage instead of ""du""","When we run datanode from the machine with big disk volume; it's found du operations from org.apache.hadoop.fs.DU's DURefreshThread cost lots of disk performance.As we use the whole disk for hdfs storage; it is possible calculate volume usage via ""df"" command. Is it necessary adding the ""df"" option for usage calculation in hdfs (org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice)?",Resolved,Duplicate,HADOOP-12974,Unassigned,MaoYuan Xian,Tue; 25 Mar 2014 07:35:00 +0000,Sun; 18 Dec 2016 08:30:31 +0000,Sun; 18 Dec 2016 08:30:24 +0000,,2.3.0,BB2015-05-TBR,,HDFS-8791,https://issues.apache.org/jira/browse/HADOOP-10434
MAPREDUCE-5809,Improvement,Major,distcp,Enhance distcp to support preserving HDFS ACLs.,This issue tracks enhancing distcp to add a new command-line argument for preserving HDFS ACLs from the source at the copy destination.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Tue; 25 Mar 2014 17:50:53 +0000,Wed; 3 Sep 2014 20:33:52 +0000,Fri; 16 May 2014 18:36:02 +0000,,2.4.0,,,MAPREDUCE-5639;HDFS-4685;HADOOP-10557,https://issues.apache.org/jira/browse/MAPREDUCE-5809
HADOOP-10436,Bug,Major,util,ToolRunner is not thread-safe,ToolRunner class is not thread-safe because it uses GenericOptionsParser.  The constructor of GenericOptionsParser uses 'OptionBuilder' which is a singleton class that uses instance variables.  In other words; OptionBuilder is NOT thread safe.  As a result; when multiple Hadoop jobs are triggered simultaneously using ToolRunner they end up stepping on each other.The easiest way to fix it is by making 'buildGeneralOptions' synchronized in GenericOptionsParser.private static synchronized Options buildGeneralOptions(Options opts) {If this seems like the correct way of fixing this; either we can provide a patch or someone can quickly fix it.  Thanks.Ajay Chitreachitre@cisco.comVirendra Singhvirsingh@cisco.com,Open,Unresolved,,Tsuyoshi Ozawa,Ajay Chitre,Tue; 25 Mar 2014 18:45:13 +0000,Tue; 9 Sep 2014 11:58:25 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10436
HADOOP-10437,Sub-task,Minor,conf;util,Fix the javac warnings in the conf and the util package,There are a few minor javac warnings in org.apache.hadoop.conf and org.apache.hadoop.util.  We should fix them.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Tue; 25 Mar 2014 20:48:20 +0000,Thu; 10 Apr 2014 13:11:42 +0000,Wed; 26 Mar 2014 19:10:28 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10437
HADOOP-10438,Bug,Major,test,TestStreamingTaskLog fails,TestStreamingTaskLog#testStreamingTaskLogWithHadoopCmd fails intermittently.,Resolved,Duplicate,MAPREDUCE-5810,Unassigned,Akira Ajisaka,Wed; 26 Mar 2014 01:35:19 +0000,Wed; 26 Mar 2014 01:36:58 +0000,Wed; 26 Mar 2014 01:36:58 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10438
HADOOP-10439,Sub-task,Major,build,Fix compilation error in branch-2 after HADOOP-10426,HADOOP-10426 removes the import of java.io.File in branch-2; which causes compilation error.,Closed,Fixed,,Haohui Mai,Haohui Mai,Wed; 26 Mar 2014 07:45:53 +0000,Fri; 15 Aug 2014 05:39:38 +0000,Wed; 26 Mar 2014 08:03:06 +0000,,2.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-10439
HADOOP-10440,Bug,Major,fs,HarFsInputStream of HarFileSystem; when reading data; computing the position has bug,"In the HarFsInputStream of HarFileSystem; when reading data by interface ""int read(byte[] b)""; ""int read(byte[] b; int offset; int len)"" wille be called and position wille be update; so position  need not be update in interface ""int read(byte[] b)""",Closed,Fixed,,guodongdong,guodongdong,Wed; 26 Mar 2014 11:21:54 +0000,Thu; 10 Apr 2014 13:12:06 +0000,Wed; 26 Mar 2014 17:50:23 +0000,,2.3.0,,,HADOOP-9361,https://issues.apache.org/jira/browse/HADOOP-10440
HADOOP-10441,Bug,Blocker,metrics,"Namenode metric ""rpc.RetryCache/NameNodeRetryCache.CacheHit"" can't be correctly processed by Ganglia","The issue is reported by Dmytro Sen:Recently added Namenode metric ""rpc.RetryCache/NameNodeRetryCache.CacheHit"" can't be correctly processed by Ganglia because its name contains ""/""Proposal: Namenode metric ""rpc.RetryCache/NameNodeRetryCache.CacheHit"" should be renamed to ""rpc.RetryCache.NameNodeRetryCache.CacheHit""Here - org.apache.hadoop.ipc.metrics.RetryCacheMetrics#RetryCacheMetrics",Closed,Fixed,,Jing Zhao,Jing Zhao,Wed; 26 Mar 2014 17:49:08 +0000,Thu; 10 Apr 2014 13:11:13 +0000,Wed; 26 Mar 2014 20:38:02 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10441
HADOOP-10442,Bug,Blocker,,Group look-up can cause segmentation fault when certain JNI-based mapping module is used.,When JniBasedUnixGroupsNetgroupMapping or JniBasedUnixGroupsMapping is used; we get segmentation fault very often. The same system ran 2.2 for months without any problem; but as soon as upgrading to 2.3; it started crashing.  This resulted in multiple name node crashes per day.The server was running nslcd (nss-pam-ldapd-0.7.5-15.el6_3.2). We did not see this problem on the servers running sssd. There was one change in the C code and it modified the return code handling after getgrouplist() call. If the function returns 0 or a negative value less than -1; it will do realloc() instead of returning failure.,Closed,Fixed,,Kihwal Lee,Kihwal Lee,Wed; 26 Mar 2014 19:22:56 +0000,Wed; 3 Sep 2014 20:39:01 +0000,Thu; 27 Mar 2014 20:21:37 +0000,,2.3.0;2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10442
HADOOP-10443,Sub-task,Minor,,limit symbol visibility in libhdfs-core.so and libyarn-core.so,We should avoid exposing all the symbols of libhdfs-core.so and libyarn-core.so.  Otherwise; we they conflict with symbols used in the applications using the libraries.  This can be done with gcc's symbol visibility directives.Also; we should probably include libuv and libprotobuf-c statically into our libraries; since most distributions don't yet include these libraries; and we don't want to have version issues there.,Resolved,Duplicate,HADOOP-10640,Colin P. McCabe,Colin P. McCabe,Thu; 27 Mar 2014 02:31:38 +0000,Thu; 12 Jun 2014 20:00:51 +0000,Thu; 12 Jun 2014 20:00:51 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10443
HADOOP-10444,Sub-task,Major,,add pom.xml infrastructure for hadoop-native-core,Add pom.xml infrastructure for hadoop-native-core; so that it builds under Maven.  We can look to how we integrated CMake into hadoop-hdfs-project and hadoop-common-project for inspiration here.  In the long term; it would be nice to use a Maven plugin here (see HADOOP-8887),Resolved,Fixed,,Binglin Chang,Colin P. McCabe,Thu; 27 Mar 2014 02:33:43 +0000,Fri; 30 May 2014 23:40:34 +0000,Fri; 30 May 2014 23:40:34 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10444
HADOOP-10445,Sub-task,Major,,Implement DataTransferProtocol in libhdfs-core.so,We need to implement DataTransferProtocol so that we can send and receive data to and from DataNodes.  This is different protocol from Hadoop IPC; so it will require a slightly different code path.,Open,Unresolved,,Unassigned,Colin P. McCabe,Thu; 27 Mar 2014 02:35:46 +0000,Thu; 27 Mar 2014 02:35:46 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10445
HADOOP-10446,Sub-task,Major,,native code for reading Hadoop configuration XML files,We need to have a way to read Hadoop configuration XML files in the native HDFS and YARN clients.  This will allow those clients to discover the locations of NameNodes; YARN daemons; and other configuration settings; etc. etc.,Resolved,Duplicate,HADOOP-10640,Colin P. McCabe,Colin P. McCabe,Thu; 27 Mar 2014 02:38:25 +0000,Thu; 12 Jun 2014 19:59:14 +0000,Thu; 12 Jun 2014 19:59:14 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10446
HADOOP-10447,Sub-task,Major,,Implement C code for parsing Hadoop / HDFS URIs,We need some glue code for parsing Hadoop or HDFS URIs in C.  Probably we should just put a small 'Path' wrapper around a URI parsing library like http://uriparser.sourceforge.net/ (BSD licensed),Resolved,Duplicate,HADOOP-10640,Colin P. McCabe,Colin P. McCabe,Thu; 27 Mar 2014 02:43:35 +0000,Thu; 12 Jun 2014 19:57:31 +0000,Thu; 12 Jun 2014 19:57:31 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10447
HADOOP-10448,Sub-task,Major,security,Support pluggable mechanism to specify proxy user settings,We have a requirement to support large number of superusers. (users who impersonate as another user) (http://hadoop.apache.org/docs/r1.2.1/Secure_Impersonation.html) Currently each  superuser needs to be defined in the core-site.xml via proxyuser settings. This will be cumbersome when there are 1000 entries.It seems useful to have a pluggable mechanism to specify  proxy user settings with the current approach as the default.,Closed,Fixed,,Benoy Antony,Benoy Antony,Thu; 27 Mar 2014 17:10:38 +0000,Wed; 3 Sep 2014 20:36:28 +0000,Thu; 29 May 2014 23:42:15 +0000,,2.3.0,,,HIVE-7184;HADOOP-10593,https://issues.apache.org/jira/browse/HADOOP-10448
HADOOP-10449,Sub-task,Minor,security,Fix the javac warnings in the security packages.,The are a few minor javac warnings.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Fri; 28 Mar 2014 00:14:44 +0000,Thu; 10 Apr 2014 13:11:57 +0000,Fri; 28 Mar 2014 18:19:10 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10449
HADOOP-10450,Bug,Major,io;native,Build zlib native code bindings in hadoop.dll for Windows.,Currently; the zlib native code bindings are not built in to hadoop.dll for Windows.  This patch will include this in the Windows build.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Fri; 28 Mar 2014 18:20:49 +0000,Thu; 12 May 2016 18:23:06 +0000,Fri; 28 Mar 2014 20:00:51 +0000,,2.3.0;3.0.0-alpha1,,,HADOOP-8540,https://issues.apache.org/jira/browse/HADOOP-10450
HADOOP-10451,Improvement,Trivial,security,Remove unused field and imports from SaslRpcServer,There were unused fields and import remained on SaslRpcServer.This jira is to remove cleanup those  fields from SaslRpcServer.,Closed,Fixed,,Benoy Antony,Benoy Antony,Mon; 31 Mar 2014 16:14:36 +0000,Wed; 10 Feb 2016 03:25:11 +0000,Mon; 31 Mar 2014 18:44:05 +0000,,2.3.0,,,HIVE-6741;HIVE-7620;HADOOP-10221,https://issues.apache.org/jira/browse/HADOOP-10451
HADOOP-10452,Improvement,Minor,build;documentation,BUILDING.txt needs to be updated,BUILDING.txt is missing some information about native compression libraries.  Noticeably if you are missing zlib/bzip2/snappy devel libraries; those will get silently skipped unless you pass the -Drequire.$LIB option (e.x. -Drequire.snappy).,Open,Unresolved,,Travis Thompson,Travis Thompson,Mon; 31 Mar 2014 18:08:23 +0000,Sat; 7 Jan 2017 01:56:56 +0000,,,2.3.0,,,HADOOP-10409;HDFS-8346;HADOOP-8642;HADOOP-11627,https://issues.apache.org/jira/browse/HADOOP-10452
HADOOP-10453,Bug,Major,,Do not use AuthenticatedURL in hadoop core,As Daryn Sharp has suggested in HDFS-4564:AuthenticatedURL is not used because it is buggy in part to causing replay attacks; double attempts to kerberos authenticate with the fallback authenticator if the TGT is expired; incorrectly uses the fallback authenticator (required by oozie servers) to add the username parameter which webhdfs has already included in the uri.AuthenticatedURL's attempt to do SPNEGO auth is a no-op because the JDK transparently does SPNEGO when the user's Subject (UGI) contains kerberos principals. Since AuthenticatedURL is now not used; webhdfs has to check the TGT itself for token operations.Bottom line is AuthenticatedURL is unnecessary and introduces nothing but problems for webhdfs. It's only useful for oozie's anon/non-anon support.However; several functionalities that relies on SPNEGO in secure mode suffer from the same problem. For example; NNs / JNs create HTTP connections to exchange fsimage and edit logs. Currently all of them are through AuthenticatedURL. This needs to be fixed to avoid security vulnerabilities.This jira purposes to remove AuthenticatedURL from hadoop core and to move it to oozie.,Open,Unresolved,,Unassigned,Haohui Mai,Mon; 31 Mar 2014 18:36:59 +0000,Sat; 7 Jan 2017 01:56:49 +0000,,,,,,HDFS-4564;HADOOP-12787;HADOOP-10799,https://issues.apache.org/jira/browse/HADOOP-10453
HADOOP-10454,Improvement,Major,,Provide FileContext version of har file system,Add support for HarFs; the FileContext version of HarFileSystem.,Closed,Fixed,,Kihwal Lee,Kihwal Lee,Tue; 1 Apr 2014 13:50:20 +0000,Wed; 3 Sep 2014 20:31:10 +0000,Thu; 3 Apr 2014 22:48:47 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10454
HADOOP-10455,Bug,Major,ipc,When there is an exception; ipc.Server should first check whether it is an terse exception,ipc.Server allows application servers to define terse exceptions; see Server.addTerseExceptions.  For terse exception; it only prints a short message but not the stack trace.  However; if an exception is both RuntimeException and terse exception; it still prints out the stack trace of the exception.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Tue; 1 Apr 2014 22:02:12 +0000,Mon; 30 Jun 2014 08:18:12 +0000,Wed; 2 Apr 2014 01:03:49 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10455
HADOOP-10456,Bug,Major,conf,Bug in Configuration.java exposed by Spark (ConcurrentModificationException),The following exception occurs non-deterministically:java.util.ConcurrentModificationException        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)        at java.util.HashMap$KeyIterator.next(HashMap.java:960)        at java.util.AbstractCollection.addAll(AbstractCollection.java:341)        at java.util.HashSet.init(HashSet.java:117)        at org.apache.hadoop.conf.Configuration.init(Configuration.java:671)        at org.apache.hadoop.mapred.JobConf.init(JobConf.java:439)        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:110)        at org.apache.spark.rdd.HadoopRDD$$anon$1.init(HadoopRDD.scala:154)        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)        at org.apache.spark.scheduler.Task.run(Task.scala:53)        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:415)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)        at java.lang.Thread.run(Thread.java:744),Closed,Fixed,,Nishkam Ravi,Nishkam Ravi,Wed; 2 Apr 2014 01:45:40 +0000,Thu; 4 Sep 2014 01:13:42 +0000,Fri; 4 Apr 2014 09:49:11 +0000,,2.3.0,,,SPARK-1097,https://issues.apache.org/jira/browse/HADOOP-10456
HADOOP-10457,Bug,Minor,fs/s3,S3N NPEs if you do a read() after a seek() past the EOF,"if you do a seek past the EOF of an S3n file	it doesn't throw any exception	on the next read; you get to see a stack trace",Closed,Duplicate,HADOOP-10589,Steve Loughran,Steve Loughran,Wed; 2 Apr 2014 09:27:19 +0000,Thu; 12 May 2016 18:22:19 +0000,Wed; 18 Jun 2014 00:36:54 +0000,,2.4.0;3.0.0-alpha1,,,HADOOP-9361,https://issues.apache.org/jira/browse/HADOOP-10457
HADOOP-10458,Improvement,Minor,fs,swifts should throw FileAlreadyExistsException on attempt to overwrite file,the swift:// filesystem checks for and rejects create() calls over an existing file if overwrite = false; but it throws a custom exception. SwiftPathExistsExceptionIf it threw a org.apache.hadoop.fs.FileAlreadyExistsException it would match HDFS,Closed,Fixed,,Steve Loughran,Steve Loughran,Wed; 2 Apr 2014 12:11:28 +0000,Fri; 15 Aug 2014 05:39:39 +0000,Thu; 3 Jul 2014 13:14:32 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10458
HADOOP-10459,Bug,Major,tools/distcp,distcp V2 doesn't preserve root dir's attributes when -p is specified,"Two issues were observed with distcpV2ISSUE 1. when copying a source dir to target dir with ""-pu"" option using command   ""distcp -pu source-dir target-dir""The source dir's owner is not preserved at target dir. Simiarly other attributes of source dir are not preserved.  Supposedly they should be preserved when no -update and no -overwrite specified. There are two scenarios with the above command:a. when target-dir already exists. Issuing the above command will  result in target-dir/source-dir (source-dir here refers to the last component of the source-dir path in the command line) at target file system; with all contents in source-dir copied to under target-dir/src-dir. The issue in this case is; the attributes of src-dir is not preserved.b. when target-dir doesn't exist. It will result in target-dir with all contents of source-dir copied to under target-dir. This issue in this  case is; the attributes of source-dir is not carried over to target-dir.For multiple source cases; e.g.; command   ""distcp -pu source-dir1 source-dir2 target-dir""No matter whether the target-dir exists or not; the multiple sources are copied to under the target dir (target-dir is created if it didn't exist). And their attributes are preserved. ISSUE 2. with the following command:  ""distcp source-dir target-dir""when source-dir is an empty directory; and when target-dir doesn't exist; source-dir is not copied; actually the command behaves like a no-op. However; when the source-dir is not empty; it would be copied and results in target-dir at the target file system containing a copy of source-dir's children.To be consistent; empty source dir should be copied too. Basically the  above distcp command should cause target-dir get created at target file system; and the source-dir's attributes are preserved at target-dir when -p is passed.",Closed,Fixed,,Yongjun Zhang,Yongjun Zhang,Mon; 24 Mar 2014 23:44:04 +0000,Mon; 4 Apr 2016 23:11:17 +0000,Thu; 3 Apr 2014 01:00:21 +0000,,2.3.0,,,HADOOP-9705;HADOOP-13002,https://issues.apache.org/jira/browse/HADOOP-10459
HADOOP-10460,Bug,Major,documentation,Please update on-line documentation for hadoop 2.3,"Documentation on page:http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/SingleNodeSetup.htmlcontains steps like: $ cp conf/*.xml inputbut after checked out repository ""conf"" does not exists (I quess it was moved to etc/hadoop)Few lines below in section ""Execution"" there are steps:  $ bin/hadoop namenode -format  - OK  $ bin/start-all.sh  - this file has been removed",Resolved,Invalid,,Unassigned,Darek,Thu; 3 Apr 2014 11:22:21 +0000,Mon; 19 May 2014 05:31:57 +0000,Mon; 19 May 2014 05:31:41 +0000,,2.3.0,newbie,,HADOOP-10618,https://issues.apache.org/jira/browse/HADOOP-10460
HADOOP-10461,Bug,Minor,,Create an extensible single point entry for of HCFS tests - using RawLocalFileSystem as example impl.,"Currently alot of manual inheritance and stub classes are required in order to run the FileSystemBaseContract and FSMainOperations tests.   This means that the HCFS Test suite for any given FileSystem spans many classes; and makes it confusing and cumbersome for anyone to create new tests for a file system; and also very hard to audit and update the tests for a particular file system.If each FileSystem had a single point of entry for its tests; we would be much better off.There are two ways to do this.  	Have a folder or package for each file system (i.e.  .//hadoop-common/src/test/java/org/apache/hadoop/fs/rawlocal ) Or	Wrap all the HCFS Test implementation classes into a single class (i.e. RawLocalFileSystemHCFSTestSuite.java).The purpose of this JIRA is to implement a wrapper class for the RawLocalFileSystem; which serves as a single point of entry to ALL RawLocalFileSystem tests.  This will cleanup the RawLocalFileSystem tests while providing other HCFS implementors a template they can use for their own filesystems (i.e. they can just copy the code and customize it however they want).For example:  This is a sampling of the classes necessary to glue Local FS implementations into the Existing generic FileSystem test classes:  Lets wrap these tests into a Suite for the RawLocalFileSystem; such that any once can copy that suite to build tests for their own file system. Using the ""Enclosed"" Junit (http://junit-team.github.io/junit/javadoc/4.10/org/junit/experimental/runners/Enclosed.html ) class; we can combine all of the RawLocalFileSystem tests into a single class.  That way  	Can save on boiler plate for setup and declaring the same FileSystem stubs over and over again; making the test ""logic"" more easy to focus on.  For example; here are some repeat file context declarations for ViewFS tests: 	Can easily be adopted for other FileSystems.	Can read in System properties to skip certain tests ; thus providing support for the type of variability that we know FileSystem tests require.  These can then be used to implement the semantics of HADOOP-9361.Ideally; we could replace RawLocalFileSystem tests with this injector as a second follow up patch to this; it would reduce the overall amount of code required; probably.",Resolved,Won't Fix,HADOOP-6275,Unassigned,jay vyas,Fri; 4 Apr 2014 03:24:23 +0000,Thu; 12 May 2016 18:24:21 +0000,Sat; 31 Oct 2015 11:55:42 +0000,,,,,HADOOP-7601,https://issues.apache.org/jira/browse/HADOOP-10461
HADOOP-10462,Bug,Major,,DF#getFilesystem is not parsing the command output,DF#getFileSystem returns null if DF#getMount is not called before. This is because DF#getFileSystem is not parsing the df command output.This causes NameNodeResourceChecker to print the log as follows if the available space on the volume used for saving fsimage is less than the specified value (100MB by default).,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Fri; 7 Mar 2014 00:25:56 +0000,Wed; 3 Sep 2014 20:36:29 +0000,Fri; 4 Apr 2014 09:15:53 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10462
HADOOP-10463,Test,Major,fs,Bring RawLocalFileSystem test coverage to 100%,"RawLocalFileSystem coverage is at about 80% (measured with cobertura) at the moment.  A few notable untested code paths are:	primitiveMkdir	markSupported	int read()Lets get it as close as possible to 100% coverage.  In the process of analyzing existing abstract tests which exersize RawLocalFileSystem;  we will also pave the way for HADOOP-10461.",Open,Unresolved,,jay vyas,jay vyas,Fri; 4 Apr 2014 11:14:05 +0000,Fri; 4 Apr 2014 11:14:20 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10463
HADOOP-10464,Test,Major,,Make TestTrash compatible with HADOOP-10461 .,"The current TestTrash implementation is like a  static utility class  It should be refactored to be a ""pure"" unit test that can be transparently integrated into TestSuites which don't have any prior knowledge of the tests it defines.",Open,Unresolved,,Unassigned,jay vyas,Sun; 6 Apr 2014 19:04:04 +0000,Sun; 6 Apr 2014 19:04:04 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10464
HADOOP-10465,Improvement,Minor,,Fix use of generics within SortedMapWritable,SortedMapWritable could use Generics the right way so that there is no warning within the implementation and more important for the consumer.,Resolved,Fixed,,Bertrand Dechoux,Bertrand Dechoux,Mon; 7 Apr 2014 15:03:57 +0000,Sun; 10 Jul 2016 15:35:07 +0000,Mon; 23 Nov 2015 02:18:56 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10465
HADOOP-10466,Improvement,Minor,security,Lower the log level in UserGroupInformation,That's more or less the continuation of HADOOP-10015... Some info should be debug if we apply the same policy.,Closed,Fixed,,Nicolas Liochon,Nicolas Liochon,Mon; 7 Apr 2014 15:09:21 +0000,Thu; 12 May 2016 18:24:56 +0000,Mon; 7 Apr 2014 18:08:52 +0000,,2.3.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10466
HADOOP-10467,Sub-task,Major,security,Enable proxyuser specification to support list of users in addition to list of groups.,Today ; the proxy user specification supports only list of groups. In some cases; it is useful to specify the list of users in addition to list of groups.,Closed,Fixed,,Benoy Antony,Benoy Antony,Mon; 7 Apr 2014 19:10:43 +0000,Wed; 3 Sep 2014 20:36:33 +0000,Thu; 8 May 2014 00:47:07 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10467
HADOOP-10468,Bug,Blocker,,TestMetricsSystemImpl.testMultiThreadedPublish fails intermediately,TestMetricsSystemImpl.testMultiThreadedPublish can fail intermediately due to the insufficient size of the sink queue: The unit test should increase the default queue size to avoid intermediate failure.,Closed,Fixed,,Akira Ajisaka,Haohui Mai,Tue; 8 Apr 2014 00:54:12 +0000,Fri; 15 Aug 2014 05:39:31 +0000,Tue; 15 Jul 2014 20:01:17 +0000,,2.5.0,,,HADOOP-10692,https://issues.apache.org/jira/browse/HADOOP-10468
HADOOP-10469,Improvement,Major,security,ProxyUser improvements,This is an umbrella jira which addresses few enhancements to proxyUser capability via sub tasks,Resolved,Fixed,,Benoy Antony,Benoy Antony,Tue; 8 Apr 2014 17:11:20 +0000,Fri; 31 Oct 2014 19:53:11 +0000,Fri; 31 Oct 2014 19:53:11 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10469
HADOOP-10470,Sub-task,Major,security,Change synchronization mechanism in ProxyUsers to readwrite lock,Currently ProxyUsers class achieve synchronization via synchornized. Performance on  ProxyUsers.authorize can be improved by replacing this with read/write lock.,Resolved,Duplicate,NULL,Benoy Antony,Benoy Antony,Tue; 8 Apr 2014 17:14:36 +0000,Thu; 24 Apr 2014 13:54:07 +0000,Thu; 24 Apr 2014 13:54:07 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10470
HADOOP-10471,Sub-task,Major,security,Reduce the visibility of constants in ProxyUsers,Most of the constants in proxyusers have public visibility unnecessarily. These public constants should be set to private  and their external usage should be replaced by the corresponding functions.,Closed,Fixed,,Benoy Antony,Benoy Antony,Tue; 8 Apr 2014 17:16:08 +0000,Fri; 15 Aug 2014 05:39:36 +0000,Tue; 6 May 2014 22:39:16 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10471
HADOOP-10472,Bug,Minor,,KerberosAuthenticator should use org.apache.commons.logging.LogFactory instead of org.slf4j.LoggerFactory,,Resolved,Invalid,,Jing Zhao,Jing Zhao,Tue; 8 Apr 2014 20:12:29 +0000,Tue; 22 Apr 2014 23:49:43 +0000,Tue; 22 Apr 2014 23:49:43 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10472
HADOOP-10473,Bug,Minor,test,TestCallQueueManager is still flaky,testSwapUnderContention counts the calls and then interrupts as shown below.  There could be call after counting the call but before interrupt.,Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Tue; 8 Apr 2014 21:43:36 +0000,Mon; 30 Jun 2014 08:18:09 +0000,Wed; 9 Apr 2014 16:51:31 +0000,,,,,HADOOP-10395,https://issues.apache.org/jira/browse/HADOOP-10473
HADOOP-10474,Improvement,Major,,Move o.a.h.record to hadoop-streaming,The classes in o.a.h.record have been deprecated for more than a year and a half. They should be removed. As the first step; the jira moves all these classes into the hadoop-streaming project; which is the only user of these classes.,Resolved,Fixed,,Haohui Mai,Haohui Mai,Tue; 8 Apr 2014 22:00:43 +0000,Fri; 13 May 2016 05:08:01 +0000,Mon; 19 May 2014 20:37:55 +0000,,,,,HADOOP-10485,https://issues.apache.org/jira/browse/HADOOP-10474
HADOOP-10475,Bug,Major,security,ConcurrentModificationException in AbstractDelegationTokenSelector.selectToken(),While running a hive job on a HA cluster saw ConcurrentModificationException in AbstractDelegationTokenSelector.selectToken(),Closed,Fixed,,Jing Zhao,Arpit Gupta,Tue; 8 Apr 2014 22:40:56 +0000,Fri; 15 Aug 2014 05:39:30 +0000,Wed; 9 Apr 2014 04:58:08 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10475
HADOOP-10476,Sub-task,Major,build,Bumping the findbugs version to 3.0.0,The findbug version used by hadoop is pretty old (1.3.9). The old version of Findbugs itself have some bugs (like http://sourceforge.net/p/findbugs/bugs/918/; hit by HADOOP-10474).Futhermore; Java 8 is only supported by findbugs 3.0.0 or newer.It's a good time to bump the findbugs version to 3.0.0.,Closed,Fixed,,Haohui Mai,Haohui Mai,Tue; 8 Apr 2014 23:43:03 +0000,Fri; 24 Apr 2015 22:49:01 +0000,Tue; 9 Dec 2014 18:39:04 +0000,,,,,HADOOP-10574,https://issues.apache.org/jira/browse/HADOOP-10476
HADOOP-10477,Improvement,Major,,Clean up findbug warnings found by findbugs 3.0.0,This is an umbrella jira to clean up the new findbug warnings found by findbugs 3.0.0.,Resolved,Fixed,,Haohui Mai,Haohui Mai,Wed; 9 Apr 2014 00:15:16 +0000,Thu; 17 Mar 2016 20:37:45 +0000,Tue; 24 Feb 2015 00:39:38 +0000,,,,,HADOOP-8594;HADOOP-12937,https://issues.apache.org/jira/browse/HADOOP-10477
HADOOP-10478,Sub-task,Major,,Fix new findbugs warnings in hadoop-maven-plugins,The following findbug warning needs to be fixed:,Closed,Fixed,,Li Lu,Haohui Mai,Wed; 9 Apr 2014 00:19:36 +0000,Fri; 24 Apr 2015 22:49:11 +0000,Tue; 24 Feb 2015 00:38:47 +0000,,,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10478
HADOOP-10479,Sub-task,Major,,Fix new findbugs warnings in hadoop-minikdc,The following findbugs warnings need to be fixed:,Closed,Fixed,,Swarnim Kulkarni,Haohui Mai,Wed; 9 Apr 2014 00:24:37 +0000,Fri; 15 Aug 2014 05:39:40 +0000,Fri; 20 Jun 2014 22:25:40 +0000,,,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10479
HDFS-7515,Bug,Major,,Fix new findbugs warnings in hadoop-hdfs,The following findbugs warnings need to be fixed:,Closed,Fixed,HDFS-7510;HDFS-7511;HDFS-7512,Haohui Mai,Haohui Mai,Wed; 9 Apr 2014 00:28:46 +0000,Fri; 24 Apr 2015 23:22:39 +0000,Thu; 11 Dec 2014 20:37:31 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-7515
HADOOP-10481,Sub-task,Major,,Fix new findbugs warnings in hadoop-auth,The following findbugs warnings need to be fixed:,Resolved,Duplicate,HADOOP-11379,Swarnim Kulkarni,Haohui Mai,Wed; 9 Apr 2014 00:35:37 +0000,Wed; 10 Dec 2014 04:49:48 +0000,Tue; 9 Dec 2014 23:22:52 +0000,,,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10481
HADOOP-10482,Sub-task,Major,,Fix various findbugs warnings in hadoop-common,The following findbugs warnings need to be fixed:,Closed,Fixed,HADOOP-9285,Haohui Mai,Haohui Mai,Wed; 9 Apr 2014 00:39:25 +0000,Fri; 24 Apr 2015 22:48:59 +0000,Wed; 10 Dec 2014 20:44:58 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10482
HADOOP-10483,Bug,Trivial,,AbstractDelegationTokenSecretManager.ExpiredTokenRemover should use adaptable unit to print time,Currently; ExpiredTokenRemover uses minute(s) as unit to print tokenRemoverScanInterval. If user entered a value less than 1 min; it will become 0 in output log. It's better to use format like 0d:1h:3m:2s:3ms to print time.,Open,Unresolved,,Unassigned,Wangda Tan,Wed; 9 Apr 2014 02:06:51 +0000,Wed; 3 Dec 2014 23:00:20 +0000,,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10483
HADOOP-10484,Improvement,Major,,Remove o.a.h.conf.Reconfig*,A search reveals that these classes are not used by hadoop and any downstream projects after 0.20. The have not been maintained since 2011.This jira proposes to remove them from hadoop-common.,Resolved,Invalid,,Haohui Mai,Haohui Mai,Wed; 9 Apr 2014 03:08:29 +0000,Wed; 27 Aug 2014 18:24:19 +0000,Wed; 27 Aug 2014 18:24:19 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10484
HADOOP-10485,Improvement,Major,tools,Remove dead classes in hadoop-streaming,Hadoop-streaming no longer requires many classes in o.a.h.record. This jira removes the dead code.,Resolved,Fixed,,Haohui Mai,Haohui Mai,Wed; 9 Apr 2014 04:35:25 +0000,Fri; 13 May 2016 05:11:02 +0000,Wed; 9 Apr 2014 18:13:29 +0000,,,,,HADOOP-10474,https://issues.apache.org/jira/browse/HADOOP-10485
HADOOP-10486,Improvement,Major,,Remove typedbytes support from hadoop-streaming,The typed record support in hadoop-streaming is based upon the deprecated records package. Neither of them are actively maintained. This jira proposes to remove them.,Resolved,Invalid,,Haohui Mai,Haohui Mai,Wed; 9 Apr 2014 04:57:11 +0000,Wed; 27 Aug 2014 18:25:06 +0000,Wed; 27 Aug 2014 18:25:06 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10486
HADOOP-10487,Bug,Major,,Racy code in UserGroupInformation#ensureInitialized(),UserGroupInformation#ensureInitialized() uses the double-check-locking pattern to reduce the synchronization cost: As Todd Lipcon pointed out in the original jira (HADOOP-9748). This pattern is incorrect. Please see more details in http://en.wikipedia.org/wiki/Double-checked_locking and http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.htmlThis jira proposes to use the static class holder pattern to do it correctly.,Open,Unresolved,,Haohui Mai,Haohui Mai,Thu; 10 Apr 2014 00:55:16 +0000,Mon; 14 Apr 2014 21:22:36 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10487
HADOOP-10488,Bug,Major,test,TestKeyProviderFactory fails randomly,This test is fail randomly depending on the order of execution of the test methods; the reason is that the keystore used by the different testmethods is the same. We should either delete it before/after each test; or we should use a diff diff or each run.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Thu; 10 Apr 2014 05:22:46 +0000,Thu; 12 May 2016 18:27:59 +0000,Thu; 10 Apr 2014 17:31:51 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10488
HADOOP-10489,Bug,Major,,UserGroupInformation#getTokens and UserGroupInformation#addToken can lead to ConcurrentModificationException,Currently UserGroupInformation#getTokens and UserGroupInformation#addToken uses UGI's monitor to protect the iteration and modification of Credentials#tokenMap. Per discussion in HADOOP-10475; this can still lead to ConcurrentModificationException.,Closed,Fixed,,Robert Kanter,Jing Zhao,Thu; 10 Apr 2014 21:24:20 +0000,Fri; 15 Aug 2014 05:39:33 +0000,Mon; 19 May 2014 19:59:32 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10489
HADOOP-10490,Bug,Minor,test,TestMapFile and TestBloomMapFile leak file descriptors.,Multiple tests in TestMapFile and TestBloomMapFile open files but don't close them.  On Windows; the leaked file descriptors cause subsequent tests to fail; because file locks are still held while trying to delete the test data directory.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Thu; 10 Apr 2014 23:42:46 +0000,Thu; 12 May 2016 18:27:28 +0000,Fri; 11 Apr 2014 04:55:14 +0000,,2.4.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10490
HADOOP-10491,Improvement,Major,security,Add Collection of Labels to KeyProvider API,A set of arbitrary labels would provide opportunity for interesting access policy decisions based on things like classification; etc.,Open,Unresolved,,Larry McCay,Larry McCay,Fri; 11 Apr 2014 12:46:32 +0000,Fri; 11 Apr 2014 12:50:05 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10491
HADOOP-10492,Bug,Major,,Help Commands needs change after deprecation,As hadoop dfs is deprecated; the help should show usage with HDFSe.g in the following command it still refers to Usage: hadoop fs generic optionsD:\Apps\java\BI\hadoop\hw\hdp\hadoop-2.2.0.2.0.6.0-0009hdfs dfsUsage: hadoop fs generic options        -appendToFile localsrc ... dst&#93;        [-cat -ignoreCrc src ...]        -checksum src ...        [-chgrp -R GROUP PATH...]        [-chmod -R MODE;MODE... | OCTALMODE PATH...]        [-chown -R OWNER[:GROUP] PATH...]        [-copyFromLocal -f -p localsrc ... dst]        [-copyToLocal -p -ignoreCrc -crc src ... localdst]        [-count -q path ...]        [-cp -f -p src ... dst]        [-createSnapshot snapshotDir &lt;snapshotName&#93;]        -deleteSnapshot snapshotDir snapshotName&#93;        [-df -h &lt;path ...]        [-du -s -h path ...],Resolved,Not A Problem,,Unassigned,Raja Nagendra Kumar,Fri; 11 Apr 2014 12:57:42 +0000,Fri; 8 May 2015 21:43:14 +0000,Fri; 8 May 2015 21:43:14 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10492
HADOOP-10493,Bug,Major,,callid for RPCrequest and RPCresponse should match but they are not always which creates problem for someone like me who consumes these two types of TCP packets  ,release used: HW 2.1 callid for RPCrequest and RPCresponse should match but they are not always which creates problem for someone like me who consumes these two types of TCP packets and need to correlate the two.TCP packets could be provided if needed for demonstrate the problem.ThanksCharlene Sun,Open,Unresolved,,Unassigned,Charlene Sun,Sun; 13 Apr 2014 14:00:33 +0000,Sat; 19 Apr 2014 15:13:04 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10493
HADOOP-10494,Improvement,Trivial,fs/s3,Ensure Jets3tFileSystemStore respects URI path for block files,The Jets3tFileSystemStore will currently store all block files in the root of an S3 bucket. If you specify a path in your OutputFormat; those paths are created; and some of the supporting meta files are stored there; but all block files will reside in the root of the bucket.We tracked this down to this method here; which does not use the provided URI from the the job: https://github.com/apache/hadoop-common/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/s3/Jets3tFileSystemStore.java#L381,Open,Unresolved,,Unassigned,Mark J Greene,Sun; 13 Apr 2014 14:15:21 +0000,Mon; 14 Apr 2014 08:56:55 +0000,,,,easyfix,,,https://issues.apache.org/jira/browse/HADOOP-10494
HADOOP-10495,Bug,Trivial,fs;test,TestFileUtil fails on Windows due to bad permission assertions.,TestFileUtil contains some assertions that check java.io.File#canRead.  These assertions fail on Windows.  The java.io.File methods for checking permissions are known to be buggy in JDK 6.  We can replace these calls with our own custom FileUtil#canRead; which is correct on all platforms.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Sun; 13 Apr 2014 15:05:33 +0000,Thu; 12 May 2016 18:22:36 +0000,Mon; 14 Apr 2014 04:15:26 +0000,,2.4.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10495
HADOOP-10496,Bug,Major,metrics,Metrics system FileSink can leak file descriptor.,FileSink opens a file.  If the MetricsSystem is shutdown; then the sink is discarded and the file is never closed; causing a file descriptor leak.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Sun; 13 Apr 2014 16:29:21 +0000,Thu; 12 May 2016 18:22:38 +0000,Mon; 14 Apr 2014 04:32:51 +0000,,2.4.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10496
HDFS-6261,Task,Major,documentation,Document for enabling node group layer in HDFS,Most of patches from Umbrella JIRA HADOOP-8468  have committed; However there is no site to introduce NodeGroup-aware(HADOOP Virtualization Extensisons) and how to do configuration. so we need to doc it.1.  Doc NodeGroup-aware relate in http://hadoop.apache.org/docs/current 2.  Doc NodeGroup-aware properties in core-default.xml.,Patch Available,Unresolved,HADOOP-10512,Binglin Chang,Wenwu Peng,Mon; 14 Apr 2014 07:23:24 +0000,Mon; 8 Jun 2015 03:37:28 +0000,,,,documentation,,HADOOP-10512,https://issues.apache.org/jira/browse/HDFS-6261
HADOOP-10498,New Feature,Major,util,Add support for proxy server,HDFS-6218  HDFS-6219 require support for configurable proxy servers.,Closed,Fixed,,Daryn Sharp,Daryn Sharp,Mon; 14 Apr 2014 16:07:30 +0000,Thu; 12 May 2016 18:21:56 +0000,Tue; 15 Apr 2014 15:27:58 +0000,,2.0.0-alpha;3.0.0-alpha1,,,HADOOP-10566,https://issues.apache.org/jira/browse/HADOOP-10498
HADOOP-10499,Sub-task,Minor,security,Remove unused parameter from ProxyUsers.authorize(),The Configuration parameter is not used in the authorize() function. It can be removed and callers can be updated.Attaching the simple patch which removes the unused conf parameter and updates the callers.The ProxyUsers is defined as a private audience and so there shouldn't be any external callers.,Closed,Fixed,,Benoy Antony,Benoy Antony,Mon; 14 Apr 2014 17:10:42 +0000,Wed; 3 Sep 2014 20:36:28 +0000,Wed; 16 Apr 2014 23:18:20 +0000,,2.4.0,,,HBASE-11067;HDFS-6251;MAPREDUCE-5840,https://issues.apache.org/jira/browse/HADOOP-10499
HADOOP-10500,Bug,Trivial,security;test,TestDoAsEffectiveUser fails on JDK7 due to failure to reset proxy user configuration.,The proxy user configuration is held in a static variable.  Various tests in TestDoAsEffectiveUser mutate this data.  JDK7 executes tests in a different order than JDK6; so this can cause a failure if the state hasn't been set explicitly to what the test needs.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Mon; 14 Apr 2014 20:40:15 +0000,Thu; 12 May 2016 18:23:23 +0000,Mon; 14 Apr 2014 23:52:36 +0000,,2.4.0;3.0.0-alpha1,,,HADOOP-9875,https://issues.apache.org/jira/browse/HADOOP-10500
HADOOP-10501,Bug,Minor,ipc,Server#getHandlers() accesses handlers without synchronization,All the other methods accessing handlers are synchronized methods.,Resolved,Later,,Unassigned,Ted Yu,Mon; 14 Apr 2014 23:36:34 +0000,Sat; 7 Mar 2015 23:17:57 +0000,Sat; 7 Mar 2015 23:17:57 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10501
HADOOP-10502,Improvement,Major,,Enhancements to LdapGroupsMapping.java,This is an umbrella Jira for enhancements proposed to org/apache/hadoop/security/LdapGroupsMapping.javaEnhancements Proposed1. support configurable hadoop.security.group.mapping.ldap.userdntemplate2. support configurable  hadoop.security.group.mapping.ldap.scope3. support configurable hadoop.security.group.mapping.ldap.base.user4. support configurable hadoop.security.group.mapping.ldap.scope.user5. support configurable hadoop.security.group.mapping.ldap.base.group6. support configurable hadoop.security.group.mapping.ldap.scope.groupWhy are these enhancements proposed:LdapGroupsMapping.java; as it stands does provide a good starting point for looking group memberships from LDAP.  However; when its use is attempted in enterprise deployments; we are hitting limits of its configurability.,Open,Unresolved,,Dilli Arumugam,Dilli Arumugam,Tue; 15 Apr 2014 00:09:05 +0000,Tue; 15 Apr 2014 00:12:58 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10502
HADOOP-10503,Sub-task,Minor,build,Move junit up to v 4.11,JUnit 4.11 has been out for a while; other projects are happy with it; so update it.,Closed,Fixed,,Chris Nauroth,Steve Loughran,Tue; 15 Apr 2014 13:57:46 +0000,Wed; 3 Sep 2014 20:36:32 +0000,Tue; 22 Apr 2014 20:01:03 +0000,,2.4.0,,,HADOOP-10532;HDFS-6265;MAPREDUCE-5852;YARN-1970,https://issues.apache.org/jira/browse/HADOOP-10503
HADOOP-10504,Improvement,Major,documentation,Document proxy server support,Document http proxy support introduced by HADOOP-10498.,Open,Unresolved,,Unassigned,Daryn Sharp,Tue; 15 Apr 2014 16:23:24 +0000,Thu; 12 May 2016 18:23:05 +0000,,,2.5.0;3.0.0-alpha1,,,HDFS-6219,https://issues.apache.org/jira/browse/HADOOP-10504
YARN-1943,Bug,Critical,nodemanager,Multitenant LinuxContainerExecutor is incompatible with Simple Security mode.,"As of hadoop 2.3.0; commit cc74a18c makes it so that nonsecureLocalUser replaces the user who submits a job if security is disabled:  However; the only way to enable security; is to NOT use SIMPLE authentication mode: Thus; the framework ENFORCES that ""SIMPLE"" login security -- nonSecureuser for submission of LinuxExecutorContainer.This results in a confusing issue; wherein we submit a job as ""sally"" and then get an exception that user ""nobody"" is not whitelisted and has UID  MAX_ID.My proposed solution is that we should be able to leverage LinuxContainerExector regardless of hadoop's view of the security settings on the cluster; i.e. decouple LinuxContainerExecutor logic from the ""isSecurityEnabled"" return value.",Closed,Duplicate,YARN-2424,Unassigned,jay vyas,Tue; 15 Apr 2014 16:44:30 +0000,Tue; 30 Jun 2015 07:20:34 +0000,Wed; 25 Feb 2015 17:52:19 +0000,,2.3.0,linux,,,https://issues.apache.org/jira/browse/YARN-1943
HADOOP-10506,Bug,Major,,LimitedPrivate annotation not useful,The LimitedPrivate annotation isn't useful.  The intention seems to have been those interfaces were only intended to be used by these components.  But in many cases those components are separate from core hadoop.  This means any changes to them will break backwards compatibility with those; which breaks the new compatibility rules in Hadoop.  Note that many of the annotation are also not marked properly; or have fallen out of date.  I see Public Interfaces that use LimitedPrivate classes in the api.  (TokenCache using Credentials is an example).,Open,Unresolved,,Unassigned,Thomas Graves,Tue; 15 Apr 2014 18:48:02 +0000,Sat; 7 Jan 2017 01:56:49 +0000,,,2.4.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10506
HADOOP-10507,Bug,Minor,fs,FsShell setfacl can throw ArrayIndexOutOfBoundsException when no perm is specified,If users don't specify the perm of an acl when using the FsShell's setfacl command; a fatal internal error ArrayIndexOutOfBoundsException will be thrown. An improvement would be if it returned something like this:,Closed,Fixed,,sathish,Stephen Chu,Tue; 8 Apr 2014 22:13:39 +0000,Thu; 12 May 2016 18:25:36 +0000,Thu; 10 Jul 2014 17:03:54 +0000,,2.4.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10507
HADOOP-10508,Bug,Major,ipc,RefreshCallQueue fails when authorization is enabled,When hadoop.security.authorization=true; the callqueue cannot be refreshed with hadoop dfsadmin -refreshCallQueue (protocol not found).,Closed,Fixed,,Chris Li,Chris Li,Tue; 15 Apr 2014 20:36:20 +0000,Fri; 15 Aug 2014 05:39:34 +0000,Tue; 29 Apr 2014 06:08:40 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10508
MAPREDUCE-5884,Bug,Major,jobhistoryserver;security,History server uses short user name when canceling tokens,When the owner of a token tries to explicitly cancel the token; it gets the following error/exception Details:AbstractDelegationTokenSecretManager.cacelToken() gets the owner as full principal name where as the canceller is the short name.The potential code snippets: The code shows 'owner' gets the full principal name. Where as the value of 'canceller' depends on who is calling it. In some cases; it is the short name. REF: HistoryClientService.java In other cases; the value could be full principal name. REF: FSNamesystem.java. Possible resolution:--------------------------Option 1: in cancelToken() method; compare with both : short name and full principal name.Pros: Easy. Have to change in one place.Cons: Someone can argue that it is hacky!Option 2:All the caller sends the consistent value as 'canceller' : either short name or full principal name.Pros: Cleaner.Cons: A lot of code changes and potential bug injections.I'm open for both options.Please give your opinion.Btw; how it is working now in most cases?  The short name and the full principal name are usually the same for end-users.,Closed,Fixed,,Mohammad Kamrul Islam,Mohammad Kamrul Islam,Wed; 16 Apr 2014 01:07:22 +0000,Wed; 3 Sep 2014 20:33:52 +0000,Thu; 8 May 2014 21:28:53 +0000,,2.3.0,,,,https://issues.apache.org/jira/browse/MAPREDUCE-5884
HADOOP-10510,Bug,Major,fs,TestSymlinkLocalFSFileContext tests are failing,Test results:https://gist.github.com/oza/9965197This was mentioned on hadoop-common-dev:http://mail-archives.apache.org/mod_mbox/hadoop-common-dev/201404.mbox/%3CCAAD07OKRSmx9VSjmfk1YxyBmnFM8mwZSp%3DizP8yKKwoXYvn3Qg%40mail.gmail.com%3ECan you suggest a workaround in the meantime? I'd like to send a pull request for an unrelated bug; but these failures mean I cannot build hadoop-common to test my fix. Thanks.,Resolved,Duplicate,HADOOP-10866,Unassigned,Daniel Darabos,Wed; 16 Apr 2014 11:30:15 +0000,Tue; 22 Jul 2014 00:37:17 +0000,Tue; 22 Jul 2014 00:37:17 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10510
HADOOP-10511,Bug,Major,fs/s3,s3n:// incorrectly handles URLs with secret keys that contain a slash,"This is similar to HADOOP-3733; but happens on s3n:// instead of s3://.Essentially if I have a path like ""s3n://key:pass%2Fword@example.com/test""; it will under certain circumstances be replaced with ""s3n://key:pass/test"" which then causes ""Invalid hostname in URI"" exceptions.I have a unit test and a fix for this. I'll make a pull request in a moment.",Resolved,Duplicate,HADOOP-3733,Unassigned,Daniel Darabos,Wed; 16 Apr 2014 12:55:29 +0000,Mon; 25 Jul 2016 16:45:42 +0000,Mon; 25 Jul 2016 16:45:42 +0000,,2.6.0,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10511
HADOOP-10512,Sub-task,Major,documentation,Document usage of node-group layer topology,For work under umbrella of HADOOP-8468; user can enable nodegroup layer between node and rack in some situations. We should document it after YARN-18 and YARN-19 is figured out.,Resolved,Duplicate,HDFS-6261,Unassigned,Junping Du,Wed; 16 Apr 2014 21:06:41 +0000,Tue; 24 Mar 2015 00:12:30 +0000,Tue; 24 Mar 2015 00:12:30 +0000,,,,YARN-18;YARN-19,HADOOP-11326;HDFS-6261,https://issues.apache.org/jira/browse/HADOOP-10512
HADOOP-10513,Bug,Major,util,ReflectionUtils::CONSTRUCTOR_CACHE leaks class loaders,Any code which uses custom class-loaders needs to be aware of the ReflectionUtils::CONSTRUCTOR_CACHE  This is not a problem when using only 1 AppClassLoader.But in cases where the application uses multiple classloaders (to isolate UDFs); this holds onto class references and their associated class loaders by ref ( that leaks all the statics).The clear method for this cache is unfortunately marked only for testing. The cache shows up as the only reference for the class loaders.,Open,Unresolved,,Unassigned,Gopal V,Wed; 16 Apr 2014 21:16:02 +0000,Thu; 30 Jul 2015 17:01:10 +0000,,,2.4.0,leak;permgen,,HADOOP-11771;HIVE-11408,https://issues.apache.org/jira/browse/HADOOP-10513
HADOOP-10514,New Feature,Major,fs,Common side changes to support  HDFS extended attributes (HDFS-2006),This is an umbrella issue for tracking all Hadoop Common changes required to support HDFS extended attributes implementation,Closed,Fixed,,Yi Liu,Uma Maheswara Rao G,Thu; 17 Apr 2014 02:33:00 +0000,Wed; 3 Sep 2014 20:36:31 +0000,Fri; 13 Jun 2014 01:07:43 +0000,,HDFS XAttrs (HDFS-2006),,,HADOOP-10561,https://issues.apache.org/jira/browse/HADOOP-10514
HDFS-6254,Bug,Major,libhdfs,hdfsConnect segment fault where namenode not connected,When namenode is not started; the libhdfs client will cause segment fault while connecting.,Resolved,Not A Problem,HDFS-866,Chris Nauroth,huang ken,Thu; 17 Apr 2014 09:18:51 +0000,Thu; 7 Aug 2014 19:01:45 +0000,Thu; 24 Apr 2014 18:12:56 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HDFS-6254
HADOOP-10516,Bug,Critical,,the NodeManager in slave does not starting,the Node Manager cannot start in the slave node and this is the log:2014-04-17 14:47:54;217 FATAL org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting NodeManagerorg.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager        at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:205)        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:328)        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:370)Caused by: org.apache.avro.AvroRuntimeException: java.lang.reflect.UndeclaredThrowableException        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:139)        at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)        ... 3 moreCaused by: java.lang.reflect.UndeclaredThrowableException        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:66)        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:176)        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:135)        ... 4 moreCaused by: com.google.protobuf.ServiceException: java.net.ConnectException: Call From fatima-HP-ProBook-4520s/127.0.1.1 to 0.0.0.0:8031 failed $        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:144)        at com.sun.proxy.$Proxy24.registerNodeManager(Unknown Source)        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)        ... 6 moreCaused by: java.net.ConnectException: Call From fatima-HP-ProBook-4520s/127.0.1.1 to 0.0.0.0:8031 failed on connection exception: java.net.Conn$        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:700)        at org.apache.hadoop.ipc.Client.call(Client.java:1098)        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:141)        ... 8 moreCaused by: java.net.ConnectException: Connection refused        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:708)        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:462)        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:559)        at org.apache.hadoop.ipc.Client$Connection.access$2100(Client.java:207)        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1204)        at org.apache.hadoop.ipc.Client.call(Client.java:1074)        ... 9 more2014-04-17 14:47:54;221 INFO org.apache.hadoop.ipc.Server: Stopping server on 417902014-04-17 14:47:54;222 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NodeManager metrics system.. 2014-04-17 14:47:54;222 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NodeManager metrics system...2014-04-17 14:47:54;224 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NodeManager metrics system stopped.2014-04-17 14:47:54;224 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NodeManager metrics system shutdown complete.2014-04-17 14:47:54;224 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: SHUTDOWN_MSG:/************************************************************,Resolved,Fixed,,Unassigned,Sami Abobala,Thu; 17 Apr 2014 12:17:27 +0000,Thu; 1 May 2014 14:58:36 +0000,Thu; 1 May 2014 14:58:36 +0000,,0.23.9,cloud;hadoop;yarn,,,https://issues.apache.org/jira/browse/HADOOP-10516
HADOOP-10517,Bug,Minor,test;util,InputStream is not closed in two methods of JarFinder,JarFinder#jarDir() and JarFinder#zipDir() have such code: The InputStream is closed in copyToZipStream() but should be enclosed in finally block.,Closed,Fixed,,Ted Yu,Ted Yu,Thu; 17 Apr 2014 13:57:33 +0000,Thu; 12 May 2016 18:25:11 +0000,Tue; 6 May 2014 19:54:32 +0000,,2.4.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10517
HADOOP-10518,Bug,Major,ipc,TestSaslRPC fails on Windows,TestSaslRPC fails with exceptions such as the following: The exact location/number of failures varies by run.,Resolved,Duplicate,HADOOP-8980,Unassigned,Arpit Agarwal,Thu; 17 Apr 2014 18:33:58 +0000,Thu; 17 Apr 2014 22:21:42 +0000,Thu; 17 Apr 2014 22:21:42 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10518
HDFS-6667,Bug,Major,security,In HDFS HA mode; Distcp/SLive with webhdfs on secure cluster fails with Client cannot authenticate via:[TOKEN; KERBEROS] error,Opening on Arpit Gupta's behalf.We observed that; in HDFS HA mode; running Distcp/SLive with webhdfs will fail on YARN.  In non-HA mode; it'll pass. The reason is in HA mode; only webhdfs delegation token is generated for the job; but YARN also requires the regular hdfs token to do localization; log-aggregation etc.In non-HA mode; both tokens are generated for the job.,Closed,Fixed,,Jing Zhao,Jian He,Fri; 18 Apr 2014 01:14:15 +0000,Mon; 1 Dec 2014 03:08:06 +0000,Thu; 17 Jul 2014 23:16:52 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-6667
HADOOP-10520,Sub-task,Major,fs,Extended attributes definition and FileSystem APIs for extended attributes.,This JIRA defines XAttr (Extended Attribute); it consists of a name and associated data; and 4 namespaces are defined: user; trusted; security and system. FileSystem APIs for XAttr include setXAttrs; getXAttrs; removeXAttrs and so on. For more information; please refer to HDFS-2006.,Resolved,Fixed,,Yi Liu,Yi Liu,Fri; 18 Apr 2014 15:53:45 +0000,Mon; 28 Apr 2014 19:49:09 +0000,Mon; 28 Apr 2014 19:49:09 +0000,,HDFS XAttrs (HDFS-2006),,,,https://issues.apache.org/jira/browse/HADOOP-10520
HADOOP-10521,Sub-task,Major,fs,FsShell commands for extended attributes.,setfattr  and  getfattr  commands are added to FsShell for XAttr; and these are the same as in Linux.,Resolved,Fixed,,Yi Liu,Yi Liu,Fri; 18 Apr 2014 15:57:41 +0000,Tue; 29 Apr 2014 09:00:00 +0000,Tue; 29 Apr 2014 08:01:48 +0000,,HDFS XAttrs (HDFS-2006),,,,https://issues.apache.org/jira/browse/HADOOP-10521
HADOOP-10522,Bug,Critical,,JniBasedUnixGroupMapping mishandles errors,The mishandling of errors in the jni user-to-groups mapping modules can cause segmentation faults in subsequent calls.  Here are the bugs:1) If hadoop_user_info_fetch() returns an error code that is not ENOENT; the error may not be handled at all.  This bug was found by Chris Nauroth.2)  In hadoop_user_info_fetch() and hadoop_group_info_fetch(); the global errno is directly used. This is not thread-safe and could be the cause of some failures that disappeared after enabling the big lookup lock.3) In the above methods; there is no limit on retries.,Closed,Fixed,,Kihwal Lee,Kihwal Lee,Fri; 18 Apr 2014 22:09:56 +0000,Thu; 4 Sep 2014 01:13:42 +0000,Mon; 21 Apr 2014 18:30:30 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10522
HADOOP-10523,Bug,Major,security,Hadoop services (such as RM; NN and JHS) throw confusing exception during token auto-cancelation ,When a user explicitly cancels the token; the system (such as RM; NN and JHS) also periodically tries to cancel the same token. During the second cancel (originated by RM/NN/JHS); Hadoop processes throw the following error/exception in the  log file. Although the exception is harmless; it creates a lot of confusions and causes the dev to spend a lot of time to investigate.This JIRA is to make sure if the token is available/not cancelled before attempting to cancel the token and  finally replace this exception with proper warning message.,Patch Available,Unresolved,,Mohammad Kamrul Islam,Mohammad Kamrul Islam,Fri; 18 Apr 2014 23:17:55 +0000,Fri; 5 Feb 2016 15:38:58 +0000,,,2.3.0,,,,https://issues.apache.org/jira/browse/HADOOP-10523
HADOOP-10524,Bug,Minor,,Race condition around MutableMetric and its subclasses,For example; MutableGaugeInt has two methods: and the lack of synchronization of the set method causes a problem that calling the set method might be ignored while another thread is calling the incr method; such as:(1) Thread1 takes the current value in the incr method.(2) Thread2 sets the new value in the set method.(3) Thread1 adds +1 to the taken value and sets the value in the incr method.Also; in the first place; MutableMetric has a volatile instance variable changed; but lack of synchronization causes a problem that it drops the latest notification which is called just before clearing the changed variable. That means; the volatile keyword is useless unless it is needed to just check the flag itself. Indeed; the implementation of the method snapshot in MutableCounterInt has this problem because of lack of synchronization.Anyway; synchronization around MutableMetric and its subclasses is doubtful and should be reviewed.,Open,Unresolved,,Unassigned,Hiroshi Ikeda,Mon; 21 Apr 2014 02:49:47 +0000,Mon; 21 Apr 2014 02:49:47 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10524
HADOOP-10525,Improvement,Minor,,Remove DRFA.MaxBackupIndex config from log4j.properties,From hadoop-user mailing list. In log4j.properties; the above lines should be removed because DailyRollingFileAppender(DRFA) doesn't support MaxBackupIndex config.,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Mon; 21 Apr 2014 04:55:23 +0000,Fri; 24 Apr 2015 22:48:55 +0000,Thu; 29 Jan 2015 07:19:52 +0000,,2.4.0,newbie,,HDFS-6263,https://issues.apache.org/jira/browse/HADOOP-10525
HADOOP-10526,Bug,Minor,,Chance for Stream leakage in CompressorStream,In CompressorStream.close ; finish() can throw IOException . But out will not be closed in that situation since it is not in finally,Closed,Fixed,,Rushabh S Shah,SreeHari,Mon; 5 Dec 2011 15:12:56 +0000,Wed; 3 Sep 2014 20:36:28 +0000,Mon; 21 Apr 2014 19:22:44 +0000,,0.23.0,,,,https://issues.apache.org/jira/browse/HADOOP-10526
HADOOP-10527,Bug,Major,,Fix incorrect return code and allow more retries on EINTR,After HADOOP-10522; user/group look-up will only try up to 5 times on EINTR.  More retries should be allowed just in case.Also; when a user/group lookup returns no entries; the wrapper methods are returning EIO; instead of ENOENT.,Closed,Fixed,,Kihwal Lee,Kihwal Lee,Mon; 21 Apr 2014 21:55:18 +0000,Thu; 4 Sep 2014 01:13:42 +0000,Wed; 23 Apr 2014 13:30:38 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10527
HADOOP-10528,Sub-task,Major,security,A TokenKeyProvider for a Centralized Key Manager Server (BEE: bee-key-manager),This is a key provider based on HADOOP-9331. HADOOP-9331 has designed a complete Hadoop crypto codec framework; but the key can only be retrieved from a local Java KeyStore file. To the convenience; we design a Centralized Key Manager Server (BEE: bee-key-manager) and user can use this TokenKeyProvider to retrieve keys from the Centralized Key Manager Server. By the way; to secure the key exchange; we leverage HTTPS + SPNego/SASL to protect the key exchange. To the detail design and usage; please refer to https://github.com/trendmicro/BEE. Moreover; there are still much more requests about Hadoop Data Encryption (such as provide standalone module; support KMIP...etc.); if anyone has interested in those features; pleas let us know. Ps. Because this patch based on HADOOP-9331; please use patch HADOOP-9333; and HADOOP-9332 and before use our patch HADOOP-10528.patch.,Patch Available,Unresolved,HADOOP-10141;HADOOP-10177;HADOOP-10433;HADOOP-10529,Unassigned,howie yu,Tue; 22 Apr 2014 02:44:38 +0000,Wed; 6 May 2015 03:32:42 +0000,,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10528
HADOOP-10529,Sub-task,Major,security,A TokenKeyProvider for a Centralized Key Manager Server (BEE: bee-key-manager),This is a key provider based on HADOOP-9331. HADOOP-9331 has designed a complete Hadoop crypto codec framework; but the key can only be retrieved from a local Java KeyStore file. To the convenience; we design a Centralized Key Manager Server (BEE: bee-key-manager) and user can use this TokenKeyProvider to retrieve keys from the Centralized Key Manager Server. By the way; to secure the key exchange; we leverage HTTPS + SPNego/SASL to protect the key exchange. To the detail design and usage; please refer to https://github.com/trendmicro/BEE. Moreover; there are still much more requests about Hadoop Data Encryption (such as provide standalone module; support KMIP...etc.); if anyone has interested in those features; pleas let us know.,Resolved,Duplicate,HADOOP-10528,Unassigned,howie yu,Tue; 22 Apr 2014 02:44:49 +0000,Tue; 22 Apr 2014 02:46:29 +0000,Tue; 22 Apr 2014 02:46:29 +0000,,,patch,,,https://issues.apache.org/jira/browse/HADOOP-10529
HADOOP-10530,Improvement,Blocker,build,Make hadoop trunk build on Java7+ only,"As discussed on hadoop-common; hadoop 3 is envisaged to be Java7+ only -this JIRA covers switching the build for this	maven enforcer plugin to set Java version = [1.7)	compiler to set language to java 1.7",Closed,Fixed,HADOOP-10963;HADOOP-11118,Steve Loughran,Steve Loughran,Tue; 22 Apr 2014 08:49:13 +0000,Thu; 12 May 2016 18:27:42 +0000,Mon; 8 Dec 2014 15:32:09 +0000,,2.6.0;3.0.0-alpha1,,HADOOP-11246,HADOOP-9991;HADOOP-11858,https://issues.apache.org/jira/browse/HADOOP-10530
HADOOP-10531,Bug,Major,,hadoop-config.sh - bug in --hosts argument,"I think there's a typo in the hadoop-config.sh which broke slave start in cluster mode.To reproduce it :/usr/local/hadoop/sbin/hadoop-daemons.sh --config /etc/hadoop/conf --hosts slaves start datanodehere's the patch : hadoop-dist/target/hadoop-2.4.0/libexec/hadoop-config.sh    2014-04-22 12:20:45.000000000 -0400+++ hadoop-dist/target/hadoop-2.4.0/libexec/hadoop-config.sh    2014-04-22 12:21:00.000000000 -0400@@ -93;7 +93;7 @@     if [ ""--hosts"" = ""$1"" ]     then         shift	export HADOOP_SLAVES=""${HADOOP_CONF_DIR}/$$1""+        export HADOOP_SLAVES=""${HADOOP_CONF_DIR}/$1""         shift     elif [ ""--hostnames"" = ""$1"" ]     then",Closed,Fixed,,Sebastien Barrier,Sebastien Barrier,Tue; 22 Apr 2014 16:34:50 +0000,Fri; 15 Aug 2014 05:39:43 +0000,Wed; 23 Apr 2014 22:37:43 +0000,,2.4.0,patch,,,https://issues.apache.org/jira/browse/HADOOP-10531
HADOOP-10532,Test,Major,build,Jenkins test-patch timed out on a large patch touching files in multiple modules.,On HADOOP-10503; I had posted a consolidated patch touching multiple files across all sub-modules: Hadoop; HDFS; YARN and MapReduce.  The Jenkins test-patch runs for these consolidated patches timed out.  I also experimented with a dummy patch that simply added one-line comment changes to files.  This patch also timed out; which seems to indicate a bug in our automation rather than a problem with any patch in particular.,Resolved,Duplicate,YETUS-151,Jean-Pierre Matsumoto,Chris Nauroth,Tue; 22 Apr 2014 20:00:14 +0000,Thu; 12 May 2016 18:27:25 +0000,Thu; 5 Nov 2015 22:44:19 +0000,,3.0.0-alpha1,,,HADOOP-10503;MAPREDUCE-5868,https://issues.apache.org/jira/browse/HADOOP-10532
HADOOP-10533,Bug,Minor,fs/s3,S3 input stream NPEs in MapReduce job,I'm running a wordcount MR as followshadoop jar WordCount.jar wordcount.WordCountDriver s3n://bucket/wordcount/input s3n://bucket/wordcount/outputs3n://bucket/wordcount/input is a s3 object that contains other input files.However I get following NPE error12/10/02 18:56:23 INFO mapred.JobClient:  map 0% reduce 0%12/10/02 18:56:54 INFO mapred.JobClient:  map 50% reduce 0%        12/10/02 18:56:56 INFO mapred.JobClient: Task Id : attempt_201210021853_0001_m_000001_0; Status : FAILEDjava.lang.NullPointerException        at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.close(NativeS3FileSystem.java:106)        at java.io.BufferedInputStream.close(BufferedInputStream.java:451)        at java.io.FilterInputStream.close(FilterInputStream.java:155)        at org.apache.hadoop.util.LineReader.close(LineReader.java:83)        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.close(LineRecordReader.java:144)        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.close(MapTask.java:497)        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:765)        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:396)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)        at org.apache.hadoop.mapred.Child.main(Child.java:249)MR runs fine if i specify more specific input path such as s3n://bucket/wordcount/input/file.txtMR fails if I pass s3 folder as a parameterIn summary;This works hadoop jar ./hadoop-examples-1.0.3.jar wordcount /user/hadoop/wordcount/input/ s3n://bucket/wordcount/output/This doesn't work hadoop jar ./hadoop-examples-1.0.3.jar wordcount s3n://bucket/wordcount/input/ s3n://bucket/wordcount/output/(both input path are directories),Closed,Fixed,,Steve Loughran,Benjamin Kim,Wed; 10 Oct 2012 11:30:51 +0000,Thu; 12 May 2016 18:27:04 +0000,Thu; 3 Jul 2014 13:04:02 +0000,,1.0.0;1.0.3;2.4.0;3.0.0-alpha1,,,HADOOP-10589,https://issues.apache.org/jira/browse/HADOOP-10533
HADOOP-10534,Bug,Major,,KeyProvider API should using windowing for retrieving metadata,The KeyProvider API should refrain from bulk operations that fetch information about all keys. in HADOOP-10430; we settled on providing an accessor that given a set of key names returns a list of metadata.,Closed,Fixed,,Owen O'Malley,Owen O'Malley,Wed; 23 Apr 2014 17:44:24 +0000,Mon; 1 Dec 2014 03:07:36 +0000,Thu; 24 Apr 2014 15:53:00 +0000,,,,HADOOP-10433,,https://issues.apache.org/jira/browse/HADOOP-10534
HADOOP-10535,Improvement,Minor,,Make the retry numbers in ActiveStandbyElector configurable,Currently in ActiveStandbyElector; when its zookeeper client cannot successfully communicate with ZooKeeper service; the retry number is hard coded to 3 (ActiveStandbyElector#NUM_RETRIES). After retrying 3 times a fatal error will be thrown and the ZKFC will quit. It will be better to make the retry times configurable.,Closed,Fixed,,Jing Zhao,Jing Zhao,Wed; 23 Apr 2014 20:46:18 +0000,Fri; 15 Aug 2014 05:39:49 +0000,Thu; 24 Apr 2014 23:05:47 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10535
HADOOP-10536,Sub-task,Major,,Pure Native Client: implement C code native_mini for YARN for unit test,We need to Implement C code native_mini for YARN for unit test.,Open,Unresolved,,Wenwu Peng,Wenwu Peng,Thu; 24 Apr 2014 06:13:06 +0000,Thu; 17 Jul 2014 17:50:45 +0000,,,HADOOP-10388,,,HADOOP-1842,https://issues.apache.org/jira/browse/HADOOP-10536
HADOOP-10537,Bug,Major,,H,,Resolved,Incomplete,,Unassigned,Fengdong Yu,Thu; 24 Apr 2014 08:56:04 +0000,Thu; 24 Apr 2014 12:21:42 +0000,Thu; 24 Apr 2014 12:21:42 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10537
HADOOP-10538,Bug,Major,,NumberFormatException happened  when hadoop 1.2.1 running on Cygwin,"The TaskTracker always failed to startup when it running on Cygwin. And the error info logged in xxx-tasktracker-xxxx.log is :2014-04-21 22:13:51;439 DEBUG org.apache.hadoop.mapred.TaskRunner: putting jobToken file name into environment D:/hadoop/mapred/local/taskTracker/pxie/jobcache/job_201404212205_0001/jobToken2014-04-21 22:13:51;439 INFO org.apache.hadoop.mapred.JvmManager: Killing JVM: jvm_201404212205_0001_m_18951771592014-04-21 22:13:51;439 WARN org.apache.hadoop.mapred.TaskRunner: attempt_201404212205_0001_m_000000_0 : Child Errorjava.lang.NumberFormatException: For input string: """"	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)	at java.lang.Integer.parseInt(Integer.java:504)	at java.lang.Integer.parseInt(Integer.java:527)	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType$JvmRunner.kill(JvmManager.java:552)	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.killJvmRunner(JvmManager.java:314)	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:378)	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:189)	at org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:122)	at org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)2014-04-21 22:13:51;511 DEBUG org.apache.hadoop.ipc.Server: IPC Server listener on 59983: disconnecting client 127.0.0.1:60154. Number of active connections: 12014-04-21 22:13:51;531 WARN org.apache.hadoop.fs.FileUtil: Failed to set permissions of path:",Resolved,Won't Fix,,Unassigned,peter xie,Thu; 24 Apr 2014 10:02:40 +0000,Fri; 10 Nov 2017 13:19:43 +0000,Fri; 10 Nov 2017 13:19:43 +0000,,1.2.1,,,,https://issues.apache.org/jira/browse/HADOOP-10538
HADOOP-10539,Improvement,Minor,security,Provide backward compatibility for ProxyUsers.authorize() call,HADOOP-10499  removed the unused Configuration parameter from ProxyUsers.authorize()  method. This broke few components like HBase who invoked that function and forced it to rely on Reflection.,Closed,Fixed,,Benoy Antony,Benoy Antony,Thu; 24 Apr 2014 16:21:28 +0000,Thu; 12 May 2016 18:21:57 +0000,Thu; 24 Apr 2014 21:27:35 +0000,,2.5.0;3.0.0-alpha1,,,HBASE-11067,https://issues.apache.org/jira/browse/HADOOP-10539
HADOOP-10540,Bug,Major,tools,Datanode upgrade in Windows fails with hardlink error.,I try to upgrade Hadoop from 1.x and 2.4; but DataNode failed to start due to hard link exception.Repro steps:*Installed Hadoop 1.x*hadoop dfsadmin -safemode enter*hadoop dfsadmin -saveNamespace*hadoop namenode -finalize*Stop all services*Uninstall Hadoop 1.x *Install Hadoop 2.4 *Start namenode with -upgrade option*Try to start datanode; begin to see Hardlink exception in datanode service log.,Closed,Fixed,,Arpit Agarwal,Huan Huang,Fri; 11 Apr 2014 05:14:07 +0000,Wed; 3 Sep 2014 20:36:30 +0000,Fri; 25 Apr 2014 03:13:05 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10540
HADOOP-10541,Bug,Minor,test,InputStream in MiniKdc#initKDCServer for minikdc.ldiff is not closed,The same InputStream variable is used for minikdc.ldiff and minikdc-krb5.conf : Before the second assignment; is should be closed.,Closed,Fixed,,Swarnim Kulkarni,Ted Yu,Sun; 27 Apr 2014 15:23:56 +0000,Thu; 12 May 2016 18:23:52 +0000,Tue; 6 May 2014 16:56:42 +0000,,2.4.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10541
HADOOP-10542,Bug,Minor,fs/s3,Potential null pointer dereference in Jets3tFileSystemStore#retrieveBlock(),get() may return null.The while loop dereferences in without null check.,Closed,Fixed,,Ted Yu,Ted Yu,Sun; 27 Apr 2014 15:27:30 +0000,Mon; 4 Jan 2016 23:32:30 +0000,Sat; 17 Jan 2015 18:26:41 +0000,,2.6.0,,,,https://issues.apache.org/jira/browse/HADOOP-10542
HADOOP-10543,Bug,Major,,RemoteException's unwrapRemoteException method failed for PathIOException,If the cause of a RemoteException is PathIOException; RemoteException's unwrapRemoteException methods would fail; because some PathIOException constructors initialize the cause to null; which makes Throwable to throw exception at,Closed,Fixed,,Yongjun Zhang,Yongjun Zhang,Sun; 27 Apr 2014 18:04:26 +0000,Fri; 15 Aug 2014 05:39:39 +0000,Wed; 30 Apr 2014 03:24:23 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10543
HADOOP-10544,Sub-task,Minor,,Find command - add operator functions to find command,Add operator functions (OR; NOT) to the find command created under HADOOP-8989.,Patch Available,Unresolved,,Jonathan Allen,Jonathan Allen,Sun; 27 Apr 2014 19:23:00 +0000,Thu; 8 Dec 2016 05:58:56 +0000,,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10544
HADOOP-10545,Bug,Minor,,hdfs zkfc NullPointerException,"Running hdfs zkfc on a node which is not a Namenode result of NullPointerException.An error message should be displayed telling that zkfc must be run only on a Namenode server and/or to verify configuration parameters.	hdfs zkfc -formatZKException in thread ""main"" java.lang.NullPointerException        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)        at org.apache.hadoop.hdfs.tools.NNHAServiceTarget.init(NNHAServiceTarget.java:57)        at org.apache.hadoop.hdfs.tools.DFSZKFailoverController.create(DFSZKFailoverController.java:128)        at org.apache.hadoop.hdfs.tools.DFSZKFailoverController.main(DFSZKFailoverController.java:172)",Resolved,Duplicate,HDFS-6731,Unassigned,Sebastien Barrier,Mon; 28 Apr 2014 13:51:50 +0000,Mon; 21 Jul 2014 20:13:34 +0000,Mon; 21 Jul 2014 20:12:40 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10545
HADOOP-10546,Sub-task,Minor,fs,Javadoc and other small fixes for extended attributes in hadoop-common,There are some additional comments from Charles Lamb and Vinayakumar B related to javadoc and other small fixes on HADOOP-10520; let's fix them in this follow-on JIRA.,Resolved,Fixed,,Charles Lamb,Andrew Wang,Mon; 28 Apr 2014 19:25:20 +0000,Mon; 28 Apr 2014 20:38:31 +0000,Mon; 28 Apr 2014 20:38:31 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10546
HADOOP-10547,Bug,Major,security,Give SaslPropertiesResolver.getDefaultProperties() public scope,Trying to use SaslPropertiesResolver.getDefaultProperties() in Hive project but the method has protected scope. Please make this a public method if appropriate.,Closed,Fixed,,Benoy Antony,Jason Dere,Tue; 29 Apr 2014 01:16:29 +0000,Wed; 3 Sep 2014 20:36:31 +0000,Tue; 29 Apr 2014 20:45:34 +0000,,2.4.0,,,HADOOP-10221,https://issues.apache.org/jira/browse/HADOOP-10547
HADOOP-10548,Sub-task,Minor,fs,Improve FsShell xattr error handling and other fixes,A couple small remaining issues from HADOOP-10521 we should address in this follow-on JIRA.,Resolved,Fixed,,Charles Lamb,Andrew Wang,Tue; 29 Apr 2014 07:58:11 +0000,Wed; 30 Apr 2014 06:45:05 +0000,Wed; 30 Apr 2014 06:44:13 +0000,,HDFS XAttrs (HDFS-2006),,,,https://issues.apache.org/jira/browse/HADOOP-10548
HADOOP-10549,Improvement,Major,conf,MAX_SUBST and varPat should be final in Configuration.java,In Configuration; expansion of variables is handled using the following constants that are not declared final:,Closed,Fixed,,Gera Shegalov,Gera Shegalov,Tue; 29 Apr 2014 11:24:01 +0000,Wed; 3 Sep 2014 20:36:33 +0000,Fri; 2 May 2014 19:15:59 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10549
HADOOP-10550,Bug,Minor,documentation,HttpAuthentication.html is out of date,It is still saying:,Resolved,Not A Problem,,Vrushali C,Zhijie Shen,Tue; 29 Apr 2014 18:23:12 +0000,Mon; 14 Aug 2017 17:51:30 +0000,Mon; 14 Aug 2017 17:51:30 +0000,,2.4.0;3.0.0-alpha1,newbie;site,,,https://issues.apache.org/jira/browse/HADOOP-10550
HDFS-6304,Improvement,Major,namenode,Consolidate the logic of path resolution in FSDirectory,Currently both FSDirectory and INodeDirectory provide helpers to resolve paths to inodes. This jira proposes to move all these helpers into FSDirectory to simplify the code.,Closed,Fixed,,Haohui Mai,Haohui Mai,Tue; 29 Apr 2014 18:37:31 +0000,Fri; 15 Aug 2014 05:41:30 +0000,Thu; 1 May 2014 00:29:29 +0000,,,,,,https://issues.apache.org/jira/browse/HDFS-6304
HADOOP-10552,Bug,Trivial,documentation,Fix usage and example at FileSystemShell.apt.vm,"Usage at moveFromLocal needs ""hdfs"" command; and example for touchz should use ""hdfs dfs"".",Closed,Fixed,,Kenji Kikushima,Kenji Kikushima,Wed; 30 Apr 2014 07:26:35 +0000,Mon; 1 Dec 2014 03:08:02 +0000,Fri; 26 Sep 2014 21:58:59 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10552
YARN-2014,Bug,Major,,Performance: AM scaleability is 10% slower in 2.4 compared to 0.23.9,Performance comparison benchmarks from 2.x against 0.23 shows AM scalability benchmark's runtime is approximately 10% slower in 2.4.0. The trend is consistent across later releases in both lines; latest release numbers are:2.4.0.0     runtime 255.6 seconds (avg 5 passes)0.23.9.12 runtime 230.4 seconds (avg 5 passes)Diff: -9.9% AM Scalability test is essentially a sleep job that measures time to launch and complete a large number of mappers.The diff is consistent and has been reproduced in both a larger (350 node; 100;000 mappers) perf environment; as well as a small (10 node; 2;900 mappers) demo cluster.,Open,Unresolved,,Jason Lowe,patrick white,Wed; 30 Apr 2014 22:17:17 +0000,Fri; 29 Sep 2017 20:55:19 +0000,,,2.4.0,,,,https://issues.apache.org/jira/browse/YARN-2014
HADOOP-10554,Bug,Major,,Performance: Scan metrics for 2.4 are down compared to 0.23.9,Performance comparison benchmarks for Scan test's runtime and throughput metrics are slightly out of 5% tolerance in 2.x compared against 0.23. The trend is consistent across later releases in both lines; latest release numbers are;Runtime:2.4.0.0     -    73.6 seconds (avg 5 passes)0.23.9.12 -    69.4 seconds (avg 5 passes)Diff: -5.7%Throughput:2.4.0.0     -   28.67 GB/s (avg 5 passes)0.23.9.12 -  30.41 GB/s (avg 5 passes)Diff: -6.1%Scan test is specifically measuring the average map's input read performance. The diff is consistent when run on a larger (350 node) perf environment; we are in process of seeing if this reproduces in a smaller cluster; using appropriately scaled inputs.,Open,Unresolved,,Unassigned,patrick white,Wed; 30 Apr 2014 22:50:27 +0000,Wed; 14 May 2014 18:17:31 +0000,,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10554
HADOOP-10555,Improvement,Trivial,,Add offset support to MurmurHash,From HIVE-6430 code review,Resolved,Fixed,,Sergey Shelukhin,Sergey Shelukhin,Wed; 30 Apr 2014 23:06:09 +0000,Tue; 30 Aug 2016 01:32:30 +0000,Mon; 23 Nov 2015 02:03:27 +0000,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10555
HADOOP-10556,Improvement,Major,security,Add toLowerCase support to auth_to_local rules for service name,When using Vintela to integrate Linux with AD; principals are lowercased. If the accounts in AD have uppercase characters (ie FooBar) the Kerberos principals have also uppercase characters (ie FooBar/HOST). Because of this; when a service (Yarn/HDFS) extracts the service name from the Kerberos principal (FooBar) and uses it for obtain groups the user is not found because via Linux the user FooBar is unknown; it has been converted to foobar.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Thu; 1 May 2014 05:57:46 +0000,Fri; 15 Aug 2014 05:39:27 +0000,Wed; 7 May 2014 18:25:03 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10556
HADOOP-10557,Improvement,Major,fs,FsShell -cp -pa option for preserving extended ACLs,This issue tracks enhancing FsShell cp to add a new command-line option (-pa) for preserving extended ACLs.,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Thu; 1 May 2014 07:54:17 +0000,Wed; 3 Sep 2014 20:36:29 +0000,Tue; 17 Jun 2014 17:12:18 +0000,,2.4.0,,,HADOOP-9705;MAPREDUCE-5809;HADOOP-9705;HADOOP-10561,https://issues.apache.org/jira/browse/HADOOP-10557
HADOOP-10558,Bug,Major,,java.net.UnknownHostException: Invalid host name: local host is: (unknown),"I had this exception  every time i try to run map-red job; I went to http://wiki.apache.org/hadoop/UnknownHostand tried every possible solution and still have the same result Task Id : attempt_1398945803120_0001_m_000004_0; Status : FAILEDContainer launch failed for container_1398945803120_0001_01_000006 : java.lang.reflect.UndeclaredThrowableException..Caused by: com.google.protobuf.ServiceException: java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: """"fatima-HP-ProBook-4520s"":8042; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.	.",Resolved,Invalid,,Unassigned,Sami Abobala,Thu; 1 May 2014 12:45:34 +0000,Wed; 9 Jul 2014 08:49:39 +0000,Wed; 9 Jul 2014 08:49:39 +0000,,,debian;hadoop;hadoop-3.0.0-SNAPSHOT;mapreduce;ubuntu,,,https://issues.apache.org/jira/browse/HADOOP-10558
HADOOP-10559,Improvement,Major,security,add a method to UserGroupInformation to load settings from a given conf file,"There's no easy way to set up the security parameters of a process unless it's set in core-site.xml; because it's just inited via: initialize(new Configuration(); false);	If it is defined in an XML resource injected in to the config resource list via Configuration.addResource() -then it may get picked up; but only if nothing has already created the configs.	If it is defined in any other means -you can't get it in.This is an issue with client apps that don't have core-site XML files on their classpath; and which are loading their configs more dynamically. Everything works on an insecure cluster; but try to target a secure one and things break.",Open,Unresolved,,Unassigned,Steve Loughran,Thu; 1 May 2014 16:34:26 +0000,Thu; 1 May 2014 16:48:42 +0000,,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10559
HADOOP-10560,Improvement,Minor,fs/s3,Update NativeS3FileSystem to issue copy commands for files with in a directory with a configurable number of threads,In NativeS3FileSystem if you do a copy of a directory it will copy all the files to the new location; but it will do this with one thread. Code is below. This jira will allow a configurable number of threads to be used to issue the copy commands to S3.do {PartialListing listing = store.list(srcKey; S3_MAX_LISTING_LENGTH; priorLastKey; true);for (FileMetadata file : listing.getFiles()){ keysToDelete.add(file.getKey()); store.copy(file.getKey(); dstKey + file.getKey().substring(srcKey.length())); }priorLastKey = listing.getPriorLastKey();} while (priorLastKey != null);,Patch Available,Unresolved,,Theodore michael Malaska,Theodore michael Malaska,Thu; 1 May 2014 17:35:34 +0000,Thu; 7 May 2015 09:20:33 +0000,,,,BB2015-05-TBR;performance,,,https://issues.apache.org/jira/browse/HADOOP-10560
HADOOP-10561,Improvement,Major,fs,Copy command with preserve option should handle Xattrs,The design docs for Xattrs stated that we handle preserve options with copy commandsFrom doc:Preserve option of commands like  cp -p  shell command and  distcp -p  should work on XAttrs. In the case of source fs supports XAttrs but target fs does not support; exception about XAttrs not supported will be thrown.,Closed,Fixed,,Yi Liu,Uma Maheswara Rao G,Thu; 1 May 2014 18:49:54 +0000,Thu; 12 May 2016 18:27:43 +0000,Thu; 12 Jun 2014 17:02:03 +0000,,3.0.0-alpha1,,,HADOOP-10557;HADOOP-10514,https://issues.apache.org/jira/browse/HADOOP-10561
HADOOP-10562,Bug,Critical,,Namenode exits on exception without printing stack trace in AbstractDelegationTokenSecretManager,Not printing the stack trace makes debugging harder.,Closed,Fixed,,Suresh Srinivas,Suresh Srinivas,Thu; 1 May 2014 20:38:48 +0000,Tue; 9 Sep 2014 21:23:49 +0000,Tue; 6 May 2014 00:26:10 +0000,,1.2.1;2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10562
HADOOP-10563,Improvement,Major,,Remove the dependency of jsp in trunk,After HDFS-6252 neither hdfs nor yarn uses jsp; thus the dependency of the jsp can be removed from the pom.,Closed,Fixed,,Haohui Mai,Haohui Mai,Thu; 1 May 2014 21:08:58 +0000,Fri; 24 Apr 2015 22:49:05 +0000,Fri; 2 May 2014 22:26:23 +0000,,,,,HDFS-6252,https://issues.apache.org/jira/browse/HADOOP-10563
HADOOP-10564,Sub-task,Major,native,Add username to native RPCv9 client,Add the ability for the native RPCv9 client to set a username when initiating a connection.,Resolved,Fixed,,Colin P. McCabe,Colin P. McCabe,Thu; 1 May 2014 21:17:00 +0000,Thu; 15 May 2014 20:29:08 +0000,Thu; 15 May 2014 20:29:08 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10564
HADOOP-10565,Sub-task,Major,security,Support IP ranges (CIDR) in  proxyuser.hosts,"In some use cases; there will be many hosts from which the user can impersonate. This requires specifying many ips  in the XML configuration. It is cumbersome to specify and maintain long list of ips in proxyuser.hostsThe problem can be solved if we enable proxyuser.hosts to accept ip ranges in CIDR format.In addition; the current ip authorization involve a liner scan of the ips and an attempt to do InetAddress.getByName()  for each ip/host. It may be beneficial to group this functionality of ip authorization by looking up  ""ip addresses/host names/ip-ranges"" into a separate class. This could be reused in other usecases which require similar functionality",Closed,Fixed,,Benoy Antony,Benoy Antony,Fri; 2 May 2014 06:13:42 +0000,Wed; 3 Sep 2014 20:36:30 +0000,Fri; 27 Jun 2014 08:37:26 +0000,,,,,HADOOP-10335,https://issues.apache.org/jira/browse/HADOOP-10565
HADOOP-10566,Sub-task,Major,security,Refactor proxyservers out of ProxyUsers,HADOOP-10498 added proxyservers feature in ProxyUsers. It is beneficial to treat this as a separate feature since 1 The ProxyUsers is per proxyuser where as proxyservers is per cluster. The cardinality is different. 2 The ProxyUsers.authorize() and ProxyUsers.isproxyUser() are synchronized and hence share the same lock  and impacts performance.Since these are two separate features; it will be an improvement to keep them separate. It also enables one to fine-tune each feature independently.,Closed,Fixed,,Benoy Antony,Benoy Antony,Fri; 2 May 2014 06:28:32 +0000,Wed; 3 Sep 2014 20:36:27 +0000,Wed; 28 May 2014 21:12:38 +0000,,2.4.0,,,HADOOP-10498,https://issues.apache.org/jira/browse/HADOOP-10566
HADOOP-10567,Sub-task,Minor,fs,Shift XAttr value encoding code out for reuse.,XAttr value encoding(encode byte[] to string; hex string or base64 string for better display and input) is common; can be reused.  It can be used by FsShell; in http request as parameter and json response.,Resolved,Fixed,,Yi Liu,Yi Liu,Fri; 2 May 2014 08:55:12 +0000,Sat; 3 May 2014 06:18:33 +0000,Sat; 3 May 2014 06:18:33 +0000,,HDFS XAttrs (HDFS-2006),,,,https://issues.apache.org/jira/browse/HADOOP-10567
HADOOP-10568,Bug,Major,fs/s3,Add s3 server-side encryption,Add s3 server-side encryption as described here:http://docs.aws.amazon.com/AmazonS3/latest/dev/SSEUsingJavaSDK.html,Closed,Fixed,,David S. Wang,David S. Wang,Fri; 2 May 2014 16:56:36 +0000,Wed; 29 Jun 2016 17:35:16 +0000,Sat; 3 May 2014 00:26:49 +0000,,2.4.0,s3,,,https://issues.apache.org/jira/browse/HADOOP-10568
HADOOP-10569,Improvement,Major,security,Normalize Hadoop Audit Logs,It will be very useful to normalize the audit format across various Hadoop components.A common audit format will help both tools parse the audit record consistently across sub-projects and will be easier for humans to interpret audit details.If a new common audit format is devised it will be useful to consider the following W's of audit 1. What Action  with What Results  - E.g What was done; action initiated; API invoked; Job Submitted and etc. - What were the results (success; failure etc)2. Who - E.g User; Proxy User (If available); IP Address (if available)3. When - Timestamp; 4. Where - What subsystem; component; node name5. Why : Now why is difficult to answer. However with Audit event correction we can provide better context. E.g A user submitted PIG script that results in some MR jobs and HDFS read/writes can be correlated. There are perhaps 2 ways to achieve the goal of normalized audit records.1. A common audit facility - as components can start to uptake this common audit facility; their audit records start adopting to the normalized audit record format.2. Change each component to produce audit record in a common format.Approach 1 appears to be more doable.,Open,Unresolved,,Unassigned,Vinay Shukla,Fri; 2 May 2014 17:34:00 +0000,Fri; 2 May 2014 18:11:10 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10569
HADOOP-10570,Bug,Major,,TestDU.testDUSetInitialValue() fails with usage didn't get updated,"testDUSetInitialValue(org.apache.hadoop.fs.TestDU)  Time elapsed: 5.067 sec  &lt; FAILURE!junit.framework.AssertionFailedError: Usage didn't get updated 12288	at junit.framework.Assert.fail(Assert.java:50)	at junit.framework.Assert.assertTrue(Assert.java:20)	at org.apache.hadoop.fs.TestDU.testDUSetInitialValue(TestDU.java:132)Results :Failed tests:   TestDU.testDUSetInitialValue:132 Usage didn't get updated",Patch Available,Unresolved,,Unassigned,Siqi Li,Fri; 2 May 2014 20:24:11 +0000,Wed; 6 May 2015 03:35:01 +0000,,,2.3.0,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10570
HADOOP-10571,Bug,Major,,Use Log.*(Object; Throwable) overload to log exceptions,When logging an exception; we often convert the exception to string or call .getMessage. Instead we can use the log method overloads which take Throwable as a parameter.,Reopened,Unresolved,,Unassigned,Arpit Agarwal,Fri; 2 May 2014 21:10:17 +0000,Tue; 1 Sep 2015 04:20:01 +0000,,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10571
HADOOP-10572,Improvement,Trivial,nfs,Example NFS mount command must pass noacl as it isn't supported by the server yet,Use of the documented default mount command results in the below server side log WARN event; cause the client tries to locate the ACL program (#100227): The client mount command must pass noacl to avoid this.,Closed,Fixed,,Harsh J,Harsh J,Sat; 3 May 2014 08:10:22 +0000,Fri; 15 Aug 2014 05:39:47 +0000,Tue; 13 May 2014 17:13:28 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10572
HADOOP-10573,Sub-task,Major,native,fix hadoop native client CMakeLists.txt issue with older cmakes,In GeneratProtobufs.cmake; use variable CMAKE_CURRENT_LIST_DIR; it is new for cmake version 2.8; so we should change cmake_minimum_required(VERSION 2.6) to (VERSION 2.8) in CMakeLists.txt,Resolved,Fixed,,Wenwu Peng,Wenwu Peng,Mon; 5 May 2014 07:33:53 +0000,Tue; 6 May 2014 17:01:50 +0000,Tue; 6 May 2014 17:01:50 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10573
HADOOP-10574,Sub-task,Major,build,Bump the maven plugin versions too -moving the numbers into properties,"the maven build plugins are a bit dated -example; compiler plugin is at 2.5..1; when the latest is 3.1	move the version definition from inline into properties	increment the versionsThis has no effect on downstream projects; so is low risk",Closed,Fixed,,Akira Ajisaka,Steve Loughran,Mon; 5 May 2014 08:15:19 +0000,Fri; 10 Apr 2015 20:04:40 +0000,Thu; 29 Jan 2015 17:30:37 +0000,,2.4.0,,,HADOOP-10476;HADOOP-10197;YARN-3113,https://issues.apache.org/jira/browse/HADOOP-10574
HADOOP-10575,Sub-task,Minor,fs,Small fixes for XAttrCommands and test.,Small fixes for XAttrCommands and test.,Resolved,Fixed,,Yi Liu,Yi Liu,Mon; 5 May 2014 15:32:42 +0000,Mon; 5 May 2014 16:01:36 +0000,Mon; 5 May 2014 16:01:36 +0000,,HDFS XAttrs (HDFS-2006),,,,https://issues.apache.org/jira/browse/HADOOP-10575
HADOOP-10576,Improvement,Major,build,Fail build on new findbugs warnings,Issues like HADOOP-10477 seem like an unnecessary technical debt for developers I think which can be easily fixed by simply failing the build anytime a code change introduces a new findbugs warning. This might help keep the code base cleaner and avoid us going back to fix those warnings.,Resolved,Not A Problem,,Swarnim Kulkarni,Swarnim Kulkarni,Mon; 5 May 2014 19:15:55 +0000,Mon; 5 May 2014 22:11:54 +0000,Mon; 5 May 2014 22:06:59 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10576
HADOOP-10577,Sub-task,Minor,,Fix some minors error and compile on macosx,,Resolved,Fixed,,Binglin Chang,Binglin Chang,Tue; 6 May 2014 14:57:59 +0000,Thu; 8 May 2014 22:58:06 +0000,Thu; 8 May 2014 05:21:31 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10577
HADOOP-10578,Sub-task,Minor,,Find command - add navigation and execution expressions to find command,"Add the navigation and execution expressions to the find command created under HADOOP-8989; e.g.	exec	maxDepth	minDepth	prune	depth",Open,Unresolved,,Jonathan Allen,Jonathan Allen,Tue; 6 May 2014 18:36:59 +0000,Sun; 15 Feb 2015 17:32:43 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10578
HADOOP-10579,Sub-task,Minor,,Find command - add match expressions to find command,"Add match expressions to the find command created under HADOOP-8989; e.g.:	atime	empty	group	mtime	newer	nogroup	nouser	perm	regex	size	user",In Progress,Unresolved,,Jonathan Allen,Jonathan Allen,Tue; 6 May 2014 18:40:15 +0000,Fri; 13 Jun 2014 18:47:51 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10579
HADOOP-10580,Sub-task,Minor,,Find command - add documentation and CLI tests to find command,Add documentation and CLI tests to the find command created under HADOOP-8989.,In Progress,Unresolved,,Jonathan Allen,Jonathan Allen,Tue; 6 May 2014 18:41:40 +0000,Fri; 13 Jun 2014 18:48:10 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10580
HADOOP-10581,Bug,Major,,TestUserGroupInformation#testGetServerSideGroups fails because groups stored in Set and ArrayList are compared,The test fails on some machines that has variety of user groups.Initially the groups are extracted and stored in a setSetString groups = new LinkedHashSetString ();when the user groups are collected by calling the login.getGroupNames(); they are stored in an array listString[] gi = login.getGroupNames();Because these groups are stored in different structure; there will be inconsistency in the group count. Sets have unique list of keys while array list emits everything they have.assertEquals(groups.size(); gi.length); fails when there are more than one groups with same name as the count in sets will be less than the arraylist.,Closed,Fixed,,Mit Desai,Mit Desai,Tue; 6 May 2014 20:57:48 +0000,Thu; 12 May 2016 18:22:48 +0000,Thu; 8 May 2014 18:05:46 +0000,,2.4.1;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10581
HADOOP-10582,Bug,Minor,fs,Fix the test case for copying to non-existent dir in TestFsShellCopy,In TestFsShellCopy#checkPut; there is a following code. The code expect to test the case when we put a file to non-exsistent directory. But the code delete childPath and then copy to its parent directory (dstPath).,Resolved,Fixed,,Kousuke Saruta,Kousuke Saruta,Tue; 6 May 2014 21:40:20 +0000,Tue; 30 Aug 2016 01:32:28 +0000,Mon; 18 May 2015 07:34:45 +0000,,3.0.0-alpha1,BB2015-05-RFC,,,https://issues.apache.org/jira/browse/HADOOP-10582
HADOOP-10583,Bug,Minor,bin,bin/hadoop key throws NPE with no args and assorted other fixups,bin/hadoop key throws NPE.,Closed,Fixed,,Charles Lamb,Charles Lamb,Wed; 7 May 2014 21:01:09 +0000,Mon; 1 Dec 2014 03:08:52 +0000,Tue; 13 May 2014 18:30:09 +0000,,,patch,,,https://issues.apache.org/jira/browse/HADOOP-10583
HADOOP-10584,Bug,Major,ha,ActiveStandbyElector goes down if ZK quorum become unavailable,ActiveStandbyElector retries operations for a few times. If the ZK quorum itself is down; it goes down and the daemons will have to be brought up again. Instead; it should log the fact that it is unable to talk to ZK; call becomeStandby on its client; and continue to attempt connecting to ZK.,Reopened,Unresolved,,Daniel Templeton,Karthik Kambatla,Wed; 7 May 2014 22:04:41 +0000,Tue; 14 Nov 2017 19:38:46 +0000,,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10584
HADOOP-10585,Bug,Critical,ipc,Retry polices ignore interrupted exceptions,Retry polices should not use ThreadUtil.sleepAtLeastIgnoreInterrupts.  This prevents FsShell commands from being aborted during retries.  It also causes orphaned webhdfs DN DFSClients to keep running after the webhdfs client closes the connection.  Jetty goes into a loop constantly sending interrupts to the handler thread.  Webhdfs retries cause multiple nodes to have these orphaned clients.  The DN cannot shutdown until orphaned clients complete.,Closed,Fixed,,Daryn Sharp,Daryn Sharp,Thu; 8 May 2014 14:50:40 +0000,Thu; 12 May 2016 18:24:55 +0000,Tue; 13 May 2014 16:33:05 +0000,,2.0.0-alpha;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10585
HADOOP-10586,Bug,Minor,bin,KeyShell doesn't allow setting Options via CLI,You should be able to set any of the Options passed to the KeyProvider via the CLI.,Closed,Fixed,,Charles Lamb,Charles Lamb,Thu; 8 May 2014 16:19:32 +0000,Thu; 12 May 2016 18:25:05 +0000,Fri; 16 May 2014 05:07:32 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10586
HADOOP-10587,Improvement,Minor,,Use a thread-local cache in TokenIdentifier#getBytes to avoid creating many DataOutputBuffer objects,We can use a thread-local cache in TokenIdentifier#getBytes to avoid creating many DataOutputBuffer objects.  This will reduce our memory usage (for example; when loading edit logs); and help prevent OOMs.,Resolved,Won't Fix,,Colin P. McCabe,Colin P. McCabe,Thu; 8 May 2014 20:33:08 +0000,Thu; 22 May 2014 18:48:03 +0000,Thu; 22 May 2014 18:48:03 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10587
HADOOP-10588,Bug,Major,,Workaround for jetty6 acceptor startup issue,When a cluster is restarted; jetty is not functioning for a small percentage of datanodes; requiring restart of those datanodes.  This is caused by JETTY-1316.We've tried overriding isRunning() and retrying on super.isRunning() returning false; as the reporter of JETTY-1316 mentioned in the description.  It looks like the code was actually exercised (i.e. the issue was caused by this jetty bug)  and the acceptor was working fine after retry.Since we will probably move to a later version of jetty after branch-3 is cut; we can put this workaround in branch-2 only.,Closed,Fixed,,Kihwal Lee,Kihwal Lee,Thu; 8 May 2014 22:22:19 +0000,Fri; 19 Aug 2016 16:59:09 +0000,Thu; 15 May 2014 21:38:31 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10588
HADOOP-10589,Bug,Major,fs/s3,NativeS3FileSystem throw NullPointerException when the file is empty,"An empty file in the s3 path.NativeS3FsInputStream dose not check the InputStream .2014-05-06 20:29:26;961 INFO main org.apache.hadoop.hive.ql.exec.ReduceSinkOperator: 4 forwarded 0 rows2014-05-06 20:29:26;961 INFO main org.apache.hadoop.hive.ql.exec.GroupByOperator: 3 Close done2014-05-06 20:29:26;961 INFO main org.apache.hadoop.hive.ql.exec.SelectOperator: 2 Close done2014-05-06 20:29:26;961 INFO main org.apache.hadoop.hive.ql.exec.FilterOperator: 1 Close done2014-05-06 20:29:26;961 INFO main org.apache.hadoop.hive.ql.exec.TableScanOperator: 0 Close done2014-05-06 20:29:26;961 INFO main org.apache.hadoop.hive.ql.exec.MapOperator: 5 Close done2014-05-06 20:29:26;961 INFO main org.apache.hadoop.hive.ql.exec.mr.ExecMapper: ExecMapper: processed 0 rows: used memory = 6022214882014-05-06 20:29:26;964 WARN main org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.NullPointerException	at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.close(NativeS3FileSystem.java:147)	at java.io.BufferedInputStream.close(BufferedInputStream.java:472)	at java.io.FilterInputStream.close(FilterInputStream.java:181)	at org.apache.hadoop.util.LineReader.close(LineReader.java:150)	at org.apache.hadoop.mapred.LineRecordReader.close(LineRecordReader.java:244)	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doClose(CombineHiveRecordReader.java:72)	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.close(HiveContextAwareRecordReader.java:96)	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.close(HadoopShimsSecure.java:248)	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.close(MapTask.java:209)	at org.apache.hadoop.mapred.MapTask.closeQuietly(MapTask.java:1950)	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:445)	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:415)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)2014-05-06 20:29:26;970 INFO main org.apache.hadoop.mapred.Task: Runnning cleanup for the task",Closed,Fixed,HADOOP-10457,Steve Loughran,shuisheng wei,Fri; 9 May 2014 05:36:55 +0000,Tue; 30 Jun 2015 07:22:36 +0000,Tue; 10 Feb 2015 09:08:13 +0000,,2.2.0,,,HADOOP-10737;HADOOP-10533,https://issues.apache.org/jira/browse/HADOOP-10589
HADOOP-10590,Bug,Major,security,ServiceAuthorizationManager  is not threadsafe,The mutators in ServiceAuthorizationManager  are synchronized. The accessors are not synchronized.This results in visibility issues when  ServiceAuthorizationManager's state is accessed from different threads.,Closed,Fixed,,Benoy Antony,Benoy Antony,Fri; 9 May 2014 06:43:58 +0000,Fri; 15 Aug 2014 05:39:21 +0000,Wed; 18 Jun 2014 05:32:55 +0000,,2.4.0,,,HADOOP-10593;HADOOP-10648,https://issues.apache.org/jira/browse/HADOOP-10590
HADOOP-10591,Bug,Major,,Compression codecs must used pooled direct buffers or deallocate direct buffers when stream is closed,Currently direct buffers allocated by compression codecs like Gzip (which allocates 2 direct buffers per instance) are not deallocated when the stream is closed. Eventually for long running processes which create a huge number of files; these direct buffers are left hanging till a full gc; which may or may not happen in a reasonable amount of time - especially if the process does not use a whole lot of heap.Either these buffers should be pooled or they should be deallocated when the stream is closed.,Closed,Fixed,,Colin P. McCabe,Hari Shreedharan,Fri; 9 May 2014 07:23:38 +0000,Wed; 20 May 2015 14:39:48 +0000,Thu; 17 Jul 2014 18:21:15 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10591
HADOOP-10592,Sub-task,Major,,Add unit test case for net in hadoop native client ,Add unit test case for net.c in hadoop native client,Resolved,Fixed,,Wenwu Peng,Wenwu Peng,Fri; 9 May 2014 08:30:49 +0000,Tue; 20 May 2014 17:16:23 +0000,Tue; 20 May 2014 17:16:23 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10592
HADOOP-10593,Improvement,Major,,Concurrency Improvements,This is an umbrella jira to improve the concurrency of a few classes by making use of safe publication idioms. Most of the improvements are based on the following:,In Progress,Unresolved,,Benoy Antony,Benoy Antony,Fri; 9 May 2014 16:11:05 +0000,Fri; 9 May 2014 16:19:20 +0000,,,,,,HADOOP-10448;HADOOP-10590;HDFS-6363,https://issues.apache.org/jira/browse/HADOOP-10593
HADOOP-10594,Sub-task,Major,,Improve Concurrency in Groups,The static field GROUPS in Groups can be accessed by holding a lock only.This object is effectively immutable after construction and hence can safely published using a volatile field. This enables threads to access this GROUPS object without holding lock,Patch Available,Unresolved,,Benoy Antony,Benoy Antony,Fri; 9 May 2014 16:15:33 +0000,Wed; 6 May 2015 03:35:04 +0000,,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10594
HADOOP-10595,Sub-task,Major,,Improve concurrency in HostFileReader,HostsFileReader is used to keep track of included and excluded hosts in the cluster. The threads needs to synchronize to access the set of hosts.This can be improved.,Open,Unresolved,,Benoy Antony,Benoy Antony,Fri; 9 May 2014 16:38:59 +0000,Fri; 9 May 2014 16:38:59 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10595
HADOOP-10596,Bug,Major,,HttpServer2 should apply the authentication filter to some urls instead of null,HttpServer2 should apply the authentication filter to some urls instead of null. In addition; it should be more flexible for users to configure SPNEGO.,Resolved,Won't Fix,,Zhijie Shen,Zhijie Shen,Mon; 12 May 2014 19:13:17 +0000,Fri; 13 Mar 2015 06:11:48 +0000,Fri; 13 Mar 2015 06:11:48 +0000,,,,YARN-1935,YARN-2049,https://issues.apache.org/jira/browse/HADOOP-10596
HADOOP-10597,Sub-task,Major,,RPC Server signals backoff to clients when all request queues are full,Currently if an application hits NN too hard; RPC requests be in blocking state; assuming OS connection doesn't run out. Alternatively RPC or NN can throw some well defined exception back to the client based on certain policies when it is under heavy load; client will understand such exception and do exponential back off; as another implementation of RetryInvocationHandler.,Resolved,Fixed,,Ming Ma,Ming Ma,Tue; 13 May 2014 03:59:41 +0000,Mon; 31 Oct 2016 18:31:07 +0000,Mon; 31 Oct 2016 18:31:06 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10597
HADOOP-10598,Sub-task,Major,,Support configurable RPC fair share,It will be useful if we can support RPC min fair share on a per user or group basis. That will be useful for SLA jobs in a shared cluster. It will be complementary to the history-based soft policy defined in fair queue's history RPC server.,Open,Unresolved,,Unassigned,Ming Ma,Tue; 13 May 2014 04:00:18 +0000,Tue; 8 Sep 2015 15:10:38 +0000,,,,,,HDFS-8994,https://issues.apache.org/jira/browse/HADOOP-10598
HADOOP-10599,Sub-task,Major,,Support prioritization of DN RPCs over client RPCs,We might need to prioritize DN RPC over client RPC so that no matter what application do to NN RPC and FSNamesystem's global lock; DN's requests will be processed timely. After a cluster is configured to have service RPC server separated from client RPC server; it is mitigated to some degree with fair FSNamesystem's global lock. Also if the NN global lock can be made more fine grained; such need becomes less important. Still; it will be good to evaluate if this is a good option.,Open,Unresolved,,Unassigned,Ming Ma,Tue; 13 May 2014 04:11:34 +0000,Tue; 13 May 2014 13:57:48 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10599
HADOOP-10600,Bug,Major,,AuthenticationFilterInitializer doesn't allow null signature secret file,AuthenticationFilterInitializer doesn't allow null signature secret file. However; null signature secret is acceptable in AuthenticationFilter; and a random signature secret is going to be created instead.,Resolved,Duplicate,HADOOP-10670,Kai Zheng,Zhijie Shen,Tue; 13 May 2014 18:07:01 +0000,Tue; 5 Aug 2014 05:42:18 +0000,Tue; 5 Aug 2014 05:42:18 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10600
HADOOP-10601,Bug,Major,,We should prevent AuthenticationFilter to be installed twice,It seems that we have two way to install the authentication filter (at least from YARN aspect):1. have SPNEGO configs and  use withHttpSpnegoXXXX when starting the web app;2. add AuthenticationFilterInitializer into the configuration of filter initializer list.If both ways are used; it seems that two AuthenticationFilter will be instantiated; which is not expected. It's good to allow one or the other.,Open,Unresolved,,Unassigned,Zhijie Shen,Tue; 13 May 2014 18:13:21 +0000,Tue; 13 May 2014 18:13:21 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10601
HADOOP-10602,Bug,Trivial,documentation,"Documentation has broken ""Go Back"" hyperlinks.","Multiple pages of our documentation have ""Go Back"" links that are broken; because they point to an incorrect relative path.",Closed,Fixed,,Akira Ajisaka,Chris Nauroth,Tue; 13 May 2014 18:44:34 +0000,Thu; 12 May 2016 18:27:29 +0000,Thu; 29 May 2014 17:36:21 +0000,,2.4.0;3.0.0-alpha1,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10602
HADOOP-10603,Sub-task,Major,security,Crypto input and output streams implementing Hadoop stream interfaces,A common set of Crypto Input/Output streams. They would be used by CryptoFileSystem; HDFS encryption; MapReduce intermediate data and spills. Note we cannot use the JDK Cipher Input/Output streams directly because we need to support the additional interfaces that the Hadoop FileSystem streams implement (Seekable; PositionedReadable; ByteBufferReadable; HasFileDescriptor; CanSetDropBehind; CanSetReadahead; HasEnhancedByteBufferAccess; Syncable; CanSetDropBehind).,Resolved,Fixed,,Yi Liu,Alejandro Abdelnur,Tue; 13 May 2014 20:24:41 +0000,Sat; 24 May 2014 01:43:40 +0000,Sat; 24 May 2014 01:43:40 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,HDFS-6405,https://issues.apache.org/jira/browse/HADOOP-10603
HADOOP-10604,New Feature,Major,fs,CryptoFileSystem decorator using xAttrs and KeyProvider,A FileSystem implementation that wraps an existing filesystem and provides encryption. It will require the underlying filesystem to support xAttrs. It  will use the KeyProvider API to retrieve encryption keys.This is mostly the work in the patch HADOOP-10150 minus the crypto streams,Resolved,Incomplete,,Yi Liu,Alejandro Abdelnur,Tue; 13 May 2014 20:25:23 +0000,Wed; 19 Apr 2017 18:21:35 +0000,Wed; 19 Apr 2017 18:21:35 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,HDFS-6537,https://issues.apache.org/jira/browse/HADOOP-10604
HADOOP-10605,Task,Major,fs,CryptoFileSystem decorator documentation,Documentation explaining how the Crypto filesystem works and how it is configured.,Open,Unresolved,,Yi Liu,Alejandro Abdelnur,Tue; 13 May 2014 20:25:46 +0000,Tue; 5 Aug 2014 00:14:29 +0000,,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10605
HADOOP-10606,Bug,Critical,fs;io;util,NodeManager cannot launch container when using RawLocalFileSystem for fs.file.impl,"Node manager failed to launch container when I set fs.file.impl to org.apache.hadoop.fs.RawLocalFileSystem in core-site.xml.The log is:WARN ContainersLauncher #11 org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch - Failed to launch container.java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:339)	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:270)	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:344)	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:150)	at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.getLogPathForWrite(LocalDirsHandlerService.java:307)	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:185)	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:81)	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)	at java.util.concurrent.FutureTask.run(FutureTask.java:166)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)	at java.lang.Thread.run(Thread.java:722)The issue is in hadoop-common-project\hadoop-common\src\main\java\org\apache\hadoop\fs\LocalDirAllocator.java. It invokes FileSystem.getLocal(); which tries to convert the FileSystem to LocalFileSystem and will fail when FileSystem object is RawLocalFileSystem (RawLocalFileSystem is not a sub class of LocalFileSystem).  public static LocalFileSystem getLocal(Configuration conf)    throws IOException {    return (LocalFileSystem)get(LocalFileSystem.NAME; conf);  }The fix for LocalDirAllocator.java seems to be invoking LocalFileSystem.get() instead of LocalFileSystem.getLocal()?",Open,Unresolved,,Unassigned,BoYang,Wed; 14 May 2014 00:54:39 +0000,Tue; 12 Apr 2016 06:54:40 +0000,,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10606
HADOOP-10607,New Feature,Major,security,Create an API to Separate Credentials/Password Storage from Applications,As with the filesystem API; we need to provide a generic mechanism to support multiple credential storage mechanisms that are potentially from third parties. We need the ability to eliminate the storage of passwords and secrets in clear text within configuration files or within code.Toward that end; I propose an API that is configured using a list of URLs of CredentialProviders. The implementation will look for implementations using the ServiceLoader interface and thus support third party libraries.Two providers will be included in this patch. One using the credentials cache in MapReduce jobs and the other using Java KeyStores from either HDFS or local file system. A CredShell CLI will also be included in this patch which provides the ability to manage the credentials within the stores.,Closed,Fixed,,Larry McCay,Larry McCay,Wed; 14 May 2014 02:08:56 +0000,Thu; 3 Dec 2015 22:31:40 +0000,Wed; 18 Jun 2014 15:45:46 +0000,,,,,HADOOP-10829;HADOOP-10831;HADOOP-10904;HADOOP-10141;HADOOP-10830;HIVE-7634;ACCUMULO-2464;HADOOP-10834;HADOOP-10833;HADOOP-9534;HIVE-7175,https://issues.apache.org/jira/browse/HADOOP-10607
MAPREDUCE-5899,Improvement,Major,distcp,Support incremental data copy in DistCp,Currently when doing distcp with -update option; for two files with the same file names but with different file length or checksum; we overwrite the whole file. It will be good if we can detect the case where (sourceFile = targetFile + appended_data); and only transfer the appended data segment to the target. This will be very useful if we're doing incremental distcp.,Closed,Fixed,,Jing Zhao,Jing Zhao,Wed; 14 May 2014 02:32:31 +0000,Tue; 8 Sep 2015 18:54:54 +0000,Thu; 22 May 2014 18:50:47 +0000,,,,,MAPREDUCE-6471,https://issues.apache.org/jira/browse/MAPREDUCE-5899
HADOOP-10609,Bug,Major,,.gitignore should ignore .orig and .rej files,.gitignore file should ignore .orig and .rej files,Closed,Fixed,,Karthik Kambatla,Karthik Kambatla,Wed; 14 May 2014 18:24:34 +0000,Fri; 21 Nov 2014 17:55:02 +0000,Sat; 17 May 2014 01:41:20 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10609
HADOOP-10610,Improvement,Minor,fs/s3,Upgrade S3n fs.s3.buffer.dir to support multi directories,fs.s3.buffer.dir defines the tmp folder where files will be written to before getting sent to S3.  Right now this is limited to a single folder which causes to major issues.1. You need a drive with enough space to store all the tmp files at once2. You are limited to the IO speeds of a single driveThis solution will resolve both and has been tested to increase the S3 write speed by 2.5x with 10 mappers on hs1.,Closed,Fixed,,Theodore michael Malaska,Theodore michael Malaska,Tue; 13 May 2014 14:28:53 +0000,Mon; 22 Aug 2016 05:00:41 +0000,Thu; 17 Jul 2014 21:58:29 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10610
HADOOP-10611,Bug,Major,security,KMS; keyVersion name should not be assumed to be keyName@versionNumber,Some KMS classes are assuming the keyVersion is keyName@versionNumber. The keyVersion should be handled as an opaque value.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Thu; 15 May 2014 16:41:37 +0000,Thu; 12 May 2016 18:23:32 +0000,Fri; 30 May 2014 23:19:22 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10611
HADOOP-10612,Bug,Major,nfs,NFS failed to refresh the user group id mapping table,Found by Preetham Kukillaya. The user/group id mapping table is not updated periodically.,Closed,Fixed,,Brandon Li,Brandon Li,Thu; 15 May 2014 22:56:27 +0000,Mon; 30 Jun 2014 08:18:21 +0000,Mon; 19 May 2014 22:23:46 +0000,,2.4.0,,,HDFS-6044;HDFS-6416,https://issues.apache.org/jira/browse/HADOOP-10612
HADOOP-10613,Bug,Major,fs,Potential Resource Leaks in FileSystem.CACHE ,There is no size limit of the hashmap in  FileSystem.CACHE; which can cause a potential memory leak.If every time i use a new UGI object to invoke FileSystem.get(conf)  and never invoke FileSystem's close method;this issue will raise.If there is a size limit of the hashmap or changing fileSystem instances to soft reference;then user's code don't need to consider too much about the cache leak issues.,Open,Unresolved,,Unassigned,Nemon Lou,Fri; 16 May 2014 02:50:15 +0000,Mon; 19 May 2014 09:09:55 +0000,,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10613
HADOOP-10614,Improvement,Major,,CBZip2InputStream is not threadsafe,Hadoop uses CBZip2InputStream to decode bzip2 files. However; the implementation is not threadsafe. This is not a really problem for Hadoop MapReduce because Hadoop runs each task in a separate JVM. But for other libraries that utilize multithreading and use Hadoop's InputFormat; e.g.; Spark; it will cause exceptions like the following:,Closed,Fixed,,Xiangrui Meng,Xiangrui Meng,Fri; 16 May 2014 02:32:34 +0000,Fri; 15 Aug 2014 05:39:44 +0000,Sat; 17 May 2014 18:34:44 +0000,,1.2.1;2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10614
HADOOP-10615,Bug,Minor,,FileInputStream in JenkinsHash#main() is never closed,The above FileInputStream is not closed upon exit of main.,Resolved,Fixed,,Chen He,Ted Yu,Sat; 17 May 2014 15:18:08 +0000,Tue; 30 Aug 2016 01:32:23 +0000,Thu; 16 Jul 2015 05:13:59 +0000,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10615
HADOOP-10616,Improvement,Major,,LoadSnappy should log the error message from UnsatisfiedLinkError,"org.apache.hadoop.io.compress.snappy.LoadSnappy currently does this: So; the UnsatisfiedLinkError is swallowed without us ever knowing what the error message was. However the message from UnsatisfiedLinkError can contain useful information for figuring out why the library isn't loading. In my case it was:""Exception in thread ""main"" java.lang.UnsatisfiedLinkError: snappy (/lib64/libc.so.6: version `GLIBC_2.14' not found (required by /path removed/libsnappy.so))""Telling me that snappy is built against the wrong version of libc.I'd suggest logging the message from UnsatisfiedLinkError later on when we log whether snappy was loaded or not. So; the full code for the static initializer be something like (not compiled or tested):  Though this could perhaps be a little neater.",Open,Unresolved,,Unassigned,Neil Ferguson,Sat; 17 May 2014 15:56:13 +0000,Sat; 17 May 2014 15:57:28 +0000,,,0.23.10,,,,https://issues.apache.org/jira/browse/HADOOP-10616
HADOOP-10617,Sub-task,Major,security,Tests for Crypto input and output streams using fake streams implementing Hadoop streams interfaces.,Tests for Crypto input and output streams using fake input and output streams implementing Hadoop streams interfaces. To cover functionality of Hadoop streams with crypto.,Resolved,Fixed,,Yi Liu,Yi Liu,Sun; 18 May 2014 16:28:44 +0000,Sat; 24 May 2014 01:48:10 +0000,Sat; 24 May 2014 01:48:10 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10617
HADOOP-10618,Improvement,Minor,documentation,Remove SingleNodeSetup.apt.vm,http://hadoop.apache.org/docs/r2.4.0/hadoop-project-dist/hadoop-common/SingleNodeSetup.html is deprecated and not linked from the left side page.We should remove the document and use http://hadoop.apache.org/docs/r2.4.0/hadoop-project-dist/hadoop-common/SingleCluster.html instead.,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Mon; 19 May 2014 05:28:25 +0000,Wed; 3 Sep 2014 20:36:28 +0000,Thu; 22 May 2014 20:42:27 +0000,,2.4.0,newbie,,HADOOP-10460,https://issues.apache.org/jira/browse/HADOOP-10618
HADOOP-10620,Improvement,Major,documentation,/docs/current doesn't point to the latest version 2.4.0,http://hadoop.apache.org/docs/current/ points to 2.3.0 while 2.4.0's out.,Closed,Fixed,,Unassigned,Jacek Laskowski,Mon; 19 May 2014 20:59:47 +0000,Fri; 12 Feb 2016 02:07:27 +0000,Thu; 1 Jan 2015 22:28:56 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10620
HADOOP-10621,Sub-task,Minor,,Remove CRLF for xattr value base64 encoding for better display.,Base64.encodeBase64String(value) encodes binary data using the base64 algorithm into 76 character blocks separated by CRLF.In fs shell; xattrs display like: We don't need multiple line and CRLF for xattr value; and we can use:,Resolved,Fixed,,Yi Liu,Yi Liu,Sat; 17 May 2014 10:59:24 +0000,Wed; 21 May 2014 16:09:11 +0000,Tue; 20 May 2014 08:57:36 +0000,,HDFS XAttrs (HDFS-2006),,,,https://issues.apache.org/jira/browse/HADOOP-10621
HADOOP-10622,Bug,Critical,,Shell.runCommand can deadlock,Ran into a deadlock in Shell.runCommand.  Stacktrace details to follow.,Closed,Fixed,,Gera Shegalov,Jason Lowe,Tue; 20 May 2014 21:06:35 +0000,Tue; 28 Mar 2017 22:06:58 +0000,Wed; 11 Jun 2014 22:06:35 +0000,,2.3.0,,,HADOOP-14084,https://issues.apache.org/jira/browse/HADOOP-10622
HADOOP-10623,New Feature,Major,,Provide a utility to be able inspect the config as seen by a hadoop client / daemon ,To ease debugging of config issues it is convenient to be able to generate a config as seen by the job client or a hadoop daemon,Open,Unresolved,,Gera Shegalov,Gera Shegalov,Wed; 21 May 2014 03:55:09 +0000,Sat; 7 Jan 2017 01:57:04 +0000,,,,,,HADOOP-7947;HADOOP-9044;HADOOP-12118,https://issues.apache.org/jira/browse/HADOOP-10623
HADOOP-10624,Sub-task,Major,,Fix some minor typos and add more test cases for hadoop_err,Changes:1. Add more test cases to cover method hadoop_lerr_alloc and hadoop_uverr_alloc2. Fix typo as following:    1) Change hadoop_uverr_alloc(int cod to hadoop_uverr_alloc(int code in hadoop_err.h    2) Change OutOfMemory to OutOfMemoryException to consistent with other Exception in hadoop_err.c    3) Change DBUG to DEBUG in messenger.c    4) Change DBUG to DEBUG in reactor.c,Resolved,Fixed,,Wenwu Peng,Wenwu Peng,Wed; 21 May 2014 06:57:56 +0000,Fri; 30 May 2014 23:48:13 +0000,Fri; 30 May 2014 23:13:21 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10624
HADOOP-10625,Bug,Major,conf,Configuration: names should be trimmed when putting/getting to properties,"Currently; Hadoop will not trim name when putting a pair of k/v to property. But when loading configuration from file; names will be trimmed:(In Configuration.java) With this behavior; following steps will be problematic:1. User incorrectly set "" hadoop.key=value"" (with a space before hadoop.key)2. User try to get ""hadoop.key""; cannot get ""value""3. Serialize/deserialize configuration (Like what did in MR)4. User try to get ""hadoop.key""; can get ""value""; which will make inconsistency problem.",Closed,Fixed,,Wangda Tan,Wangda Tan,Thu; 22 May 2014 05:18:08 +0000,Fri; 15 Aug 2014 05:39:24 +0000,Wed; 28 May 2014 17:12:41 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10625
HADOOP-10626,Improvement,Major,security,Limit Returning Attributes for LDAP search,When using Hadoop Ldap Group mappings in an enterprise environment; searching groups and returning all members can take a long time causing a timeout.  This causes not all groups to be returned for a user.  Because the first search only searches for the user dn and the second search retrieves the group member attribute; we only need to return the group member attribute on the search speeding up the search.,Closed,Fixed,,Jason Hubbard,Jason Hubbard,Thu; 22 May 2014 14:02:24 +0000,Fri; 10 Apr 2015 20:04:19 +0000,Tue; 27 Jan 2015 21:54:27 +0000,,2.3.0,easyfix;newbie;performance,,,https://issues.apache.org/jira/browse/HADOOP-10626
HADOOP-10627,Improvement,Major,documentation,Add documentation for http/https policy configuration/setup,HADOOP-10022/HDFS-5305 etc. adds new http/https policy support in HDFS and Yarn. We should have documentation for its configuration and setup.,Open,Unresolved,,Unassigned,Jing Zhao,Thu; 22 May 2014 17:53:29 +0000,Thu; 22 May 2014 17:53:54 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10627
HADOOP-10628,Sub-task,Major,security,Javadoc and few code style improvement for Crypto input and output streams,There are some additional comments from Charles Lamb related to javadoc and few code style on HADOOP-10603; let's fix them in this follow-on JIRA.,Resolved,Fixed,,Yi Liu,Yi Liu,Sat; 24 May 2014 01:38:24 +0000,Thu; 29 May 2014 22:28:43 +0000,Thu; 29 May 2014 22:28:43 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10628
HADOOP-10629,Improvement,Major,ipc,security diagnostics info being dropped in exceptions seen by client,When there are some security problems; not all the info goes back to the client; which sees It's only server-side the diagnostics surface; here some javax crypto issues -the inner exception text isn't making it back to the client...,Resolved,Cannot Reproduce,,Unassigned,Steve Loughran,Sat; 24 May 2014 13:41:05 +0000,Thu; 19 Nov 2015 16:09:29 +0000,Thu; 19 Nov 2015 16:09:29 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10629
HADOOP-10630,Bug,Major,,Possible race condition in RetryInvocationHandler,"In one of our system tests with NameNode HA setup; we ran 300 threads in LoadGenerator. While one of the NameNodes was already in the active state and started to serve; we still saw one of the client thread failed all the retries in a 20 seconds window. In the meanwhile; we saw a lot of following warning msg in the log: After checking the code; we see the following code in RetryInvocationHandler: We can see we refresh the value of currentProxy only when the thread performs the failover (while holding the monitor of the proxyProvider). Because ""currentProxy"" is not volatile;  a thread that does not perform the failover (in which case it will log the warning msg) may fail to get the new value of currentProxy.",Closed,Fixed,,Jing Zhao,Jing Zhao,Tue; 27 May 2014 22:11:05 +0000,Fri; 15 Aug 2014 05:39:41 +0000,Mon; 2 Jun 2014 21:45:56 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10630
HADOOP-10631,Sub-task,Trivial,,Native Hadoop Client: make clean should remove pb-c.h.s files,In GenerateProtobufs.cmake; pb-c.h.s files are not added to output; so when make clean is called; those files are not cleaned.,Resolved,Fixed,,Binglin Chang,Binglin Chang,Wed; 28 May 2014 03:27:21 +0000,Thu; 29 May 2014 06:43:06 +0000,Wed; 28 May 2014 17:47:11 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10631
HADOOP-10632,Sub-task,Major,security,Minor improvements to Crypto input and output streams,Minor follow up feedback on the crypto streams,Closed,Fixed,,Yi Liu,Alejandro Abdelnur,Wed; 28 May 2014 04:14:23 +0000,Mon; 1 Dec 2014 03:08:40 +0000,Fri; 30 May 2014 08:09:49 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10632
HADOOP-10633,Improvement,Major,io;security,use Time#monotonicNow to avoid system clock reset,let's replace System#currentTimeMillis with Time#monotonicNow,Patch Available,Unresolved,,Liang Xie,Liang Xie,Wed; 28 May 2014 06:17:56 +0000,Thu; 12 May 2016 18:24:10 +0000,,,3.0.0-alpha1,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10633
HADOOP-10634,Improvement,Major,fs/s3,Add recursive list apis to FileSystem to give implementations an opportunity for optimization,Currently different code flows in hadoop use recursive listing to discover files/folders in a given path. For example in FileInputFormat (both mapreduce and mapred implementations) this is done while calculating splits. They however do this by doing listing level by level. That means to discover files in /foo/bar means they do listing at /foo/bar first to get the immediate children; then make the same call on all immediate children for /foo/bar to discover their immediate children and so on. This doesn't scale well for fs implementations like s3 because every listStatus call ends up being a webservice call to s3. In cases where large number of files are considered for input; this makes getSplits() call slow. This patch adds a new set of recursive list apis that give opportunity to the s3 fs implementation to optimize. The behavior remains the same for other implementations (that is a default implementation is provided for other fs so they don't have to implement anything new). However for s3 it provides a simple change (as shown in the patch) to improve listing performance.,Resolved,Duplicate,NULL,Unassigned,Sumit Kumar,Wed; 28 May 2014 17:49:10 +0000,Mon; 2 Jun 2014 16:34:01 +0000,Thu; 29 May 2014 18:40:23 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10634
HADOOP-10635,Sub-task,Major,security,Add a method to CryptoCodec to generate SRNs for IV,SRN generators are provided by crypto libraries. the CryptoCodec gives access to a crypto library; thus it makes sense to expose the SRN generator on the CryptoCodec API.,Closed,Fixed,,Yi Liu,Alejandro Abdelnur,Thu; 29 May 2014 05:08:04 +0000,Mon; 1 Dec 2014 03:10:03 +0000,Fri; 30 May 2014 08:26:43 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10635
HADOOP-10636,Sub-task,Major,,Native Hadoop Client:add unit test case for call&client_id,,Resolved,Fixed,,Wenwu Peng,Wenwu Peng,Thu; 29 May 2014 05:22:03 +0000,Wed; 18 Jun 2014 17:56:34 +0000,Wed; 18 Jun 2014 03:52:11 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10636
HADOOP-10637,Improvement,Major,test,Add snapshot and several dfsadmin tests into TestCLI,Add the following commands to TestCLI:appendToFiletextrmdirrmdir with ignore-fail-on-non-emptydfexpungegetmergeallowSnapshotdisallowSnapshotcreateSnapshotrenameSnapshotdeleteSnapshotrefreshUserToGroupsMappingsrefreshSuperUserGroupsConfigurationsetQuotaclrQuotasetSpaceQuotasetBalancerBandwidthfinalizeUpgrade,Resolved,Duplicate,HDFS-6297,Unassigned,Dasha Boudnik,Thu; 29 May 2014 09:32:06 +0000,Thu; 12 May 2016 18:25:09 +0000,Fri; 30 May 2014 23:15:24 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10637
HADOOP-10638,Bug,Major,nfs,Updating hadoop-daemon.sh to work as expected when nfs is started as a privileged user. ,When NFS is started as a privileged user; this change sets up required environment variables:HADOOP_PID_DIR = $HADOOP_PRIVILEGED_NFS_PID_DIRHADOOP_LOG_DIR = $HADOOP_PRIVILEGED_NFS_LOG_DIRHADOOP_IDENT_STRING = $HADOOP_PRIVILEGED_NFS_USERAlso; along with the above; we also now collect ulimits for the right user.,Closed,Fixed,,Manikandan Narayanaswamy,Manikandan Narayanaswamy,Thu; 29 May 2014 17:52:40 +0000,Fri; 15 Aug 2014 05:39:52 +0000,Fri; 30 May 2014 01:53:56 +0000,,2.4.0,patch,,,https://issues.apache.org/jira/browse/HADOOP-10638
HADOOP-10639,Bug,Major,security,FileBasedKeyStoresFactory initialization is not using default for SSL_REQUIRE_CLIENT_CERT_KEY,The FileBasedKeyStoresFactory initialization is defaulting SSL_REQUIRE_CLIENT_CERT_KEY to true instead of the default DEFAULT_SSL_REQUIRE_CLIENT_CERT (false).,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Thu; 29 May 2014 18:47:03 +0000,Fri; 15 Aug 2014 05:39:24 +0000,Thu; 29 May 2014 21:40:31 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10639
HADOOP-10640,Sub-task,Major,native,Implement Namenode RPCs in HDFS native client,Implement the parts of libhdfs that just involve making RPCs to the Namenode; such as mkdir; rename; etc.,Resolved,Fixed,HADOOP-10446;HADOOP-10447;HADOOP-10443,Colin P. McCabe,Colin P. McCabe,Thu; 29 May 2014 20:02:55 +0000,Mon; 16 Jun 2014 09:15:19 +0000,Thu; 12 Jun 2014 19:56:36 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10640
HADOOP-10641,New Feature,Major,,Introduce Coordination Engine interface,Coordination Engine (CE) is a system; which allows to agree on a sequence of events in a distributed system. In order to be reliable CE should be distributed by itself.Coordination Engine can be based on different algorithms (paxos; raft; 2PC; zab) and have different implementations; depending on use cases; reliability; availability; and performance requirements.CE should have a common API; so that it could serve as a pluggable component in different projects. The immediate beneficiaries are HDFS (HDFS-6469) and HBase (HBASE-10909).First implementation is proposed to be based on ZooKeeper.,Open,Unresolved,,Plamen Jeliazkov,Konstantin Shvachko,Thu; 29 May 2014 20:55:01 +0000,Thu; 12 May 2016 18:26:23 +0000,,,3.0.0-alpha1,,HBASE-11241,,https://issues.apache.org/jira/browse/HADOOP-10641
HADOOP-10642,Improvement,Major,metrics,Provide option to limit heap memory consumed by dynamic metrics2 metrics,User sunweiei provided the following jmap output in HBase 0.96 deployment: Heap consumption by Interns$CacheWith2Keys$2 (and indirectly by [C) could be due to calls to Interns.info() in DynamicMetricsRegistry which was cloned off metrics2/lib/MetricsRegistry.java.This scenario would arise when large number of regions are tracked through metrics2 dynamically.Interns class doesn't provide API to remove entries in its internal Map.One solution is to provide an option that allows skipping calls to Interns.info() in metrics2/lib/MetricsRegistry.java,Resolved,Later,,Unassigned,Ted Yu,Fri; 30 May 2014 00:54:12 +0000,Wed; 11 Oct 2017 18:14:36 +0000,Wed; 11 Oct 2017 18:14:36 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10642
HADOOP-10643,Sub-task,Major,fs/s3,Add NativeS3Fs that delegates calls from FileContext apis to native s3 fs implementation,The new set of file system related apis (FileContext/AbstractFileSystem) already support local filesytem; hdfs; viewfs) however they don't support s3n. This patch is to add that support using configurations likefs.AbstractFileSystem.s3n.impl = org.apache.hadoop.fs.s3native.NativeS3FsThis patch however doesn't provide a new implementation; instead relies on DelegateToFileSystem abstract class to delegate all calls from FileContext apis for s3n to the NativeS3FileSystem implementation.,Resolved,Duplicate,NULL,Sumit Kumar,Sumit Kumar,Fri; 30 May 2014 01:10:34 +0000,Thu; 12 May 2016 19:36:32 +0000,Wed; 20 Jan 2016 19:18:25 +0000,,2.4.0,,,HADOOP-11262,https://issues.apache.org/jira/browse/HADOOP-10643
HADOOP-10644,Bug,Major,security,Remote principal name case sensitivity issue introduced on Windows by HADOOP-10418,HADOOP-10418 caused the SPN to be generated using KRB_NT_SRV_HST type. This results in a wrong case FQDN name and the  check fails due to case difference.,Resolved,Not A Problem,,Unassigned,Remus Rusanu,Fri; 30 May 2014 13:29:15 +0000,Fri; 30 May 2014 13:55:40 +0000,Fri; 30 May 2014 13:55:40 +0000,,2.4.0,windows,,,https://issues.apache.org/jira/browse/HADOOP-10644
HADOOP-10645,Bug,Major,security,TestKMS fails because race condition writing acl files,The TestKMS#testACLs() test randomly fails because a race condition while updating the acls files which is hot-reloaded.We should disable the background thread that does the reload and do it manually for the purposes of the test.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Fri; 30 May 2014 23:06:46 +0000,Thu; 12 May 2016 18:27:07 +0000,Fri; 30 May 2014 23:12:12 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10645
HADOOP-10646,Bug,Major,security,KeyProvider buildVersionName method should be moved to a utils class,The buildVersionName() method should not be part of the KeyProvider public API because keyversions could be opaque (not built based on the keyname and key generation counter).KeyProvider implementations may choose to use buildVersionName() for reasons such as described in HADOOP-10611.,Open,Unresolved,,Unassigned,Alejandro Abdelnur,Fri; 30 May 2014 23:17:22 +0000,Thu; 12 May 2016 18:27:08 +0000,,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10646
HADOOP-10647,Bug,Minor,fs/swift,String Format Exception in SwiftNativeFileSystemStore.java,If Swift.debug is given a string containing a % character; a format exception will occur. This happens when the path for any of the FileStatus objects contain a % encoded character. The bug is located at hadoop/src/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/snative/SwiftNativeFileSystemStore.java:931.,Closed,Fixed,,Gene Kim,Gene Kim,Sun; 1 Jun 2014 22:36:12 +0000,Fri; 15 Aug 2014 05:39:39 +0000,Fri; 6 Jun 2014 18:01:41 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10647
HADOOP-10648,Improvement,Major,security,Service Authorization Improvements,Umbrella jira for a set of improvements on service Authorization,Open,Unresolved,,Benoy Antony,Benoy Antony,Mon; 2 Jun 2014 01:35:42 +0000,Sun; 8 Jun 2014 16:36:08 +0000,,,,,,HADOOP-10590,https://issues.apache.org/jira/browse/HADOOP-10648
HADOOP-10649,Sub-task,Major,security,Allow overriding the default ACL for service authorization ,The default service authorization for a protocol is  *  and this authorizes everyone for the specific protocol.It should be possible to override the default ACL and specify a different acl as the default ACL value.,Closed,Fixed,,Benoy Antony,Benoy Antony,Mon; 2 Jun 2014 01:44:03 +0000,Wed; 3 Sep 2014 20:36:27 +0000,Fri; 27 Jun 2014 18:45:33 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10649
HADOOP-10650,Sub-task,Major,security,Add ability to specify a reverse ACL (black list) of users and groups,Currently ; it is possible to define a ACL (user and groups) for a service. To temporarily remove authorization for a set of users; administrator needs to remove the users from the specific group and this may be a lengthy process ( update ldap groups; flush caches on machines). If there is a facility to define a reverse ACL for services; then administrator can disable users by specifying the users in reverse ACL. In other words; one can specify a whitelist of users and groups as well as a blacklist of users and groups. One can also specify a default blacklist to disable the users from accessing any service.,Closed,Fixed,,Benoy Antony,Benoy Antony,Mon; 2 Jun 2014 01:45:09 +0000,Mon; 1 Dec 2014 03:11:19 +0000,Sun; 17 Aug 2014 17:07:19 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10650
HADOOP-10651,Sub-task,Major,security,Add ability to restrict service access using IP addresses and hostnames,In some use cases; it make sense to authorize the usage of some services only from specific hosts. Just like ACLS for Service Authorization ; there can be a list of hosts for each service and this list can be checked during authorization. Similar to ACLS; there can be a whitelist of ip and blacklist of ips. The default whitelist will be * and default blacklist will be empty. It should be possible to override the default whitelist and default blacklist. It should be possible to define whitelist and blacklist per service.It should be possible to define ip ranges in blacklists and whitelists,Closed,Fixed,,Benoy Antony,Benoy Antony,Mon; 2 Jun 2014 01:47:20 +0000,Mon; 31 Jul 2017 09:30:46 +0000,Thu; 8 Jan 2015 18:12:34 +0000,,2.5.0,,,HADOOP-14702,https://issues.apache.org/jira/browse/HADOOP-10651
HADOOP-10652,Sub-task,Major,security,Refactor Proxyusers to use AccessControlList  ,Currently Proxyuser specification  accepts a list of users and groups including wildcard values. Same functionality is already encapsulated in AccessControlList . It will be better to refactor ProxyUsers to use AccessControlList instead of maintaining separate logic.,Closed,Fixed,,Benoy Antony,Benoy Antony,Mon; 2 Jun 2014 01:51:57 +0000,Wed; 3 Sep 2014 20:36:31 +0000,Tue; 24 Jun 2014 18:05:52 +0000,,,,,HADOOP-10659,https://issues.apache.org/jira/browse/HADOOP-10652
HADOOP-10653,Sub-task,Major,security,Add a new constructor for CryptoInputStream that receives current position of wrapped stream.,Add a new constructor for CryptoInputStream that receives current position of wrapped stream.We need it for shuffle stream over http.,Resolved,Fixed,,Yi Liu,Yi Liu,Mon; 2 Jun 2014 06:12:17 +0000,Mon; 2 Jun 2014 14:36:06 +0000,Mon; 2 Jun 2014 14:36:06 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10653
HADOOP-10654,Sub-task,Major,security,Support pluggable mechanism to specify Service Authorization,The service authorization can be specified via hadoop-policy.xml for different services(protocols) using ACLS ( list of users and groups) . The other subtasks will improve the standard behavior. In some case; further special handling is required to restrict access based on few other external characteristics of the user for special clusters. Such special authorization requirements can be met if it is possible to plugin authorization mechanisms at this point.,Open,Unresolved,,Benoy Antony,Benoy Antony,Mon; 2 Jun 2014 16:31:39 +0000,Wed; 7 Jan 2015 22:42:51 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10654
HADOOP-10655,Bug,Major,fs/swift,swift native store returns true even when it doesn't delete the directory,Wasn't sure if this was desired behavior but the javadoc comments and implementation seem to contradict; hence this JIRA. See http://tiny.cc/aa6tgx,Open,Unresolved,,Unassigned,Sumit Kumar,Mon; 2 Jun 2014 16:48:27 +0000,Fri; 6 Jun 2014 16:22:01 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10655
HADOOP-10656,Bug,Major,security,The password keystore file is not picked by LDAP group mapping,The user configured password file(LDAP_KEYSTORE_PASSWORD_FILE_KEY) will not be picked by LdapGroupsMapping:In setConf():,Closed,Fixed,,Brandon Li,Brandon Li,Mon; 2 Jun 2014 23:37:19 +0000,Fri; 15 Aug 2014 05:39:44 +0000,Wed; 11 Jun 2014 18:59:29 +0000,,2.2.0,,,HADOOP-8121,https://issues.apache.org/jira/browse/HADOOP-10656
HADOOP-10657,Bug,Major,,Have RetryInvocationHandler log failover attempt at INFO level,RetryInovcationHandler uses worthLogging flag to decide if it will do logging. worthLogging will be false for first fails over given invocationFailoverCount is zero. That addresses the log noise where the second-listed NN is active.For other failover scenarios; it will be useful to log the error message at info level for analysis purpose.,Closed,Fixed,,Ming Ma,Ming Ma,Mon; 2 Jun 2014 23:00:40 +0000,Fri; 15 Aug 2014 05:39:23 +0000,Mon; 16 Jun 2014 17:53:58 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10657
HADOOP-10658,Bug,Major,security,SSLFactory expects truststores being configured,The FileBasedKeyStoresFactory used by the SSLFactory expects truststores to be always present; if using CA signed certificates; they are not needed.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 3 Jun 2014 04:32:29 +0000,Fri; 15 Aug 2014 05:39:51 +0000,Mon; 9 Jun 2014 18:47:27 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10658
HADOOP-10659,Sub-task,Minor,security,Refactor AccessControlList to reuse utility functions and to improve performance,Minor improvements can be done on AccessControlList.Code Reusability:AccessControlList sanitizes the input list to remove duplicate entries; trim entries. StringUtils.getTrimmedStringCollection can be used in this case.Performance:AccessControlList  uses TreeSet to maintain set of users and groups.HashSet improves the performance slightly.,Closed,Fixed,,Benoy Antony,Benoy Antony,Tue; 3 Jun 2014 20:14:07 +0000,Wed; 3 Sep 2014 20:36:32 +0000,Mon; 23 Jun 2014 23:16:37 +0000,,,,,HADOOP-10652,https://issues.apache.org/jira/browse/HADOOP-10659
HADOOP-10660,Bug,Major,,GraphiteSink should implement Closeable,GraphiteSink wraps OutputStreamWriter around socket's output stream.Currently the socket is never closed.GraphiteSink should implement Closeable such that MetricsSystem can close the socket when it is stopped.,Closed,Fixed,,Chen He,Ted Yu,Tue; 3 Jun 2014 22:53:22 +0000,Fri; 15 Aug 2014 05:39:37 +0000,Wed; 18 Jun 2014 09:29:13 +0000,,,,,HADOOP-10715,https://issues.apache.org/jira/browse/HADOOP-10660
HADOOP-10661,Bug,Minor,,Ineffective user/passsword check in FTPFileSystem#initialize(),Here is related code: The intention seems to be checking that username / password should not be null.But due to the presence of colon; the above check is not effective.,Resolved,Not A Problem,,Chen He,Ted Yu,Wed; 4 Jun 2014 04:44:26 +0000,Thu; 28 May 2015 17:59:28 +0000,Thu; 28 May 2015 17:59:28 +0000,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10661
HADOOP-10662,Sub-task,Major,security,NullPointerException in CryptoInputStream while wrapped stream is not ByteBufferReadable. Add tests using normal stream.,NullPointerException in CryptoInputStream while wrapped stream is not ByteBufferReadable. Add tests for crypto streams using normal stream which does not support the additional interfaces that the Hadoop FileSystem streams implement (Seekable; PositionedReadable; ByteBufferReadable; HasFileDescriptor; CanSetDropBehind; CanSetReadahead; HasEnhancedByteBufferAccess; Syncable; CanSetDropBehind).,Resolved,Fixed,,Yi Liu,Yi Liu,Wed; 4 Jun 2014 06:03:02 +0000,Thu; 5 Jun 2014 01:31:32 +0000,Thu; 5 Jun 2014 01:31:32 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10662
HADOOP-10663,Bug,Major,,Path.getFileSystem should identify Windows drive (c:) like files and return the localFS,While investigating the problem with MAPREDUCE-5912 I considered that the fact that asking a Windows specific Path to get its file system should not return the default FS; but the localFS. Specifically; I believe that Path.initialize should identify such names as belonging to the 'file://' scheme (when null) and so create appropriate URIs.I do not have the know-how to evaluate the bigger impact of such a change.,Resolved,Won't Fix,,Remus Rusanu,Remus Rusanu,Wed; 4 Jun 2014 16:01:22 +0000,Thu; 12 May 2016 18:22:40 +0000,Thu; 12 Jun 2014 21:38:24 +0000,,3.0.0-alpha1,,,MAPREDUCE-5912,https://issues.apache.org/jira/browse/HADOOP-10663
HADOOP-10664,Bug,Major,,TestNetUtils.testNormalizeHostName fails,"java.lang.AssertionError: null	at org.junit.Assert.fail(Assert.java:86)	at org.junit.Assert.assertTrue(Assert.java:41)	at org.junit.Assert.assertFalse(Assert.java:64)	at org.junit.Assert.assertFalse(Assert.java:74)	at org.apache.hadoop.net.TestNetUtils.testNormalizeHostName(TestNetUtils.java:617)",Closed,Fixed,,Aaron T. Myers,Chen He,Wed; 4 Jun 2014 20:25:53 +0000,Fri; 15 Aug 2014 05:39:39 +0000,Mon; 9 Jun 2014 19:06:52 +0000,,2.4.0,test,,,https://issues.apache.org/jira/browse/HADOOP-10664
HADOOP-10665,Improvement,Minor,security,Make Hadoop Authentication Handler loads case in-sensitive,"The authentication mechanism for http specified via hadoop.http.authentication.type is matched in in a case sensitive way. If one specifies the type {simple;kerberos}  in the wrong case; the following exception is thrown . Caused by: java.lang.ClassNotFoundException: Simple	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)	at java.security.AccessController.doPrivileged(Native Method)	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:148)	... 24 moreI believe ; it is safe to ignore case during this comparison.",Closed,Fixed,,Benoy Antony,Benoy Antony,Thu; 5 Jun 2014 19:39:37 +0000,Fri; 15 Aug 2014 05:39:51 +0000,Tue; 24 Jun 2014 10:33:07 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10665
HADOOP-10666,Improvement,Minor,documentation,Remove Copyright /d/d/d/d Apache Software Foundation from the source files license header,"Some of the files have ""Copyright 2011 Apache Software Foundation"" in the license header comment.This is not the right license header per ASF rule 1From the ASF header rule 1 :""Source File Headers for Code Developed at the ASF""""This section refers only to works submitted directly to the ASF by the copyright owner or owner's agent.""Seems like the one without Copyright notice is used for software developed directly by ASF which Apache Hadoop is.When it was under external organization it does need to use the one from ASF 2.0 license 2 which requires Copyright included.1 http://www.apache.org/legal/src-headers.html2 http://www.apache.org/licenses/LICENSE-2.0",Closed,Fixed,,Henry Saputra,Henry Saputra,Thu; 5 Jun 2014 22:59:28 +0000,Fri; 15 Aug 2014 05:39:25 +0000,Mon; 16 Jun 2014 23:27:57 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10666
HADOOP-10667,Sub-task,Major,native,implement TCP connection reuse for native client,The HDFS / YARN native clients should re-use TCP connections to avoid the overhead of the three-way handshake; similar to how the Java code does.,Resolved,Fixed,HADOOP-10705,Colin P. McCabe,Colin P. McCabe,Fri; 6 Jun 2014 07:42:55 +0000,Thu; 26 Jun 2014 18:05:39 +0000,Thu; 26 Jun 2014 18:05:39 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10667
HADOOP-10668,Test,Major,test,TestZKFailoverControllerStress#testExpireBackAndForth occasionally fails,From https://builds.apache.org/job/PreCommit-HADOOP-Build/4018//testReport/org.apache.hadoop.ha/TestZKFailoverControllerStress/testExpireBackAndForth/ :,Closed,Fixed,,Ming Ma,Ted Yu,Fri; 6 Jun 2014 18:59:29 +0000,Tue; 30 Aug 2016 01:32:22 +0000,Mon; 19 Jan 2015 19:36:20 +0000,,,test,,HADOOP-9555,https://issues.apache.org/jira/browse/HADOOP-10668
HADOOP-10669,Bug,Major,io,Avro serialization does not flush buffered serialized values causing data lost,Found this debugging Nutch. MapTask serializes keys and values to the same stream; in pairs: keySerializer.serialize(key); ..... valSerializer.serialize(value); ..... bb.write(b0; 0; 0); AvroSerializer does not flush its buffer after each serialization. So if it is used for valSerializer; the values are only partially written or not written at all to the output stream before the record is marked as complete (the last line above).EDIT Added HADOOP-10699_all.patch. This is a less intrusive fix; as it does not try to flush MapTask stream. Instead; we write serialized values directly to MapTask stream and avoid using a buffer on avro side.,Open,Unresolved,HADOOP-11678,Unassigned,Mikhail Bernadsky,Sun; 8 Jun 2014 05:36:42 +0000,Sun; 30 Jul 2017 04:12:24 +0000,,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10669
HADOOP-10670,Improvement,Minor,security,Allow AuthenticationFilters to load secret from signature secret files,In Hadoop web console; by using AuthenticationFilterInitializer; it's allowed to configure AuthenticationFilter for the required signature secret by specifying signature.secret.file property. This improvement would also allow this when AuthenticationFilterInitializer isn't used in situations like webhdfs.,Closed,Fixed,,Kai Zheng,Kai Zheng,Mon; 9 Jun 2014 05:28:50 +0000,Thu; 18 Jun 2015 22:56:18 +0000,Wed; 25 Mar 2015 18:13:33 +0000,,,,,HADOOP-11567;HADOOP-11763;HADOOP-11748,https://issues.apache.org/jira/browse/HADOOP-10670
HADOOP-10671,Improvement,Major,security,Unify and simplify common configurations for authentication filters between web console and web hdfs,Currently it's not able to single sign on between hadoop web console and webhdfs since they don't share common configurations as required to; such as signature secret to sign authenticaton token; and domain cookie etc. This improvement would allow sso between the two; and also simplify the configuration by removing the duplicate effort for the two parts.The sso makes sense because in current web console; it integrates webhdfs and we should avoid redundant sign on in different mechanisms. This is necessary when a certain authentication mechanism other than SPNEGO is desired across web console and webhdfs.,Patch Available,Unresolved,,Kai Zheng,Kai Zheng,Mon; 9 Jun 2014 05:58:30 +0000,Wed; 6 May 2015 03:27:46 +0000,,,,BB2015-05-TBR,,HADOOP-10709;HADOOP-10679,https://issues.apache.org/jira/browse/HADOOP-10671
HADOOP-10672,New Feature,Minor,metrics,Add support for pushing metrics to OpenTSDB,We wish to add support for pushing metrics to OpenTSDB from hadoop Code and instructions at - https://github.com/eBay/hadoop-tsdb-connector,Patch Available,Unresolved,,zhangyubiao,Kamaldeep Singh,Mon; 9 Jun 2014 11:12:05 +0000,Wed; 8 Jun 2016 14:17:43 +0000,,,0.21.0,,,HADOOP-13003,https://issues.apache.org/jira/browse/HADOOP-10672
HADOOP-10673,Bug,Major,,Update rpc metrics when the call throws an exception,Currently RPC metrics isn't updated when the call throws an exception. We can either update the existing metrics or have a new set of metrics in the case of exception.,Closed,Fixed,,Ming Ma,Ming Ma,Mon; 9 Jun 2014 18:01:13 +0000,Mon; 1 Dec 2014 03:07:25 +0000,Tue; 15 Jul 2014 23:15:40 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10673
HADOOP-10674,Improvement,Major,performance;util,Rewrite the PureJavaCrc32 loop for performance improvement,"Below are some performance improvement opportunities performance improvement in PureJavaCrc32.	eliminate ""off += 8; len -= 8;""	replace T8_x_start with hard coded constants	eliminate c0 - c7 local variablesIn my machine; there are 30% to 50% improvement for most of the cases.",Closed,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Mon; 9 Jun 2014 22:14:29 +0000,Mon; 13 Nov 2017 10:56:36 +0000,Wed; 25 Jun 2014 02:23:25 +0000,,,,,HADOOP-15033,https://issues.apache.org/jira/browse/HADOOP-10674
HADOOP-10675,Improvement,Major,fs/s3,Add server-side encryption functionality to s3a,The current patch for s3a in HADOOP-10400 does not have the capability to specify server-side encryption.  This JIRA will track the addition of such functionality to HADOOP-10400; similar to what was done in HADOOP-10568 for s3n.,Closed,Fixed,,David S. Wang,David S. Wang,Tue; 10 Jun 2014 13:30:48 +0000,Mon; 1 Dec 2014 03:08:42 +0000,Tue; 16 Sep 2014 00:48:16 +0000,,2.4.0,,HADOOP-10400,,https://issues.apache.org/jira/browse/HADOOP-10675
HADOOP-10676,Bug,Major,fs/s3,S3AOutputStream not reading new config knobs for multipart configs,S3AOutputStream.java does not have the code to read the new config knobs for multipart configs.  This patch will add that.,Closed,Fixed,,David S. Wang,David S. Wang,Tue; 10 Jun 2014 15:53:21 +0000,Mon; 1 Dec 2014 03:10:06 +0000,Tue; 16 Sep 2014 00:49:10 +0000,,2.4.0,,HADOOP-10400,,https://issues.apache.org/jira/browse/HADOOP-10676
HADOOP-10677,Bug,Major,fs/s3,ExportSnapshot fails on kerberized cluster using s3a,"When using HBase ExportSnapshot on a kerberized cluster; exporting to s3a using HADOOP-10400; we see the following problem:Caused by: java.lang.IllegalArgumentException: java.net.UnknownHostException: patch283two	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:414)The problem seems to be that the patch in HADOOP-10400 does not have getCanonicalServiceName().",Closed,Fixed,,David S. Wang,David S. Wang,Tue; 10 Jun 2014 16:01:45 +0000,Mon; 1 Dec 2014 03:11:57 +0000,Tue; 16 Sep 2014 00:49:46 +0000,,2.4.0,,HADOOP-10400,,https://issues.apache.org/jira/browse/HADOOP-10677
HADOOP-10678,Bug,Minor,security,SecurityUtil has unnecessary synchronization on collection used for only tests,The function SecurityUtil.getKerberosInfo()  is a function used during authentication and authorization. It has two synchronized blocks and one of them is on testProviders. This is an unnecessary lock given that the testProviders is empty in real scenario.,Closed,Fixed,,Benoy Antony,Benoy Antony,Tue; 10 Jun 2014 18:48:43 +0000,Thu; 12 May 2016 18:22:31 +0000,Fri; 13 Jun 2014 16:53:19 +0000,,2.4.0;3.0.0-alpha1,,,MAPREDUCE-2746,https://issues.apache.org/jira/browse/HADOOP-10678
HADOOP-10679,New Feature,Major,security,Authorize webui access using ServiceAuthorizationManager,Currently accessing Hadoop via RPC can be authorized using ServiceAuthorizationManager. But there is no uniform authorization of the HTTP access. Some of the servlets check for admin privilege. This creates an inconsistency of authorization between access via RPC vs HTTP. The fix is to enable authorization of the webui access also using ServiceAuthorizationManager.,Patch Available,Unresolved,,Benoy Antony,Benoy Antony,Tue; 10 Jun 2014 22:02:21 +0000,Thu; 2 Mar 2017 20:42:02 +0000,,,,BB2015-05-TBR,,YARN-6254;HADOOP-10671,https://issues.apache.org/jira/browse/HADOOP-10679
HADOOP-10680,Improvement,Major,documentation,Document metrics included in MXBean,Enhancement of HADOOP-6350.For example; SnapshotInfo metrics in NameNode; DataNodeInfo and FsDatasetState metrics in DataNode are not collected by MetricsSystem but registered to MXBean via MBeanServer; so these metrics can be seen by jmx/jconsole.These metrics should also be documented.,Open,Unresolved,,Unassigned,Akira Ajisaka,Wed; 11 Jun 2014 17:36:12 +0000,Wed; 11 Jun 2014 17:36:30 +0000,,,,,,HADOOP-6350,https://issues.apache.org/jira/browse/HADOOP-10680
HADOOP-10681,Bug,Major,performance,Remove synchronized blocks from SnappyCodec and ZlibCodec buffering inner loop,The current implementation of SnappyCompressor spends more time within the java loop of copying from the user buffer into the direct buffer allocated to the compressor impl; than the time it takes to compress the buffers.The bottleneck was found to be java monitor code inside SnappyCompressor.The methods are neatly inlined by the JIT into the parent caller (BlockCompressorStream::write); which unfortunately does not flatten out the synchronized blocks.The loop does a write of small byte[] buffers (each IFile key+value). I counted approximately 6 monitor enter/exit blocks per k-v pair written.,Closed,Fixed,HADOOP-10116,Gopal V,Gopal V,Wed; 11 Jun 2014 17:41:42 +0000,Thu; 8 Sep 2016 06:49:18 +0000,Sun; 5 Oct 2014 14:49:19 +0000,,2.2.0;2.4.0;2.5.0,perfomance,,,https://issues.apache.org/jira/browse/HADOOP-10681
HADOOP-10682,Bug,Major,metrics,Metrics are not output in trunk,"Metrics are not output in trunk by the following configuration: The below change worked well. It means that an old configuration doesn't work on trunk. We should fix it or document to use ""NameNode"".",Resolved,Not A Problem,,Unassigned,Akira Ajisaka,Wed; 11 Jun 2014 18:54:14 +0000,Mon; 8 Aug 2016 19:25:44 +0000,Fri; 13 Jun 2014 01:50:32 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10682
HADOOP-10683,Bug,Major,security,Users authenticated with KERBEROS are recorded as being authenticated with SIMPLE,We have enabled kerberos authentication in our clusters; but we see the following in the log files 2014-06-11 11:07:05;903 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for X@Y.COM (auth:SIMPLE)2014-06-11 11:07:05;914 INFO SecurityLogger.org.apache.hadoop.security.authorize.ServiceAuthorizationManager: Authorization successful for X@Y.COM (auth:KERBEROS) for protocol=interface This is quite confusing for administrators.,Closed,Fixed,,Benoy Antony,Benoy Antony,Wed; 11 Jun 2014 19:36:58 +0000,Thu; 12 May 2016 18:23:31 +0000,Mon; 16 Jun 2014 20:32:25 +0000,,2.4.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10683
HADOOP-10684,Improvement,Minor,ha,Extend HA support for more use cases,"We'd like the current HA framework to be more configurable from a behavior standpoint.  In particular:	Add the ability for a HAServiceTarget to survive a configurable number of health check failures (default of 0) before HealthMonitor (HM) reports service not responding or service unhealthy.  For instance; predicate the HM on a state machine whose default implementation can be overridden by method or constructor argument.  The default would behave the same as today.			If a target fails a health check but does not exceed the maximum number of consecutive check failures; it d be desirable if the target and/or controller were alerted.					i.e. Introduce a SERVICE_DYING state--Additionally; it d be desirable if a mechanism existed; similar to fencing semantics; for  reviving  a service that transitioned to SERVICE_DYING.			i.e. attemptRevive( )							Add the ability to allow a service to completely fail (no failover or failback possible).  There are scenarios where allowing a failover or failback could cause more damage.			E.g. a recovered master with stale data.  The master may have been manually recovered (human error).			Add affinity to a particular HAServiceTarget.			In other words; allow the controller to prefer one target over another when deciding leadership.		If a higher affinity; but previously unhealthy target; becomes healthy then it should be allowed to become the leader.		Likewise; if two targets are racing for a ZooKeeper lock; then the controller should ""prefer"" the higher the affinity target.		It might make more sense to add a different implementation/subclass of the ZKFailoverController (i.e. ZKAffinityFailoverController) than modify current behavior.		Please comment with thoughts/ideas/etc...Thanks.",Open,Unresolved,,Unassigned,Paul Rubio,Wed; 11 Jun 2014 21:53:02 +0000,Wed; 11 Jun 2014 21:53:02 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10684
HADOOP-10685,Improvement,Minor,,Migrate Standalone Refresh Protocols to the GenericRefreshProto,Now that we have a GenericRefreshProtocol; we should migrate existing protocols towards it.First; we will duplicate the functionality. If all goes well we can mark the old methods as deprecated; and remove them later.,Open,Unresolved,,Chris Li,Chris Li,Thu; 12 Jun 2014 01:55:34 +0000,Thu; 12 Jun 2014 02:19:01 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10685
HADOOP-10686,Bug,Major,,Writables are not always configured,Seeing the following exception: It turns out that WritableComparator does not configure Writable objects :https://github.com/apache/hadoop-common/blob/branch-2.3.0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableComparator.java. This is during the sort phase for an MR job.,Closed,Fixed,,Abraham Elmahrek,Abraham Elmahrek,Thu; 5 Jun 2014 17:45:02 +0000,Fri; 15 Aug 2014 05:39:45 +0000,Tue; 29 Jul 2014 23:16:04 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10686
HADOOP-10687,Improvement,Major,conf,improve Configuration.CACHE_CLASSES concurrentcy,"In one of my high concurrent HBase in-memory read only testing scenario; the following strace traces show CACHE_CLASSES becoming hotspot; since there's a ""synchronized"" or ""synchronizedMap""; but for most of hadoop applications; when goes into getClassByNameOrNull(); most of code paths should expect there's a cached class already; that means it's a read mostly scenario; the write should be rare enough. So one possible optimization here is using concurrent weakHashMap; the MapMaker could do this.ps: i added more debug logging above ""cache hit"" and ""Class.forName"" and reran my case; shows an expected result (almost all of them go to ""cache hit"" path); and TestConfiguration also passed in my box.",Patch Available,Unresolved,,Liang Xie,Liang Xie,Thu; 12 Jun 2014 09:04:26 +0000,Thu; 12 May 2016 18:23:04 +0000,,,2.4.0;3.0.0-alpha1,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10687
HADOOP-10688,Improvement,Major,fs,Expose thread-level FileSystem StatisticsData,FileSystems collect data on the bytes read and written by each thread.  It would be helpful for it to make this data public.,Closed,Fixed,,Sandy Ryza,Sandy Ryza,Thu; 12 Jun 2014 20:34:11 +0000,Fri; 15 Aug 2014 05:39:20 +0000,Fri; 13 Jun 2014 20:57:59 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10688
HADOOP-10689,Bug,Minor,tools,InputStream is not closed in AzureNativeFileSystemStore#retrieve(),In the catch block: We check against in but try to close inDataStream which should have been closed by the if statement above.,Closed,Fixed,,Chen He,Ted Yu,Thu; 12 Jun 2014 21:52:42 +0000,Fri; 10 Apr 2015 20:04:37 +0000,Thu; 18 Dec 2014 00:18:24 +0000,,,,,HADOOP-9629,https://issues.apache.org/jira/browse/HADOOP-10689
HADOOP-10690,Bug,Minor,tools,Lack of synchronization on access to InputStream in NativeAzureFileSystem#NativeAzureFsInputStream#close(),The close() method should be protected by synchronized keyword.,Closed,Fixed,,Chen He,Ted Yu,Thu; 12 Jun 2014 21:56:27 +0000,Fri; 10 Apr 2015 20:04:53 +0000,Thu; 18 Dec 2014 00:18:31 +0000,,,,,HADOOP-9629,https://issues.apache.org/jira/browse/HADOOP-10690
HADOOP-10691,Improvement,Minor,tools,Improve the readability of 'hadoop fs -help','hadoop fs -help` displays help informations with numbers of different formats. This patch borrows the format used in `hdfs cacheadmin -help`: all options are formatted by using org.apache.hadoop.tools.TableListing.,Closed,Fixed,,Lei (Eddy) Xu,Lei (Eddy) Xu,Thu; 12 Jun 2014 04:06:18 +0000,Fri; 15 Aug 2014 05:39:26 +0000,Fri; 13 Jun 2014 06:45:29 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10691
HADOOP-10692,Bug,Major,conf;metrics,Update metrics2 document and examples to be case sensitive,After HADOOP-10468; the prefix of the properties in metrics2 become case sensitive. We should also update package-info and hadoop-metrics2.properties examples to be case sensitive.,Resolved,Invalid,,Unassigned,Akira Ajisaka,Fri; 13 Jun 2014 01:58:09 +0000,Thu; 17 Jul 2014 07:58:10 +0000,Thu; 17 Jul 2014 07:58:10 +0000,,2.5.0,newbie,,HADOOP-10468,https://issues.apache.org/jira/browse/HADOOP-10692
HADOOP-10693,Sub-task,Major,security,Implementation of AES-CTR CryptoCodec using JNI to OpenSSL,In HADOOP-10603; we have an implementation of AES-CTR CryptoCodec using Java JCE provider. To get high performance; the configured JCE provider should utilize native code and AES-NI; but in JDK6;7 the Java embedded provider doesn't support it.Considering not all hadoop user will use the provider like Diceros or able to get signed certificate from oracle to develop a custom provider; so this JIRA will have an implementation of AES-CTR CryptoCodec using JNI to OpenSSL directly.,Resolved,Fixed,,Yi Liu,Yi Liu,Fri; 13 Jun 2014 08:37:23 +0000,Thu; 3 Jul 2014 23:40:54 +0000,Thu; 3 Jul 2014 23:40:54 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10693
HADOOP-10694,Bug,Major,io,Remove synchronized input streams from Writable deserialization,Writable deserialization is slowing down due to a synchronized block within DataInputBuffer$Buffer.ByteArrayInputStream::read() is synchronized and this shows up as a slow uncontested lock.Hive ships with its own faster thread-unsafe version with hive.common.io.NonSyncByteArrayInputStream.The DataInputBuffer and Writable deserialization should not require a lock per readInt()/read().,Resolved,Fixed,,Gopal V,Gopal V,Fri; 13 Jun 2014 19:14:33 +0000,Thu; 12 May 2016 18:25:29 +0000,Wed; 11 May 2016 00:29:50 +0000,,,BB2015-05-TBR,,TEZ-2605,https://issues.apache.org/jira/browse/HADOOP-10694
HADOOP-10695,Improvement,Major,,KMSClientProvider should respect a configurable timeout.,It'd be good if KMSClientProvider used a timeout; so it doesn't hang forever if the KMServer is down.,Closed,Fixed,HADOOP-13318,Mike Yoder,Andrew Wang,Fri; 13 Jun 2014 22:41:13 +0000,Mon; 3 Oct 2016 04:44:11 +0000,Mon; 30 Jun 2014 20:55:20 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10695
HADOOP-10696,Improvement,Major,security,Add optional attributes to KeyProvider Options and Metadata,In addition to having an optional description; KeyProvider Options and Metadata should support optional key value pairs to help categorize keys.This would be useful for visualization purposes.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Fri; 13 Jun 2014 22:54:24 +0000,Thu; 12 May 2016 18:25:34 +0000,Thu; 19 Jun 2014 22:18:17 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10696
HADOOP-10697,Improvement,Major,security,KMS DelegationToken support,Add DelegationToken support to KMS as per discussion in HDFS-6134.,Resolved,Duplicate,HDFS-6134,Alejandro Abdelnur,Alejandro Abdelnur,Sat; 14 Jun 2014 00:13:47 +0000,Thu; 12 May 2016 18:25:14 +0000,Wed; 20 Aug 2014 20:11:05 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10697
HADOOP-10698,Improvement,Major,security,KMS; add proxyuser support,Add proxyuser support to KMS as per discussion in HDFS-6134.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Sat; 14 Jun 2014 00:14:38 +0000,Thu; 12 May 2016 18:25:12 +0000,Fri; 15 Aug 2014 15:53:54 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10698
HADOOP-10699,Bug,Major,,Fix build native library on mac osx,Some patches for fixing build a hadoop native library on os x 10.7/10.8.,Closed,Fixed,HADOOP-9963;HADOOP-7147;HADOOP-10194;HADOOP-10358,Binglin Chang,Kirill A. Korinskiy,Sun; 16 Jun 2013 13:26:03 +0000,Fri; 13 Feb 2015 20:31:52 +0000,Tue; 17 Jun 2014 02:25:23 +0000,,,,,HADOOP-7147;YARN-2161;HDFS-6534,https://issues.apache.org/jira/browse/HADOOP-10699
HDFS-6534,Bug,Minor,,Fix build on macosx: HDFS parts,When compiling native code on macosx using clang; compiler find more warning and errors which gcc ignores; those should be fixed.,Closed,Fixed,HDFS-7136,Binglin Chang,Binglin Chang,Sat; 14 Jun 2014 16:24:54 +0000,Mon; 1 Dec 2014 03:09:38 +0000,Wed; 24 Sep 2014 15:23:15 +0000,,,,,HADOOP-10699,https://issues.apache.org/jira/browse/HDFS-6534
HADOOP-10701,Bug,Major,nfs,NFS should not validate the access premission only based on the user's primary group,The bug is while accessing NFS Mounted File System the permission is always validated based on the primary Unix group the user is associated with and Secondary Unix groups are ignored.,Closed,Fixed,,Harsh J,Premchandra Preetham Kukillaya,Sun; 15 Jun 2014 07:50:14 +0000,Fri; 15 Aug 2014 05:39:32 +0000,Fri; 27 Jun 2014 12:03:51 +0000,,2.3.0,,,HADOOP-10761,https://issues.apache.org/jira/browse/HADOOP-10701
HADOOP-10702,Bug,Minor,security,KerberosAuthenticationHandler does not log the principal names correctly,With HADOOP-10158; it is possible to load multiple principal names or all HTTP principals in the key tab by specifying  * .Each principal name is logged when when the principal is loaded from key tab. But there is a bug due to which principal name is logged each time  as either  *  or the full list of principals.The log snippet is as below:2014-06-15 00:19:13;288 INFO org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler: Login using keytab /etc/hadoop/hadoop.keytab; for principal *2014-06-15 00:19:13;292 INFO org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler: Login using keytab /etc/hadoop/hadoop.keytab; for principal *2014-06-15 00:19:13;294 INFO org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler: Login using keytab /etc/hadoop/hadoop.keytab; for principal *2014-06-15 00:19:13;295 INFO org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler: Login using keytab /etc/hadoop/hadoop.keytab; for principal *,Closed,Fixed,,Benoy Antony,Benoy Antony,Sun; 15 Jun 2014 08:44:37 +0000,Wed; 3 Sep 2014 20:36:27 +0000,Mon; 16 Jun 2014 23:12:09 +0000,,2.5.0,,,HADOOP-10158,https://issues.apache.org/jira/browse/HADOOP-10702
HADOOP-10703,Bug,Major,security,HttpServer2 creates multiple authentication filters,The HttpServer2.defineFilter creates a Filter instance for each context. By default; there are 3 contexts.So there will be 3 separate AuthenticationFilter instances and corresponding AuthenticationHandler instances. This also results in 3 separate initializations of AuthenticationHandler.The log file illustrating this repeated initialization is attached.,Closed,Fixed,,Benoy Antony,Benoy Antony,Sun; 15 Jun 2014 08:55:12 +0000,Fri; 24 Apr 2015 22:49:18 +0000,Thu; 19 Mar 2015 05:33:58 +0000,,2.4.0,,,HADOOP-11728,https://issues.apache.org/jira/browse/HADOOP-10703
HADOOP-10704,Bug,Minor,fs,Corrupt Files are not moved to bad_files in LocalFileSystem,"A file is created using LocalFileSystem. When the file is corrupted/changed and tried to read; it throws ChecksumException as the CRC does not match. But this process does not move the affected file to ""bad_files"" directory.Is this expected behavior ?  I Checked in FSInputChecker. At line 286; the number of retries are checked if 0. For LocalFileSystem there will be only 1 try as there is no replication. But the move of corrupted to bad_files file happens after line 286. Thus leaving the corrupted file in the same path.What should be the expected behavior ?I tried the below test case :",Open,Unresolved,,Unassigned,Suraj Nayak,Sun; 15 Jun 2014 20:38:00 +0000,Fri; 20 Jun 2014 15:47:48 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10704
HADOOP-10705,Sub-task,Major,,Fix namenode-rpc-unit  warning reported by memory leak check tool(valgrind),Rum valgrind to check memory leak for namenode-rpc.unit.There are many warning; need to fix it.valgrind --tool=memcheck --leak-check=full --show-reachable=yes ./namenode-rpc-unit ==24085== ==24085== HEAP SUMMARY:==24085==     in use at exit: 1;216 bytes in 20 blocks==24085==   total heap usage: 89 allocs; 69 frees; 12;222 bytes allocated==24085== ==24085== 128 bytes in 1 blocks are definitely lost in loss record 18 of 20==24085==    at 0x4A069EE: malloc (vg_replace_malloc.c:270)==24085==    by 0x4A06B62: realloc (vg_replace_malloc.c:662)==24085==    by 0x4C2BD6A: uv__io_start (core.c:679)==24085==    by 0x4C31868: uv_signal_init (signal.c:225)==24085==    by 0x4C3000E: uv__loop_init (loop.c:136)==24085==    by 0x4275EF: hrpc_reactor_create (reactor.c:183)==24085==    by 0x426829: hrpc_messenger_create (messenger.c:83)==24085==    by 0x42590F: main (namenode-rpc-unit.c:115)==24085== ==24085== LEAK SUMMARY:==24085==    definitely lost: 128 bytes in 1 blocks==24085==    indirectly lost: 0 bytes in 0 blocks==24085==      possibly lost: 0 bytes in 0 blocks==24085==    still reachable: 1;088 bytes in 19 blocks==24085==         suppressed: 0 bytes in 0 blocks==24085== Reachable blocks (those to which a pointer was found) are not shown.==24085== To see them; rerun with: --leak-check=full --show-reachable=yes==24085== ==24085== For counts of detected and suppressed errors; rerun with: -v==24085== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 6 from 6),Resolved,Duplicate,HADOOP-10667,Wenwu Peng,Wenwu Peng,Mon; 16 Jun 2014 07:36:16 +0000,Fri; 20 Jun 2014 07:04:52 +0000,Fri; 20 Jun 2014 07:04:52 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10705
HADOOP-10706,Sub-task,Major,,Fix initialization of hrpc_sync_ctx,1.  Doing this will alway make return value to 02.hrpc_release_sync_ctx should changed to hrpc_proxy_release_sync_ctx; all the functions in this .h/.c file follow this rule,Resolved,Fixed,,Binglin Chang,Binglin Chang,Mon; 16 Jun 2014 09:11:53 +0000,Tue; 17 Jun 2014 04:58:37 +0000,Tue; 17 Jun 2014 04:58:37 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10706
HADOOP-10707,Improvement,Minor,tools,support bzip2 in python avro tool,The Python tool to decode avro files is currently missing support for bzip2 compression.,Resolved,Invalid,,Unassigned,Eustache,Mon; 16 Jun 2014 13:44:50 +0000,Wed; 18 Jun 2014 06:09:52 +0000,Tue; 17 Jun 2014 16:17:23 +0000,,,avro,,,https://issues.apache.org/jira/browse/HADOOP-10707
HADOOP-10708,Improvement,Minor,tools,support bzip2 in python avro tool,The Python tool to decode avro files is currently missing support for bzip2 compression.,Resolved,Invalid,,Unassigned,Eustache,Mon; 16 Jun 2014 13:45:04 +0000,Mon; 16 Jun 2014 13:46:04 +0000,Mon; 16 Jun 2014 13:46:04 +0000,,,avro,,,https://issues.apache.org/jira/browse/HADOOP-10708
HADOOP-10709,Improvement,Major,security,Reuse Filters across web apps,Currently; we need to define separate authentication filters for webhdfs and general webui. This also involves defining parameters for those filters.It will be better if one could reuse filters for web apps if desired.,Patch Available,Unresolved,,Benoy Antony,Benoy Antony,Mon; 16 Jun 2014 20:46:03 +0000,Thu; 19 Nov 2015 00:29:33 +0000,,,,,,HADOOP-10307;HADOOP-10671,https://issues.apache.org/jira/browse/HADOOP-10709
HADOOP-10710,Bug,Major,security,hadoop.auth cookie is not properly constructed according to RFC2109,It seems that HADOOP-10379 introduced a bug on how hadoop.auth cookies are being constructed.Before HADOOP-10379; cookies were constructed using Servlet's Cookie class and corresponding HttpServletResponse methods. This was taking care of setting attributes like 'Version=1' and double-quoting the cookie value if necessary.HADOOP-10379 changed the Cookie creation to use a StringBuillder and setting values and attributes by hand. This is not taking care of setting required attributes like Version and escaping the cookie value.While this is not breaking HadoopAuth AuthenticatedURL access; it is breaking access done using HtttpClient. I.e. Solr uses HttpClient and its access is broken since this change.It seems that HADOOP-10379 main objective was to set the 'secure' attribute. Note this can be done using the Cookie API.We should revert the cookie creation logic to use the Cookie API and take care of the security flag via setSecure(boolean).,Closed,Fixed,,Juan Yu,Alejandro Abdelnur,Mon; 16 Jun 2014 21:26:16 +0000,Wed; 29 Oct 2014 19:37:44 +0000,Mon; 30 Jun 2014 20:42:49 +0000,,2.4.0,,,HADOOP-10911,https://issues.apache.org/jira/browse/HADOOP-10710
HADOOP-10711,Bug,Major,security,Cleanup some extra dependencies from hadoop-auth,HADOOP-10322 added apacheds-kerberos-codec as a dependency; which brought in some additional dependencies.   It looks like we don't need most of them.,Closed,Fixed,,Robert Kanter,Robert Kanter,Mon; 16 Jun 2014 23:49:54 +0000,Fri; 15 Aug 2014 05:39:30 +0000,Wed; 18 Jun 2014 21:50:52 +0000,,2.5.0,,,HADOOP-10322,https://issues.apache.org/jira/browse/HADOOP-10711
HDFS-6549,Bug,Major,nfs,Add support for accessing the NFS gateway from the AIX NFS client,"We've identified two issues when trying to access the HDFS NFS Gateway from an AIX NFS client:	In the case of COMMITs; the AIX NFS client will always send 4096; or a multiple of the page size; for the offset to be committed; even if fewer bytes than this have ever; or will ever; be written to the file. This will cause a write to a file from the AIX NFS client to hang on close unless the size of that file is a multiple of 4096.	In the case of READDIR and READDIRPLUS; the AIX NFS client will send the same cookie verifier for a given directory seemingly forever after that directory is first accessed over NFS; instead of getting a new cookie verifier for every set of incremental readdir calls. This means that if a directory's mtime ever changes; the FS must be unmounted/remounted before readdir calls on that dir from AIX will ever succeed again.From my interpretation of RFC-1813; the NFS Gateway is in fact doing the correct thing in both cases; but we can introduce simple changes on the NFS Gateway side to be able to optionally work around these incompatibilities.",Closed,Fixed,,Aaron T. Myers,Aaron T. Myers,Tue; 17 Jun 2014 01:07:36 +0000,Mon; 4 Apr 2016 10:45:44 +0000,Thu; 19 Jun 2014 19:42:07 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HDFS-6549
HADOOP-10713,Sub-task,Trivial,security,Refactor CryptoCodec#generateSecureRandom to take a byte[],Following suit with the Java Random implementations; it'd be better if we switched CryptoCodec#generateSecureRandom to take a byte[] for parity.Also; let's document that this method needs to be thread-safe; which is an important consideration for CryptoCodec implementations.,Resolved,Fixed,,Andrew Wang,Andrew Wang,Tue; 17 Jun 2014 05:00:29 +0000,Sun; 22 Jun 2014 07:21:20 +0000,Sun; 22 Jun 2014 07:21:20 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10713
HADOOP-10714,Bug,Critical,fs/s3,AmazonS3Client.deleteObjects() need to be limited to 1000 entries per call,"In the patch for HADOOP-10400; calls to AmazonS3Client.deleteObjects() need to have the number of entries at 1000 or below. Otherwise we get a Malformed XML error similar to:com.amazonaws.services.s3.model.AmazonS3Exception: Status Code: 400; AWS Service: Amazon S3; AWS Request ID: 6626AD56A3C76F5B; AWS Error Code: MalformedXML; AWS Error Message: The XML you provided was not well-formed or did not validate against our published schema; S3 Extended Request ID: DOt6C+Y84mGSoDuaQTCo33893VaoKGEVC3y1k2zFIQRm+AJkFH2mTyrDgnykSL+vat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:798)at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:421)at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3480)at com.amazonaws.services.s3.AmazonS3Client.deleteObjects(AmazonS3Client.java:1739)at org.apache.hadoop.fs.s3a.S3AFileSystem.rename(S3AFileSystem.java:388)at org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(ExportSnapshot.java:829)at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)at org.apache.hadoop.hbase.snapshot.ExportSnapshot.innerMain(ExportSnapshot.java:874)at org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(ExportSnapshot.java:878)Note that this is mentioned in the AWS documentation:http://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html""The Multi-Object Delete request contains a list of up to 1000 keys that you want to delete. In the XML; you provide the object key names; and optionally; version IDs if you want to delete a specific version of the object from a versioning-enabled bucket. For each key; Amazon S3 . Thanks to Matteo Bertozzi and Rahul Bhartia from AWS for identifying the problem.",Closed,Fixed,,Juan Yu,David S. Wang,Tue; 17 Jun 2014 13:32:27 +0000,Thu; 21 Jul 2016 22:25:43 +0000,Thu; 6 Nov 2014 01:28:08 +0000,,2.5.0,s3,,HADOOP-11128;HADOOP-13402,https://issues.apache.org/jira/browse/HADOOP-10714
HADOOP-10715,Task,Minor,,Remove public GraphiteSink#setWriter(),During review of HADOOP-10660; Ravi brought up the notion of making GraphiteSink#setWriter() private.This JIRA is to address Ravi's comment.,Closed,Fixed,,Unassigned,Ted Yu,Tue; 17 Jun 2014 14:35:16 +0000,Fri; 15 Aug 2014 05:39:46 +0000,Thu; 26 Jun 2014 01:59:59 +0000,,2.5.0,,,HADOOP-10660,https://issues.apache.org/jira/browse/HADOOP-10715
HADOOP-10716,Bug,Critical,conf;fs,Cannot use more than 1 har filesystem,"Filesystems are cached purely on scheme + authority.  Har filesystems actually need further differentiation based on path to the har file itself.  For this reason; the fs cache used to be explicitly disable for har via ""fs.har.impl.cache.disable"" in core-default.xml.",Closed,Fixed,,Rushabh S Shah,Daryn Sharp,Tue; 17 Jun 2014 21:35:53 +0000,Thu; 12 May 2016 18:27:09 +0000,Wed; 18 Jun 2014 23:32:14 +0000,,2.0.0-alpha;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10716
HADOOP-10717,Bug,Blocker,,HttpServer2 should load jsp DTD from local jars instead of going remote,When user want to start NameNode; user would got the following exception; it is caused by missing org.mortbay.jetty:jsp-2.1-jetty:jar:6.1.26 in the pom.xml14/06/18 14:55:30 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)14/06/18 14:55:30 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs14/06/18 14:55:30 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static14/06/18 14:55:30 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs14/06/18 14:55:30 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)14/06/18 14:55:30 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources; pathSpec=/webhdfs/v1/*14/06/18 14:55:30 INFO http.HttpServer2: Jetty bound to port 5007014/06/18 14:55:30 INFO mortbay.log: jetty-6.1.2614/06/18 14:55:30 INFO mortbay.log: NO JSP Support for /; did not find org.apache.jasper.servlet.JspServlet14/06/18 14:57:38 WARN mortbay.log: EXCEPTIONjava.net.ConnectException: Connection timed out        at java.net.PlainSocketImpl.socketConnect(Native Method)        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)        at java.net.Socket.connect(Socket.java:529)        at java.net.Socket.connect(Socket.java:478)        at sun.net.NetworkClient.doConnect(NetworkClient.java:163)        at sun.net.www.http.HttpClient.openServer(HttpClient.java:395)        at sun.net.www.http.HttpClient.openServer(HttpClient.java:530)        at sun.net.www.http.HttpClient.init(HttpClient.java:234)        at sun.net.www.http.HttpClient.New(HttpClient.java:307)        at sun.net.www.http.HttpClient.New(HttpClient.java:324)        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:970)        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:911)        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:836)        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1172)        at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:677)        at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.startEntity(XMLEntityManager.java:1315)        at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.startDTDEntity(XMLEntityManager.java:1282)        at com.sun.org.apache.xerces.internal.impl.XMLDTDScannerImpl.setInputSource(XMLDTDScannerImpl.java:283)        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$DTDDriver.dispatch(XMLDocumentScannerImpl.java:1194)        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$DTDDriver.next(XMLDocumentScannerImpl.java:1090)        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:1003)        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:648)        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:140)        at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:511)        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:808)        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)        at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:119)        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)        at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:522)        at javax.xml.parsers.SAXParser.parse(SAXParser.java:395)        at org.mortbay.xml.XmlParser.parse(XmlParser.java:188)        at org.mortbay.xml.XmlParser.parse(XmlParser.java:204)        at org.mortbay.jetty.webapp.TagLibConfiguration.configureWebApp(TagLibConfiguration.java:238)        at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1279)        at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)        at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)        at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)        at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)        at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)        at org.mortbay.jetty.Server.doStart(Server.java:224)        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)        at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:142)        at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:690)        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:581)        at org.apache.hadoop.hdfs.server.namenode.NameNode.init(NameNode.java:748)        at org.apache.hadoop.hdfs.server.namenode.NameNode.init(NameNode.java:732)        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1386)        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1452),Closed,Fixed,,Dapeng Sun,Dapeng Sun,Wed; 18 Jun 2014 08:46:01 +0000,Fri; 24 Apr 2015 22:49:01 +0000,Tue; 24 Jun 2014 17:52:33 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10717
HADOOP-10718,Bug,Major,ipc,IOException: An existing connection was forcibly closed by the remote host frequently happens on Windows,After HADOOP-317; we still observed that on windows platform; there're a number of IOException: An existing connection was forcibly closed by the remote host when running a MR job. For example;  And the latter one results in the issue of MAPREDUCE-5924.,Resolved,Duplicate,NULL,Unassigned,Zhijie Shen,Wed; 18 Jun 2014 12:11:37 +0000,Thu; 19 Jun 2014 00:25:35 +0000,Thu; 19 Jun 2014 00:25:35 +0000,,,,,HADOOP-8980,https://issues.apache.org/jira/browse/HADOOP-10718
HADOOP-10719,New Feature,Major,security,Add generateEncryptedKey and decryptEncryptedKey methods to KeyProvider,"This is a follow up on HDFS-6134KeyProvider API should  have 2 new methods:	KeyVersion generateEncryptedKey(String keyVersionName; byte[] iv)	KeyVersion decryptEncryptedKey(String keyVersionName; byte[] iv; KeyVersion encryptedKey)The implementation would do a known transformation on the IV (i.e.: xor with 0xff the original IV).",Closed,Fixed,,Arun Suresh,Alejandro Abdelnur,Wed; 18 Jun 2014 18:08:34 +0000,Thu; 12 May 2016 18:21:45 +0000,Fri; 4 Jul 2014 19:41:31 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10719
HADOOP-10720,Improvement,Major,security,KMS: Implement generateEncryptedKey and decryptEncryptedKey in the REST API,KMS client/server should implement support for generating encrypted keys and decrypting them via the REST API being introduced by HADOOP-10719.,Closed,Fixed,,Arun Suresh,Alejandro Abdelnur,Wed; 18 Jun 2014 20:48:23 +0000,Wed; 25 May 2016 23:47:00 +0000,Mon; 21 Jul 2014 20:56:51 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10720
HADOOP-10721,Bug,Major,fs/swift,The result does not show up after running hive query on Swift.,I configured Hadoop and Swift system as the site is mentioned : http://docs.openstack.org/developer/sahara/userdoc/hadoop-swift.html.So; I succeeded to access the Swift from Hadoop.I am running TPC-H performance test on Hadoop system integrated with Swift.I ran the below hive query.---------------------------------------------------------------------------------------------DROP TABLE lineitem;DROP TABLE q1_pricing_summary_report; create tables and load dataCreate external table lineitem (L_ORDERKEY INT; L_PARTKEY INT; L_SUPPKEY INT; L_LINENUMBER INT; L_QUANTITY DOUBLE; L_EXTENDEDPRICE DOUBLE; L_DISCOUNT DOUBLE; L_TAX DOUBLE; L_RETURNFLAG STRING; L_LINESTATUS STRING; L_SHIPDATE STRING; L_COMMITDATE STRING; L_RECEIPTDATE STRING; L_SHIPINSTRUCT STRING; L_SHIPMODE STRING; L_COMMENT STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION 'swift://test.provider/tpch/lineitem'; create the target tableCREATE external TABLE q1_pricing_summary_report ( L_RETURNFLAG STRING; L_LINESTATUS STRING; SUM_QTY DOUBLE; SUM_BASE_PRICE DOUBLE; SUM_DISC_PRICE DOUBLE; SUM_CHARGE DOUBLE; AVE_QTY DOUBLE; AVE_PRICE DOUBLE; AVE_DISC DOUBLE; COUNT_ORDER INT) LOCATION 'swift://test.provider/user/result/q1_pricing_summary_report';set mapred.min.split.size=536870912; the queryINSERT OVERWRITE TABLE q1_pricing_summary_report SELECT   L_RETURNFLAG; L_LINESTATUS; SUM(L_QUANTITY); SUM(L_EXTENDEDPRICE); SUM(L_EXTENDEDPRICE*(1-L_DISCOUNT)); SUM(L_EXTENDEDPRICE*(1-L_DISCOUNT)*(1+L_TAX)); AVG(L_QUANTITY); AVG(L_EXTENDEDPRICE); AVG(L_DISCOUNT); COUNT(1) FROM   lineitem WHERE   L_SHIPDATE='1998-09-02' GROUP BY L_RETURNFLAG; L_LINESTATUS ORDER BY L_RETURNFLAG; L_LINESTATUS;---------------------------------------------------------------------------------------------You can get the files(such as lineitem) for the test through running dbgen which is in this site : http://www.tpc.org/tpch/.I saw the some temporary files are generated and deleted. However; the result does not show up after running hive query.,Open,Unresolved,,Unassigned,YongHun Jeon,Thu; 19 Jun 2014 02:42:47 +0000,Thu; 19 Jun 2014 02:46:39 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10721
HADOOP-10722,Bug,Major,auto-failover;ha,Standby NN continuing as standby when active NN machine got shutdown.,I have HA cluster with 3 ZK; 3 QJM.My Active NN machine got shutdown; but still my standby NN is standby only.It should be activeZKFC logs========,Resolved,Not A Problem,,Unassigned,Surendra Singh Lilhore,Thu; 19 Jun 2014 09:59:16 +0000,Thu; 19 Jun 2014 10:55:05 +0000,Thu; 19 Jun 2014 10:55:05 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10722
HADOOP-10723,Bug,Major,fs,FileSystem deprecated filesystem name warning : Make error message HCFS compliant,In the FileSystem.java class we've found that if we have an alternative filesystem (i.e. xyz://) and we enter it without slashes; we get an hdfs specific error message.  There are two problems with this.1) This is a conceptual bug because the hadoop-common FileSystem interface shouldn't have explicit knowledge/dependencies on any particular implementation and 2) This results in a misleading error message:hadoop fs -fs xyz: -mkdir -p /foo/bar Yields an error message which suggests an hdfs URI.   Would be better if the error message threw an Exception (as suggested in the comments) and was something that didnt hardcode the hdfs uri to the beggining; as its very confusing when running on any alternative filesystem.,Open,Unresolved,,Unassigned,jay vyas,Thu; 19 Jun 2014 13:13:59 +0000,Fri; 28 Nov 2014 14:55:55 +0000,,,,hcfs,,,https://issues.apache.org/jira/browse/HADOOP-10723
HADOOP-10724,Bug,Major,fs,`hadoop fs -du -h` incorrectly formatted,"hadoop fs -du -h prints sizes with a space between the number and the unit: The standard unix du -h does not: the result is that the output of du -h is properly sorted by sort -h while the output of hadoop fs -du -h is not properly sorted by it.Please see 	sort: ""-h --human-numeric-sort    compare human readable numbers (e.g.; 2K 1G) ""	du: ""-h; --human-readable    print sizes in human readable format (e.g.; 1K 234M 2G) """,Open,Unresolved,,Sam Steingold,Sam Steingold,Thu; 19 Jun 2014 15:22:44 +0000,Fri; 29 Sep 2017 23:16:44 +0000,,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10724
HADOOP-10725,Sub-task,Major,native,Implement listStatus and getFileInfo in the native client,Implement listStatus and getFileInfo in the native client.,Resolved,Fixed,,Colin P. McCabe,Colin P. McCabe,Thu; 19 Jun 2014 21:39:51 +0000,Tue; 12 Aug 2014 18:59:01 +0000,Tue; 12 Aug 2014 18:59:01 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10725
HADOOP-10726,Improvement,Minor,,Shell.ExitCodeException to implement getExitCode(),Once HADOOP-9626 adds an interface to get the exit code of an exception; this should be implemented by Shell.ExitCodeException to serve up its exit code.,Open,Unresolved,,Steve Loughran,Steve Loughran,Thu; 19 Jun 2014 22:27:24 +0000,Fri; 10 Apr 2015 19:24:09 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10726
HADOOP-10727,Improvement,Minor,,"GraphiteSink metric names should not contain "".""","Sometimes the names of the metrics sent to Graphite contain ""."" in them (such as hostnames). Graphite interpret these dots as a separator for directories causing long directory hierarchies. It would be easier to replace them to ""_"" in order to have easier to read metric names.",Open,Unresolved,,Unassigned,Babak Behzad,Fri; 20 Jun 2014 00:03:02 +0000,Wed; 9 Jul 2014 17:14:18 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10727
HADOOP-10728,New Feature,Major,tools,Metrics system for Windows Azure Storage Filesystem,Add a metrics2 source for the Windows Azure Filesystem driver that was introduced with HADOOP-9629.AzureFileSystemInstrumentation is the new MetricsSource.  AzureNativeFilesystemStore and NativeAzureFilesystem have been modified to record statistics and some machinery is added for the accumulation of 'rolling average' statistics.Primary new code appears in org.apache.hadoop.fs.azure.metrics namespace.Credits and historyCredit for this work goes to the early team: Min Wei; David Lao; Lengning Liu and Alexander Stojanovic as well as multiple people who have taken over this work since then (hope I don't forget anyone): Dexter Bradshaw; Johannes Klein; Ivan Mitic; Michael Rys; Mostafa Elhemali; Brian Swan; ~mikelid; Xi Fang; and Chuan Liu.,Closed,Fixed,,Mike Liddell,Mike Liddell,Fri; 20 Jun 2014 01:18:18 +0000,Tue; 12 Sep 2017 16:01:05 +0000,Thu; 18 Dec 2014 00:18:38 +0000,,,,,HADOOP-9629;HADOOP-14862,https://issues.apache.org/jira/browse/HADOOP-10728
HADOOP-10729,Test,Major,ipc,Add tests for PB RPC in case version mismatch of client and server,We have ProtocolInfo specified in protocol interface with version info; but we don't have unit test to verify if/how it works. We should have tests to track this annotation work as expectation.,Resolved,Fixed,,Junping Du,Junping Du,Fri; 20 Jun 2014 01:56:56 +0000,Tue; 30 Aug 2016 01:32:20 +0000,Tue; 8 Dec 2015 21:45:09 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10729
HADOOP-10730,Bug,Critical,security,Login with kerberos should always set refreshKrb5Config to true,"When logined with kerberos by ""kinit"" way; it never refreshes the kerberos configurations. It is really often needed to be refreshed same as ""keytab"" way(The bug of ""keytab"" has been fixed by HADOOP-6947).",Open,Unresolved,,Unassigned,ycbai,Fri; 20 Jun 2014 10:11:54 +0000,Tue; 8 Jul 2014 07:58:59 +0000,,,,,,HADOOP-6947,https://issues.apache.org/jira/browse/HADOOP-10730
HADOOP-10731,Improvement,Trivial,documentation,Remove @date JavaDoc comment in ProgramDriver class,Remove JavaDoc @date in the ProgramDriver class for consistency.,Closed,Fixed,,Henry Saputra,Henry Saputra,Fri; 20 Jun 2014 23:20:31 +0000,Mon; 1 Dec 2014 03:08:12 +0000,Fri; 26 Sep 2014 21:42:19 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10731
HADOOP-10732,Bug,Minor,,Update without holding write lock in JavaKeyStoreProvider#innerSetCredential(),In hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/JavaKeyStoreProvider.java;innerSetCredential() doesn't wrap update with writeLock.lock() / writeLock.unlock().,Closed,Fixed,,Ted Yu,Ted Yu,Sat; 21 Jun 2014 03:40:47 +0000,Mon; 1 Dec 2014 03:10:06 +0000,Thu; 17 Jul 2014 17:48:45 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10732
HADOOP-10733,Bug,Minor,,Potential null dereference in CredentialShell#promptForCredential(),newPassword1 might be null; leading to NullPointerException in Arrays.fill() call.Similar issue for the following call on line 381:,Closed,Fixed,,Ted Yu,Ted Yu,Sat; 21 Jun 2014 03:45:53 +0000,Mon; 1 Dec 2014 03:11:37 +0000,Thu; 17 Jul 2014 18:13:41 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10733
HADOOP-10734,Sub-task,Major,security,Implement high-performance secure random number sources,This JIRA is to implement Secure random using JNI to OpenSSL; and implementation should be thread-safe.Utilize RdRand to return random numbers from hardware random number generator. It's TRNG(True Random Number generators) having much higher performance than java.security.SecureRandom. https://wiki.openssl.org/index.php/Random_Numbershttp://en.wikipedia.org/wiki/RdRandhttps://software.intel.com/en-us/articles/performance-impact-of-intel-secure-key-on-openssl,Resolved,Fixed,,Yi Liu,Yi Liu,Sun; 22 Jun 2014 09:32:08 +0000,Sat; 12 Jul 2014 01:29:41 +0000,Sat; 12 Jul 2014 01:29:41 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10734
HADOOP-10735,Sub-task,Major,security,Fall back AesCtrCryptoCodec implementation from OpenSSL to JCE if non native support.,If there is no native support or OpenSSL version is too low not supporting AES-CTR; but OpensslAesCtrCryptoCodec is configured; we need to fall back it to JCE implementation.,Resolved,Fixed,HADOOP-10785,Yi Liu,Yi Liu,Sun; 22 Jun 2014 09:42:46 +0000,Wed; 16 Jul 2014 18:05:04 +0000,Wed; 16 Jul 2014 00:36:58 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10735
HADOOP-10736,Improvement,Major,security,Add key attributes to the key shell,The recent work in HADOOP-10696 added attribute-value pairs to keys in the KMS.  Now the key shell needs to be updated to set/get these attributes.,Closed,Fixed,,Mike Yoder,Mike Yoder,Mon; 23 Jun 2014 17:03:45 +0000,Thu; 12 May 2016 18:23:46 +0000,Sat; 12 Jul 2014 00:24:19 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10736
HADOOP-10737,Bug,Major,fs/s3,S3n silent failure on copy; data loss on rename,"Jets3tNativeFileSystemStore.copy(String; String) handles its exceptions with handleServiceException(String; ServiceException); which behaves like:1) Throw FileNotFoundException if the exception's error code is NoSuchKey2) Otherwise; throw IOException if the exception's cause is an IOException3) Otherwise; LOG.debug a message and throw nothingSo S3 exceptions other than NoSuchKey (like RequestTimeout; ServiceUnavailable) are suppressed. This makes ""copy"" fail while still returning as if it succeeded. Furthermore since NativeS3FileSystem's ""rename"" is implemented as a copy followed by a delete; this means ""rename"" can delete the source key even though the copy has failed.",Closed,Fixed,,Steve Loughran,Gian Merlino,Mon; 23 Jun 2014 17:19:02 +0000,Tue; 30 Jun 2015 07:22:38 +0000,Tue; 10 Feb 2015 09:09:22 +0000,,2.4.0,,,HADOOP-10589,https://issues.apache.org/jira/browse/HADOOP-10737
HADOOP-10738,Bug,Major,,Dynamically adjust distcp configuration by adding distcp-site.xml into code base,For now; the configuration of distcp resides in hadoop-distcp.jar. This makes it difficult to adjust the configuration dynamically.,Patch Available,Unresolved,HADOOP-14189,Unassigned,Siqi Li,Mon; 23 Jun 2014 18:00:53 +0000,Tue; 3 Oct 2017 18:22:09 +0000,,,2.4.0,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10738
HADOOP-10739,Bug,Major,fs,Renaming a file into a directory containing the same filename results in a confusing I/O error,"Renaming a file to another existing filename says ""Fileexists"" but colliding with a file in a directory results in the cryptic""Input/output error"".",Closed,Fixed,,Chang Li,Jason Lowe,Mon; 23 Jun 2014 18:22:59 +0000,Wed; 3 Sep 2014 20:36:30 +0000,Fri; 27 Jun 2014 23:24:18 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10739
HADOOP-10740,Improvement,Minor,scripts,Add missing shebang line for the shell scripts,Add missing shebang with bash line for .sh files:hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.shhadoop-common-project/hadoop-common/src/main/conf/hadoop-env.shhadoop-mapreduce-project/conf/mapred-env.shhadoop-yarn-project/hadoop-yarn/bin/yarn-config.shhadoop-yarn-project/hadoop-yarn/conf/yarn-env.sh,Resolved,Won't Fix,,Henry Saputra,Henry Saputra,Mon; 23 Jun 2014 21:57:28 +0000,Wed; 3 Sep 2014 21:16:38 +0000,Wed; 3 Sep 2014 21:16:38 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10740
HADOOP-10741,New Feature,Major,tools,A lightweight WebHDFS client library,"One of the motivations for creating WebHDFS is for applications connecting to HDFS from outside the cluster.  In order to do so; users have to either	install Hadoop and use WebHdfsFileSsytem; or	develop their own client using the WebHDFS REST API.For #1; it is very difficult to manage and unnecessarily complicated for other applications since Hadoop is not a lightweight library.  For #2; it is not easy to deal with security and handle transient errors.Therefore; we propose adding a lightweight WebHDFS client as a separated library which does not depend on Common and HDFS.  The client can be packaged as a standalone jar.  Other applications simply add the jar to their classpath for using it.",Open,Unresolved,,Mohammad Kamrul Islam,Tsz Wo Nicholas Sze,Mon; 23 Jun 2014 23:00:43 +0000,Thu; 2 Oct 2014 04:03:30 +0000,,,,,,HDFS-6200,https://issues.apache.org/jira/browse/HADOOP-10741
HADOOP-10742,Bug,Major,build,Problems building with -Pnative on FreeBSD 10,"mkdir Created dir: /usr/ports/devel/hadoop2/work/hadoop-2.4.0-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native/target     exec  The C compiler identification is Clang 3.4.1     exec  The CXX compiler identification is Clang 3.4.1     exec  Check for working C compiler: /usr/bin/cc     exec  Check for working C compiler: /usr/bin/cc  works     exec  Detecting C compiler ABI info     exec  Detecting C compiler ABI info - done     exec  Check for working CXX compiler: /usr/bin/CC     exec  Check for working CXX compiler: /usr/bin/CC  works     exec  Detecting CXX compiler ABI info     exec  Detecting CXX compiler ABI info - done     exec  Found JNI: /usr/local/openjdk6/jre/lib/amd64/libjawt.so       exec  Configuring done     exec  Generating done     exec  Build files have been written to: /usr/ports/devel/hadoop2/work/hadoop-2.4.0-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native...     exec /usr/ports/devel/hadoop2/work/hadoop-2.4.0-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c:501:33: error: use of undeclared identifier 'LOGIN_NAME_MAX'     exec       if (strncmp(*users; user; LOGIN_NAME_MAX) == 0) {     exec                                 ^     exec /usr/ports/devel/hadoop2/work/hadoop-2.4.0-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c:842:25: warning: implicit declaration of function 'basename' is invalid in C99 -Wimplicit-function-declaration     exec                                    primary_app_dir; basename(nmPrivate_credentials_file_copy));     exec                                                     ^     exec /usr/ports/devel/hadoop2/work/hadoop-2.4.0-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c:1221:15: warning: using the result of an assignment as a condition without parentheses -Wparentheses     exec     while (ep = readdir(dp)) {     exec            ~~^~~~~~~~~~~~     exec /usr/ports/devel/hadoop2/work/hadoop-2.4.0-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c:1221:15: note: place parentheses around the assignment to silence this warning     exec     while (ep = readdir(dp)) {     exec               ^     exec            (               )     exec /usr/ports/devel/hadoop2/work/hadoop-2.4.0-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c:1221:15: note: use '==' to turn this assignment into an equality comparison     exec     while (ep = readdir(dp)) {     exec               ^     exec               ==     exec /usr/ports/devel/hadoop2/work/hadoop-2.4.0-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c:1250:48: error: too many arguments to function call; expected 4; have 5     exec     if (mount(""none""; mount_path; ""cgroup""; 0; controller) == 0) {     exec         ~~~~~                                  ^~~~~~~~~~     exec /usr/include/sys/mount.h:929:1: note: 'mount' declared here     exec int     mount(const char *; const char *; int; void *);     exec ^     exec 2 warnings and 2 errors generated.     exec *** Error code 1     exec      exec Stop.     exec make2: stopped in /usr/ports/devel/hadoop2/work/hadoop-2.4.0-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native     exec *** Error code 1     exec      exec Stop.     exec make1: stopped in /usr/ports/devel/hadoop2/work/hadoop-2.4.0-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native     exec *** Error code 1     exec      exec Stop.     exec make: stopped in /usr/ports/devel/hadoop2/work/hadoop-2.4.0-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/nativeINFO ------------------------------------------------------------------------INFO Reactor Summary:INFO INFO Apache Hadoop Main ................................ SUCCESS 1.741sINFO Apache Hadoop Project POM ......................... SUCCESS 1.994sINFO Apache Hadoop Annotations ......................... SUCCESS 1.559sINFO Apache Hadoop Project Dist POM .................... SUCCESS 0.385sINFO Apache Hadoop Assemblies .......................... SUCCESS 0.555sINFO Apache Hadoop Maven Plugins ....................... SUCCESS 5.486sINFO Apache Hadoop MiniKDC ............................. SUCCESS 3.057sINFO Apache Hadoop Auth ................................ SUCCESS 1.327sINFO Apache Hadoop Auth Examples ....................... SUCCESS 0.316sINFO Apache Hadoop Common .............................. SUCCESS 4:08.982sINFO Apache Hadoop NFS ................................. SUCCESS 15.122sINFO Apache Hadoop Common Project ...................... SUCCESS 0.208sINFO Apache Hadoop HDFS ................................ SUCCESS 3:17.590sINFO Apache Hadoop HttpFS .............................. SUCCESS 23.520sINFO Apache Hadoop HDFS BookKeeper Journal ............. SUCCESS 16.332sINFO Apache Hadoop HDFS-NFS ............................ SUCCESS 1.315sINFO Apache Hadoop HDFS Project ........................ SUCCESS 0.240sINFO hadoop-yarn ....................................... SUCCESS 0.207sINFO hadoop-yarn-api ................................... SUCCESS 20.082sINFO hadoop-yarn-common ................................ SUCCESS 23.888sINFO hadoop-yarn-server ................................ SUCCESS 0.153sINFO hadoop-yarn-server-common ......................... SUCCESS 3.231sINFO hadoop-yarn-server-nodemanager .................... FAILURE 45.247sINFO hadoop-yarn-server-web-proxy ...................... SKIPPED...",Resolved,Duplicate,YARN-1327,Unassigned,Pedro Giffuni,Mon; 23 Jun 2014 23:37:31 +0000,Tue; 24 Jun 2014 04:18:50 +0000,Tue; 24 Jun 2014 04:18:50 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10742
HADOOP-10743,Bug,Major,build,Problem building hadoop -2.4.0 on FreeBSD 10 (without -Pnative),mapreduce-client-core fails to compile with java 1.6 on FreeBSD 10.,Resolved,Won't Fix,,Unassigned,Pedro Giffuni,Tue; 24 Jun 2014 01:08:40 +0000,Fri; 10 Nov 2017 15:19:30 +0000,Fri; 10 Nov 2017 15:19:30 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10743
HADOOP-10744,Bug,Major,io;native,LZ4 Compression fails to recognize PowerPC Little Endian Architecture,Lz4 Compression fails to identify the PowerPC Little Endian Architecture. It recognizes it as Big Endian and several testcases( TestCompressorDecompressor; TestCodec; TestLz4CompressorDecompressor)  fails due to this.Running org.apache.hadoop.io.compress.TestCompressorDecompressorTests run: 2; Failures: 2; Errors: 0; Skipped: 0; Time elapsed: 0.435 sec &lt; FAILURE! - in org.apache.hadoop.io.compress.TestCompressorDecompressortestCompressorDecompressor(org.apache.hadoop.io.compress.TestCompressorDecompressor)  Time elapsed: 0.308 sec  &lt; FAILURE!org.junit.internal.ArrayComparisonFailure: org.apache.hadoop.io.compress.lz4.Lz4Compressor_org.apache.hadoop.io.compress.lz4.Lz4Decompressor-  byte arrays not equals error !!!: arrays first differed at element 1428; expected:4 but was:10        at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:50)        at org.junit.Assert.internalArrayEquals(Assert.java:473)        at org.junit.Assert.assertArrayEquals(Assert.java:294)        at org.apache.hadoop.io.compress.CompressDecompressTester$CompressionTestStrategy$2.assertCompression(CompressDecompressTester.java:325)        at org.apache.hadoop.io.compress.CompressDecompressTester.test(CompressDecompressTester.java:135)        at org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor(TestCompressorDecompressor.java:58)...................,Closed,Fixed,,Bert Sanders,Ayappan,Tue; 24 Jun 2014 09:39:13 +0000,Tue; 30 Jun 2015 07:22:38 +0000,Wed; 4 Feb 2015 10:12:48 +0000,,2.4.1;2.5.2,,,,https://issues.apache.org/jira/browse/HADOOP-10744
HADOOP-10745,Improvement,Minor,security,Improve the delete key rest api of KMS,Deploy KMS on 1 node:create key successfully; No any message are given when deleting a key although the delete action succeeds. #curl -X DELETE http://localhost:16000/kms/v1/key/k2?user.name=1,Open,Unresolved,,Unassigned,liyunzhang,Tue; 24 Jun 2014 10:25:17 +0000,Tue; 24 Jun 2014 10:25:17 +0000,,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10745
HADOOP-10746,Bug,Major,test,TestSocketIOWithTimeout#testSocketIOWithTimeout fails on Power PC ,SocketOutputStream closes its writer if it's partial written. But on PPC; after writing for some time; buf.capacity still equals buf.remaining. The reason might be what's written on PPC is buffered;so the buf.remaining will not change till a flush.,Closed,Fixed,,Jinghui Wang,Jinghui Wang,Fri; 20 Jun 2014 18:00:47 +0000,Wed; 3 Sep 2014 20:36:29 +0000,Tue; 24 Jun 2014 17:17:00 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10746
HADOOP-10747,Improvement,Minor,ipc,Support configurable retries on SASL connection failures in RPC client.,The RPC client includes a retry loop around SASL connection failures.  Currently; this is hard-coded to a maximum of 5 retries.  Let's make this configurable.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Tue; 24 Jun 2014 17:52:50 +0000,Thu; 12 May 2016 18:24:43 +0000,Wed; 25 Jun 2014 00:02:25 +0000,,2.4.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10747
HADOOP-10748,Bug,Major,,HttpServer2 should not load JspServlet,Currently HttpServer2 loads the JspServlet by default. It should be removed as JSP support is no longer required.,Closed,Fixed,,Haohui Mai,Haohui Mai,Tue; 24 Jun 2014 17:56:38 +0000,Fri; 24 Apr 2015 22:49:05 +0000,Wed; 27 Aug 2014 20:31:47 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10748
HADOOP-10749,Bug,Major,,Error : Server ipc version 9 cannot connect with client version 4 ,All Hadoop daemons up and runningWhen I'm trying to connect HDFS from Eclipse; getting the errorError : Server ipc version 9 cannot connect with client version 4 Have used both hadoop-eclipse-kepler-plugin-2.2.0.jar andhadoop-eclipse-plugin-1.2.1.jarI can able to connect ( Hadoop 1.1.0) HDFS from eclipse. any help appreciated;Thanks,Open,Unresolved,,Unassigned,Solaimurugan.V,Wed; 25 Jun 2014 15:19:30 +0000,Wed; 25 Jun 2014 15:19:30 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10749
HADOOP-10750,Improvement,Major,security,KMSKeyProviderCache should be in hadoop-common,KMS has KMSCacheKeyProvider; this class should be available in hadoop-common for users of  KeyProvider instances to wrap them and avoid several; potentially expensive; key retrievals.,Closed,Fixed,,Arun Suresh,Alejandro Abdelnur,Wed; 25 Jun 2014 23:39:22 +0000,Thu; 12 May 2016 18:27:02 +0000,Fri; 18 Jul 2014 22:01:47 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10750
SLIDER-174,Task,Major,,HBase Master needs data locality,Currently: But in RoleHistory#findNodeForNewInstance(): This implies that HBase master instances might be scheduled on one host; obviating the goal for HA.,Resolved,Fixed,,Ted Yu,Ted Yu,Thu; 26 Jun 2014 02:18:56 +0000,Thu; 10 Jul 2014 19:52:45 +0000,Thu; 26 Jun 2014 16:52:13 +0000,,,,,,https://issues.apache.org/jira/browse/SLIDER-174
HADOOP-10752,Improvement,Minor,native;performance,Add support for hardware crc on ARM aarch64 architecture,This patch adds support for hardware crc for ARM's new 64 bit architecture.The patch is completely conditionalized on _arch64_For the moment I have only done the non pipelined version as the hw I have only has 1 crc execute unit.Some initial benchmarks on terasort givesw crc: 107 sechw crc: 103 secThe performance improvement is quite small; but this is limited by the fact that I am using early stage hw which is not performant.I have also built it on x86 and I think the change is fairly safe for other architectures because post conditionalization the src is identical on other architectures.This is the first patch I have submitted for Hadoop so I would welcome any feedback and help.,Resolved,Duplicate,NULL,Unassigned,Edward Nevill,Thu; 26 Jun 2014 08:54:13 +0000,Tue; 31 Mar 2015 08:43:07 +0000,Tue; 31 Mar 2015 08:43:06 +0000,,trunk-win,performance,,,https://issues.apache.org/jira/browse/HADOOP-10752
HADOOP-10753,Improvement,Minor,,Could be better to move CommonConfigurationKeysPublic to org.apache.hadoop.common package,As discussed in HADOOP-8943; it would be better to place CommonConfigurationKeysPublic into a more neutral place like org.apache.hadoop.common instead of org.apache.hadoop.fs package. This would help common facilities like security related codes avoid coupling with fs stuffs.Could anyone post your thought about this; and if confirmed; I would provide a patch for it.,Open,Unresolved,,Kai Zheng,Kai Zheng,Thu; 26 Jun 2014 09:59:43 +0000,Fri; 27 Jun 2014 10:06:43 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10753
HADOOP-10754,Test,Trivial,ha;test,Reenable several HA ZooKeeper-related tests on Windows.,Now that our version of ZooKeeper has been upgraded in HADOOP-9555; we can reenable several tests that had been broken on Windows.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Thu; 26 Jun 2014 16:35:48 +0000,Thu; 12 May 2016 18:26:26 +0000,Thu; 26 Jun 2014 23:06:17 +0000,,2.4.0;3.0.0-alpha1,,,HADOOP-9555;HADOOP-9556,https://issues.apache.org/jira/browse/HADOOP-10754
HADOOP-10755,Improvement,Major,security,Support negative caching of user-group mapping,We've seen a situation at a couple of our customers where interactions from an unknown user leads to a high-rate of group mapping calls. In one case; this was happening at a rate of 450 calls per second with the shell-based group mapping; enough to severely impact overall namenode performance and also leading to large amounts of log spam (prints a stack trace each time).Let's consider negative caching of group mapping; as well as quashing the rate of this log message.,Closed,Fixed,,Lei (Eddy) Xu,Andrew Wang,Wed; 16 Oct 2013 17:07:09 +0000,Mon; 1 Dec 2014 03:10:56 +0000,Mon; 21 Jul 2014 21:53:07 +0000,,2.2.0,,,HADOOP-8088,https://issues.apache.org/jira/browse/HADOOP-10755
HADOOP-10756,Improvement,Major,security,KMS audit log should consolidate successful similar requests,Every rejected access should be audited; but successful accesses should be consolidated within a given amount of time if the request is from the same user for he same key.,Closed,Fixed,,Arun Suresh,Alejandro Abdelnur,Thu; 26 Jun 2014 22:17:12 +0000,Thu; 12 May 2016 18:27:55 +0000,Wed; 30 Jul 2014 17:54:42 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10756
HADOOP-10757,Improvement,Major,security,KeyProvider KeyVersion should provide the key name,Currently the KeyVersion does not provide a way to get the key name to do a reverse lookup to get the metadata of the key.For the JavaKeyStoreProvider and the UserProvider this is not an issue because the key name is encoded in the key version name. This encoding of the key name in the key version name cannot be expected in all KeyProvider implementations. It is common for key management systems to use UUID to refer to specific key materials (KeyVersions in Hadoop parlance).,Closed,Fixed,,Arun Suresh,Alejandro Abdelnur,Fri; 27 Jun 2014 00:15:43 +0000,Thu; 12 May 2016 18:27:57 +0000,Fri; 4 Jul 2014 17:32:24 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10757
HADOOP-10758,Improvement,Major,security,KMS: add ACLs on per key basis.,The KMS server should enforce ACLs on per key basis.,Closed,Fixed,,Arun Suresh,Alejandro Abdelnur,Fri; 27 Jun 2014 00:23:06 +0000,Mon; 31 Jul 2017 19:04:20 +0000,Wed; 10 Sep 2014 21:30:58 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10758
HADOOP-10759,Bug,Minor,bin,Remove hardcoded JAVA_HEAP_MAX in hadoop-config.sh,In hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh; there is a hard code for Java parameter: 'JAVA_HEAP_MAX=-Xmx1000m'. It should be removed.,Resolved,Fixed,,sam liu,sam liu,Fri; 27 Jun 2014 01:26:15 +0000,Sat; 12 Sep 2015 19:22:44 +0000,Thu; 7 Aug 2014 15:04:51 +0000,,2.4.0,,,,https://issues.apache.org/jira/browse/HADOOP-10759
HADOOP-10760,Improvement,Major,,Helper scirpt: looping tests until it fails,Some tests can fail intermittently because of timing bugs. To reproduce the test failure; it's useful to add script which launches specified test until it fails.,Patch Available,Unresolved,,Tsuyoshi Ozawa,Tsuyoshi Ozawa,Fri; 27 Jun 2014 14:35:05 +0000,Wed; 6 May 2015 03:35:02 +0000,,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10760
HADOOP-10761,New Feature,Major,nfs,Provide an NFS gateway option to get groups from local name service for easy deployment in cloud,In cloud deployment; client machines and Hadoop Cluster are sometimes in different DMZ and LDAP in some other. Integration client machines with LDAP might require lot of firewall tickets; which can be a pain especially when users keep on adding new client machines.When the client is not integrated with LDAP for the group mapping; NFS should not use the group list in the request. Instead; NFS gateway can get the group list from local name service.This JIRA is to track the effort to provide a configuration option to get list of GID from LDAP/OS for making the deployment at Cloud easier. Also this can remove the limitation of 15 GID in the NFS request.,Reopened,Unresolved,,Unassigned,Brandon Li,Fri; 27 Jun 2014 17:20:12 +0000,Thu; 12 May 2016 18:27:22 +0000,,,3.0.0-alpha1,,,HADOOP-10701,https://issues.apache.org/jira/browse/HADOOP-10761
HADOOP-10762,Bug,Major,conf,Config parameter 'fs.permissions.umask-mode' with value 0002 doesn't have any affect,When is set parameter 'fs.permissions.umask-mode' with value 0002 org.apache.hadoop.fs.FileContext always use internally 0022. After investigation found that org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService-serviceInit(Configuration conf) sets always FsPermission.DEFAULT_UMASK value.,Open,Unresolved,,Unassigned,Svetozar Ivanov,Fri; 27 Jun 2014 17:41:46 +0000,Fri; 27 Jun 2014 17:49:43 +0000,,,2.4.0,conf;config;permissions,,,https://issues.apache.org/jira/browse/HADOOP-10762
HADOOP-10763,Test,Minor,,TestZKFailoverControllerStress#testExpireBackAndForth fails occasionally in trunk,See https://builds.apache.org/job/Hadoop-Common-trunk/1153/consoleI was able to reproduce on Mac:,Resolved,Duplicate,NULL,Unassigned,Ted Yu,Sat; 28 Jun 2014 14:54:14 +0000,Sat; 28 Jun 2014 15:36:05 +0000,Sat; 28 Jun 2014 15:36:05 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10763
HADOOP-10764,Bug,Major,,Unable to build hadoop-2.2.0 under FreeBSD-10,"FreeBSD does not have libdl.so (dlopen() et al are in libc).  So the buld fails:     exec /usr/bin/cc  -fPIC  -g -Wall -O2 -D_REENTRANT -D_GNU_SOURCE -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64   -shared -Wl;-soname;libhdfs.so.0.0.0 -o target/usr/local/lib/libhdfs.so.0.0.0 CMakeFiles/hdfs.dir/main/native/libhdfs/exception.c.o CMakeFiles/hdfs.dir/main/native/libhdfs/jni_helper.c.o CMakeFiles/hdfs.dir/main/native/libhdfs/hdfs.c.o /usr/local/openjdk7/jre/lib/amd64/server/libjvm.so -ldl -lpthread -Wl;-rpath;/usr/local/openjdk7/jre/lib/amd64/server     exec *** Error code 1The solution is not to use ""-ldl"" under FreeBSD at all.",Resolved,Duplicate,HDFS-5365,Unassigned,Dmitry Sivachenko,Sat; 28 Jun 2014 18:23:14 +0000,Mon; 30 Jun 2014 18:32:24 +0000,Sun; 29 Jun 2014 21:32:08 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10764
HADOOP-10765,Bug,Major,,Unable to build hadoop-2.2.0 under FreeBSD-10,"hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.cfile uses Linux-specific semantic for mount():if (mount(""none""; mount_path; ""cgroup""; 0; controller) == 0) {FreeBSD has different format for mount(2) system call:int mount(const char *type; const char *dir; int flags; void *data);So the build breaks.Please port this piece of code to be usable under FreeBSD too.",Resolved,Duplicate,YARN-1327,Unassigned,Dmitry Sivachenko,Sat; 28 Jun 2014 18:27:13 +0000,Sun; 29 Jun 2014 21:25:39 +0000,Sun; 29 Jun 2014 21:25:39 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10765
HADOOP-10766,Bug,Major,,Unable to build hadoop 2.4.1 (FreeBSD),mvn compile -Pnative  ends like this:ERROR Failed to execute goal on project hadoop-hdfs: Could not resolve dependencies for project org.apache.hadoop:hadoop-hdfs:jar:2.4.1: Failure to find org.apache.hadoop:hadoop-common:jar:tests:2.4.1 in https://repository.apache.org/content/repositories/snapshots was cached in the local repository; resolution will not be reattempted until the update interval of apache.snapshots.https has elapsed or updates are forced - Help 1,Resolved,Fixed,,Unassigned,Dmitry Sivachenko,Mon; 30 Jun 2014 11:04:05 +0000,Tue; 1 Jul 2014 11:24:34 +0000,Tue; 1 Jul 2014 11:24:34 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10766
HADOOP-10767,Improvement,Trivial,fs,Clean up unused code in Ls shell command.,After recent ACL changes; there is some unused code left in Ls that we can clean up.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Mon; 30 Jun 2014 20:24:43 +0000,Thu; 12 May 2016 18:22:37 +0000,Tue; 1 Jul 2014 16:36:38 +0000,,2.4.1;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10767
HADOOP-10768,Improvement,Major,performance;security,Optimize Hadoop RPC encryption performance,"Hadoop RPC encryption is enabled by setting hadoop.rpc.protection to ""privacy"". It utilized SASL GSSAPI and DIGEST-MD5 mechanisms for secure authentication and data protection. Even GSSAPI supports using AES; but without AES-NI support by default; so the encryption is slow and will become bottleneck.After discuss with Aaron T. Myers; Alejandro Abdelnur and Uma Maheswara Rao G; we can do the same optimization as in HDFS-6606. Use AES-NI with more than 20x speedup.On the other hand; RPC message is small; but RPC is frequent and there may be lots of RPC calls in one connection; we needs to setup benchmark to see real improvement and then make a trade-off.",Patch Available,Unresolved,HADOOP-14558;HADOOP-14558,Dapeng Sun,Yi Liu,Tue; 1 Jul 2014 07:38:40 +0000,Tue; 9 Jan 2018 05:44:32 +0000,,,3.0.0-alpha1,,,HBASE-16633;HDFS-6606;HADOOP-15032;HADOOP-12725,https://issues.apache.org/jira/browse/HADOOP-10768
HADOOP-10769,Improvement,Major,security,Create KeyProvider extension to handle delegation tokens,The KeyProvider API needs to return delegation tokens to enable access to the KeyProvider from processes without Kerberos credentials (ie Yarn containers).This is required for HDFS encryption and KMS integration.,Closed,Fixed,,Arun Suresh,Alejandro Abdelnur,Tue; 1 Jul 2014 19:21:42 +0000,Thu; 12 May 2016 18:23:40 +0000,Sun; 6 Jul 2014 19:18:01 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10769
HADOOP-10770,Improvement,Major,security,KMS add delegation token support,This is a follow up on HADOOP-10769 for KMS itself.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 1 Jul 2014 19:22:55 +0000,Thu; 12 May 2016 18:23:38 +0000,Fri; 15 Aug 2014 05:04:10 +0000,,3.0.0-alpha1,,HADOOP-10771,,https://issues.apache.org/jira/browse/HADOOP-10770
HADOOP-10771,Improvement,Major,security,Refactor HTTP delegation support out of httpfs to common,HttpFS implements delegation token support in AuthenticationFilter  AuthenticationHandler subclasses.For HADOOP-10770 we need similar functionality for KMS.Not to duplicate code; we should refactor existing code to common.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 1 Jul 2014 19:25:32 +0000,Thu; 12 May 2016 18:23:39 +0000,Fri; 8 Aug 2014 05:05:04 +0000,,3.0.0-alpha1,,HADOOP-10880;HADOOP-10770,HADOOP-11207;YARN-2292,https://issues.apache.org/jira/browse/HADOOP-10771
HADOOP-10772,Improvement,Major,build,Generating RPMs for common; hdfs; httpfs; mapreduce ; yarn and tools ,Generating RPMs for hadoop-common; hadoop-hdfs; hadoop-hdfs-httpfs; hadoop-mapreduce ; hadoop-yarn-project and hadoop-tools-dist with dist build profile.,Open,Unresolved,,Jinghui Wang,Jinghui Wang,Wed; 2 Jul 2014 00:49:27 +0000,Mon; 7 Jul 2014 01:01:05 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10772
HADOOP-10773,Bug,Minor,security,testLocalHostNameForNullOrWild() will failed if we have UpperCase hostname,"In the method getServerPrincipal() of SecurityUtil; we change the fqdn to lowcase. The testLocalHostNameForNullOrWild() of TestSecurityUtil will failed if the hostname contains uppercase.We could fix it simply change ""String local = SecurityUtil.getLocalHostName();"" to ""String local = SecurityUtil.getLocalHostName().toLowCase();"" in testLocalHostNameForNullOrWild().But I think we should rethink the implement of SecurityUtil.replacePattern(). Is it need to hange the fqdn to lowcase?",Open,Unresolved,,Unassigned,Beckham007,Wed; 2 Jul 2014 03:42:17 +0000,Sat; 7 Jan 2017 01:56:48 +0000,,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10773
HADOOP-10774,Bug,Major,security,Update KerberosTestUtils for hadoop-auth tests when using IBM Java,"There are two issues if IBM Java is used while testing hadoop-auth tests.Looks like there are lot of changes haven been done to properly handle the kerbserose authentication using the JIRA defect: HADOOP-9446 for IBM JAVA.But their are still some failures can been seen in ""Apache Hadoop Common"" tests in case of IBM JAVA.Available patch for HADOOP-10774 will solve the authentication issues plus the path issues.Two issue issue related to IBM java are.1) Bad JAAS configuration: unrecognized option: isInitiator2) Cannot retrieve key from keytab HTTP/localhost@EXAMPLE.COM#1 Is caused as isInitiator isn't defined when we use IBM JAVA. #2 IS caused as; For IBM_JAVA keytab file must be a absolute path with file:// as the prefix for the useKeytab option.   But the file path is relative. This change will work with both openjdk  IBM_JAVA. Attached patch will resolve all failures happening if we use IBM Java.",Closed,Fixed,HADOOP-10421,sangamesh,sangamesh,Wed; 2 Jul 2014 11:41:37 +0000,Thu; 12 May 2016 18:22:45 +0000,Sun; 1 Mar 2015 07:23:08 +0000,,2.6.0,,,,https://issues.apache.org/jira/browse/HADOOP-10774
HADOOP-10775,Improvement,Minor,util,Shell operations to fail with meaningful errors on windows if winutils.exe not found,If winutils.exe can't be found HADOOP_HOME wrong/unset or other causes; then an error is logged -but when any of the Shell operations are used; an NPE is raised rather than something meaningful.The error message at setup time should be preserved and then raised before any attempt to invoke a winutils-driven process made,Resolved,Fixed,SPARK-2356;HADOOP-11003;HADOOP-14586,Steve Loughran,Steve Loughran,Wed; 2 Jul 2014 13:17:50 +0000,Fri; 29 Sep 2017 12:18:58 +0000,Tue; 13 Oct 2015 20:30:39 +0000,,trunk-win;2.7.1,,,SLIDER-201;SPARK-2356;HADOOP-11003;SPARK-6961,https://issues.apache.org/jira/browse/HADOOP-10775
HADOOP-10776,Improvement,Blocker,,Open up already widely-used APIs for delegation-token fetching & renewal to ecosystem projects,Storm would like to be able to fetch delegation tokens and forward them on to running topologies so that they can access HDFS (STORM-346).  But to do so we need to open up access to some of APIs. Most notably FileSystem.addDelegationTokens(); Token.renew; Credentials.getAllTokens; and UserGroupInformation but there may be others.At a minimum adding in storm to the list of allowed API users. But ideally making them public. Restricting access to such important functionality to just MR really makes secure HDFS inaccessible to anything except MR; or tools that reuse MR input formats.,Resolved,Fixed,,Vinod Kumar Vavilapalli,Robert Joseph Evans,Wed; 2 Jul 2014 17:46:00 +0000,Tue; 29 Nov 2016 01:57:36 +0000,Thu; 24 Nov 2016 17:57:08 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10776
HADOOP-10777,Bug,Major,security,Provide Kerberos login user with password,Please provide a means to do a Kerberos login with a given principal name and a password. Currently; UserGroupInformation provides loginUserWithKeytab; but no means to do login with a password.   As background; I am a developer for an ISV developing a native YARN application that works with an existing data-quality/integration suite. This suite has a GUI that can ask the user for their Kerberos principal name and password. We have successfully used the UserGroupInformation loginUserWIthKeytab; but can not find a means to do a login with a password given the current API.,Open,Unresolved,,Unassigned,Geoff Thompson,Wed; 2 Jul 2014 18:04:16 +0000,Wed; 2 Jul 2014 20:26:28 +0000,,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10777
HADOOP-10778,Improvement,Major,util,Use NativeCrc32 only if it is faster,From the benchmark post in this comment; NativeCrc32 is slower than java.util.zip.CRC32 for Java 7 and above when bytesPerChecksum  512.,Patch Available,Unresolved,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Wed; 2 Jul 2014 23:53:43 +0000,Wed; 6 May 2015 03:33:29 +0000,,,,BB2015-05-TBR,,HADOOP-10865;HADOOP-10838,https://issues.apache.org/jira/browse/HADOOP-10778
HADOOP-10779,Wish,Minor,documentation;fs,Generalize DFS_PERMISSIONS_SUPERUSERGROUP_KEY for any HCFS,HDFS has configuration option dfs.permissions.superusergroup stored inhdfs-site.xml configuration file: Since we have an option to use alternative Hadoop filesystems (HCFS); there isa question how to specify a supergroup in such case.Eg. would introducing HCFS option in say core-site.xml for this as shownbelow make sense? Or would you solve it in different way? I would like to at least declare a recommended approach for alternative Hadoop filesystems to follow.,Open,Unresolved,,Unassigned,Martin Bukatovic,Thu; 3 Jul 2014 08:23:43 +0000,Fri; 28 Nov 2014 14:55:08 +0000,,,,hcfs,,,https://issues.apache.org/jira/browse/HADOOP-10779
HADOOP-10780,Bug,Major,,hadoop_user_info_alloc fails on FreeBSD due to incorrect sysconf use,I am trying hadoop-2.4.1 on FreeBSD-10/stable.namenode starts up; but after first datanode contacts it; it throws an exception.All limits seem to be high enough:% limits -aResource limits (current):  cputime              infinity secs  filesize             infinity kB  datasize             33554432 kB  stacksize              524288 kB  coredumpsize         infinity kB  memoryuse            infinity kB  memorylocked         infinity kB  maxprocesses           122778  openfiles              140000  sbsize               infinity bytes  vmemoryuse           infinity kB  pseudo-terminals     infinity  swapuse              infinity kB14944  1  S        0:06.59 /usr/local/openjdk7/bin/java -Dproc_namenode -Xmx1000m -Dhadoop.log.dir=/var/log/hadoop -Dhadoop.log.file=hadoop-hdfs-namenode-nezabudka3-00.log -Dhadoop.home.dir=/usr/local -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO;RFA -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx32768m -Xms32768m -Djava.library.path=/usr/local/lib -Xmx32768m -Xms32768m -Djava.library.path=/usr/local/lib -Xmx32768m -Xms32768m -Djava.library.path=/usr/local/lib -Dhadoop.security.logger=INFO;RFAS org.apache.hadoop.hdfs.server.namenode.NameNodeFrom the namenode's log:2014-07-03 23:28:15;070 WARN  IPC Server handler 5 on 8020 ipc.Server (Server.java:run(2032)) - IPC Server handler 5 on 8020; call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 5.255.231.209:57749 Call#842 Retry#0java.lang.OutOfMemoryError        at org.apache.hadoop.security.JniBasedUnixGroupsMapping.getGroupsForUser(Native Method)        at org.apache.hadoop.security.JniBasedUnixGroupsMapping.getGroups(JniBasedUnixGroupsMapping.java:80)        at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)        at org.apache.hadoop.security.Groups.getGroups(Groups.java:139)        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1417)        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.init(FSPermissionChecker.java:81)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3331)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkSuperuserPrivilege(FSNamesystem.java:5491)        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.versionRequest(NameNodeRpcServer.java:1082)        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.versionRequest(DatanodeProtocolServerSideTranslatorPB.java:234)        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:28069)        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:415)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)I did not have such an issue with hadoop-1.2.1.,Closed,Fixed,,Dmitry Sivachenko,Dmitry Sivachenko,Thu; 3 Jul 2014 19:32:43 +0000,Mon; 1 Dec 2014 03:09:36 +0000,Mon; 14 Jul 2014 17:59:49 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10780
HADOOP-10781,Bug,Major,,Unportable getgrouplist() usage breaks FreeBSD,"getgrouplist() has different return values on Linux and FreeBSD:Linux: either the number of groups (positive) or -1 on errorFreeBSD: 0 on success or -1 on errorThe return value of getgrouplist() is analyzed in Linux-specific way in hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/security/hadoop_user_info.c; in function hadoop_user_info_getgroups() which breaks FreeBSD.In this function you have 3 choices for the return value ret = getgrouplist(uinfo-pwd.pw_name; uinfo-pwd.pw_gid;                         uinfo-gids; ngroups);1) ret  0 : OK for Linux; it will be zero on FreeBSD.  I propose to change this to ret = 02) First condition is false and ret != -1:  impossible according to manpage3) ret == 1  OK for both Linux and FreeBSDSo I propose to change ""ret  0"" to ""ret = 0"" and (optionally) return 2nd case.",Closed,Fixed,,Dmitry Sivachenko,Dmitry Sivachenko,Fri; 4 Jul 2014 06:43:00 +0000,Mon; 1 Dec 2014 03:08:23 +0000,Tue; 8 Jul 2014 18:17:51 +0000,,2.4.1,,,HADOOP-10989,https://issues.apache.org/jira/browse/HADOOP-10781
HADOOP-10782,Improvement,Trivial,,Typo in DataChecksum classs,,Closed,Fixed,,Jingguo Yao,Jingguo Yao,Fri; 4 Jul 2014 08:00:01 +0000,Fri; 15 Aug 2014 05:39:32 +0000,Mon; 7 Jul 2014 18:22:01 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10782
HADOOP-10783,Bug,Major,,apache-commons-lang.jar 2.6 does not support FreeBSD -upgrade to 3.x needed,Hadoop-2.4.1 ships with apache-commons.jar version 2.6.It does not support FreeBSD (IS_OS_UNIX returns False).This is fixed in recent versions of apache-commons.jarPlease update apache-commons.jar to recent version so it correctly recognizes FreeBSD as UNIX-like system.Right now I get in datanode's log:2014-07-04 11:58:10;459 DEBUG org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: Disabling ShortCircuitRegistryjava.io.IOException: The OS is not UNIX.        at org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory.create(SharedFileDescriptorFactory.java:77)        at org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry.init(ShortCircuitRegistry.java:169)        at org.apache.hadoop.hdfs.server.datanode.DataNode.initDataXceiver(DataNode.java:583)        at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:771)        at org.apache.hadoop.hdfs.server.datanode.DataNode.init(DataNode.java:289)        at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1931)        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1818)        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1865)        at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2041)        at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2065),Open,Unresolved,,Steve Loughran,Dmitry Sivachenko,Fri; 4 Jul 2014 08:43:46 +0000,Mon; 2 Mar 2015 19:01:07 +0000,,,2.4.1,,,HADOOP-9991,https://issues.apache.org/jira/browse/HADOOP-10783
HADOOP-10784,Improvement,Minor,security,Need add more in KMS document,"Now i can only find the kms document in http://aajisaka.github.io/hadoop-project/hadoop-kms/index.html; but it is very simple. for example; i don't know how to enabling Kerberos HTTP SPNEGO Authentication although i configure the kms-site.xml according to the reference page.How to test it ?I send following request to KMS server: curl -g --header  ""Authorization:Negotiate123455"" http://localhost:16000/kms/v1/key/k1I read the KMS code and found that  i need add parameters in request header and  the format is ""Authorization:Negotiate $token"". But how the token is generated?",Open,Unresolved,,Unassigned,liyunzhang,Fri; 4 Jul 2014 08:57:17 +0000,Tue; 22 Jul 2014 05:56:51 +0000,,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10784
HADOOP-10785,Sub-task,Major,fs,UnsatisfiedLinkError in cryptocodec tests with OpensslCipher#initContext,,Resolved,Duplicate,HADOOP-10735,Uma Maheswara Rao G,Uma Maheswara Rao G,Fri; 4 Jul 2014 14:29:42 +0000,Thu; 12 May 2016 18:24:50 +0000,Mon; 7 Jul 2014 18:18:33 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10785
HADOOP-10786,Bug,Major,security,Fix UGI#reloginFromKeytab on Java 8,Krb5LoginModule changed subtly in java 8: in particular; if useKeyTab and storeKey are specified; then only a KeyTab object is added to the Subject's private credentials; whereas in java = 7 both a KeyTab and some number of KerberosKey objects were added.The UGI constructor checks whether or not a keytab was used to login by looking if there are any KerberosKey objects in the Subject's private credentials. If there are; then isKeyTab is set to true; and otherwise it's set to false.Thus; in java 8 isKeyTab is always false given the current UGI implementation; which makes UGI#reloginFromKeytab fail silently.Attached patch will check for a KeyTab object on the Subject; instead of a KerberosKey object. This fixes relogins from kerberos keytabs on Oracle java 8; and works on Oracle java 7 as well.,Closed,Fixed,,Stephen Chu,Tobi Vollebregt,Sat; 5 Jul 2014 05:02:51 +0000,Tue; 30 Aug 2016 01:32:17 +0000,Mon; 10 Nov 2014 01:49:18 +0000,,2.6.0,2.6.1-candidate,,HADOOP-11287;HADOOP-11090;HBASE-7608;SLIDER-1010,https://issues.apache.org/jira/browse/HADOOP-10786
HADOOP-10787,Improvement,Blocker,scripts,Rename/remove non-HADOOP_*; etc from the shell scripts,We should make an effort to clean up the shell env var name space by removing unsafe variables.  See comments for list.,Resolved,Fixed,,Allen Wittenauer,Allen Wittenauer,Sun; 6 Jul 2014 15:35:05 +0000,Thu; 12 May 2016 18:26:05 +0000,Wed; 4 Nov 2015 10:27:41 +0000,,3.0.0-alpha1,scripts,HADOOP-12366;HADOOP-10788;HADOOP-11460,HADOOP-11010,https://issues.apache.org/jira/browse/HADOOP-10787
HADOOP-10788,Improvement,Major,scripts,Rewrite kms to use new shell framework,kms was not rewritten to use the new shell framework.  It should be reworked to take advantage of it.,Resolved,Fixed,,John Smith,Allen Wittenauer,Sun; 6 Jul 2014 15:37:16 +0000,Thu; 12 May 2016 18:26:06 +0000,Fri; 2 Jan 2015 18:51:33 +0000,,3.0.0-alpha1,scripts,HADOOP-10787;HDFS-7460;HADOOP-11331,HADOOP-11300;HADOOP-11010,https://issues.apache.org/jira/browse/HADOOP-10788
HADOOP-10789,Bug,Minor,build,Print revision number in Hadoop QA comment,The trunk revision number to test patch is not printed in Hadoop QA.+1 overall.  Here are the results of testing the latest attachment   http://issues.apache.org/jira/secure/attachment/12654270/MAPREDUCE-5868.4.patch  against trunk revision .,Resolved,Not A Problem,,Unassigned,Akira Ajisaka,Mon; 7 Jul 2014 07:50:16 +0000,Thu; 12 May 2016 18:25:56 +0000,Thu; 28 Aug 2014 08:20:19 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10789
HADOOP-10790,Bug,Major,documentation,http://hadoop.apache.org/index.html only lists versions up to 2.2,Although the releases page is current; the root index.html page stops at release 2.2,Resolved,Fixed,,Unassigned,Steve Loughran,Mon; 7 Jul 2014 11:19:50 +0000,Sat; 6 Dec 2014 16:46:10 +0000,Sat; 6 Dec 2014 16:19:07 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10790
HADOOP-10791,Improvement,Major,security,AuthenticationFilter should support externalizing the secret for signing and provide rotation support,It should be possible to externalize the secret used to sign the hadoop-auth cookies.In the case of WebHDFS the shared secret used by NN and DNs could be used. In the case of Oozie HA; the secret could be stored in Oozie HA control data in ZooKeeper.In addition; it is desirable for the secret to change periodically; this means that the AuthenticationService should remember a previous secret for the max duration of hadoop-auth cookie.,Closed,Fixed,,Robert Kanter,Alejandro Abdelnur,Mon; 7 Jul 2014 17:42:54 +0000,Mon; 9 Feb 2015 23:28:13 +0000,Tue; 5 Aug 2014 21:25:47 +0000,,2.4.1,,,HADOOP-11567,https://issues.apache.org/jira/browse/HADOOP-10791
HADOOP-10792,Improvement,Major,fs,Add FileSystem#closeIfNotReferred method,FileSystem#close closes FileSystem even if the same instance of FileSystem is referred by someone.For instance; a library using FileSystem calls FileSystem.get; and a program using the library calls FileSystem.get; both of instances of FileSystem is same. When the library and the program is implemented as different threads and one calls FileSystem.close; another fails most of operations of FileSystem.So; we need the method like cloesIfNotReferred; which closes FileSystem only if a instance of FileSystem is not referred by anyone.,Resolved,Won't Fix,,Unassigned,Kousuke Saruta,Mon; 7 Jul 2014 19:21:44 +0000,Thu; 18 May 2017 12:32:41 +0000,Thu; 18 May 2017 12:32:41 +0000,,,,,HADOOP-4655,https://issues.apache.org/jira/browse/HADOOP-10792
HADOOP-10793,Improvement,Major,security,KeyShell args should use single-dash style,Follow-on from HADOOP-10736 as per Andrew Wang - the key shell uses the gnu double dash style for command line args; while other command line programs use a single dash.  Consider changing this; and consider another argument parsing scheme; like the CommandLine class.,Closed,Fixed,,Andrew Wang,Mike Yoder,Tue; 8 Jul 2014 07:07:37 +0000,Thu; 12 May 2016 18:26:26 +0000,Fri; 1 Aug 2014 17:46:33 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10793
YARN-2327,Bug,Major,,YARN should warn about nodes with poor clock synchronization,YARN should warn about nodes with poor clock synchronization.YARN relies on approximate clock synchronization to report certain elapsed time statistics (see YARN-2251); but we currently don't warn if this assumption is violated.,Open,Unresolved,,Unassigned,Zhijie Shen,Tue; 8 Jul 2014 07:35:55 +0000,Wed; 3 Sep 2014 22:01:15 +0000,,,,,,YARN-2251,https://issues.apache.org/jira/browse/YARN-2327
HADOOP-10795,Bug,Major,build,unale to build hadoop 2.4.1(redhat5.8 x64),unale to build hadoop 2.4.1(redhat5.8 x64)WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-project:pom:2.4.1WARNING 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-enforcer-plugin @ line 1015; column 15WARNING Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-common:jar:2.4.1WARNING 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ line 479; column 15WARNING 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-enforcer-plugin @ org.apache.hadoop:hadoop-project:2.4.1; /home/software/Server/hadoop-2.4.1-src/hadoop-project/pom.xml; line 1015; column 15WARNING] /home/software/Server/hadoop-2.4.1-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FastByteComparisons.java:25;15 Unsafe is internal proprietary API and may be removed in a future releaseWARNING /home/software/Server/hadoop-2.4.1-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java:42;15 Unsafe is internal proprietary API and may be removed in a future releaseWARNING /home/software/Server/hadoop-2.4.1-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:21;15 Signal is internal proprietary API and may be removed in a future releaseWARNING /home/software/Server/hadoop-2.4.1-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:22;15 SignalHandler is internal proprietary API and may be removed in a future releaseWARNING /home/software/Server/hadoop-2.4.1-src/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/ssl/KeyStoreTestUtil.java:22;24 AlgorithmId is internal proprietary API and may be removed in a future releaseWARNING /home/software/Server/hadoop-2.4.1-src/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/ssl/KeyStoreTestUtil.java:23;24 CertificateAlgorithmId is internal proprietary API and may be removed in a future releasetestROBufferDirAndRWBufferDir1(org.apache.hadoop.fs.TestLocalDirAllocator)  Time elapsed: 0.014 sec  &lt; FAILURE!ERROR Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.16:test (default-test) on project hadoop-common: There are test failures.ERROR,Resolved,Invalid,,Unassigned,moses.wang,Tue; 8 Jul 2014 09:43:34 +0000,Sat; 14 Mar 2015 00:57:08 +0000,Sat; 14 Mar 2015 00:57:08 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10795
HADOOP-10796,Bug,Major,,Porting Hadoop to FreeBSD,"This is ""high""-level issue to accumulate other issues about porting Hadoop to FreeBSD in one place.As suggested by Steve Loughran.",Open,Unresolved,,Unassigned,Dmitry Sivachenko,Tue; 8 Jul 2014 11:03:46 +0000,Tue; 2 Aug 2016 23:19:43 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10796
HDFS-7755,Bug,Major,scripts,httpfs shell code has hardcoded path to bash,"Most of shell scripts use shebang ling in the following format:#!/usr/bin/env bashBut some scripts contain hardcoded ""/bin/bash"" which is not portable.Please use #!/usr/bin/env bash instead for portability.PS: it would be much better to switch to standard Bourne Shell /bin/sh; do these scripts really need bash?",Resolved,Fixed,,Dmitry Sivachenko,Dmitry Sivachenko,Tue; 8 Jul 2014 11:38:46 +0000,Thu; 12 May 2016 18:19:02 +0000,Mon; 9 Feb 2015 21:06:09 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HDFS-7755
HADOOP-10798,Bug,Minor,,globStatus() should always return a sorted list of files,"(FileSystem) globStatus() does not return a sorted file list anymore.But the API says: "" ... Results are sorted by their names.""Seems to be lost; when the Globber Object was introduced. Can't find a sort in actual code.code to check this behavior:",Resolved,Fixed,,Colin P. McCabe,Felix Borchers,Tue; 8 Jul 2014 13:22:01 +0000,Tue; 30 Aug 2016 01:32:15 +0000,Tue; 30 Jun 2015 23:43:15 +0000,,2.3.0,BB2015-05-TBR,,HADOOP-12177,https://issues.apache.org/jira/browse/HADOOP-10798
HADOOP-10799,Sub-task,Major,security,Refactor HTTP delegation token logic from httpfs into reusable code in hadoop-common.,,Resolved,Invalid,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 8 Jul 2014 16:44:58 +0000,Thu; 12 May 2016 18:27:45 +0000,Fri; 8 Aug 2014 06:10:53 +0000,,3.0.0-alpha1,,HADOOP-10800;HADOOP-10835,HADOOP-10453;HADOOP-10832,https://issues.apache.org/jira/browse/HADOOP-10799
HADOOP-10800,Sub-task,Major,security,Refactor HttpFS to use hadoop-common HTTP delegation token support.,,Resolved,Invalid,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 8 Jul 2014 16:45:34 +0000,Fri; 8 Aug 2014 06:11:05 +0000,Fri; 8 Aug 2014 06:11:05 +0000,,,,HADOOP-10799,,https://issues.apache.org/jira/browse/HADOOP-10800
HADOOP-10801,Bug,Major,documentation,Fix dead link in site.xml,Documents for FileSystem API definition were created in HADOOP-9361 but not linked.In hadoop-project/src/site/site.xml; should be,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Tue; 8 Jul 2014 17:13:21 +0000,Fri; 15 Aug 2014 05:39:46 +0000,Wed; 9 Jul 2014 10:57:41 +0000,,2.5.0,newbie,,HADOOP-9361,https://issues.apache.org/jira/browse/HADOOP-10801
HADOOP-10802,Improvement,Major,,Add metrics for KMS client and server encrypted key caches,HADOOP-10720 is adding KMS server and client caches for encrypted keys for performance reasons. It would be good to add metrics to make sure that the cache is working as expected; and to inform future dynamic cache sizing and refilling policies.,Open,Unresolved,,Arun Suresh,Andrew Wang,Tue; 8 Jul 2014 20:19:52 +0000,Thu; 12 May 2016 18:27:37 +0000,,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10802
HADOOP-10803,Sub-task,Major,security,Update OpensslCipher#getInstance to accept CipherSuite#name format.,The name format of org.apache.hadoop.crypto.CipherSuite is the same as   transformation of javax.crypto.Cipher#getInstance. Let's update the OpensslCipher#getInstance to accept same format; then we can get OpensslCipher instance using CipherSuite.,Resolved,Fixed,,Yi Liu,Yi Liu,Tue; 8 Jul 2014 22:28:00 +0000,Thu; 10 Jul 2014 07:16:23 +0000,Thu; 10 Jul 2014 06:28:14 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10803
HADOOP-10804,Bug,Blocker,build,Jenkins is failing due to the upgrade of svn client,Now Jenkins is failing with the following message: https://builds.apache.org/job/PreCommit-HADOOP-Build/4231/console,Resolved,Fixed,,Giridharan Kesavan,Akira Ajisaka,Wed; 9 Jul 2014 02:27:13 +0000,Thu; 10 Jul 2014 01:13:23 +0000,Thu; 10 Jul 2014 01:13:23 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10804
HADOOP-10805,Sub-task,Major,native,ndfs hdfsDelete should check the return boolean,The delete RPC to the NameNode returns a boolean.  We need to check this in the pure native client to ensure that the delete actually succeeded.,Resolved,Fixed,,Colin P. McCabe,Colin P. McCabe,Wed; 9 Jul 2014 19:18:16 +0000,Thu; 10 Jul 2014 01:30:46 +0000,Wed; 9 Jul 2014 19:29:39 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10805
HADOOP-10806,Sub-task,Major,native,ndfs: need to implement umask; pass permission bits to hdfsCreateDirectory,We need to pass in permission bits to hdfsCreateDirectory.  Also; we need to read fs.permissions.umask-mode so that we know what to mask off of the permission bits (umask is always implemented client-side),Resolved,Fixed,,Colin P. McCabe,Colin P. McCabe,Wed; 9 Jul 2014 19:27:14 +0000,Sat; 12 Jul 2014 00:08:53 +0000,Sat; 12 Jul 2014 00:08:53 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10806
HADOOP-10807,Bug,Minor,util,GenericOptionsParser needs updating for Hadoop 2.x+,The options presented to users; the comments; etc; are all woefully out of date and don't reflect the current reality. These should be updated for Hadoop 2.x and up.,Open,Unresolved,,Unassigned,Allen Wittenauer,Wed; 9 Jul 2014 20:47:20 +0000,Thu; 10 Jul 2014 10:12:26 +0000,,,,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10807
HADOOP-10808,Improvement,Minor,native,Remove unused native code for munlock.,The Centralized Cache Management project added a native function for calling munlock.  This function is unused though; because Centralized Cache Management calls munmap; which implicitly unlocks the memory too.  Let's remove the unused code.  This is a private/unstable class; so there is no backwards-compatibility concern.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Wed; 9 Jul 2014 21:43:24 +0000,Thu; 12 May 2016 18:21:37 +0000,Thu; 10 Jul 2014 17:14:49 +0000,,2.5.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10808
HADOOP-10809,Improvement,Major,tools,hadoop-azure: page blob support,"Azure Blob Storage provides two flavors: block-blobs and page-blobs.  Block-blobs are the general purpose kind that support convenient APIs and are the basis for the Azure Filesystem for Hadoop (see HADOOP-9629).Page-blobs use the same namespace as block-blobs but provide a different low-level feature set.  Most importantly; page-blobs can cope with an effectively infinite number of small accesses whereas block-blobs can only tolerate 50K appends before relatively manual rewriting of the data is necessary.  A simple analogy is that page-blobs are like a regular disk and the basic API is like a low-level device driver.See http://msdn.microsoft.com/en-us/library/azure/ee691964.aspx for some introductory material.The primary driving scenario for page-blob support is for HBase transaction log files which require an access pattern of many small writes.  Additional scenarios can also be supported.Configuration:The Hadoop Filesystem abstraction needs a mechanism so that file-create can determine whether to create a block- or page-blob.  To permit scenarios where application code doesn't know about the details of azure storage we would like the configuration to be Aspect-style; ie configured by the Administrator and transparent to the application. The current solution is to use hadoop configuration to declare a list of page-blob folders  Azure Filesystem for Hadoop will create files in these folders using page-blob flavor.  The configuration key is ""fs.azure.page.blob.dir""; and description can be found in AzureNativeFileSystemStore.java.Code changes:	refactor of basic Azure Filesystem code to use a general BlobWrapper and specialized BlockBlobWrapper vs PageBlobWrapper	introduction of PageBlob support (read; write; etc)	miscellaneous changes such as umask handling; implementation of createNonRecursive(); flush/hflush/hsync.	new unit tests.Credit for the primary patch: Dexter Bradshaw; Mostafa Elhemali; Eric Hanson; Mike Liddell.Also included in the patch is support for atomic folder rename over the Azure blob store through the Azure file system layer for Hadoop. See the README file for more details; including how to use the fs.azure.atomic.rename.dir configuration variable to control where atomic folder rename logic is applied. By default; folders under /hbase have atomic rename applied; which is needed for correct operation of HBase.",Closed,Fixed,HADOOP-10206,Eric Hanson,Mike Liddell,Wed; 9 Jul 2014 22:44:53 +0000,Fri; 10 Apr 2015 20:04:20 +0000,Thu; 18 Dec 2014 00:18:44 +0000,,,,,HADOOP-9629,https://issues.apache.org/jira/browse/HADOOP-10809
HADOOP-10810,Bug,Minor,native,Clean up native code compilation warnings.,There are several compilation warnings coming from the native code on both Linux and Windows.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Thu; 10 Jul 2014 00:09:45 +0000,Thu; 12 May 2016 18:21:20 +0000,Mon; 14 Jul 2014 20:58:26 +0000,,2.5.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10810
HADOOP-10811,New Feature,Minor,conf,Allow classes to be reloaded at runtime,Currently hadoop loads its classes and caches them in the Configuration class. Even if the user swaps a class's jar at runtime; hadoop will continue to use the cached classes when using reflection to instantiate objects. This limits the usefulness of things like HADOOP-10285; because the admin would need to restart each time they wanted to change their queue class.This patch is to add a way to refresh the class cache; by creating a new refresh handler to do so (using HADOOP-10376),Open,Unresolved,,Chris Li,Chris Li,Thu; 10 Jul 2014 03:29:47 +0000,Thu; 12 May 2016 18:21:22 +0000,,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10811
HADOOP-10812,Improvement,Trivial,,Delegate KeyProviderExtension#toString to underlying KeyProvider,Would be nice to delegate this; else we get the default reference toString,Closed,Fixed,,Andrew Wang,Andrew Wang,Thu; 10 Jul 2014 00:58:23 +0000,Thu; 12 May 2016 18:21:22 +0000,Thu; 10 Jul 2014 08:10:57 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10812
HADOOP-10813,Improvement,Minor,fs,Define general filesystem exceptions (usable by any HCFS),While Hadoop defines filesystem API which makes possible to use differentfilesystem implementation than HDFS (aka HCFS); we are missing HCFSexceptions for some failures wrt to namenode federation.For namenode federation; one can specify different namenode like this:hdfs://namenode_hostname/some/path. So when the given namenode doesn'texist; UnknownHostException is thrown: Which is ok for HDFS; but there are other hadoop filesystem with differentimplementation and raising UnknownHostException doesn't make sense forthem. For example the following path: glusterfs://bugcheck/foo/bar pointsto file /foo/bar on GlusterFS volume named bugcheck. That said; themeaning is the same compared to HDFS; both namenode hostname and glusterfsvolume specifies different filesystem tree available for Hadoop.Would it make sense to define general HCFS exception which would wrap suchcases so that it would be possible to fail in the same way when givenfilesystem tree is not available/defined; not matter which hadoop filesystemis used?,Open,Unresolved,,Unassigned,Martin Bukatovic,Thu; 10 Jul 2014 17:20:59 +0000,Fri; 28 Nov 2014 14:54:34 +0000,,,2.2.0,hcfs,,,https://issues.apache.org/jira/browse/HADOOP-10813
HADOOP-10814,Bug,Major,,Update Tomcat version used by HttpFS and KMS to latest 6.x version,KMS and HttpFS are using Tomcat 6.0.37; we should move it to 6.0.41 to get bug fixes and security fixes.We should add a property with the tomcat version in the hadoop-project POM and use that property from KMS and HttpFS.,Closed,Fixed,HDFS-4836,Robert Kanter,Alejandro Abdelnur,Thu; 10 Jul 2014 18:14:21 +0000,Mon; 1 Dec 2014 03:09:29 +0000,Fri; 29 Aug 2014 18:54:06 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10814
HADOOP-10815,Improvement,Major,native,Implement Windows equivalent of mlock.,To support HDFS Centralized Cache Management on Windows; we need a native code function for Windows that is equivalent to POSIX mlock.,Closed,Fixed,,Chris Nauroth,Chris Nauroth,Thu; 10 Jul 2014 19:09:17 +0000,Thu; 12 May 2016 18:22:05 +0000,Fri; 11 Jul 2014 03:01:25 +0000,,2.5.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10815
HADOOP-10816,Bug,Major,security,KeyShell returns -1 on error to the shell; should be 1,I've seen this in several places now - commands returning -1 on failure to the shell. It's a bug. Someone confused their posix style returns (0 on success;  0 on failure) with program returns; which are an unsigned character. Thus; a return of -1 actually becomes 255 to the shell. A return value of 1 instead of -1 does the right thing.,Closed,Fixed,,Mike Yoder,Mike Yoder,Fri; 11 Jul 2014 22:29:15 +0000,Thu; 12 May 2016 18:23:18 +0000,Thu; 17 Jul 2014 00:49:29 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10816
HADOOP-10817,Improvement,Major,security,ProxyUsers configuration should support configurable prefixes ,Currently ProxyUsers and the ImpersonationProvider are hardcoded to use hadoop.proxyuser. prefixes for loading proxy user configuration.Adding the possibility of using a custom prefix will enable reusing the ProxyUsers class from other components (i.e. HttpFS and KMS).,Closed,Fixed,HDFS-7362,Alejandro Abdelnur,Alejandro Abdelnur,Fri; 11 Jul 2014 22:32:19 +0000,Thu; 12 May 2016 18:23:17 +0000,Fri; 18 Jul 2014 19:43:42 +0000,,3.0.0-alpha1,,HADOOP-10835,HIVE-7184,https://issues.apache.org/jira/browse/HADOOP-10817
HADOOP-10818,Sub-task,Major,native,native client: refactor URI code to be clearer,Refactor the common/uri.c code to be a bit clearer.  We should just be able to refer to user_info; auth; port; path; etc. fields in the structure; rather than calling accessors.  hdfsBuilder should just have a connection URI rather than separate fields for all these things.,Resolved,Fixed,,Colin P. McCabe,Colin P. McCabe,Sat; 12 Jul 2014 00:48:30 +0000,Tue; 22 Jul 2014 17:55:12 +0000,Tue; 22 Jul 2014 17:55:12 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10818
HADOOP-10819,Bug,Major,build,build hadoop on 64 bit linux failure ..,"I have insall all pre dependency software for building the hadoop 64bit native lib. like: cmake lzo-devel  zlib-devel  gcc autoconf automake libtool   ncurses-devel openssl-deve  and ant. then run : mvn clean package -DskipTests -Pdist;native -Dtar -e  get Exception :INFO INFO  native-maven-plugin:1.0-alpha-7:javah (default) @ hadoop-common   12; 2014 3:08:13   org.sonatype.guice.bean.reflect.Logs$JULSink warn : Error injecting: org.codehaus.mojo.natives.javah.JavahExecutablejava.lang.NoClassDefFoundError: org/codehaus/plexus/util/cli/Commandline	at java.lang.Class.getDeclaredMethods0(Native Method)	at java.lang.Class.privateGetDeclaredMethods(Class.java:2531)	at java.lang.Class.getDeclaredMethods(Class.java:1855)	at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:664)	at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:358)	at com.google.inject.internal.ConstructorBindingImpl.getInternalDependencies(ConstructorBindingImpl.java:155)	at com.google.inject.internal.InjectorImpl.getInternalDependencies(InjectorImpl.java:585)	at com.google.inject.internal.InjectorImpl.cleanup(InjectorImpl.java:542)	at com.google.inject.internal.InjectorImpl.initializeJitBinding(InjectorImpl.java:528)	at com.google.inject.internal.InjectorImpl.createJustInTimeBinding(InjectorImpl.java:833)	at com.google.inject.internal.InjectorImpl.createJustInTimeBindingRecursive(InjectorImpl.java:758)	at com.google.inject.internal.InjectorImpl.getJustInTimeBinding(InjectorImpl.java:255)	at com.google.inject.internal.InjectorImpl.getBindingOrThrow(InjectorImpl.java:204)	at com.google.inject.internal.InjectorImpl.getProviderOrThrow(InjectorImpl.java:954)	at com.google.inject.internal.InjectorImpl.getProvider(InjectorImpl.java:987)	at com.google.inject.internal.InjectorImpl.getProvider(InjectorImpl.java:950)	at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1000)	at org.sonatype.guice.bean.reflect.AbstractDeferredClass.get(AbstractDeferredClass.java:45)	at com.google.inject.internal.ProviderInternalFactory.provision(ProviderInternalFactory.java:84)	at com.google.inject.internal.InternalFactoryToInitializableAdapter.provision(InternalFactoryToInitializableAdapter.java:52)	at com.google.inject.internal.ProviderInternalFactory$1.call(ProviderInternalFactory.java:70)	at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:100)	at org.sonatype.guice.plexus.lifecycles.PlexusLifecycleManager.onProvision(PlexusLifecycleManager.java:138)	at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:108)	at com.google.inject.internal.ProvisionListenerStackCallback.provision(ProvisionListenerStackCallback.java:55)	at com.google.inject.internal.ProviderInternalFactory.circularGet(ProviderInternalFactory.java:68)	at com.google.inject.internal.InternalFactoryToInitializableAdapter.get(InternalFactoryToInitializableAdapter.java:45)	at com.google.inject.internal.InjectorImpl$3$1.call(InjectorImpl.java:965)	at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1011)	at com.google.inject.internal.InjectorImpl$3.get(InjectorImpl.java:961)	at com.google.inject.Scopes$1$1.get(Scopes.java:59)	at org.sonatype.guice.bean.locators.LazyBeanEntry.getValue(LazyBeanEntry.java:83)	at org.sonatype.guice.plexus.locators.LazyPlexusBean.getValue(LazyPlexusBean.java:49)	at java.util.AbstractMap.get(AbstractMap.java:182)	at org.codehaus.mojo.natives.manager.DefaultJavahManager.getJavah(DefaultJavahManager.java:62)	at org.codehaus.mojo.natives.plugin.NativeJavahMojo.getJavah(NativeJavahMojo.java:197)	at org.codehaus.mojo.natives.plugin.NativeJavahMojo.execute(NativeJavahMojo.java:179)	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)	at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)	at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)Caused by: java.lang.ClassNotFoundException: org.codehaus.plexus.util.cli.Commandline	at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass(SelfFirstStrategy.java:50)	at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:244)	at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:230)	... 58 moreINFO ------------------------------------------------------------------------INFO Reactor Summary:INFO INFO Apache Hadoop Main ................................ SUCCESS 1.126sINFO Apache Hadoop Project POM ......................... SUCCESS 0.755sINFO Apache Hadoop Annotations ......................... SUCCESS 2.958sINFO Apache Hadoop Assemblies .......................... SUCCESS 0.251sINFO Apache Hadoop Project Dist POM .................... SUCCESS 1.844sINFO Apache Hadoop Maven Plugins ....................... SUCCESS 2.539sINFO Apache Hadoop MiniKDC ............................. SUCCESS 2.329sINFO Apache Hadoop Auth ................................ SUCCESS 3.178sINFO Apache Hadoop Auth Examples ....................... SUCCESS 1.842sINFO Apache Hadoop Common .............................. FAILURE 8.757sINFO Apache Hadoop NFS ................................. SKIPPEDINFO Apache Hadoop Common Project ...................... SKIPPEDINFO Apache Hadoop HDFS ................................ SKIPPEDINFO Apache Hadoop HttpFS .............................. SKIPPEDINFO Apache Hadoop HDFS BookKeeper Journal ............. SKIPPEDINFO Apache Hadoop HDFS-NFS ............................ SKIPPEDINFO Apache Hadoop HDFS Project ........................ SKIPPEDINFO hadoop-yarn ....................................... SKIPPEDINFO hadoop-yarn-api ................................... SKIPPEDINFO hadoop-yarn-common ................................ SKIPPEDINFO hadoop-yarn-server ................................ SKIPPEDINFO hadoop-yarn-server-common ......................... SKIPPEDINFO hadoop-yarn-server-nodemanager .................... SKIPPEDINFO hadoop-yarn-server-web-proxy ...................... SKIPPEDINFO hadoop-yarn-server-applicationhistoryservice ...... SKIPPEDINFO hadoop-yarn-server-resourcemanager ................ SKIPPEDINFO hadoop-yarn-server-tests .......................... SKIPPEDINFO hadoop-yarn-client ................................ SKIPPEDINFO hadoop-yarn-applications .......................... SKIPPEDINFO hadoop-yarn-applications-distributedshell ......... SKIPPEDINFO hadoop-yarn-applications-unmanaged-am-launcher .... SKIPPEDINFO hadoop-yarn-site .................................. SKIPPEDINFO hadoop-yarn-project ............................... SKIPPEDINFO hadoop-mapreduce-client ........................... SKIPPEDINFO hadoop-mapreduce-client-core ...................... SKIPPEDINFO hadoop-mapreduce-client-common .................... SKIPPEDINFO hadoop-mapreduce-client-shuffle ................... SKIPPEDINFO hadoop-mapreduce-client-app ....................... SKIPPEDINFO hadoop-mapreduce-client-hs ........................ SKIPPEDINFO hadoop-mapreduce-client-jobclient ................. SKIPPEDINFO hadoop-mapreduce-client-hs-plugins ................ SKIPPEDINFO Apache Hadoop MapReduce Examples .................. SKIPPEDINFO hadoop-mapreduce .................................. SKIPPEDINFO Apache Hadoop MapReduce Streaming ................. SKIPPEDINFO Apache Hadoop Distributed Copy .................... SKIPPEDINFO Apache Hadoop Archives ............................ SKIPPEDINFO Apache Hadoop Rumen ............................... SKIPPEDINFO Apache Hadoop Gridmix ............................. SKIPPEDINFO Apache Hadoop Data Join ........................... SKIPPEDINFO Apache Hadoop Extras .............................. SKIPPEDINFO Apache Hadoop Pipes ............................... SKIPPEDINFO Apache Hadoop OpenStack support ................... SKIPPEDINFO Apache Hadoop Client .............................. SKIPPEDINFO Apache Hadoop Mini-Cluster ........................ SKIPPEDINFO Apache Hadoop Scheduler Load Simulator ............ SKIPPEDINFO Apache Hadoop Tools Dist .......................... SKIPPEDINFO Apache Hadoop Tools ............................... SKIPPEDINFO Apache Hadoop Distribution ........................ SKIPPEDINFO ------------------------------------------------------------------------INFO BUILD FAILUREINFO ------------------------------------------------------------------------INFO Total time: 27.249sINFO Finished at: Sat Jul 12 15:08:13 CST 2014INFO Final Memory: 54M/435MINFO ------------------------------------------------------------------------ERROR Failed to execute goal org.codehaus.mojo:native-maven-plugin:1.0-alpha-7:javah (default) on project hadoop-common: Execution default of goal org.codehaus.mojo:native-maven-plugin:1.0-alpha-7:javah failed: A required class was missing while executing org.codehaus.mojo:native-maven-plugin:1.0-alpha-7:javah: org/codehaus/plexus/util/cli/CommandlineERROR -----------------------------------------------------ERROR realm =    pluginorg.codehaus.mojo:native-maven-plugin:1.0-alpha-7ERROR strategy = org.codehaus.plexus.classworlds.strategy.SelfFirstStrategyERROR urls0 = file:/home/suan/.m2/repository/org/codehaus/mojo/native-maven-plugin/1.0-alpha-7/native-maven-plugin-1.0-alpha-7.jarERROR urls1 = file:/home/suan/.m2/repository/bcel/bcel/5.1/bcel-5.1.jarERROR urls2 = file:/home/suan/.m2/repository/regexp/regexp/1.2/regexp-1.2.jarERROR urls3 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-utils/2.0.1/plexus-utils-2.0.1.jarERROR urls4 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-api/1.0-alpha-7/maven-native-api-1.0-alpha-7.jarERROR urls5 = file:/home/suan/.m2/repository/backport-util-concurrent/backport-util-concurrent/3.1/backport-util-concurrent-3.1.jarERROR urls6 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-javah/1.0-alpha-7/maven-native-javah-1.0-alpha-7.jarERROR urls7 = file:/home/suan/.m2/repository/commons-lang/commons-lang/2.4/commons-lang-2.4.jarERROR urls8 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-generic-c/1.0-alpha-7/maven-native-generic-c-1.0-alpha-7.jarERROR urls9 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-msvc/1.0-alpha-7/maven-native-msvc-1.0-alpha-7.jarERROR urls10 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-bcc/1.0-alpha-7/maven-native-bcc-1.0-alpha-7.jarERROR urls11 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-manager/1.0-alpha-7/maven-native-manager-1.0-alpha-7.jarERROR urls12 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-archiver/1.0-alpha-12/plexus-archiver-1.0-alpha-12.jarERROR urls13 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-io/1.0-alpha-4/plexus-io-1.0-alpha-4.jarERROR Number of foreign imports: 1ERROR import: Entry[import  from realm ClassRealm[projectorg.apache.hadoop:hadoop-main:2.4.1; parent: ClassRealmmaven.api; parent: null]]ERROR ERROR -----------------------------------------------------: org.codehaus.plexus.util.cli.CommandlineERROR - Help 1org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.codehaus.mojo:native-maven-plugin:1.0-alpha-7:javah (default) on project hadoop-common: Execution default of goal org.codehaus.mojo:native-maven-plugin:1.0-alpha-7:javah failed: A required class was missing while executing org.codehaus.mojo:native-maven-plugin:1.0-alpha-7:javah: org/codehaus/plexus/util/cli/Commandline-----------------------------------------------------realm =    pluginorg.codehaus.mojo:native-maven-plugin:1.0-alpha-7strategy = org.codehaus.plexus.classworlds.strategy.SelfFirstStrategyurls0 = file:/home/suan/.m2/repository/org/codehaus/mojo/native-maven-plugin/1.0-alpha-7/native-maven-plugin-1.0-alpha-7.jarurls1 = file:/home/suan/.m2/repository/bcel/bcel/5.1/bcel-5.1.jarurls2 = file:/home/suan/.m2/repository/regexp/regexp/1.2/regexp-1.2.jarurls3 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-utils/2.0.1/plexus-utils-2.0.1.jarurls4 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-api/1.0-alpha-7/maven-native-api-1.0-alpha-7.jarurls5 = file:/home/suan/.m2/repository/backport-util-concurrent/backport-util-concurrent/3.1/backport-util-concurrent-3.1.jarurls6 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-javah/1.0-alpha-7/maven-native-javah-1.0-alpha-7.jarurls7 = file:/home/suan/.m2/repository/commons-lang/commons-lang/2.4/commons-lang-2.4.jarurls8 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-generic-c/1.0-alpha-7/maven-native-generic-c-1.0-alpha-7.jarurls9 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-msvc/1.0-alpha-7/maven-native-msvc-1.0-alpha-7.jarurls10 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-bcc/1.0-alpha-7/maven-native-bcc-1.0-alpha-7.jarurls11 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-manager/1.0-alpha-7/maven-native-manager-1.0-alpha-7.jarurls12 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-archiver/1.0-alpha-12/plexus-archiver-1.0-alpha-12.jarurls13 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-io/1.0-alpha-4/plexus-io-1.0-alpha-4.jarNumber of foreign imports: 1import: Entry[import  from realm ClassRealm[projectorg.apache.hadoop:hadoop-main:2.4.1; parent: ClassRealmmaven.api; parent: null]]-----------------------------------------------------	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:225)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)	at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)	at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)Caused by: org.apache.maven.plugin.PluginExecutionException: Execution default of goal org.codehaus.mojo:native-maven-plugin:1.0-alpha-7:javah failed: A required class was missing while executing org.codehaus.mojo:native-maven-plugin:1.0-alpha-7:javah: org/codehaus/plexus/util/cli/Commandline-----------------------------------------------------realm =    pluginorg.codehaus.mojo:native-maven-plugin:1.0-alpha-7strategy = org.codehaus.plexus.classworlds.strategy.SelfFirstStrategyurls0 = file:/home/suan/.m2/repository/org/codehaus/mojo/native-maven-plugin/1.0-alpha-7/native-maven-plugin-1.0-alpha-7.jarurls1 = file:/home/suan/.m2/repository/bcel/bcel/5.1/bcel-5.1.jarurls2 = file:/home/suan/.m2/repository/regexp/regexp/1.2/regexp-1.2.jarurls3 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-utils/2.0.1/plexus-utils-2.0.1.jarurls4 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-api/1.0-alpha-7/maven-native-api-1.0-alpha-7.jarurls5 = file:/home/suan/.m2/repository/backport-util-concurrent/backport-util-concurrent/3.1/backport-util-concurrent-3.1.jarurls6 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-javah/1.0-alpha-7/maven-native-javah-1.0-alpha-7.jarurls7 = file:/home/suan/.m2/repository/commons-lang/commons-lang/2.4/commons-lang-2.4.jarurls8 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-generic-c/1.0-alpha-7/maven-native-generic-c-1.0-alpha-7.jarurls9 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-msvc/1.0-alpha-7/maven-native-msvc-1.0-alpha-7.jarurls10 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-bcc/1.0-alpha-7/maven-native-bcc-1.0-alpha-7.jarurls11 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-manager/1.0-alpha-7/maven-native-manager-1.0-alpha-7.jarurls12 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-archiver/1.0-alpha-12/plexus-archiver-1.0-alpha-12.jarurls13 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-io/1.0-alpha-4/plexus-io-1.0-alpha-4.jarNumber of foreign imports: 1import: Entry[import  from realm ClassRealm[projectorg.apache.hadoop:hadoop-main:2.4.1; parent: ClassRealmmaven.api; parent: null]]-----------------------------------------------------	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:127)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)	... 19 moreCaused by: org.apache.maven.plugin.PluginContainerException: A required class was missing while executing org.codehaus.mojo:native-maven-plugin:1.0-alpha-7:javah: org/codehaus/plexus/util/cli/Commandline-----------------------------------------------------realm =    pluginorg.codehaus.mojo:native-maven-plugin:1.0-alpha-7strategy = org.codehaus.plexus.classworlds.strategy.SelfFirstStrategyurls0 = file:/home/suan/.m2/repository/org/codehaus/mojo/native-maven-plugin/1.0-alpha-7/native-maven-plugin-1.0-alpha-7.jarurls1 = file:/home/suan/.m2/repository/bcel/bcel/5.1/bcel-5.1.jarurls2 = file:/home/suan/.m2/repository/regexp/regexp/1.2/regexp-1.2.jarurls3 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-utils/2.0.1/plexus-utils-2.0.1.jarurls4 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-api/1.0-alpha-7/maven-native-api-1.0-alpha-7.jarurls5 = file:/home/suan/.m2/repository/backport-util-concurrent/backport-util-concurrent/3.1/backport-util-concurrent-3.1.jarurls6 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-javah/1.0-alpha-7/maven-native-javah-1.0-alpha-7.jarurls7 = file:/home/suan/.m2/repository/commons-lang/commons-lang/2.4/commons-lang-2.4.jarurls8 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-generic-c/1.0-alpha-7/maven-native-generic-c-1.0-alpha-7.jarurls9 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-msvc/1.0-alpha-7/maven-native-msvc-1.0-alpha-7.jarurls10 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-bcc/1.0-alpha-7/maven-native-bcc-1.0-alpha-7.jarurls11 = file:/home/suan/.m2/repository/org/codehaus/mojo/natives/maven-native-manager/1.0-alpha-7/maven-native-manager-1.0-alpha-7.jarurls12 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-archiver/1.0-alpha-12/plexus-archiver-1.0-alpha-12.jarurls13 = file:/home/suan/.m2/repository/org/codehaus/plexus/plexus-io/1.0-alpha-4/plexus-io-1.0-alpha-4.jarNumber of foreign imports: 1import: Entry[import  from realm ClassRealm[projectorg.apache.hadoop:hadoop-main:2.4.1; parent: ClassRealmmaven.api; parent: null]]-----------------------------------------------------	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:125)	... 20 moreCaused by: java.lang.NoClassDefFoundError: org/codehaus/plexus/util/cli/Commandline	at java.lang.Class.getDeclaredMethods0(Native Method)	at java.lang.Class.privateGetDeclaredMethods(Class.java:2531)	at java.lang.Class.getDeclaredMethods(Class.java:1855)	at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:664)	at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:358)	at com.google.inject.internal.ConstructorBindingImpl.getInternalDependencies(ConstructorBindingImpl.java:155)	at com.google.inject.internal.InjectorImpl.getInternalDependencies(InjectorImpl.java:585)	at com.google.inject.internal.InjectorImpl.cleanup(InjectorImpl.java:542)	at com.google.inject.internal.InjectorImpl.initializeJitBinding(InjectorImpl.java:528)	at com.google.inject.internal.InjectorImpl.createJustInTimeBinding(InjectorImpl.java:833)	at com.google.inject.internal.InjectorImpl.createJustInTimeBindingRecursive(InjectorImpl.java:758)	at com.google.inject.internal.InjectorImpl.getJustInTimeBinding(InjectorImpl.java:255)	at com.google.inject.internal.InjectorImpl.getBindingOrThrow(InjectorImpl.java:204)	at com.google.inject.internal.InjectorImpl.getProviderOrThrow(InjectorImpl.java:954)	at com.google.inject.internal.InjectorImpl.getProvider(InjectorImpl.java:987)	at com.google.inject.internal.InjectorImpl.getProvider(InjectorImpl.java:950)	at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1000)	at org.sonatype.guice.bean.reflect.AbstractDeferredClass.get(AbstractDeferredClass.java:45)	at com.google.inject.internal.ProviderInternalFactory.provision(ProviderInternalFactory.java:84)	at com.google.inject.internal.InternalFactoryToInitializableAdapter.provision(InternalFactoryToInitializableAdapter.java:52)	at com.google.inject.internal.ProviderInternalFactory$1.call(ProviderInternalFactory.java:70)	at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:100)	at org.sonatype.guice.plexus.lifecycles.PlexusLifecycleManager.onProvision(PlexusLifecycleManager.java:138)	at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:108)	at com.google.inject.internal.ProvisionListenerStackCallback.provision(ProvisionListenerStackCallback.java:55)	at com.google.inject.internal.ProviderInternalFactory.circularGet(ProviderInternalFactory.java:68)	at com.google.inject.internal.InternalFactoryToInitializableAdapter.get(InternalFactoryToInitializableAdapter.java:45)	at com.google.inject.internal.InjectorImpl$3$1.call(InjectorImpl.java:965)	at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1011)	at com.google.inject.internal.InjectorImpl$3.get(InjectorImpl.java:961)	at com.google.inject.Scopes$1$1.get(Scopes.java:59)	at org.sonatype.guice.bean.locators.LazyBeanEntry.getValue(LazyBeanEntry.java:83)	at org.sonatype.guice.plexus.locators.LazyPlexusBean.getValue(LazyPlexusBean.java:49)	at java.util.AbstractMap.get(AbstractMap.java:182)	at org.codehaus.mojo.natives.manager.DefaultJavahManager.getJavah(DefaultJavahManager.java:62)	at org.codehaus.mojo.natives.plugin.NativeJavahMojo.getJavah(NativeJavahMojo.java:197)	at org.codehaus.mojo.natives.plugin.NativeJavahMojo.execute(NativeJavahMojo.java:179)	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)	... 20 moreCaused by: java.lang.ClassNotFoundException: org.codehaus.plexus.util.cli.Commandline	at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass(SelfFirstStrategy.java:50)	at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:244)	at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:230)	... 58 moreERROR ERROR Re-run Maven using the -X switch to enable full debug logging.ERROR ERROR For more information about the errors and possible solutions; please read the following articles:ERROR Help 1 http://cwiki.apache.org/confluence/display/MAVEN/PluginContainerExceptionERROR ERROR After correcting the problems; you can resume the build with the commandERROR   mvn goals -rf :hadoop-common",Open,Unresolved,,Unassigned,suanping,Sat; 12 Jul 2014 07:28:34 +0000,Mon; 21 Jul 2014 16:31:18 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10819
HADOOP-10820,Bug,Minor,,Throw an exception in GenericOptionsParser when passed an empty Path,"An empty token (e.g. ""a.jar;;b.jar"") in the -libjars option causes the current working directory to be recursively localized.Here's an example of this in action (using Hadoop 2.2.0):",Closed,Fixed,,zhihai xu,Alex Holmes,Sun; 13 Jul 2014 17:41:02 +0000,Mon; 1 Dec 2014 03:10:25 +0000,Tue; 12 Aug 2014 17:48:00 +0000,,2.2.0,,,,https://issues.apache.org/jira/browse/HADOOP-10820
HADOOP-10821,Task,Blocker,,Prepare the release notes for Hadoop 2.5.0,The release notes for 2.3.0+ (http://hadoop.apache.org/docs/r2.4.1/index.html) still talk about federation and MRv2being new features. We should update them.,Closed,Fixed,,Andrew Wang,Akira Ajisaka,Mon; 14 Jul 2014 07:12:16 +0000,Fri; 15 Aug 2014 05:39:52 +0000,Tue; 29 Jul 2014 00:49:43 +0000,,2.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-10821
HADOOP-10822,Improvement,Major,security,Refactor HTTP proxyuser support out of HttpFS into common,HttpFS implements HTTP proxyuser support inline in httpfs code.For HADOOP-10698 we need similar functionality for KMS.Not to duplicate code; we should refactor existing code to common.We should also leverage HADOOP-10817.,Resolved,Invalid,,Alejandro Abdelnur,Alejandro Abdelnur,Mon; 14 Jul 2014 22:37:26 +0000,Wed; 13 Aug 2014 20:25:48 +0000,Wed; 13 Aug 2014 20:25:48 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10822
HADOOP-10823,Bug,Major,,TestReloadingX509TrustManager is flaky,"Pasting the logError Messageexpected:2 but was:1Stacktracejunit.framework.AssertionFailedError: expected:2 but was:1	at junit.framework.Assert.fail(Assert.java:50)	at junit.framework.Assert.failNotEquals(Assert.java:287)	at junit.framework.Assert.assertEquals(Assert.java:67)	at junit.framework.Assert.assertEquals(Assert.java:199)	at junit.framework.Assert.assertEquals(Assert.java:205)	at org.apache.hadoop.security.ssl.TestReloadingX509TrustManager.testReload(TestReloadingX509TrustManager.java:112)Standard Output2014-07-06 06:12:21;170 WARN  ssl.ReloadingX509TrustManager (ReloadingX509TrustManager.java:run(197)) - Could not load truststore (keep using existing one) : java.io.EOFExceptionjava.io.EOFException	at java.io.DataInputStream.readInt(DataInputStream.java:375)	at sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:628)	at sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:38)	at java.security.KeyStore.load(KeyStore.java:1185)	at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:166)	at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run(ReloadingX509TrustManager.java:195)	at java.lang.Thread.run(Thread.java:662)",Resolved,Fixed,,Mingliang Liu,Ratandeep Ratti,Tue; 15 Jul 2014 07:20:00 +0000,Tue; 30 Aug 2016 01:32:11 +0000,Mon; 8 Aug 2016 18:12:31 +0000,,2.3.0,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10823
HADOOP-10824,Improvement,Major,security,Refactor KMSACLs to avoid locking,Currently KMSACLs is made thread safe using ReadWriteLock. It is possible to safely publish the acls collection using volatile.Similar refactoring has been done in HADOOP-10448,Closed,Fixed,,Benoy Antony,Benoy Antony,Tue; 15 Jul 2014 10:15:18 +0000,Mon; 1 Dec 2014 03:10:36 +0000,Wed; 16 Jul 2014 11:27:03 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10824
HADOOP-10825,Sub-task,Minor,conf,Refactor class creation logic in Configuration into nested class,This first patch refactors class creation inside Configuration into a nested class called ClassCreator (a pretty uninspired name; I m up for better naming suggestions). Since this is a refactor that adds no new features; I have not included a test.,Patch Available,Unresolved,,Chris Li,Chris Li,Tue; 15 Jul 2014 10:20:16 +0000,Wed; 6 May 2015 03:35:11 +0000,,,,BB2015-05-TBR,,,https://issues.apache.org/jira/browse/HADOOP-10825
HADOOP-10826,Improvement,Major,security,Iteration on KeyProviderFactory.serviceLoader  is thread-unsafe,KeyProviderFactory uses ServiceLoader framework to load KeyProviderFactory The ServiceLoader framework does lazy initialization of services which makes it thread unsafe. If accessed from multiple threads; it is better to synchronize the access.Similar synchronization has been done while loading compression codec providers via HADOOP-8406.,Closed,Fixed,,Benoy Antony,Benoy Antony,Tue; 15 Jul 2014 10:33:00 +0000,Mon; 1 Dec 2014 03:08:50 +0000,Tue; 22 Jul 2014 15:44:56 +0000,,,,,HADOOP-10829,https://issues.apache.org/jira/browse/HADOOP-10826
HADOOP-10827,Improvement,Major,security,Iteration on KeyProviderFactory.serviceLoader  is thread-unsafe,KeyProviderFactory uses ServiceLoader framework to load KeyProviderFactory The ServiceLoader framework does lazy initialization of services which makes it thread unsafe. If accessed from multiple threads; it is better to synchronize the access.Similar synchronization has been done while loading compression codec providers via HADOOP-8406.,Resolved,Duplicate,NULL,Benoy Antony,Benoy Antony,Tue; 15 Jul 2014 10:34:11 +0000,Tue; 15 Jul 2014 10:37:26 +0000,Tue; 15 Jul 2014 10:37:26 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10827
HADOOP-10828,Sub-task,Minor,conf,Allow user to reload classes,"This patch should allow reloading classes at runtime. The software will maintain a set of extra jars to load classes from. The interface for adding and removing from this set will be done using the refresh protocol and a RefreshHandler.Considerations include:	Which classes are eligible for reloading?	Does reloading change existing classes?	Does reloading affect classes created from existing Configuration objects?Details will be added in comments for each patch preview.",Open,Unresolved,,Chris Li,Chris Li,Tue; 15 Jul 2014 11:09:30 +0000,Tue; 15 Jul 2014 11:24:18 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10828
HADOOP-10829,Bug,Major,security,Iteration on CredentialProviderFactory.serviceLoader  is thread-unsafe,CredentialProviderFactory uses ServiceLoader framework to load CredentialProviderFactory The ServiceLoader framework does lazy initialization of services which makes it thread unsafe. If accessed from multiple threads; it is better to synchronize the access.Similar synchronization has been done while loading compression codec providers via HADOOP-8406.,Resolved,Fixed,,Benoy Antony,Benoy Antony,Tue; 15 Jul 2014 17:00:35 +0000,Thu; 31 Aug 2017 23:26:33 +0000,Tue; 11 Jul 2017 01:01:59 +0000,,2.6.0,BB2015-05-TBR,,HADOOP-10607;HADOOP-10826,https://issues.apache.org/jira/browse/HADOOP-10829
HADOOP-10830,Bug,Major,security,Missing lock in JavaKeyStoreProvider.createCredentialEntry,JavaKeyStoreProvider uses ReentrantReadWriteLock  to provide thread safety.The createCredentialEntry should hold writeLock before adding the entry.,Closed,Fixed,,Benoy Antony,Benoy Antony,Tue; 15 Jul 2014 17:03:20 +0000,Mon; 1 Dec 2014 03:09:58 +0000,Wed; 23 Jul 2014 18:43:48 +0000,,2.4.1,,,HADOOP-10607,https://issues.apache.org/jira/browse/HADOOP-10830
HADOOP-10831,Bug,Major,security,UserProvider is not thread safe,While JavaKeyStoreProvider is thread safe; UserProvider is not thread safe.,Resolved,Fixed,,Benoy Antony,Benoy Antony,Tue; 15 Jul 2014 17:04:42 +0000,Thu; 12 May 2016 18:25:17 +0000,Tue; 15 Jul 2014 18:17:03 +0000,,2.4.1,,,HADOOP-10607,https://issues.apache.org/jira/browse/HADOOP-10831
HADOOP-10832,Task,Major,,Add support for passing delegation tokens via headers for web services,HADOOP-10799 refactors the WebHDFS code to handle delegation tokens a part of hadoop-common. We should add support to pass delegation tokens as a header instead of passing it as part of the url.,Resolved,Duplicate,NULL,Varun Vasudev,Varun Vasudev,Tue; 15 Jul 2014 16:31:18 +0000,Fri; 8 Aug 2014 08:17:28 +0000,Fri; 8 Aug 2014 08:17:28 +0000,,,,YARN-2292,HADOOP-10799,https://issues.apache.org/jira/browse/HADOOP-10832
HADOOP-10833,Improvement,Major,security,Remove unused cache in UserProvider,UserProvider contains the field cache. It is referenced only in deleteCredentialEntry and so there is no real usage of cache.,Closed,Fixed,,Benoy Antony,Benoy Antony,Tue; 15 Jul 2014 17:06:56 +0000,Mon; 1 Dec 2014 03:10:07 +0000,Sat; 30 Aug 2014 20:09:52 +0000,,2.4.1,,,HADOOP-10607,https://issues.apache.org/jira/browse/HADOOP-10833
HADOOP-10834,Improvement,Minor,security,Typo in CredentialShell usage,There is a typo in one of the informational messages in _ CredentialShell_,Resolved,Fixed,,Benoy Antony,Benoy Antony,Tue; 15 Jul 2014 17:09:06 +0000,Thu; 12 May 2016 18:25:19 +0000,Tue; 15 Jul 2014 18:23:10 +0000,,2.4.1,,,HADOOP-10607,https://issues.apache.org/jira/browse/HADOOP-10834
HADOOP-10835,Improvement,Major,security,Implement HTTP proxyuser support in HTTP authentication client/server libraries,This is to implement generic handling of proxyuser in the DelegationTokenAuthenticatedURL and DelegationTokenAuthenticationFilter classes and to wire properly UGI on the server side.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 15 Jul 2014 18:16:59 +0000,Mon; 1 Dec 2014 03:07:29 +0000,Tue; 12 Aug 2014 00:12:01 +0000,,2.4.1,,HADOOP-10880;HDFS-6849;HADOOP-10799;HADOOP-10817,,https://issues.apache.org/jira/browse/HADOOP-10835
HDFS-6849,Improvement,Major,security,Replace HttpFS custom proxyuser handling with common implementation,Use HADOOP-10835 to implement proxyuser logic in HttpFS,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 15 Jul 2014 18:18:33 +0000,Mon; 1 Dec 2014 03:10:24 +0000,Wed; 13 Aug 2014 20:30:05 +0000,,2.4.1,,HADOOP-10835,,https://issues.apache.org/jira/browse/HDFS-6849
HADOOP-10837,Bug,Major,fs,Fix the failures from TestSymlinkLocalFSFileContext & TestSymlinkLocalFSFileSystem,There are failures in trunk: log:,Resolved,Duplicate,NULL,Unassigned,Uma Maheswara Rao G,Tue; 15 Jul 2014 18:27:57 +0000,Thu; 12 May 2016 18:25:25 +0000,Tue; 15 Jul 2014 20:52:42 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10837
HADOOP-10838,Improvement,Major,performance,Byte array native checksumming,,Closed,Fixed,HADOOP-9601,James Thomas,James Thomas,Wed; 18 Jun 2014 21:02:57 +0000,Mon; 1 Dec 2014 03:11:08 +0000,Thu; 14 Aug 2014 04:30:07 +0000,,,,,HADOOP-10778,https://issues.apache.org/jira/browse/HADOOP-10838
HADOOP-10839,Improvement,Major,metrics,Add unregisterSource() to MetricsSystem API,Currently the MetrisSystem API has register() method to register a MetricsSource but doesn't have unregister() method. This means once a MetricsSource is registered with the MetricsSystem; it will be there forever until the MetricsSystem is shut down. This in some cases can cause Java OutOfMemoryError.One such case is in file system metrics implementation. The new AbstractFileSystem/FileContext framework does not implement a cache so every file system access can lead to the creation of a NativeFileSystem instance. (refer to HADOOP-6356). And all these NativeFileSystem needs to share the same instance of MetricsSystemImpl; which means we cannot shut down MetricsSystem to clean up all the MetricsSources that has been registered but no longer active. Over time the MetricsSource instance accumulates and eventually we saw OutOfMemoryError.,Closed,Fixed,,shanyu zhao,shanyu zhao,Tue; 15 Jul 2014 20:35:24 +0000,Mon; 1 Dec 2014 03:08:05 +0000,Wed; 16 Jul 2014 18:53:12 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10839
HADOOP-10840,Bug,Major,metrics,Fix OutOfMemoryError caused by metrics system in Azure File System,In Hadoop 2.x the Hadoop File System framework changed and no cache is implemented (refer to HADOOP-6356). This means for every WASB access; a new NativeAzureFileSystem is created; along which a Metrics source created and added to MetricsSystemImpl. Over time the sources accumulated; eating memory and causing Java OutOfMemoryError.The fix is to utilize the unregisterSource() method added to MetricsSystem in HADOOP-10839.,Closed,Fixed,,shanyu zhao,shanyu zhao,Tue; 15 Jul 2014 20:59:58 +0000,Fri; 10 Apr 2015 20:04:18 +0000,Thu; 30 Oct 2014 00:51:30 +0000,,2.4.1,,,HADOOP-9629,https://issues.apache.org/jira/browse/HADOOP-10840
HADOOP-10841,Improvement,Major,security,EncryptedKeyVersion should have a key name property,having a keyname will help the NN to efficiently (without additional keyprovider calls; which can translate into remote calls) determine the key name of an EDEK.,Closed,Fixed,,Arun Suresh,Alejandro Abdelnur,Tue; 15 Jul 2014 22:43:15 +0000,Thu; 12 May 2016 18:24:37 +0000,Fri; 18 Jul 2014 05:41:01 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10841
HADOOP-10842,Bug,Major,security,CryptoExtension generateEncryptedKey method should receive the key name,Generating an EEK should be done using always the current keyversion of a key name. We should enforce that by API by handing off EEKs for the last keyversion of a keyname only; thus we should ask for EEKs for a keyname and the CryptoExtension should use the last keyversion.,Closed,Fixed,,Arun Suresh,Alejandro Abdelnur,Tue; 15 Jul 2014 22:46:23 +0000,Thu; 12 May 2016 18:24:39 +0000,Fri; 18 Jul 2014 06:13:32 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10842
HADOOP-10843,Bug,Major,test;tools,TestGridmixRecord unit tests failure on PowerPC,"In TestGridmixRecord#binSortTest; the test expects the comparison result of WritableComparator.compareBytes; which uses UnsafeComparer; to be the integer difference rather than the documented ""@return 0 if equal;  0 if left is less than right; etc."". TestGridmixRecord#binSortTest code snippet The code snippet below shows the Unsafe comparator behavior for non-little-endian machines.",Closed,Fixed,,Jinghui Wang,Jinghui Wang,Tue; 15 Jul 2014 23:17:21 +0000,Mon; 1 Dec 2014 03:07:28 +0000,Thu; 14 Aug 2014 01:36:49 +0000,,2.2.0;2.3.0;2.4.0;2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10843
HADOOP-10844,Bug,Major,fs;test,Add common tests for ACLs in combination with viewfs.,Add tests in Hadoop Common for the ACL APIs in combination with viewfs.,Resolved,Duplicate,HADOOP-10845,Stephen Chu,Chris Nauroth,Tue; 15 Jul 2014 23:18:02 +0000,Thu; 12 May 2016 18:24:40 +0000,Wed; 23 Jul 2014 15:03:35 +0000,,2.5.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10844
HADOOP-10845,Improvement,Major,fs;test,Add common tests for ACLs in combination with viewfs.,Add tests in Hadoop Common for the ACL APIs in combination with viewfs.,Closed,Fixed,HADOOP-10844,Stephen Chu,Chris Nauroth,Tue; 15 Jul 2014 23:18:03 +0000,Thu; 12 May 2016 18:24:40 +0000,Wed; 16 Jul 2014 05:21:47 +0000,,2.5.0;3.0.0-alpha1,,,HADOOP-10184,https://issues.apache.org/jira/browse/HADOOP-10845
HADOOP-10846,Bug,Major,util,DataChecksum#calculateChunkedSums not working for PPC when buffers not backed by array,Got the following exception when running Hadoop on Power PC. The implementation for computing checksum when the data buffer and checksum buffer are not backed by arrays.13/09/16 04:06:57 ERROR security.UserGroupInformation: PriviledgedActionException as:biadmin (auth:SIMPLE) cause:org.apache.hadoop.ipc.RemoteException(java.io.IOException): org.apache.hadoop.fs.ChecksumException: Checksum error,Patch Available,Unresolved,HDFS-6903,Ayappan,Jinghui Wang,Wed; 16 Jul 2014 01:31:19 +0000,Mon; 11 Sep 2017 06:25:16 +0000,,,2.4.1;2.5.2,BB2015-05-TBR,,HADOOP-11665,https://issues.apache.org/jira/browse/HADOOP-10846
HADOOP-10847,Improvement,Minor,security,Remove the usage of sun.security.x509.* in testing code,As was told by Max (Oracle); JDK9 is likely to block all accesses to sun.* classes.Below is from email of Andrew Purtell:The use of sun.* APIs to create a certificate in Hadoop and HBase test code can be removed. Someone (Intel? Oracle?) can submit a JIRA that replaces the programmatic construction with a stringified binary cert for use in the relevant unit tests. In Hadoop; the calls in question are below:,Closed,Fixed,,pascal oliva,Kai Zheng,Wed; 16 Jul 2014 02:21:46 +0000,Thu; 6 Aug 2015 18:29:44 +0000,Tue; 4 Nov 2014 18:52:43 +0000,,,,,HADOOP-11090;HADOOP-10848;HADOOP-12300;KNOX-422;HADOOP-11230,https://issues.apache.org/jira/browse/HADOOP-10847
HADOOP-10848,Improvement,Minor,,Cleanup calling of sun.security.krb5.Config,As was told by Max (Oracle); JDK9 is likely to block all accesses to sun.* classes.In ./hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosUtil.java;sun.security.krb5.Config is called against the method getDefaultRealm() to get default Kerberos realm. It was proposed to remove the call by Oracle:,Open,Unresolved,,Unassigned,Kai Zheng,Wed; 16 Jul 2014 02:36:25 +0000,Tue; 8 Aug 2017 10:18:03 +0000,,,,,,HADOOP-10847;HADOOP-11123,https://issues.apache.org/jira/browse/HADOOP-10848
HADOOP-10849,Improvement,Major,conf,Implement conf substitution with UGI.current/loginUser,Many path properties and similar in hadoop code base would be easily configured if we had substitutions with UserGroupInformation#getCurrentUser. Currently we often use less elegant concatenation code if we want to express currentUser as opposed to ${user.name} system property representing the user owning the JVM.This JIRA proposes the corresponding substitution support for keys ugi.current.user and ugi.login.user,Open,Unresolved,,Gera Shegalov,Gera Shegalov,Wed; 16 Jul 2014 06:07:37 +0000,Sat; 7 Jan 2017 01:56:51 +0000,,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10849
HADOOP-10850,Bug,Major,security,KerberosAuthenticator should not do the SPNEGO handshake,As mentioned in HADOOP-10453; the JDK automatically does a SPNEGO handshake when opening a connection with a URL within a Kerberos login context; there is no need to do the SPNEGO handshake in the KerberosAuthenticator; simply extract the auth token (hadoop-auth cookie) and do the fallback if necessary.,Open,Unresolved,,Alejandro Abdelnur,Alejandro Abdelnur,Wed; 16 Jul 2014 13:25:14 +0000,Fri; 29 Sep 2017 20:50:49 +0000,,,2.4.1,,,HADOOP-12787,https://issues.apache.org/jira/browse/HADOOP-10850
HADOOP-10851,Bug,Major,security,NetgroupCache does not remove group memberships,"NetgroupCache is used by GroupMappingServiceProvider implementations based on net groups.But it has a serious flaw in that once a user to group membership is established; it remains forever even if user is actually removed from the netgroup and cache is cleared.  It is cleared only if the server is restarted.To reproduce this: 	Cache a group with a set of users.	Test membership correctness.	Clear cache; remove a user from the group and cache the group again	Expected result : user s groups should not include the group from which he/she is removed.	Actual result : user s groups includes the group from which he/she was removed.It is also noted that NetgroupCache has concurrency issues and a separate jira is filed to rectify them.",Closed,Fixed,,Benoy Antony,Benoy Antony,Wed; 16 Jul 2014 19:14:52 +0000,Mon; 1 Dec 2014 03:09:57 +0000,Tue; 12 Aug 2014 22:33:05 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10851
HADOOP-10852,Bug,Major,security,NetgroupCache is not thread-safe,NetgroupCache internally uses two ConcurrentHashMaps and a boolean variable to signal updates on one of the ConcurrentHashMapNone of the functions are synchronized  and hence is possible to have unexpected results due to race condition between different threads.As an example; consider the following sequence:Thread1 :add a groupnetgroupToUsersMap is updated.netgroupToUsersMapUpdated is set to true.Thread 2:calls getNetgroups for a userDue to re-ordering; netgroupToUsersMapUpdated=true is visible; but updates in netgroupToUsersMap is not visible.Does a wrong update with older netgroupToUsersMap values.,Closed,Fixed,,Benoy Antony,Benoy Antony,Wed; 16 Jul 2014 19:20:09 +0000,Fri; 10 Apr 2015 20:04:25 +0000,Mon; 15 Dec 2014 22:06:25 +0000,,2.5.0;2.4.1,,,HADOOP-11234,https://issues.apache.org/jira/browse/HADOOP-10852
HADOOP-10853,Sub-task,Major,security,Refactor get instance of CryptoCodec and support create via algorithm/mode/padding.,"We should be able to create instance of CryptoCodec:	via codec class name. (Applications may have config for different crypto codecs)	via algorithm/mode/padding. (For automatically decryption; we need to find correct crypto codec and proper implementation)	a default crypto codec through specific config.This JIRA is for	Create instance through cipher suite(algorithm/mode/padding)	Refactor create instance of CryptoCodec into CryptoCodecFactoryWe need to get all crypto codecs in system; this can be done via a Java ServiceLoader + hadoop.security.crypto.codecs config.",Resolved,Fixed,,Yi Liu,Yi Liu,Thu; 17 Jul 2014 07:13:57 +0000,Tue; 22 Jul 2014 14:27:01 +0000,Tue; 22 Jul 2014 08:39:14 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10853
HADOOP-10854,Test,Major,scripts,unit tests for the shell scripts,With HADOOP-9902 moving a lot of the core functionality to functions; we should build some unit tests for them.,Resolved,Fixed,HADOOP-6383;HADOOP-4059;HADOOP-6174,Allen Wittenauer,Allen Wittenauer,Thu; 17 Jul 2014 16:16:07 +0000,Thu; 12 May 2016 18:26:57 +0000,Fri; 31 Jul 2015 21:36:28 +0000,,3.0.0-alpha1,scripts,,HADOOP-11745;HADOOP-584;HADOOP-11010;HADOOP-12364,https://issues.apache.org/jira/browse/HADOOP-10854
HADOOP-10855,Improvement,Minor,io,Allow Text to be read with a known length,"For the native task work (MAPREDUCE-2841) it is useful to be able to store strings in a different fashion than the default (varint-prefixed) serialization. We should provide a ""read"" method in Text which takes an already-known length to support this use case while still providing Text objects back to the user.",Closed,Fixed,,Todd Lipcon,Todd Lipcon,Thu; 17 Jul 2014 16:20:12 +0000,Mon; 1 Dec 2014 03:10:04 +0000,Wed; 23 Jul 2014 00:29:07 +0000,,2.6.0,,,MAPREDUCE-2841,https://issues.apache.org/jira/browse/HADOOP-10855
HADOOP-10856,Bug,Major,,HarFileSystem and HarFs support for HDFS encryption,We need to examine support for Har with HDFS encryption.,Resolved,Won't Fix,,Andrew Wang,Andrew Wang,Thu; 17 Jul 2014 18:49:40 +0000,Mon; 6 Mar 2017 22:16:58 +0000,Mon; 6 Mar 2017 22:16:58 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,HDFS-6891,https://issues.apache.org/jira/browse/HADOOP-10856
HADOOP-10857,Bug,Major,documentation,Native Libraries Guide doen't mention a dependency on openssl-development package,maven compile -Pnative fails without installing openssl-development package(e.g. libssl-dev). We should describe it in Native Libraries Guide.,Closed,Fixed,,Tsuyoshi Ozawa,Tsuyoshi Ozawa,Fri; 18 Jul 2014 03:09:07 +0000,Mon; 1 Dec 2014 03:07:41 +0000,Mon; 21 Jul 2014 19:05:38 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10857
HADOOP-10858,Improvement,Major,metrics,Specify the charset explicitly rather than rely on the default,Findbugs 2 warns about relying on the default Java charset instead of specifying it explicitly. Given that we're porting Hadoop to different platforms it's better to be explicit,Open,Unresolved,,Liang Xie,Liang Xie,Fri; 18 Jul 2014 08:47:43 +0000,Thu; 12 May 2016 18:27:45 +0000,,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10858
HADOOP-10859,Improvement,Minor,,Native implementation of java Checksum interface,Some parts of our code such as IFileInputStream/IFileOutputStream use the java Checksum interface to calculate/verify checksums. Currently we don't have a native implementation of these. For CRC32C in particular; we can get a very big speedup with a native implementation.,Open,Unresolved,,Todd Lipcon,Todd Lipcon,Fri; 18 Jul 2014 15:29:49 +0000,Sat; 7 Jan 2017 01:56:47 +0000,,,,,,MAPREDUCE-5962,https://issues.apache.org/jira/browse/HADOOP-10859
HADOOP-10860,Wish,Major,,Add serialization for Protocol Buffers,Protocol Buffers (http://code.google.com/p/protobuf/) are a way of encoding data in a compact binary format. This issue is to write a ProtocolBuffersSerialization to support using Protocol Buffers types in MapReduce programs; including an example program. This should probably go into contrib.,Open,Unresolved,,Unassigned,Tom White,Fri; 18 Jul 2008 11:01:17 +0000,Fri; 18 Jul 2014 18:12:59 +0000,,,,,,MAPREDUCE-376,https://issues.apache.org/jira/browse/HADOOP-10860
HADOOP-10861,Improvement,Minor,,Fix HTML validation warnings in web UI,Many of the web pages in the admin webapp are invalid HTML according to the W3C validator (http://validator.w3.org/). They should be fixed so they render correctly in a wide range of browsers.,Open,Unresolved,,Unassigned,Tom White,Thu; 31 Jul 2008 14:31:07 +0000,Fri; 18 Jul 2014 20:03:33 +0000,,,,newbie,,HADOOP-3866,https://issues.apache.org/jira/browse/HADOOP-10861
HADOOP-10862,Bug,Major,security,Miscellaneous trivial corrections to KMS classes,KMSRESTConstants.java; KEY_OP should be KEYS and value should be keys.KMS.java should be annotated with Jersey @Singleton to avoid creating an instance on every request; it is thread safe already.Make sure all KMS related classes are annotated with private audience.,Closed,Fixed,,Arun Suresh,Alejandro Abdelnur,Fri; 18 Jul 2014 22:25:56 +0000,Thu; 12 May 2016 18:27:25 +0000,Fri; 8 Aug 2014 23:10:36 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10862
HADOOP-10863,Improvement,Major,security,KMS should have a blacklist for decrypting EEKs,In particular; we'll need to put HDFS admin user there by default to prevent an HDFS admin from getting file encryption keys.,Closed,Fixed,,Arun Suresh,Alejandro Abdelnur,Fri; 18 Jul 2014 22:28:01 +0000,Thu; 12 May 2016 18:27:26 +0000,Wed; 3 Sep 2014 22:21:37 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10863
HADOOP-10864,Sub-task,Minor,documentation,Tool documentenation is broken,Looking at http://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/util/Tool.html; at least one of the links is non-existent.  There are likely other bugs in this documentation too.,Closed,Fixed,,Akira Ajisaka,Allen Wittenauer,Sat; 19 Jul 2014 00:06:57 +0000,Wed; 3 Sep 2014 20:36:27 +0000,Tue; 22 Jul 2014 16:59:18 +0000,,2.0.2-alpha,docuentation;newbie,,HADOOP-10894,https://issues.apache.org/jira/browse/HADOOP-10864
HADOOP-10865,Improvement,Minor,util,Add a Crc32 chunked verification benchmark for both directly and non-directly buffer cases,Currently; it is not easy to compare Crc32 chunked verification implementations.  Let's add a benchmark.,Resolved,Fixed,,Tsz Wo Nicholas Sze,Tsz Wo Nicholas Sze,Sat; 19 Jul 2014 20:57:04 +0000,Tue; 30 Aug 2016 01:32:09 +0000,Thu; 18 Feb 2016 20:01:22 +0000,,,BB2015-05-TBR,,HADOOP-10778,https://issues.apache.org/jira/browse/HADOOP-10865
HADOOP-10866,Bug,Major,,RawLocalFileSystem fails to read symlink targets via the stat command when the format of stat command uses non-curly quotes,Symlink tests failure happened from time to time;https://builds.apache.org/job/PreCommit-HDFS-Build/7383//testReport/https://builds.apache.org/job/PreCommit-HDFS-Build/7376/testReport/,Closed,Fixed,HADOOP-10510,Yongjun Zhang,Yongjun Zhang,Fri; 18 Jul 2014 22:00:15 +0000,Mon; 1 Dec 2014 03:10:34 +0000,Mon; 21 Jul 2014 23:22:28 +0000,,2.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-10866
HADOOP-10867,Bug,Major,,NPE in heartbeat when the configured topology script doesn't exist,,Open,Unresolved,MAPREDUCE-820,Mauro Murari,Vinod Kumar Vavilapalli,Thu; 26 Feb 2009 11:12:53 +0000,Mon; 2 Feb 2015 22:14:18 +0000,,,1.0.3,newbie,,HADOOP-8049,https://issues.apache.org/jira/browse/HADOOP-10867
HADOOP-10868,Sub-task,Major,security,Create a ZooKeeper-backed secret provider,Create a secret provider (see HADOOP-10791) that is backed by ZooKeeper and can synchronize amongst different servers.,Closed,Fixed,,Robert Kanter,Robert Kanter,Mon; 21 Jul 2014 23:08:49 +0000,Wed; 25 Mar 2015 18:24:37 +0000,Tue; 16 Sep 2014 00:14:11 +0000,,2.4.1,,HADOOP-11016,HADOOP-11102,https://issues.apache.org/jira/browse/HADOOP-10868
HADOOP-10869,Bug,Major,security,JavaKeyStoreProvider backing jceks file may get corrupted,Currently; flush writes to the same file jceks file; if there is a failure during a write; the jceks file will be rendered unusable losing access to all keys stored in it.,Resolved,Duplicate,HADOOP-10224,Arun Suresh,Alejandro Abdelnur,Mon; 21 Jul 2014 23:23:55 +0000,Thu; 12 May 2016 18:22:10 +0000,Tue; 22 Jul 2014 05:58:16 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10869
HADOOP-10870,Sub-task,Major,security,Failed to load OpenSSL cipher error logs on systems with old openssl versions,"I built Hadoop from fs-encryption branch and deployed Hadoop (without enabling any security confs) on a Centos 6.4 VM with an old version of openssl. When I try to do a simple ""hadoop fs -ls""; I get It would be an improvment to clean up/shorten this error log.hadoop checknative shows the error as well Thanks to cmccabe who identified this issue as a bug.",Resolved,Fixed,,Colin P. McCabe,Stephen Chu,Mon; 21 Jul 2014 23:40:37 +0000,Tue; 22 Jul 2014 01:18:53 +0000,Tue; 22 Jul 2014 00:58:23 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10870
HADOOP-10871,Sub-task,Major,util,incorrect prototype in OpensslSecureRandom.c,There is an incorrect prototype in OpensslSecureRandom.c.,Resolved,Fixed,,Colin P. McCabe,Colin P. McCabe,Mon; 21 Jul 2014 23:45:08 +0000,Tue; 22 Jul 2014 01:17:22 +0000,Tue; 22 Jul 2014 00:59:11 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10871
HADOOP-10872,Bug,Major,fs,"TestPathData fails intermittently with ""Mkdirs failed to create d1""",A bunch of TestPathData tests failed intermittently; e.g.https://builds.apache.org/job/PreCommit-HDFS-Build/7416//testReport/Example failure log:,Closed,Fixed,,Yongjun Zhang,Yongjun Zhang,Tue; 22 Jul 2014 07:09:26 +0000,Wed; 3 Sep 2014 20:36:30 +0000,Wed; 23 Jul 2014 17:51:46 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10872
HADOOP-10873,Bug,Major,documentation,Fix dead links in the API doc,There are a lot of dead links in Hadoop API doc. We should fix them.,Resolved,Fixed,,Unassigned,Akira Ajisaka,Tue; 22 Jul 2014 09:15:12 +0000,Thu; 25 May 2017 08:39:29 +0000,Thu; 25 May 2017 08:39:29 +0000,,,,,MAPREDUCE-5988;MAPREDUCE-5998;MAPREDUCE-5999,https://issues.apache.org/jira/browse/HADOOP-10873
HADOOP-10874,Wish,Minor,fs,hdfs dfs -getmerge can have an additional parameter for sort order,Default implementation sorts the array in ascending order of the files;  a parameter can be added to current implementation so that it can sort in descending order as well.Current impl:public static boolean More ...copyMerge(FileSystem srcFS; Path srcDir; FileSystem dstFS; Path dstFile; boolean deleteSource; Configuration conf; String addString) throws IOException {Proposed:public static boolean More ...copyMerge(FileSystem srcFS; Path srcDir; FileSystem dstFS; Path dstFile; boolean deleteSource; Configuration conf; String addString; boolean sort) throws IOException {,Open,Unresolved,,Unassigned,Sambit Tripathy,Tue; 22 Jul 2014 17:39:51 +0000,Tue; 22 Jul 2014 17:39:51 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10874
HADOOP-10875,Bug,Major,,Sqoop2: Attach a debugger to miniclusters,It would be nice to have a way to attach a debugger to the miniclusters: http://cargo.codehaus.org/Starting+and+stopping+a+container.For tomcat; I needed to add the following to TomcatSqoopMiniCluster: There should also be a way to attach a debugger to the Yarn container.,Resolved,Won't Fix,,Unassigned,Abraham Elmahrek,Tue; 22 Jul 2014 17:41:46 +0000,Tue; 22 Jul 2014 17:42:12 +0000,Tue; 22 Jul 2014 17:42:12 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10875
HADOOP-10876,Bug,Major,,The constructor of Path should not take an empty URL as a parameter,The constructor of Path should not take an empty URL as a parameter; As discussed in HADOOP-10820; This JIRA is to change Path constructor at public Path(URI aUri) to check the empty URI and throw IllegalArgumentException for empty URI.,Resolved,Won't Fix,,zhihai xu,zhihai xu,Tue; 22 Jul 2014 17:44:39 +0000,Fri; 8 Aug 2014 15:21:40 +0000,Fri; 8 Aug 2014 01:40:07 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-10876
HADOOP-10877,Sub-task,Major,native,native client: implement hdfsMove and hdfsCopy,In the pure native client; we need to implement hdfsMove and hdfsCopy.  These are basically recursive copy functions (in the Java code; move is copy with a delete at the end).,Resolved,Fixed,,Colin P. McCabe,Colin P. McCabe,Tue; 22 Jul 2014 18:50:11 +0000,Thu; 14 Aug 2014 20:37:39 +0000,Thu; 14 Aug 2014 20:37:39 +0000,,HADOOP-10388,,,,https://issues.apache.org/jira/browse/HADOOP-10877
HADOOP-10878,Bug,Major,metrics;security,Hadoop servlets need ACLs,As far as I'm aware; once a user gets past the HTTP-level authentication; all servlets available on that port are available to the user.  This is a security hole as there is some information and services that we don't want every user to be able to access or only want them to access from certain locations.,Open,Unresolved,HADOOP-5722,Unassigned,Allen Wittenauer,Tue; 22 Jul 2014 18:56:23 +0000,Tue; 22 Jul 2014 19:00:30 +0000,,,,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10878
HADOOP-10879,Improvement,Major,scripts,Rename *-env.sh in the tree to *-env.sh.example,With HADOOP-9902 in place; we don't have to ship *-env.sh called as such and only provide examples.  This goes a long way with being able to upgrade the binaries in place since we would no longer overwrite those files upon extraction.,Resolved,Later,,Unassigned,Allen Wittenauer,Tue; 22 Jul 2014 19:41:58 +0000,Thu; 26 Feb 2015 03:29:47 +0000,Thu; 26 Feb 2015 03:29:47 +0000,,,scripts,,HADOOP-11010;YARN-2438,https://issues.apache.org/jira/browse/HADOOP-10879
HADOOP-10880,Bug,Blocker,security,Move HTTP delegation tokens out of URL querystring to a header,Following up on a discussion in HADOOP-10799.Because URLs are often logged; delegation tokens may end up in LOG files while they are still valid. We should move the tokens to a header.We should still support tokens in the querystring for backwards compatibility.,Closed,Fixed,,Alejandro Abdelnur,Alejandro Abdelnur,Tue; 22 Jul 2014 21:03:17 +0000,Mon; 1 Dec 2014 03:09:15 +0000,Thu; 28 Aug 2014 21:49:35 +0000,,2.4.1,,HADOOP-10771;HADOOP-10835,,https://issues.apache.org/jira/browse/HADOOP-10880
HADOOP-10881,Improvement,Major,,Clarify usage of encryption and encrypted encryption key in KeyProviderCryptoExtension,,Closed,Fixed,,Andrew Wang,Andrew Wang,Tue; 22 Jul 2014 22:30:13 +0000,Thu; 12 May 2016 18:23:01 +0000,Wed; 23 Jul 2014 01:24:48 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10881
HADOOP-10882,Task,Minor,util,Move DirectBufferPool into common util,MAPREDUCE-2841 uses a direct buffer pool to pass data back and forth between native and Java code. The branch has an implementation which appears to be derived from the one in HDFS. Instead of copy-pasting; we should move the HDFS DirectBufferPool into Common so that MR can make use of it.,Closed,Fixed,,Todd Lipcon,Todd Lipcon,Wed; 23 Jul 2014 01:33:42 +0000,Mon; 1 Dec 2014 03:09:15 +0000,Thu; 24 Jul 2014 06:22:18 +0000,,2.6.0,,,,https://issues.apache.org/jira/browse/HADOOP-10882
MAPREDUCE-5998,Bug,Minor,documentation,CompositeInputFormat javadoc is broken,In CompositeInputFormat javadoc; some part of the description is converted to hyperlink by @see tag.,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Wed; 23 Jul 2014 02:01:42 +0000,Mon; 1 Dec 2014 03:08:22 +0000,Thu; 14 Aug 2014 17:19:49 +0000,,2.0.2-alpha,newbie,,HADOOP-10873,https://issues.apache.org/jira/browse/MAPREDUCE-5998
HADOOP-10884,Sub-task,Minor,documentation,Fix dead link in Configuration javadoc,In Configuration javadoc; the link to core-default.xml is dead. We should fix it.,Closed,Fixed,,Akira Ajisaka,Akira Ajisaka,Wed; 23 Jul 2014 02:28:05 +0000,Mon; 1 Dec 2014 03:09:42 +0000,Mon; 18 Aug 2014 20:04:15 +0000,,2.0.2-alpha,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10884
HADOOP-10885,Sub-task,Minor,documentation,Fix dead links to the javadocs of o.a.h.security.authorize,In API doc (my trunk build); ImpersonationProvider and DefaultImpersonationProvider classes are linked but these documents are not generated.There's an inconsistency about @InterfaceAudience between package-info and these classes; so these dead links are generated.,Resolved,Duplicate,HADOOP-12545,Yufei Gu,Akira Ajisaka,Wed; 23 Jul 2014 07:05:52 +0000,Thu; 25 Feb 2016 01:56:24 +0000,Thu; 25 Feb 2016 01:56:24 +0000,,2.6.0,newbie,,,https://issues.apache.org/jira/browse/HADOOP-10885
HADOOP-10886,Sub-task,Major,fs,CryptoCodec#getCodecclasses throws NPE when configurations not loaded.,There are some test cases which will not load the xml defaults. In this case; CryptoCodec#getCodecclasses will fail with NPE. https://builds.apache.org/job/Hadoop-fs-encryption-nightly/71/,Closed,Fixed,,Uma Maheswara Rao G,Uma Maheswara Rao G,Wed; 23 Jul 2014 17:59:08 +0000,Mon; 1 Dec 2014 03:11:01 +0000,Mon; 4 Aug 2014 09:58:00 +0000,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,https://issues.apache.org/jira/browse/HADOOP-10886
HADOOP-10887,Bug,Major,fs;test,Add XAttrs to ViewFs and make XAttrs + ViewFileSystem internal dir behavior consistent,This is very similar to the work done in HADOOP-10845 (Add common tests for ACLs in combination with viewfs)Here we make the XAttrs + ViewFileSystem internal dir behavior consistent. Right now; when users attempt XAttr operation on an internal dir; they will get an UnsupportedOperationException. Instead; we should throw the ReadOnlyMountTable AccessControlException or the NotInMountPointException.We also add the XAttrs APIs to ViewFs. This involves adding them to ChRootedFs as well. Also; listXAttrs is missing from FileContext; so we should add that in.,Closed,Fixed,,Stephen Chu,Stephen Chu,Wed; 23 Jul 2014 18:00:36 +0000,Thu; 12 May 2016 18:23:55 +0000,Wed; 23 Jul 2014 21:43:30 +0000,,2.5.0;3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10887
HADOOP-10888,Bug,Major,,org.apache.hadoop.ipc.TestIPC.testRetryProxy failed often with timeout,As an example; https://builds.apache.org/job/PreCommit-HADOOP-Build/4333//testReport/org.apache.hadoop.ipc/TestIPC/testRetryProxy/,Resolved,Duplicate,INFRA-8097,Yongjun Zhang,Yongjun Zhang,Wed; 23 Jul 2014 18:17:35 +0000,Fri; 1 Aug 2014 17:51:26 +0000,Fri; 1 Aug 2014 17:51:26 +0000,,2.5.0,,,INFRA-8097,https://issues.apache.org/jira/browse/HADOOP-10888
HADOOP-10889,Bug,Major,,Verify usage of test.build.data in test cases,Per Arpit Agarwal's comments in HDFS-6719; I'm filing this jira as a follow-up. The goal is to fix the misuse of test.build.data over quite some places. Thanks Arpit!,Open,Unresolved,,Yongjun Zhang,Yongjun Zhang,Wed; 23 Jul 2014 18:38:57 +0000,Wed; 23 Jul 2014 18:59:47 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-10889
HADOOP-10890,Bug,Major,,TestDFVariations.testMount fails intermittently,Failure message:,Closed,Fixed,,Yongjun Zhang,Yongjun Zhang,Tue; 22 Jul 2014 01:00:14 +0000,Wed; 3 Sep 2014 20:36:28 +0000,Wed; 23 Jul 2014 18:55:21 +0000,,2.4.1,,,,https://issues.apache.org/jira/browse/HADOOP-10890
HADOOP-10891,Improvement,Major,,Add EncryptedKeyVersion factory method to KeyProviderCryptoExtension,For fs-encryption; we need to create a EncryptedKeyVersion from its component parts for decryption. We also need a way of getting a KPCE from a conf. Both of these can be done with factory methods.,Closed,Fixed,,Andrew Wang,Andrew Wang,Wed; 23 Jul 2014 22:34:10 +0000,Thu; 12 May 2016 18:23:41 +0000,Thu; 24 Jul 2014 23:49:44 +0000,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-10891
HADOOP-10892,Bug,Major,,Suppress 'proprietary API' warnings,The 'proprietary API' warnings provide no useful information and clutter up the build output; hiding legitimate warnings in the noise.Most of the warnings appear to be about OutputFormat; XMLSerializer and Unsafe. I don't think these APIs are going away any time soon; and if they do we can deal with it when it happens.,Open,Unresolved,,Unassigned,Arpit Agarwal,Wed; 23 Jul 2014 22:36:18 +0000,Mon; 4 Aug 2014 22:42:00 +0000,,,,,,HDFS-4629,https://issues.apache.org/jira/browse/HADOOP-10892
HADOOP-10893,New Feature,Major,util,isolated classloader on the client side,We have the job classloader on the mapreduce tasks that run,,,,,,,,,,,,,,
HADOOP-15000,Sub-task,Minor,fs/s3,s3a new getdefaultblocksize be called in getFileStatus which has not been implemented in s3afilesystem yet,new implementation of getting block size has been called in getFileStatus method:  while we don't implement it in our s3afilesystem currently; also we need to implement this new method as the old one deprecated.,Open,Unresolved,,Unassigned,Yonger,Wed; 1 Nov 2017 09:33:33 +0000,Thu; 2 Nov 2017 17:52:32 +0000,,,2.9.0,,,HADOOP-14943,https://issues.apache.org/jira/browse/HADOOP-15000
HADOOP-15001,Sub-task,Major,fs/oss,AliyunOSS: provide one memory-based buffering mechanism in outpustream to oss,,Open,Unresolved,,Unassigned,Genmao Yu,Wed; 1 Nov 2017 13:44:37 +0000,Wed; 1 Nov 2017 13:49:15 +0000,,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15001
HADOOP-15002,Sub-task,Major,fs/oss,AliyunOSS: refactor storage statistics to oss,,Open,Unresolved,,Unassigned,Genmao Yu,Wed; 1 Nov 2017 13:53:32 +0000,Wed; 1 Nov 2017 13:53:32 +0000,,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15002
HADOOP-15003,Sub-task,Major,fs/s3,Merge S3A committers into trunk: Yetus patch checker,This is a Yetus only JIRA created to have Yetus review the HADOOP-13786/HADOOP-14971 patch as a .patch file; as the review PR https://github.com/apache/hadoop/pull/282 is stopping this happening in HADOOP-14971.Reviews should go into the PR/other task,Resolved,Duplicate,HADOOP-14971;HADOOP-13786,Steve Loughran,Steve Loughran,Wed; 1 Nov 2017 14:07:55 +0000,Wed; 22 Nov 2017 17:29:10 +0000,Wed; 22 Nov 2017 17:29:10 +0000,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15003
MAPREDUCE-6999,Bug,Trivial,,"Fix typo ""onf"" in DynamicInputChunk.java","Modify the wrong word ""onf"" to ""on""",Resolved,Fixed,,fang zhenyi,fang zhenyi,Wed; 1 Nov 2017 15:09:10 +0000,Fri; 3 Nov 2017 13:47:51 +0000,Thu; 2 Nov 2017 09:35:36 +0000,,3.0.0-alpha3,,,,https://issues.apache.org/jira/browse/MAPREDUCE-6999
HADOOP-15005,New Feature,Major,,Support meta tag element in Hadoop XML configurations,We should tag the hadoop/hdfs config so that we can retrieve properties by there usage/application like PERFORMANCE; NAMENODE etc. Right now we don't have an option available to group or list related properties together. Grouping properties through some restricted set of Meta tags and then exposing them in Configuration class will be useful for end users.For example; here is an config file with tags.,Resolved,Fixed,,Ajay Kumar,Ajay Kumar,Thu; 24 Aug 2017 18:59:27 +0000,Wed; 1 Nov 2017 16:30:29 +0000,Thu; 7 Sep 2017 20:46:37 +0000,,,,,HDFS-12529;HADOOP-15007,https://issues.apache.org/jira/browse/HADOOP-15005
HADOOP-15006,New Feature,Minor,fs/s3;kms,Encrypt S3A data client-side with Hadoop libraries & Hadoop KMS,This is for the proposal to introduce Client Side Encryption to S3 in such a way that it can leverage HDFS transparent encryption; use the Hadoop KMS to manage keys; use the `hdfs crypto` command line tools to manage encryption zones in the cloud; and enable distcp to copy from HDFS to S3 (and vice-versa) with data still encrypted.,Open,Unresolved,,Unassigned,Steve Moist,Wed; 1 Nov 2017 16:07:22 +0000,Thu; 11 Jan 2018 19:53:31 +0000,,,,,,HADOOP-13887,https://issues.apache.org/jira/browse/HADOOP-15006
HADOOP-15007,Improvement,Blocker,conf,Stabilize and document Configuration <tag> element,HDFS-12350 (moved to HADOOP-15005). Adds the ability to tag properties with a tag value.We need to make sure that this feature is backwards compatible  usable in production. That's docs; testing; marshalling etc.,Open,Unresolved,,Anu Engineer,Steve Loughran,Wed; 1 Nov 2017 16:30:03 +0000,Tue; 7 Nov 2017 15:06:05 +0000,,,3.1.0,,,HADOOP-15005,https://issues.apache.org/jira/browse/HADOOP-15007
HADOOP-15008,Bug,Minor,metrics,Metrics sinks may emit too frequently if multiple sink periods are configured,If there are multiple metrics sink periods configured; depending on what those periods are; some sinks may emit too frequently. For example with the following:  I get the following: i.e.; for both metrics files; each record is 5000 ms apart; even though one of the sinks has been configured to emit at 10s intervals,Resolved,Fixed,,Erik Krogen,Erik Krogen,Wed; 1 Nov 2017 21:08:13 +0000,Mon; 13 Nov 2017 23:39:04 +0000,Mon; 13 Nov 2017 17:49:56 +0000,,2.2.0;3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15008
HADOOP-15009,Bug,Blocker,scripts;tools,hadoop-resourceestimator's shell scripts are a mess,#1:There's no reason for estimator.sh to exist.  Just make it a subcommand under yarn or whatever.  #2:In it's current form; it's missing a BUNCH of boilerplate that makes certain functionality completely fail.#3start/stop-estimator.sh is full of copypasta that doesn't actually do anything/work correctly.  Additionally; if estimator.sh doesn't exist; neither does this since yarn --daemon start/stop will do everything as necessary.,Patch Available,Unresolved,,Ajay Kumar,Allen Wittenauer,Thu; 2 Nov 2017 20:29:33 +0000,Fri; 12 Jan 2018 22:43:56 +0000,,,3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15009
HADOOP-15010,Bug,Blocker,build;tools,hadoop-resourceestimator's assembly buries it,There's zero reason for this layout: Buried that far back; it might as well not exist.Propose:a) HADOOP-15009 to eliminate binb) Move conf file into etc/hadoopc) keep data where it's at,Open,Unresolved,,Unassigned,Allen Wittenauer,Thu; 2 Nov 2017 20:36:50 +0000,Thu; 2 Nov 2017 21:41:48 +0000,,,2.9.0;3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15010
HADOOP-15011,Bug,Trivial,fs/s3,Getting file not found exception while using distcp with s3a,I'm using the distcp option to copy the huge files from Hadoop to S3. Sometimes i'm getting the below error;Command: (Copying 378 GB data)_hadoop distcp -D HADOOP_OPTS=-Xmx12g -D HADOOP_CLIENT_OPTS='-Xmx12g -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' -D 'mapreduce.map.memory.mb=12288' -D 'mapreduce.map.java.opts=-Xmx10g' -D 'mapreduce.reduce.memory.mb=12288' -D 'mapreduce.reduce.java.opts=-Xmx10g' '-Dfs.s3a.proxy.host=edhmgrn-prod.cloud.capitalone.com' '-Dfs.s3a.proxy.port=8088' '-Dfs.s3a.access.key=XXXXXXX' '-Dfs.s3a.secret.key=XXXXXXX' '-Dfs.s3a.connection.timeout=180000' '-Dfs.s3a.attempts.maximum=5' '-Dfs.s3a.fast.upload=true' '-Dfs.s3a.fast.upload.buffer=array' '-Dfs.s3a.fast.upload.active.blocks=50' '-Dfs.s3a.multipart.size=262144000' '-Dfs.s3a.threads.max=500' '-Dfs.s3a.threads.keepalivetime=600' '-Dfs.s3a.server-side-encryption-algorithm=AES256' -bandwidth 3072 -strategy dynamic -m 220 -numListstatusThreads 30 /src/ s3a://bucket/dest_17/11/01 12:23:27 INFO mapreduce.Job: Task Id : attempt_1497120915913_2792335_m_000165_0; Status : FAILEDError: java.io.FileNotFoundException: No such file or directory: s3a://bucketname/filename        at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1132)        at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:78)        at org.apache.hadoop.tools.util.DistCpUtils.preserve(DistCpUtils.java:197)        at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:256)        at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:422)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1912)        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)17/11/01 12:28:32 INFO mapreduce.Job: Task Id : attempt_1497120915913_2792335_m_000010_0; Status : FAILEDError: java.io.IOException: File copy failed: hdfs://nameservice1/filena -- s3a://cof-prod-lake-card/src/seam/acct_scores/acctmdlscore_card_cobna_anon_vldtd/instnc_id=20161023000000/000004_0_copy_6        at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:284)        at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:252)        at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:422)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1912)        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: java.io.IOException: Couldn't run retriable-command: Copying hdfs://nameservice1/filename to s3a://bucketname/filename        at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)        at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:280)        ... 10 moreCaused by: com.cloudera.com.amazonaws.AmazonClientException: Failed to parse XML document with handler class com.cloudera.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler        at com.cloudera.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseXmlInputStream(XmlResponsesSaxParser.java:164)        at com.cloudera.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseListBucketObjectsResponse(XmlResponsesSaxParser.java:299)        at com.cloudera.com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsUnmarshaller.unmarshall(Unmarshallers.java:77)        at com.cloudera.com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsUnmarshaller.unmarshall(Unmarshallers.java:74)        at com.cloudera.com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62)        at com.cloudera.com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31)        at com.cloudera.com.amazonaws.http.AmazonHttpClient.handleResponse(AmazonHttpClient.java:1072)        at com.cloudera.com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:746)        at com.cloudera.com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)        at com.cloudera.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)        at com.cloudera.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)        at com.cloudera.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3738)        at com.cloudera.com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:653)        at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1096)        at org.apache.hadoop.fs.s3a.S3AFileSystem.deleteUnnecessaryFakeDirectories(S3AFileSystem.java:1279)        at org.apache.hadoop.fs.s3a.S3AFileSystem.finishedWrite(S3AFileSystem.java:1268)        at org.apache.hadoop.fs.s3a.S3AFastOutputStream.close(S3AFastOutputStream.java:257)        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)        at java.io.FilterOutputStream.close(FilterOutputStream.java:159)        at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyBytes(RetriableFileCopyCommand.java:261)        at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyToFile(RetriableFileCopyCommand.java:184)        at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:124)        at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:100)        at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)        ... 11 moreCaused by: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 2; XML document structures must start and end within the same entity.        at org.apache.xerces.util.ErrorHandlerWrapper.createSAXParseException(Unknown Source)        at org.apache.xerces.util.ErrorHandlerWrapper.fatalError(Unknown Source)        at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)        at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)        at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)        at org.apache.xerces.impl.XMLScanner.reportFatalError(Unknown Source)        at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.endEntity(Unknown Source)        at org.apache.xerces.impl.XMLDocumentScannerImpl.endEntity(Unknown Source)        at org.apache.xerces.impl.XMLEntityManager.endEntity(Unknown Source)        at org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)        at org.apache.xerces.impl.XMLEntityScanner.skipChar(Unknown Source)        at org.apache.xerces.impl.XMLDocumentScannerImpl$PrologDispatcher.dispatch(Unknown Source)        at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)        at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)        at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)        at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)        at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)        at com.cloudera.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseXmlInputStream(XmlResponsesSaxParser.java:151)        ... 35 moreAnd also please help me in choosing the number of mappers and what should I do to copy the data faster to S3.,Resolved,Duplicate,HADOOP-13145,Unassigned,Logesh Rangan,Wed; 1 Nov 2017 17:01:51 +0000,Thu; 2 Nov 2017 21:00:01 +0000,Thu; 2 Nov 2017 21:00:01 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15011
HADOOP-15012,Improvement,Major,fs,Add readahead; dropbehind; and unbuffer to StreamCapabilities,A split from HADOOP-14872 to track changes that enhance StreamCapabilities class with READAHEAD; DROPBEHIND; and UNBUFFER capability.Discussions and code reviews are done in HADOOP-14872.,Resolved,Fixed,,John Zhuge,John Zhuge,Thu; 2 Nov 2017 20:44:37 +0000,Fri; 29 Dec 2017 21:54:54 +0000,Fri; 8 Dec 2017 05:22:43 +0000,,2.9.0,,,,https://issues.apache.org/jira/browse/HADOOP-15012
HADOOP-15013,Bug,Blocker,,Fix ResourceEstimator findbugs issues,Just see any recent report.,Resolved,Fixed,,Arun Suresh,Allen Wittenauer,Thu; 2 Nov 2017 20:18:53 +0000,Fri; 3 Nov 2017 02:27:12 +0000,Fri; 3 Nov 2017 00:24:19 +0000,,2.9.0;3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15013
HADOOP-15014,Improvement,Major,kms,KMS should log the IP address of the clients,Currently KMSMDCFilter only captures http request url and method; but not the remote address of the client. Storing this information in a thread local variable would help external authorizer plugins to do more thorough checks.,Patch Available,Unresolved,,Unassigned,Zsombor Gegesy,Fri; 3 Nov 2017 14:57:37 +0000,Sun; 5 Nov 2017 22:58:36 +0000,,,2.8.1,kms;log,RANGER-1869,,https://issues.apache.org/jira/browse/HADOOP-15014
HADOOP-15015,Bug,Trivial,conf;test,TestConfigurationFieldsBase to use SLF4J for logging,"TestConfigurationFieldsBase has a protected ""configDebug"" field used to turn logging on/off Presumably its there to allow people with code access to debug their classes. But if we switch to SLF4J you get controllable logging @ runtime.",Resolved,Fixed,,Steve Loughran,Steve Loughran,Fri; 3 Nov 2017 14:59:06 +0000,Sat; 4 Nov 2017 05:18:15 +0000,Sat; 4 Nov 2017 04:58:22 +0000,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15015
HADOOP-15016,Improvement,Major,,Cost-Based RPC FairCallQueue with Reservation support,"FairCallQueue is introduced to provide RPC resource fairness among different users. In current implementation; each user is weighted equally; and the processing priority for different RPC calls are based on how many requests that user sent before. This works well when the cluster is shared among several end-users.However; this has some limitations when a cluster is shared among both end-users and some service jobs; like some ETL jobs which run under a service account and need to issue lots of RPC calls. When NameNode becomes quite busy; this set of jobs can be easily backoffed and low-prioritied. We cannot simply treat this type jobs as ""bad"" user who randomly issues too many calls; as their calls are normal calls. Also; it is unfair to weight a end-user and a heavy service user equally when allocating RPC resources.One idea here is to introduce reservation support to RPC resources. That is; for some services; we reserve some RPC resources for their calls. This idea is very similar to how YARN manages CPU/memory resources among different resource queues. A little more details here: Along with existing FairCallQueue setup (like using 4 queues with different priorities); we would add some additional special queues; one for each special service user. For each special service user; we provide a guarantee RPC share (like 10% which can be aligned with its YARN resource share); and this percentage can be converted to a weight used in WeightedRoundRobinMultiplexer. A quick example; we have 4 default queues with default weights (8; 4; 2; 1); and two special service users (user1 with 10% share; and user2 with 15% share). So finally we'll have 6 queues; 4 default queues (with weights 8; 4; 2; 1) and 2 special queues (user1Queue weighted 15*10%/75%=2; and user2Queue weighted 15*15%/75%=3).For new coming RPC calls from special service users; they will be put directly to the corresponding reserved queue; for other calls; just follow current implementation.By default; there is no special user and all RPC requests follow existing FairCallQueue implementation.Would like to hear more comments on this approach; also want to know any other better solutions? Will put a detailed design once get some early comments.",Open,Unresolved,,Wei Yan,Wei Yan,Fri; 3 Nov 2017 16:02:01 +0000,Thu; 4 Jan 2018 00:06:56 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15016
HADOOP-15017,Bug,Major,, is not a valid DFS filename,I encountered the following error: My code is as follows:public class SqoopT,Resolved,Invalid,,Unassigned,Chen Jia,Mon; 6 Nov 2017 06:39:16 +0000,Tue; 7 Nov 2017 01:05:02 +0000,Mon; 6 Nov 2017 14:49:01 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15017
HADOOP-15018,Bug,Blocker,build,Update JAVA_HOME in create-release for Xenial Dockerfile,create-release expects the Oracle JDK when setting JAVA_HOME. HADOOP-14816 no longer includes the Oracle JDK; so we need to update this to point to OpenJDK instead.,Resolved,Fixed,,Andrew Wang,Andrew Wang,Mon; 6 Nov 2017 20:06:48 +0000,Wed; 8 Nov 2017 01:31:47 +0000,Wed; 8 Nov 2017 00:41:26 +0000,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15018
HADOOP-15019,Bug,Major,bin,Hadoop shell script classpath de-duping ignores HADOOP_USER_CLASSPATH_FIRST ,"If a user sets HADOOP_USER_CLASSPATH_FIRST=true and furthermore includes a directory that's already in Hadoop's classpath via HADOOP_CLASSPATH; that directory will appear later than it should in the eventual $CLASSPATH. I believe this is because the de-duping at https://github.com/apache/hadoop/blob/cbc632d9abf08c56a7fc02be51b2718af30bad28/hadoop-common-project/hadoop-common/src/main/bin/hadoop-functions.sh#L1200 is ignoring the ""before/after"" parameter.My way of reproduction; first build the following trivial Java program: With that; if you happen to have an entry in HADOOP_CLASSPATH that matches what Hadoop would produce; you'll find the ordering not honored. It's easiest to reproduce this with a match for HADOOP_CONF_DIR; as in the second case below: To re-iterate; what's surprising is that you can make an entry that's first in HADOOP_USER_CLASSPATH show up not first in the resulting classpath.I ran into this configuring bin/hive with a confdir that was being used for both HDFS and Hive; and flailing as to why my log4j2.properties wasn't being read. The one in my conf dir was lower in my classpath than one bundled in some Hive jar.",Open,Unresolved,,Unassigned,Philip Zeyliger,Mon; 6 Nov 2017 23:04:49 +0000,Wed; 8 Nov 2017 18:36:37 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15019
HADOOP-15020,Bug,Minor,benchmarks,NNBench not support run more than one map task on the same host,"When benchmark NameNode performance with NNBench; I start with pseudo distributed deploy. Everything goes well with ""-maps 1"". BUT with -maps N (n1) and -operation create_write; many exceptions meet during the benchmark.Hostname is part of the file path; which can  differentiate hosts. With more than two map tasks run on the same host; more than two map tasks may operate on the same file; leading to exceptions.17/11/07 15:22:32 INFO hdfs.NNBench:                  RAW DATA: AL Total #1: 8417/11/07 15:22:32 INFO hdfs.NNBench:                  RAW DATA: AL Total #2: 4317/11/07 15:22:32 INFO hdfs.NNBench:               RAW DATA: TPS Total (ms): 257017/11/07 15:22:32 INFO hdfs.NNBench:        RAW DATA: Longest Map Time (ms): 814.017/11/07 15:22:32 INFO hdfs.NNBench:                    RAW DATA: Late maps: 017/11/07 15:22:32 INFO hdfs.NNBench:              RAW DATA: # of exceptions: 30002017-11-07 14:54:08;082 INFO org.apache.hadoop.hdfs.NNBench: Exception recorded in op: Create/Write/Close2017-11-07 14:54:08;082 INFO org.apache.hadoop.hdfs.NNBench: Exception recorded in op: Create/Write/Close2017-11-07 14:54:08;083 INFO org.apache.hadoop.hdfs.NNBench: Exception recorded in op: Create/Write/Close",Resolved,Duplicate,MAPREDUCE-6363,Unassigned,zhoutai.zt,Tue; 7 Nov 2017 07:29:20 +0000,Wed; 8 Nov 2017 04:40:25 +0000,Wed; 8 Nov 2017 04:40:25 +0000,,2.7.2,,,,https://issues.apache.org/jira/browse/HADOOP-15020
HADOOP-15021,Bug,Minor,,Excluding private and limitiedprivate from javadoc causes broken links,"Examples:http://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FSDataInputStream.htmlCheck ""All Implemented Interfaces"" sectionhttp://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TaskAttemptContext.htmlSame sectionhttp://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Cluster.html#renewDelegationToken-org.apache.hadoop.security.token.Token-Method parametersI am not sure about the correct solution. Waiting for ideas or suggestions.",Open,Unresolved,HADOOP-9324,Unassigned,Andras Bokor,Tue; 7 Nov 2017 14:54:57 +0000,Fri; 10 Nov 2017 14:27:06 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15021
HADOOP-15022,Sub-task,Minor,fs/s3;test,s3guard IT tests increase R/W capacity of the test table by 1,Just noticed playing with the CLI that my allocated IOPs was 153; reset it to 10 R  10 W; after a few of the IT test runs it is now 13 eachassumption: every test run of the S3Guard CLI is increasing the allocated IO,Open,Unresolved,,Unassigned,Steve Loughran,Tue; 7 Nov 2017 17:04:20 +0000,Tue; 7 Nov 2017 17:04:20 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15022
HADOOP-15023,Improvement,Minor,,ValueQueue should also validate (lowWatermark * numValues) > 0 on construction,ValueQueue has precondition checks for each item independently; but does not check (int)(lowWatermark * numValues)  0. If the product is low enough; casting to int will wrap that to 0; causing problems later when filling / getting from the queue.code,Resolved,Fixed,,Xiao Chen,Xiao Chen,Tue; 7 Nov 2017 20:28:00 +0000,Thu; 16 Nov 2017 01:04:09 +0000,Thu; 16 Nov 2017 00:48:42 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15023
HADOOP-15024,Sub-task,Major,fs;fs/oss,AliyunOSS: support user agent configuration and include that & Hadoop version information to oss server,Provide oss client side Hadoop version to oss server; to help build access statistic metrics.,Resolved,Fixed,HADOOP-15029,SammiChen,SammiChen,Wed; 8 Nov 2017 07:50:31 +0000,Fri; 8 Dec 2017 14:05:28 +0000,Tue; 21 Nov 2017 12:06:31 +0000,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15024
HADOOP-15025,Bug,Major,,Ensure singleton for ResourceEstimatorService,HADOOP-15013 fixed static findbugs warnings but this has lead to the singleton being broken for ResourceEstimatorService. This jira tracks the fix for the same.,Resolved,Fixed,,Rui Li,Subru Krishnan,Wed; 8 Nov 2017 22:08:47 +0000,Thu; 9 Nov 2017 02:27:21 +0000,Thu; 9 Nov 2017 02:14:02 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15025
HADOOP-15026,Bug,Major,,Rebase ResourceEstimator start/stop scripts for branch-2,HADOOP-14840 introduced the ResourceEstimatorService which was cherry-picked from trunk to branch-2. The start/stop scripts need minor alignment with branch-2.,Resolved,Fixed,,Rui Li,Subru Krishnan,Wed; 8 Nov 2017 22:10:25 +0000,Thu; 9 Nov 2017 08:34:57 +0000,Thu; 9 Nov 2017 08:34:57 +0000,,2.9.0,,,,https://issues.apache.org/jira/browse/HADOOP-15026
HADOOP-15027,Sub-task,Major,fs/oss,AliyunOSS: Support multi-thread pre-read to improve read from Hadoop to Aliyun OSS performance,Currently; AliyunOSSInputStream uses single thread to read data from AliyunOSS;  so we can do some refactoring by using multi-thread pre-read to improve read performance.,Patch Available,Unresolved,,wujinhu,wujinhu,Fri; 10 Nov 2017 06:22:01 +0000,Thu; 11 Jan 2018 15:17:32 +0000,,,3.0.0,,,HADOOP-15039,https://issues.apache.org/jira/browse/HADOOP-15027
HADOOP-15028,Bug,Major,,Got errors while running org.apache.hadoop.io.TestSequenceFileAppend,I ran the test case org.apache.hadoop.io.TestSequenceFileAppend in branch-2.6.4; I got the following errors:Running org.apache.hadoop.io.TestSequenceFileAppendTests run: 4; Failures: 0; Errors: 2; Skipped: 0; Time elapsed: 0.801 sec &lt; FAILURE! - in org.apache.hadoop.io.TestSequenceFileAppendtestAppendBlockCompression(org.apache.hadoop.io.TestSequenceFileAppend)  Time elapsed: 0.117 sec  &lt; ERROR!java.io.IOException: File is corrupt!        at org.apache.hadoop.io.SequenceFile$Reader.readBlock(SequenceFile.java:2179)        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2583)        at org.apache.hadoop.io.TestSequenceFileAppend.verifyAll4Values(TestSequenceFileAppend.java:309)        at org.apache.hadoop.io.TestSequenceFileAppend.testAppendBlockCompression(TestSequenceFileAppend.java:205)testAppendSort(org.apache.hadoop.io.TestSequenceFileAppend)  Time elapsed: 0.013 sec  &lt; ERROR!java.io.IOException: File is corrupt!        at org.apache.hadoop.io.SequenceFile$Reader.readBlock(SequenceFile.java:2179)        at org.apache.hadoop.io.SequenceFile$Reader.nextRaw(SequenceFile.java:2488)        at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:2923)        at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:2861)        at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:2809)        at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:2850)        at org.apache.hadoop.io.TestSequenceFileAppend.testAppendSort(TestSequenceFileAppend.java:286)But everything is OK in branch-2.6.5..The maven command is  'mvn test -Pnative -Dtest=TestSequenceFileAppend',Resolved,Fixed,HADOOP-7139,Unassigned,bd17kaka,Fri; 10 Nov 2017 08:23:06 +0000,Wed; 15 Nov 2017 13:41:49 +0000,Tue; 14 Nov 2017 03:51:33 +0000,,2.6.4,,,,https://issues.apache.org/jira/browse/HADOOP-15028
HADOOP-15029,Sub-task,Major,fs;fs/oss,AliyunOSS:  Add User-Agent header in HTTP requests to the OSS server,,Resolved,Duplicate,HADOOP-15024,Genmao Yu,Genmao Yu,Fri; 10 Nov 2017 09:52:05 +0000,Fri; 10 Nov 2017 10:02:48 +0000,Fri; 10 Nov 2017 09:53:59 +0000,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15029
HADOOP-15030,Bug,Critical,,[branch-2] Include hadoop-cloud-storage-project in the main hadoop pom modules,During validation of 2.9.0. RC; Vrushali C noticed that the hadoop-cloud-storage-project is not included in the main hadoop pom modules so it's not being managed including mvn versions:set for releases. This was fixed in trunk with HADOOP-14004; doing the same for branch-2.,Resolved,Fixed,,Subru Krishnan,Subru Krishnan,Fri; 10 Nov 2017 20:48:28 +0000,Fri; 10 Nov 2017 22:24:55 +0000,Fri; 10 Nov 2017 22:24:55 +0000,,2.9.0,,,HADOOP-14004,https://issues.apache.org/jira/browse/HADOOP-15030
HADOOP-15031,Bug,Minor,common,Fix javadoc issues in Hadoop Common,Fix the following javadocs warning in Hadoop Common,Resolved,Fixed,,Mukul Kumar Singh,Mukul Kumar Singh,Sun; 12 Nov 2017 17:29:04 +0000,Mon; 13 Nov 2017 18:36:29 +0000,Mon; 13 Nov 2017 14:13:11 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15031
HADOOP-15032,Improvement,Major,,Enable Optimize Hadoop RPC encryption performance for branch-2,Base on HADOOP-10768; this ticket is targeted to enable Optimize Hadoop RPC encryption performance for branch-2,Patch Available,Unresolved,,Dapeng Sun,Dapeng Sun,Mon; 13 Nov 2017 03:38:50 +0000,Thu; 16 Nov 2017 06:40:10 +0000,,,2.8.1,,,HADOOP-10768,https://issues.apache.org/jira/browse/HADOOP-15032
HADOOP-15033,Improvement,Major,performance;util,Use java.util.zip.CRC32C for Java 9 and above,java.util.zip.CRC32C implementation is available since Java 9.https://docs.oracle.com/javase/9/docs/api/java/util/zip/CRC32C.htmlPlatform specific assembler intrinsics make it more effective than any pure Java implementation.Hadoop is compiled against Java 8 but class constructor may be accessible with method handle on 9 to instances implementing Checksum in runtime.,Resolved,Fixed,,Dmitry Chuyko,Dmitry Chuyko,Mon; 13 Nov 2017 09:49:32 +0000,Thu; 11 Jan 2018 16:09:25 +0000,Thu; 11 Jan 2018 10:50:01 +0000,,3.0.0,Java9;common;jdk9,,HADOOP-11660;HADOOP-10674;HADOOP-11123,https://issues.apache.org/jira/browse/HADOOP-15033
HADOOP-15034,Sub-task,Minor,fs/s3,S3Guard instrumentation to include cost of DynamoDB ops as metric,DynamoDB ops can return the cost of the operation in ConsumedCapacity methods.by switching to the API calls which include this in the results are used in DynamoDBMetadataStore; then we could provide live/aggregate stats on IO capacity used. This could aid in live monitoring S3Guard load; and help assess the cost of queries,Open,Unresolved,,Unassigned,Steve Loughran,Mon; 13 Nov 2017 19:41:56 +0000,Mon; 13 Nov 2017 19:41:56 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15034
HADOOP-15035,Sub-task,Major,fs/s3,S3Guard to perform retry and translation of exceptions,"S3Guard doesn't translate DDB exceptions; nor does it do much in the way of retry of network problems. Some queries do a bit. HADOOP-13786 marks up the code with its new @ attribute; this JIRA is ""decide what to do and fix""Be good to  have some stack traces of failures",Resolved,Duplicate,HADOOP-13761,Unassigned,Steve Loughran,Mon; 13 Nov 2017 20:40:40 +0000,Tue; 5 Dec 2017 15:00:34 +0000,Tue; 5 Dec 2017 15:00:34 +0000,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15035
HADOOP-15036,Bug,Major,,Update LICENSE.txt for HADOOP-14840,As noticed by Anu Engineer:Looks like HADOOP-14840 added a dependency on  oj! Algorithms - version 43.0 ; but we have just added  oj! Algorithms - version 43.0  to the LICENSE.txt . The right addition to the LICENESE.txt should contain the original MIT License; especially  Copyright (c) 2003-2017 Optimatika .,Resolved,Fixed,,Arun Suresh,Arun Suresh,Mon; 13 Nov 2017 22:30:48 +0000,Mon; 13 Nov 2017 23:03:58 +0000,Mon; 13 Nov 2017 22:42:27 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15036
HADOOP-15037,Improvement,Major,,Add site release notes for OrgQueue and resource types,Let's add some small blurbs and doc links to the site release notes for these features.,Resolved,Fixed,,Andrew Wang,Andrew Wang,Tue; 14 Nov 2017 01:45:36 +0000,Tue; 14 Nov 2017 03:09:39 +0000,Tue; 14 Nov 2017 02:50:22 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15037
HADOOP-15038,New Feature,Major,fs,Abstract MetadataStore in S3Guard into a common module.,Open this JIRA to discuss if we should move MetadataStore in S3Guard into a common module. Based on this work; other filesystem or object store can implement their own metastore for optimization (known issues like consistency problem and metadata operation performance). Steve Loughran and other guys have done many base and great works in S3Guard. It is very helpful to start work. I did some perf test in HADOOP-14098; and started related work for Aliyun OSS.  Indeed there are still works to do for S3Guard; like metadata cache inconsistent with S3 and so on. It also will be a problem for other object store. However; we can do these works in parallel.Steve Loughran Aaron Fabbri Kai Zheng Any suggestion is appreciated.,Open,Unresolved,,Unassigned,Genmao Yu,Tue; 14 Nov 2017 07:20:18 +0000,Mon; 4 Dec 2017 15:15:35 +0000,,,3.0.0-beta1,,,HADOOP-14098,https://issues.apache.org/jira/browse/HADOOP-15038
HADOOP-15039,Improvement,Minor,fs;fs/oss;fs/s3,Move SemaphoredDelegatingExecutor to hadoop-common,Detailed discussions in HADOOP-14999 and HADOOP-15027.share SemaphoredDelegatingExecutor and move it to hadoop-common.cc Steve Loughran,Resolved,Fixed,,Genmao Yu,Genmao Yu,Tue; 14 Nov 2017 11:16:41 +0000,Thu; 14 Dec 2017 03:45:59 +0000,Wed; 6 Dec 2017 04:07:05 +0000,,3.0.0-beta1,,,HADOOP-14999;HADOOP-15027,https://issues.apache.org/jira/browse/HADOOP-15039
HADOOP-15040,Bug,Major,fs/s3,AWS SDK NPE bug spams logs w/ Yarn Log Aggregation,My colleagues working with Yarn log aggregation found that they were getting this message spammed in their logs when they used an s3a:// URI for logs (yarn.nodemanager.remote-app-log-dir): This happens even though the aws sdk cloudwatch metrics reporting was disabled (default); which is a bug. I filed a github issue and it looks like a fix should be coming around SDK release 1.11.229 or so.,Open,Unresolved,,Aaron Fabbri,Aaron Fabbri,Tue; 14 Nov 2017 23:13:15 +0000,Tue; 14 Nov 2017 23:13:15 +0000,,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15040
HADOOP-15041,Bug,Critical,,"XInclude support in .xml configuration file is broken after ""5eb7dbe9b31a45f57f2e1623aa1c9ce84a56c4d1"" commit",XInclude support in .xml configuration file is broken after following check-in.  Since there is no JIRA number in the following commit message; create a new JIRA to track the issue. commit 5eb7dbe9b31a45f57f2e1623aa1c9ce84a56c4d1Author: Arun Suresh asuresh@apache.orgDate:   Thu Nov 9 15:15:51 2017 -0800Fixing Job History Server Configuration parsing. (Jason Lowe via asuresh),Resolved,Duplicate,NULL,Arun Suresh,SammiChen,Wed; 15 Nov 2017 11:30:25 +0000,Mon; 20 Nov 2017 20:48:07 +0000,Mon; 20 Nov 2017 20:48:07 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15041
HADOOP-15042,Bug,Minor,fs/azure,Azure PageBlobInputStream.skip() can return negative value when numberOfPagesRemaining is 0,PageBlobInputStream::skip--skipImpl returns negative values when numberOfPagesRemaining=0. This can cause wrong position to be set in NativeAzureFileSystem::seek() and can lead to errors.,Resolved,Fixed,,Rajesh Balamohan,Rajesh Balamohan,Thu; 16 Nov 2017 00:14:17 +0000,Fri; 8 Dec 2017 19:01:25 +0000,Wed; 29 Nov 2017 22:30:14 +0000,,2.9.0,,,HADOOP-14552,https://issues.apache.org/jira/browse/HADOOP-15042
HADOOP-15043,Bug,Major,,Download page must not link to dist.apache.org,The download page http://hadoop.apache.org/releases.html currently links to dist.apache.org for KEYS; hashes and sigs.However dist.a.o is only intended as a developer staging area; it is not intended for the general public to use.Please change the links to usehttps://www.apache.org/dist/hadoop/common/KEYSetc. instead.Note that https should always be used for the KEYS; hashes and sigs.,Open,Unresolved,,Unassigned,Sebb,Thu; 16 Nov 2017 14:42:11 +0000,Thu; 16 Nov 2017 14:42:11 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15043
HADOOP-15044,Sub-task,Major,fs/azure,Wasb getFileBlockLocations() returns too many locations.,The wasb mimicking of getFileBlockLocations() uses the length of the file as the number to use to calculate the # of blocks to create (i.e. file.length/blocksize); when it should be just the range of the request.As a result; you always get the number of blocks in the total file; not the number spanning the range of (start; len). If this is less (i.e start  0 or len  file.length); you end up with some 0-byte-range blocks at the end,Open,Unresolved,,Steve Loughran,Steve Loughran,Thu; 16 Nov 2017 19:36:47 +0000,Tue; 19 Dec 2017 16:42:04 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15044
HADOOP-15045,Bug,Major,build;documentation,ISA-L build options are documented in branch-2,In branch-2; YARN-4849 unintentionally documented ISA-L build options in BUILDING.txt. This issue is similar to YARN-7398.,Resolved,Fixed,,Akira Ajisaka,Akira Ajisaka,Fri; 17 Nov 2017 05:23:06 +0000,Tue; 21 Nov 2017 07:14:26 +0000,Tue; 21 Nov 2017 04:53:27 +0000,,2.9.0,,,,https://issues.apache.org/jira/browse/HADOOP-15045
HADOOP-15046,Bug,Major,documentation,Document Apache Hadoop does not support Java 9 in BUILDING.txt,"Now the java version is documented as ""JDK 1.8+"" or ""JDK 1.7+""; we should update this to ""JDK 1.8"" or ""JDK 1.7 or 1.8"" to exclude Java 9.",Resolved,Fixed,,Hanisha Koneru,Akira Ajisaka,Fri; 17 Nov 2017 08:52:29 +0000,Tue; 21 Nov 2017 16:33:26 +0000,Tue; 21 Nov 2017 16:09:01 +0000,,,newbie,,HADOOP-11123,https://issues.apache.org/jira/browse/HADOOP-15046
HADOOP-15047,Bug,Major,build;documentation,Python is required for -Preleasedoc but not documented in branch-2.8,"Python is required for -Preleasedoc but not documented in branch-2.8.	In trunk and branch-3.0; it was documented by HADOOP-10854.	In branch-2 and branch-2.9; it was documented by YARN-4849.",Resolved,Fixed,,Bharat Viswanadham,Akira Ajisaka,Fri; 17 Nov 2017 09:04:19 +0000,Wed; 22 Nov 2017 02:15:37 +0000,Wed; 22 Nov 2017 02:15:37 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15047
HADOOP-15048,Sub-task,Minor,fs/azure,ITestNativeAzureFileSystemLive.testLeaseAsDistributedLock assert failure in parallel run,ITestNativeAzureFileSystemLive.testLeaseAsDistributedLock failed with assert(irstEndTime  secondStartTime)Fairly uniformative stack trace on an assert failure; test run -Dparallel-tests -DtestsThreadCount=10  -DscaleIrrespective of root cause; exception needs changing to provide enough information; include raised exceptions in thrown failures; etc.,Resolved,Won't Fix,,Unassigned,Steve Loughran,Fri; 17 Nov 2017 10:39:10 +0000,Fri; 17 Nov 2017 10:42:22 +0000,Fri; 17 Nov 2017 10:42:22 +0000,,3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15048
MAPREDUCE-7010,Improvement,Major,,Make Job History File Permissions configurable,Currently the mapreduce job history files are written with 770 permissions which can be accessed by job user or other user part of hadoop group.There might be users who are not part of the hadoop group but want to access these history files. We should provide ability to change the default permissions for staging files.The default should remain 770.,Open,Unresolved,,Gergely Nov√°k,Andras Bokor,Fri; 17 Nov 2017 12:10:16 +0000,Fri; 1 Dec 2017 13:34:24 +0000,,,,,,,https://issues.apache.org/jira/browse/MAPREDUCE-7010
HADOOP-15050,Bug,Major,fs/swift,swift:// doesn't support createNonRecursive. hence the new FS createFile(path) builder,swift throws an exception in createNonRecursive(); so fails to work with the new FileSystem.createFile(path) which uses that mode by default (because its the right thing to do,Open,Unresolved,,Unassigned,Steve Loughran,Fri; 17 Nov 2017 18:28:01 +0000,Fri; 17 Nov 2017 18:29:39 +0000,,,2.9.0,,,,https://issues.apache.org/jira/browse/HADOOP-15050
HADOOP-15051,New Feature,Major,fs,FSDataOutputStream returned by LocalFileSystem#createNonRecursive doesn't have hflush capability,"See HBASE-19289 for background information.Here is related hbase code (fs is instance of LocalFileSystem): StreamCapabilities is used to poll ""hflush"" capability.Sean Busbey suggested fixing this in hadoop.",Resolved,Won't Fix,,Unassigned,Ted Yu,Fri; 17 Nov 2017 21:08:35 +0000,Sun; 19 Nov 2017 11:08:26 +0000,Sun; 19 Nov 2017 11:08:26 +0000,,3.0.0-beta1,,,HADOOP-13327,https://issues.apache.org/jira/browse/HADOOP-15051
HADOOP-15052,Bug,Major,,hadoop-resourceestimator throws dependency errors during build,Noticed the following errors during mvn install/package. Seems to be a misconfigured pom.xml for the tool.,Open,Unresolved,,Unassigned,Kuhu Shukla,Mon; 20 Nov 2017 19:17:09 +0000,Mon; 20 Nov 2017 23:31:20 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15052
HADOOP-15053,Bug,Major,performance,new Server(out).write call delay occurs ,In createBlockOutputStream; Block details write call takes more time to get acknowledgement.new Sender(out).writeBlock(this.block; .............pipeline has three data nodes.,Open,Unresolved,,Unassigned,Suganya,Mon; 20 Nov 2017 20:27:18 +0000,Wed; 22 Nov 2017 10:38:12 +0000,,,2.5.1,,,,https://issues.apache.org/jira/browse/HADOOP-15053
HADOOP-15054,Bug,Major,,upgrade hadoop dependency on commons-codec to 1.11,https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-auth/3.0.0-beta1 retains the dependency on an old commons-codec version (1.4).And hadoop-common.Would it be possible to consider an upgrade to 1.11?,Resolved,Fixed,,Bharat Viswanadham,PJ Fanning,Mon; 20 Nov 2017 21:10:17 +0000,Wed; 29 Nov 2017 17:48:44 +0000,Wed; 29 Nov 2017 17:28:05 +0000,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15054
HADOOP-15055,New Feature,Major,fs/s3,Add s3 metrics from AWS SDK to s3a metrics tracking,,Resolved,Duplicate,HADOOP-13551,Sean Mackrory,Sean Mackrory,Mon; 20 Nov 2017 21:12:48 +0000,Tue; 21 Nov 2017 11:51:06 +0000,Tue; 21 Nov 2017 11:51:06 +0000,,,,,HADOOP-14475,https://issues.apache.org/jira/browse/HADOOP-15055
HADOOP-15056,Improvement,Minor,test,Fix TestUnbuffer#testUnbufferException failure,Hello! I am a new contributor and actually contributing to open source for the very first time.  I pulled down Hadoop today and when running the tests I encountered a failure with the TestUnbuffer#testUnbufferException test.The unbuffer code has recently gone through some changes and I believe this test case may have been overlooked. Using today's git commit (659e85e304d070f9908a96cf6a0e1cbafde6a434); and upon running the test case; there is an expect mock for an exception UnsupportedOperationException that is no longer being thrown. It would appear that a test like this would be valuable so my initial proposed patch did not remove it. Instead; I removed the conditions that were guarding the cast from being able to fire  as was the previous behavior. Now when we encounter an object that doesn't have the UNBUFFERED StreamCapability; it will throw an error as it did prior to the recent changes. Please review and let me know what you think!,Resolved,Fixed,HDFS-12815,Jack Bearden,Jack Bearden,Tue; 21 Nov 2017 05:08:30 +0000,Fri; 29 Dec 2017 21:55:15 +0000,Fri; 8 Dec 2017 05:21:59 +0000,,2.9.0,,,,https://issues.apache.org/jira/browse/HADOOP-15056
HADOOP-15057,Sub-task,Minor,fs/s3,s3guard bucket-info command to include default bucket encryption info,AWS S3 now has the notion of default bucket encryption http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETencryption.htmlOnce set; all data uploaded is automatically encrypted; without needing to set any client optionsWe should provide that info in the s3guard bucket-info command; so you can see that data being uploaded really is encrypted.,Open,Unresolved,,Unassigned,Steve Loughran,Tue; 21 Nov 2017 10:29:46 +0000,Tue; 21 Nov 2017 10:29:46 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15057
HADOOP-15058,Bug,Blocker,,create-release site build outputs dummy shaded jars due to skipShade,,Resolved,Fixed,,Andrew Wang,Andrew Wang,Tue; 21 Nov 2017 18:45:41 +0000,Mon; 4 Dec 2017 23:54:46 +0000,Mon; 4 Dec 2017 22:47:16 +0000,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15058
HADOOP-15059,Bug,Blocker,security,3.0 deployment cannot work with old version MR tar ball which breaks rolling upgrade,"I tried to deploy 3.0 cluster with 2.9 MR tar ball. The MR job is failed because following error: I think it is due to token incompatiblity change between 2.9 and 3.0. As we claim ""rolling upgrade"" is supported in Hadoop 3; we should fix this before we ship 3.0 otherwise all MR running applications will get stuck during/after upgrade.",Resolved,Fixed,,Jason Lowe,Junping Du,Tue; 21 Nov 2017 20:54:52 +0000,Wed; 13 Dec 2017 01:04:51 +0000,Fri; 8 Dec 2017 16:05:30 +0000,,,,,HADOOP-13123;HDFS-12920,https://issues.apache.org/jira/browse/HADOOP-15059
HADOOP-15060,Bug,Major,,TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime flaky,,Resolved,Fixed,,Miklos Szegedi,Miklos Szegedi,Wed; 22 Nov 2017 00:18:55 +0000,Wed; 10 Jan 2018 19:22:09 +0000,Wed; 10 Jan 2018 19:22:09 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15060
HDFS-12847,Task,Major,test,Regenerate editsStored and editsStored.xml in HDFS tests,From HDFS-12840; we found that the `editsStored` in HDFS tests missing a few operations; i.e.; the following operations from DFSTestUtils#runOperations(). We should re-generate to edits and related XML to sync with the code.,Resolved,Fixed,,Lei (Eddy) Xu,Lei (Eddy) Xu,Wed; 22 Nov 2017 00:26:55 +0000,Wed; 22 Nov 2017 18:49:16 +0000,Wed; 22 Nov 2017 18:26:08 +0000,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HDFS-12847
HADOOP-15062,Bug,Major,,TestCryptoStreamsWithOpensslAesCtrCryptoCodec fails on Debian 9,This happened due to the following openssl change:https://github.com/openssl/openssl/commit/ff4b7fafb315df5f8374e9b50c302460e068f188,Open,Unresolved,,Miklos Szegedi,Miklos Szegedi,Wed; 22 Nov 2017 03:13:28 +0000,Wed; 22 Nov 2017 03:32:22 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15062
HADOOP-15063,Sub-task,Major,fs/oss,IOException may be thrown when read from Aliyun OSS in some case,"IOException will be thrown in this case1. set part size = n(102400)2. assume current position = 0; then partRemaining = 1024003. we call seek(pos = 101802); with pos  position &amp; pos  position + partRemaining; so it will skip pos - position bytes; but partRemaining remains the same4. if we read bytes more than n - pos; it will throw IOException.Current code: Logs:java.io.IOException: Failed to read from stream. Remaining:101802	at org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream.read(AliyunOSSInputStream.java:182)	at org.apache.hadoop.fs.FSInputStream.read(FSInputStream.java:75)	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)How to re-produce:1. create a file with 10MB size2.",Resolved,Duplicate,NULL,wujinhu,wujinhu,Wed; 22 Nov 2017 05:44:16 +0000,Wed; 22 Nov 2017 12:01:15 +0000,Wed; 22 Nov 2017 12:01:15 +0000,,3.0.0-alpha2;3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15063
HADOOP-15064,Bug,Major,,hadoop-common and hadoop-auth 3.0.0-beta1 expose a dependency on slf4j-log4j12,https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common/3.0.0-beta1https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-auth/3.0.0-beta1 One of the ideas of SLF4J is that you should depend on the API jar and it is up to users of your lib to add a dependency to their preferred SLF4J implementation. You can only have one such implementation jar on your classpath.If the hadoop build uses log4j in its tests; then this can be made a test dependency and not a general compile or runtime dependency.,Open,Unresolved,,Unassigned,PJ Fanning,Wed; 22 Nov 2017 10:10:57 +0000,Wed; 22 Nov 2017 10:14:25 +0000,,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15064
HADOOP-15065,Improvement,Minor,,Make mapreduce specific GenericOptionsParser arguments optional,org.apache.hadoop.util.GenericOptionsParser is widely used to use common arguments in all the command line applications.Some of the common arguments are really generic: But some are mapreduce specific: In the review of HDFS-12588 it was suggested to remove/turn off the mapreduce specific arguments if they are not required (for example in case of starting namenode or datanode).,Open,Unresolved,,Unassigned,Elek; Marton,Wed; 22 Nov 2017 17:12:35 +0000,Wed; 22 Nov 2017 17:12:35 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15065
HADOOP-15066,Bug,Major,scripts,Spurious error stopping secure datanode,There is a spurious error when stopping a secure datanode. The error appears benign. The service was stopped correctly.,Patch Available,Unresolved,,Bharat Viswanadham,Arpit Agarwal,Wed; 22 Nov 2017 19:02:39 +0000,Wed; 22 Nov 2017 22:06:31 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15066
HADOOP-15067,Bug,Major,,GC time percentage reported in JvmMetrics should be a gauge; not counter,A new GcTimeMonitor class has been recently added; and the corresponding metrics added in JvmMetrics.java; line 190: Since GC time percentage can go up and down; a gauge rather than counter should be used to report it. That is; addCounter should be replaced with addGauge above.,Resolved,Fixed,,Misha Dmitriev,Misha Dmitriev,Wed; 22 Nov 2017 19:41:39 +0000,Tue; 28 Nov 2017 18:54:14 +0000,Thu; 23 Nov 2017 17:02:44 +0000,,,,,HBASE-19238,https://issues.apache.org/jira/browse/HADOOP-15067
HADOOP-15068,Improvement,Major,common,cancelToken and renewToken should use shortUserName consistently,AbstractDelegationTokenSecretManager is used by many external projects including Hive. This class provides implementations of renewToken and cancelToken which are used for the delegation token management. The methods are semantically inconsistent. Specifically; when you call cancelToken; the string value of the canceller is used to get the Kerberos shortname and then compared with the renewer value of the token to be cancelled. While in case of renewToken; the string value which is passed in is used directly to compare with the renewer value of the token.This inconsistency means that applications need to know about this subtle difference and pass in the shortname while renewing the token; while it can pass the full kerberos username during cancellation. Can we change the renewToken method such that it uses the shortName similar to the cancelToken method?,Open,Unresolved,,Unassigned,Vihang Karajgaonkar,Thu; 23 Nov 2017 04:30:10 +0000,Tue; 28 Nov 2017 23:49:19 +0000,,,2.8.2,,,HIVE-16708,https://issues.apache.org/jira/browse/HADOOP-15068
HADOOP-15069,Sub-task,Minor,build,support git-secrets commit hook to keep AWS secrets out of git,"The latest Uber breach looks like it involved AWS keys in git repos.Nobody wants that; which is why amazon provide git-secrets; a script you can use to scan a repo and its history; and add as an automated check.Anyone can set this up; but there are a few false positives in the scan; mostly from longs and a few all-upper-case constants. These can all be added to a .gitignore file.Also: mention git-secrets in the aws testing docs; say ""use it""",Patch Available,Unresolved,,Steve Loughran,Steve Loughran,Thu; 23 Nov 2017 15:23:20 +0000,Mon; 27 Nov 2017 12:04:46 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15069
HADOOP-15070,Improvement,Minor,fs;test,add test to verify FileSystem and paths differentiate on user info,"Add a test to verify that userinfo data is (correctly) used to differentiate the entries in the FS cache; so are treated as different filesystems.	This is criticalk for wasb; which uses the username to identify the container; in a path like wasb:container1@stevel.azure.net. This works in Hadoop; but SPARK-22587 shows that it may not be followed everywhere (and given there's no documentation; who can fault them?)	AbstractFileSystem.checkPath looks suspiciously like it's path validation just checks host; not authority. That needs a test too.	And we should cut the @LimitedPrivate(HDFS; Mapreduce) from Path.makeQualified. If MR needs it; it should be considered open to all apps using the Hadoop APIs. Until I looked at the code I thought it was...",Patch Available,Unresolved,,Steve Loughran,Steve Loughran,Mon; 27 Nov 2017 11:14:15 +0000,Tue; 28 Nov 2017 06:43:44 +0000,,,2.8.2,,,SPARK-22587,https://issues.apache.org/jira/browse/HADOOP-15070
HADOOP-15071,Sub-task,Minor,documentation;fs/s3,s3a troubleshooting docs to add a couple more failure modes,I've got some more troubleshooting entries to add,Resolved,Fixed,,Steve Loughran,Steve Loughran,Mon; 27 Nov 2017 16:03:35 +0000,Tue; 5 Dec 2017 15:27:07 +0000,Tue; 5 Dec 2017 15:06:38 +0000,,2.8.2,,,SPARK-22526,https://issues.apache.org/jira/browse/HADOOP-15071
HADOOP-15072,Improvement,Major,security,Upgrade Apache Kerby version to 1.1.0,Apache Kerby 1.1.0 implements cross-realm support; and also includes a GSSAPI module.,Patch Available,Unresolved,,Jiajia Li,Jiajia Li,Wed; 29 Nov 2017 02:08:22 +0000,Mon; 4 Dec 2017 03:08:16 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15072
HADOOP-15073,Bug,Major,common,SequenceFile.Reader will unexpectedly quit next() iterator while the file ends with sync and appended,The SequenceFile.Writer will insert SYNC into file every SYNC_INTERVAL.In the case that SequenceFile ends with SYNC coincidentally; and another Writer open it with mode AppendIfExits; there meets the BUG.For the AppendIfExits set appendMode _ to _true ; the init method will insert another SYNC. In such case; there will be two SYNC MAKR continuously.In SequenceFile.Reader; the method readRecordLength will only test SYNC once; when there's two SYNC MARK; the length will be -1(The begining of another SYNC); which means the end of file causing the next method quit unexpectedly.,Open,Unresolved,,Unassigned,Howard Lee,Wed; 29 Nov 2017 08:54:11 +0000,Thu; 30 Nov 2017 03:06:55 +0000,,,2.6.3;2.7.4;2.8.2;3.0.0-alpha3,,,,https://issues.apache.org/jira/browse/HADOOP-15073
HADOOP-15074,Bug,Major,,SequenceFile#Writer flush does not update the length of the written file.,SequenceFile#Writer flush does not update the length of the file. This happens because as part of the flush; UPDATE_LENGTH flag is not passed to the DFSOutputStream#hsync.,Open,Unresolved,,Mukul Kumar Singh,Mukul Kumar Singh,Wed; 29 Nov 2017 10:35:45 +0000,Tue; 5 Dec 2017 13:44:38 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15074
HADOOP-15075,New Feature,Major,hdfs-client;security,Implement KnoxSSO for hadoop web UIs (hdfs; yarn; history server etc.),Need to implement Knox SSO login feature for hadoop webUIs like HDFS Namenode; Yarn RM; MR2 Job history server; spark etc. I know that we have SPNEGO feature enabled; however having Knox SSO login feature seems to be a good option,Resolved,Not A Problem,,Unassigned,madhu raghavendra,Wed; 29 Nov 2017 11:45:44 +0000,Wed; 29 Nov 2017 12:33:17 +0000,Wed; 29 Nov 2017 12:33:16 +0000,,3.0.0-alpha3,,,HADOOP-11717,https://issues.apache.org/jira/browse/HADOOP-15075
HADOOP-15076,Sub-task,Major,documentation;fs/s3,Enhance s3a troubleshooting docs; add perf section,"A recurrent theme in s3a-related JIRAs; support calls etc is ""tried upgrading the AWS SDK JAR and then I got the error ..."". We know here ""don't do that""; but its not something immediately obvious to lots of downstream users who want to be able to drop in the new JAR to fix things/add new featuresWe need to spell this out quite clearlyi ""you cannot safely expect to do this. If you want to upgrade the SDK; you will need to rebuild the whole of hadoop-aws with the maven POM updated to the latest version; ideally rerunning all the tests to make sure something hasn't broken. Maybe near the top of the index.md file; along with ""never share your AWS credentials with anyone""",Open,Unresolved,,Steve Loughran,Steve Loughran,Thu; 30 Nov 2017 10:14:28 +0000,Fri; 12 Jan 2018 23:42:58 +0000,,,2.8.2,,,,https://issues.apache.org/jira/browse/HADOOP-15076
HADOOP-15077,New Feature,Minor,common;fs,Support for setting user agent for (GCS)Google Cloud Storage,Currently; when we use AWS/Azure/Aliyun as a FileSystem; we can set user agent for the underneath HTTP communication with these cloud providers by setting fs.s3a.user.agent.prefix; fs.azure.user.agent.prefix; or fs.oss.user.agent.prefix properties.But not GCS(Google Cloud); So is it possible to provide this new feature ?,Resolved,Invalid,,Unassigned,Yu LIU,Thu; 30 Nov 2017 16:59:25 +0000,Thu; 30 Nov 2017 17:09:13 +0000,Thu; 30 Nov 2017 17:09:13 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15077
HADOOP-15078,Bug,Major,security,dtutil ignores nonexistent files,While investigating issues in HADOOP-15059 I ran the dtutil append command like this: expecting the append command to translate the existing tokens in file foo into the currently non-existent file foo.pb.  Instead the command executed without error and overwrote foo instead of creating foo.pb as I expected.  I now understand how append works; but it was very surprising to have dtutil silently ignore filenames requested on the command-line.  At best it is a bit surprising to the user.  At worst it clobbers data the user did not expect to be overwritten.,Open,Unresolved,,Unassigned,Jason Lowe,Thu; 30 Nov 2017 19:32:08 +0000,Thu; 30 Nov 2017 19:32:08 +0000,,,3.0.0-alpha1,,,,https://issues.apache.org/jira/browse/HADOOP-15078
HADOOP-15079,Sub-task,Critical,,ITestS3AFileOperationCost#testFakeDirectoryDeletion failing after OutputCommitter patch,"I see this test failing with ""object_delete_requests expected:1 but was:2"". I printed stack traces whenever this metric was incremented; and found the root cause to be that innerMkdirs is now causing two calls to delete fake directories when it previously caused only one. It is called once inside createFakeDirectory; and once directly inside innerMkdirs later:",Patch Available,Unresolved,,Steve Loughran,Sean Mackrory,Thu; 30 Nov 2017 22:01:08 +0000,Sat; 13 Jan 2018 14:58:39 +0000,,,3.1.0,,,HADOOP-14594,https://issues.apache.org/jira/browse/HADOOP-15079
HADOOP-15080,Bug,Blocker,fs/oss,"Aliyun OSS: update oss sdk from 2.8.1 to 2.8.3 to remove its dependency on Cat-x ""json-lib""",Cat-X dependency on org.json via derived json-lib. OSS SDK has a dependency on json-lib. In LEGAL-245; the org.json library (from which json-lib may be derived) is released under a category-x license.,Resolved,Fixed,,SammiChen,Chris Douglas,Thu; 30 Nov 2017 23:40:51 +0000,Fri; 15 Dec 2017 07:05:42 +0000,Thu; 7 Dec 2017 20:25:49 +0000,,3.0.0-beta1,,,HADOOP-14964,https://issues.apache.org/jira/browse/HADOOP-15080
HADOOP-15081,Bug,Major,common,"org.apache.hadoop.util.JvmPauseMonitor	 Detected pause in JVM or host machine (eg GC)   cause  ResourceManager   exit  ","org.apache.hadoop.util.JvmPauseMonitor	Detected pause in JVM or host machine (eg GC): pause of approximately 2562msGC pool 'ConcurrentMarkSweep' had collection(s): count=4 time=3040msResourceManager   NodeManager    exit   .....",Open,Unresolved,,Unassigned,liuxiaobin,Fri; 1 Dec 2017 03:40:57 +0000,Fri; 1 Dec 2017 14:01:05 +0000,,,2.6.3,,,,https://issues.apache.org/jira/browse/HADOOP-15081
HADOOP-15082,Sub-task,Minor,fs;fs/azure;test,add AbstractContractRootDirectoryTest test for mkdir / ; wasb to implement the test,"I managed to get a stack trace on an older version of WASB with some coding doing a mkdir(new Path(""/""))....some of the ranger parentage checks didn't handle that specific case.	Add a new root Fs contract test for this operation	Have WASB implement the test suite as an integration test.	if the test fails shows a problem fix",Patch Available,Unresolved,,Steve Loughran,Steve Loughran,Fri; 1 Dec 2017 11:08:48 +0000,Tue; 19 Dec 2017 16:42:48 +0000,,,2.9.0,,,,https://issues.apache.org/jira/browse/HADOOP-15082
HADOOP-15083,Sub-task,Major,,Create base image for running hadoop in docker containers,,Patch Available,Unresolved,,Elek; Marton,Elek; Marton,Fri; 1 Dec 2017 13:33:18 +0000,Wed; 20 Dec 2017 22:20:54 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15083
HADOOP-15084,Sub-task,Major,,Create docker images for latest stable hadoop2 build,,Open,Unresolved,,Elek; Marton,Elek; Marton,Fri; 1 Dec 2017 13:33:39 +0000,Fri; 1 Dec 2017 13:33:39 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15084
HADOOP-15085,Bug,Major,,Output streams closed with IOUtils suppressing write errors,There are a few places in hadoop-common that are closing an output stream with IOUtils.cleanupWithLogger like this: This suppresses any IOException that occurs during the close() method which could lead to partial/corrupted output without throwing a corresponding exception.  The code should either use try-with-resources or explicitly close the stream within the try block so the exception thrown during close() is properly propagated as exceptions during write operations are.,Resolved,Fixed,,Jim Brennan,Jason Lowe,Fri; 1 Dec 2017 15:29:48 +0000,Fri; 15 Dec 2017 23:22:42 +0000,Fri; 15 Dec 2017 23:22:42 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15085
HADOOP-15086,Sub-task,Major,fs/azure,NativeAzureFileSystem file rename is not atomic,When multiple threads rename files to the same target path; more than 1 threads can succeed. It's because check and copy file in `rename` is not atomic.I would expect it's atomic just like HDFS.,Resolved,Fixed,,Thomas Marquardt,Shixiong Zhu,Fri; 1 Dec 2017 23:42:09 +0000,Thu; 4 Jan 2018 18:01:57 +0000,Fri; 22 Dec 2017 11:41:08 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15086
HADOOP-15087,New Feature,Major,fs/s3,S3A to support writing directly to the destination dir without creating temp directory to avoid rename ,Rename in workloads like Teragen/Terasort who use Hadoop default outputcommitters really hurt performance a lot. Stocator announce it doesn't create the temporary directories any all; and still preserves Hadoop's fault tolerance. I add a switch when creating file via integrating it's code into s3a; I got 5x performance gain in Teragen and 15% performance improvement in Terasort.,Open,Unresolved,,Unassigned,Yonger,Mon; 4 Dec 2017 03:25:05 +0000,Mon; 4 Dec 2017 14:22:27 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15087
HADOOP-15088,Bug,Major,streaming,BufferedInputStream.skip function can return 0 when the file is corrupted; causing an infinite loop,When a file is corrupted; the skip function can return 0; causing an infinite loop.Here is the code: Similar bugs are Hadoop-8614; Yarn-2905; Yarn-163; MAPREDUCE-6990,Open,Unresolved,,Unassigned,John Doe,Mon; 4 Dec 2017 04:03:26 +0000,Mon; 4 Dec 2017 14:29:09 +0000,,,2.5.0,,,,https://issues.apache.org/jira/browse/HADOOP-15088
HADOOP-15089,Sub-task,Minor,test,ADLS to implement AbstractContractDistCpTest,ADLS doesn't implement the AbstractContractDistCpTest yet. It should,Open,Unresolved,,Unassigned,Steve Loughran,Mon; 4 Dec 2017 15:13:41 +0000,Mon; 4 Dec 2017 15:13:41 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15089
HADOOP-15090,Sub-task,Major,documentation;fs/adl,add adl troubleshooting doc,Add a troubleshooting section/doc to the ADL docs based on our experiences.this should not be a substitute for improving the diagnostics/fixing the error messages.,Patch Available,Unresolved,,Steve Loughran,Steve Loughran,Mon; 4 Dec 2017 15:21:50 +0000,Tue; 5 Dec 2017 11:01:52 +0000,,,2.9.0,,,,https://issues.apache.org/jira/browse/HADOOP-15090
HADOOP-15091,Sub-task,Trivial,fs/s3,"S3aUtils.getEncryptionAlgorithm() always logs@Debug ""Using SSE-C""",even when you have encryption off or set to sse-kms/aes256; the debug logs print a comment about using SSE-C That log statement should be moved to only get printed with SSE-C enabled.,Open,Unresolved,,Unassigned,Steve Loughran,Tue; 5 Dec 2017 12:47:58 +0000,Tue; 5 Dec 2017 12:49:26 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15091
HADOOP-15092,Bug,Major,security,Proxy failures during NamenodeWebHdfsMethods are not logged ,When GetDelegationToken http request fails with proxy issue; there is no logs from NameNode to indicate it's a proxy issue. Below is the only log. IOExceptions are logged but AuthorizationException are not logged. It will be helpful to log proxy failures from JspHelper getUGI().,Patch Available,Unresolved,,Prabhu Joseph,Prabhu Joseph,Tue; 5 Dec 2017 13:26:32 +0000,Mon; 8 Jan 2018 07:33:35 +0000,,,2.7.3,,,,https://issues.apache.org/jira/browse/HADOOP-15092
HADOOP-15093,Bug,Major,documentation,Deprecation of yarn.resourcemanager.zk-address is undocumented,"yarn.resourcemanager.zk-address was deprecated in 2.9.x and moved to ""hadoop.zk.address"". However this doesn't appear in Deprecated Properties. Additionally; the Configuration base class doesn't auto-translate from ""yarn.resourcemanager.zk-address"" to ""hadoop.zk.address"". Only the sub-class YarnConfiguration does the translation. Also; the 2.9+ Resource Manager HA documentation still refers to the use of ""yarn.resourcemanager.zk-address"".https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html",Resolved,Fixed,,Ajay Kumar,Eric Wohlstadter,Tue; 5 Dec 2017 18:23:06 +0000,Wed; 3 Jan 2018 22:51:55 +0000,Wed; 3 Jan 2018 22:33:36 +0000,,2.9.0;3.0.0;3.1.0,documentation,,HADOOP-14741;TEZ-3874,https://issues.apache.org/jira/browse/HADOOP-15093
HADOOP-15094,Improvement,Minor,fs,FileSystem.getCanonicalUri() to be public,Discussion around SPARK-22587 highlights how per-fs notions of a canonical URI make it hard to determine if a file is on a specific filesystem; or; put differently; if two filesystems are equivalent.You can't reliably use this.getUri == that.getUri as it doesn't handle FQDN == unqualified DN; bit you can't do nslookup as HDFS HA doesn't use hosnames.If FileSystem.getCanonicalUri() were public; then this could be used to compare things consistently.needs: filesystem.md coverage; contract test (two filesystem instances are equal; different filesystems aren't). Or at least: this method never returns null.,Open,Unresolved,,Unassigned,Steve Loughran,Wed; 6 Dec 2017 11:22:46 +0000,Wed; 6 Dec 2017 11:25:52 +0000,,,2.7.4,,,SPARK-22587,https://issues.apache.org/jira/browse/HADOOP-15094
HADOOP-15095,Sub-task,Minor,fs/s3,S3a committer factory to warn when default FileOutputFormat committer is created,"The S3ACommitterFactory should warn when the classic FileOutputCommitter is used (i.e. the client is not configured to use a new one). Something like""this committer is neither fast nor guaranteed to be correct. See $URL"" where URL is a pointer to something (wiki? hadoop docs?).",Open,Unresolved,,Unassigned,Steve Loughran,Wed; 6 Dec 2017 13:11:17 +0000,Wed; 6 Dec 2017 13:11:17 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15095
HADOOP-15096,Bug,Major,build,start-build-env.sh can create a docker image that fills up disk,start-build-env.sh has the potential to build an image that can fill up root disks by exploding a sparse file.In my case; the right ingredients are:Ubuntu 17.04Docker 17.09.0AUFS storage driveruserId and groupid with a high numberThis happens when building the hadoop-build-${USER_ID} image; specifically in the  RUN useradd -g ${GROUP_ID} -u ${USER_ID} -k /root -m ${USER_NAME} command.The reason for this:/var/log/lastlog is a sparse file that pre-reserves based on highest seen UID and GID; in my case; those numbers are very high (above 1 billion). Locally; this result in a sparse file that reports as 443 GB. However; under docker and specifically AUFS; it appears that his file isn't sparse and it tries to allocate the whole file.If you start this script and walk away to wait for it to finish; you come back to a computer with a completely full disk.Luckily; the fix is quite easy; simply add the `-l` option to useradd which won't create those files,Open,Unresolved,,Unassigned,Addison Higham,Thu; 7 Dec 2017 06:05:24 +0000,Thu; 7 Dec 2017 06:09:19 +0000,,,3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15096
HADOOP-15097,Bug,Minor,fs;test,AbstractContractDeleteTest::testDeleteNonEmptyDirRecursive with misleading path,"@Test  public void testDeleteNonEmptyDirRecursive() throws Throwable {    Path path = path(""testDeleteNonEmptyDirNonRecursive"");    mkdirs(path);    Path file = new Path(path; ""childfile"");    ContractTestUtils.writeTextFile(getFileSystem(); file; ""goodby",Open,Unresolved,,Unassigned,zhoutai.zt,Thu; 7 Dec 2017 09:59:28 +0000,Thu; 7 Dec 2017 11:44:57 +0000,,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15097
HADOOP-15098,Bug,Major,test,TestClusterTopology#testChooseRandom fails intermittently,Flaky test failure:,Resolved,Fixed,,Zsolt Venczel,Zsolt Venczel,Tue; 5 Dec 2017 10:48:31 +0000,Thu; 7 Dec 2017 20:42:05 +0000,Thu; 7 Dec 2017 20:26:30 +0000,,3.0.0,flaky-test,,,https://issues.apache.org/jira/browse/HADOOP-15098
HADOOP-15099,Bug,Trivial,documentation,YARN Federation Link not working,YARN federation(in the last paragraph on the page) link isn't working on http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html.,Patch Available,Unresolved,,Unassigned,Anirudh,Thu; 7 Dec 2017 18:48:45 +0000,Thu; 7 Dec 2017 18:56:56 +0000,,,2.9.0,documentation;easyfix;newbie,,,https://issues.apache.org/jira/browse/HADOOP-15099
HADOOP-15100,Bug,Critical,,Configuration#Resource constructor change broke Hive tests,In CDH's C6 rebased testing; the following Hive tests started failing: The exception is Aihua Xu and Sahil Takiar helped to git bisect to a recent change on Configuration.We should fix this in Hadoop code base.,Resolved,Won't Fix,,Unassigned,Xiao Chen,Thu; 7 Dec 2017 21:22:03 +0000,Thu; 7 Dec 2017 23:07:50 +0000,Thu; 7 Dec 2017 23:07:50 +0000,,2.8.3;2.7.5;3.0.0;2.9.1,,,,https://issues.apache.org/jira/browse/HADOOP-15100
HADOOP-15101,Bug,Critical,fs;test,what testListStatusFile verified ÔøΩÔøΩnot consistent with listStatus declaration in FileSystem,In this case; first create a file f; then listStatus on f expect listStatus returns an array of one FileStatus. But this is not consistent with the declarations in FileSystem; i.e. Which is the expected? The behave in fs contract test or in FileSystem?,Open,Unresolved,,Unassigned,zhoutai.zt,Fri; 8 Dec 2017 02:24:52 +0000,Fri; 15 Dec 2017 14:36:08 +0000,,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15101
HADOOP-15102,Sub-task,Major,fs/s3;tools,HADOOP-14831,,Resolved,Invalid,,Unassigned,Steve Loughran,Fri; 8 Dec 2017 10:18:29 +0000,Sun; 10 Dec 2017 21:26:02 +0000,Fri; 8 Dec 2017 17:48:22 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15102
HDFS-12909,Bug,Major,,SSLConnectionConfigurator creation error should be printed only if security is enabled,Currently URLConnectionFactory#getSSLConnectionConfiguration attempts to create a SSL connection configurator even if security is not enabled. This raises the below false warning in the logs.,Open,Unresolved,,Lokesh Jain,Lokesh Jain,Fri; 8 Dec 2017 10:58:07 +0000,Fri; 8 Dec 2017 17:29:22 +0000,,,,,,,https://issues.apache.org/jira/browse/HDFS-12909
HADOOP-15104,Improvement,Major,fs/oss,AliyunOSS: change the default value of max error retry,Currently; default number of times we should retry errors is 20;  however; oss sdk retry delay is         when one error occurs. So; if we retry 20 times; sleep time will be about 3.64 days and it is unacceptable. So we should change the default behavior.,Resolved,Fixed,,wujinhu,wujinhu,Fri; 8 Dec 2017 14:17:24 +0000,Tue; 9 Jan 2018 07:46:11 +0000,Fri; 8 Dec 2017 15:05:23 +0000,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15104
HADOOP-15105,Sub-task,Minor,fs/s3,add htrace context to HTTP requests as ? parameter,you can add x-something query parameters to S3 REST callsThese then get included in the logs. If the htrace context were passed in this way; you could determine from the S3 logs what query/job an HTTP request ties back to,Open,Unresolved,,Unassigned,Steve Loughran,Fri; 8 Dec 2017 15:04:21 +0000,Fri; 8 Dec 2017 15:04:21 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15105
HADOOP-15106,Improvement,Minor,,FileSystem::open(PathHandle) should throw a specific exception on validation failure,Callers of FileSystem::open(PathHandle) cannot distinguish between I/O errors and an invalid handle. The signature should include a specific; checked exception for this case.,Resolved,Fixed,,Chris Douglas,Chris Douglas,Fri; 8 Dec 2017 19:39:50 +0000,Sat; 16 Dec 2017 19:17:31 +0000,Sat; 16 Dec 2017 18:59:16 +0000,,,,,HADOOP-15117,https://issues.apache.org/jira/browse/HADOOP-15106
HADOOP-15107,Sub-task,Major,fs/s3,Prove the correctness of the new committers; or fix where they are not correct,"I'm writing about the paper on the committers; one which; being a proper paper; requires me to show the committers work.	define the requirements of a ""Correct"" committed job (this applies to the FileOutputCommitter too)	show that the Staging committer meets these requirements (most of this is implicit in that it uses the V1 FileOutputCommitter to marshall .pendingset lists from committed tasks to the final destination; where they are read and committed.	Show the magic committer also works.I'm now not sure that the magic committer works.",In Progress,Unresolved,,Steve Loughran,Steve Loughran,Sun; 10 Dec 2017 22:23:07 +0000,Fri; 5 Jan 2018 20:00:47 +0000,,,3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15107
HADOOP-15108,Test,Major,,Testcase TestBalancer#testBalancerWithPinnedBlocks always fails,"When running testcases without any code changes; the function testBalancerWithPinnedBlocks in TestBalancer.java never succeeded. I tried to use Ubuntu 16.04 and redhat 7; maybe the failure is not related to various linux environment. I am not sure if there is some bug in this case or I used wrong environment and settings. Could anyone give some advice.-------------------------------------------------------------------------------Test set: org.apache.hadoop.hdfs.server.balancer.TestBalancer-------------------------------------------------------------------------------Tests run: 1; Failures: 0; Errors: 1; Skipped: 0; Time elapsed: 100.389 sec &lt; FAILURE! - in org.apache.hadoop.hdfs.server.balancer.TestBalancertestBalancerWithPinnedBlocks(org.apache.hadoop.hdfs.server.balancer.TestBalancer)  Time elapsed: 100.134 sec  &lt; ERROR!java.lang.Exception: test timed out after 100000 milliseconds	at java.lang.Object.wait(Native Method)	at org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:903)	at org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:773)	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:870)	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:842)	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)	at org.apache.hadoop.hdfs.DFSTestUtil.createFile(DFSTestUtil.java:441)	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.testBalancerWithPinnedBlocks(TestBalancer.java:515)",Open,Unresolved,,Unassigned,Jianfei Jiang,Mon; 11 Dec 2017 11:15:04 +0000,Mon; 11 Dec 2017 11:50:34 +0000,,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15108
HADOOP-15109,Bug,Minor,fs;test,TestDFSIO -read -random doesn't work on file sized 4GB,"TestDFSIO -read -random throws IllegalArgumentException on 4GB file. The cause is: When filesize exceeds signed int; (int)(filesize) will be negative and cause Random.nextInt throws  IllegalArgumentException(""n must be positive"").",Resolved,Fixed,,Ajay Kumar,zhoutai.zt,Mon; 11 Dec 2017 11:26:42 +0000,Mon; 18 Dec 2017 22:03:04 +0000,Mon; 18 Dec 2017 21:27:17 +0000,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15109
HADOOP-15110,Sub-task,Minor,metrics;security,Gauges are getting logged in exceptions from AutoRenewalThreadForUserCreds,scenario:-----------------While Running the renewal command for principal it's printing the direct objects for renewalFailures *and *renewalFailuresTotal Expected Result:it's should be user understandable value.,Open,Unresolved,,Unassigned,Harshakiran Reddy,Tue; 12 Dec 2017 05:09:52 +0000,Wed; 10 Jan 2018 10:34:29 +0000,,,2.8.0;3.0.0-alpha2,,,,https://issues.apache.org/jira/browse/HADOOP-15110
HADOOP-15111,Improvement,Major,fs/oss,AliyunOSS: backport HADOOP-14993 to branch-2,"Do a bulk listing off all entries under a path in one single operation; there is no need to recursively walk the directory tree.Updates:	override listFiles and listLocatedStatus by using bulk listing	some minor updates in hadoop-aliyun index.md",Resolved,Fixed,,Genmao Yu,Genmao Yu,Tue; 12 Dec 2017 13:00:24 +0000,Fri; 15 Dec 2017 06:48:50 +0000,Fri; 15 Dec 2017 06:48:50 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15111
HADOOP-15112,Bug,Major,,create-release didn't sign artifacts,"While building the 3.0.0 RC1; I had to re-invoke Maven because the create-release script didn't deploy signatures to Nexus. Looking at the repo (and my artifacts); it seems like ""sign"" didn't run properly.I lost my create-release output; but I noticed that it will log and continue rather than abort in some error conditions. This might have caused my lack of signatures. IMO it'd be better to explicitly fail in these situations.",Open,Unresolved,,Unassigned,Andrew Wang,Tue; 12 Dec 2017 17:58:49 +0000,Tue; 12 Dec 2017 17:58:49 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15112
HADOOP-15113,Sub-task,Major,fs/s3,NPE in S3A getFileStatus: null instrumentation on using closed instance,NPE in getFileStatus in a downstream test of mine; s3a ireland{{PathMetadata pm = metadataStore.get(path; needEmptyDirectoryFlag);. }}Something up with the bucket config?,Resolved,Fixed,,Steve Loughran,Steve Loughran,Tue; 12 Dec 2017 18:12:22 +0000,Thu; 21 Dec 2017 14:45:35 +0000,Thu; 21 Dec 2017 14:25:58 +0000,,3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15113
HADOOP-15114,Improvement,Major,,Add closeStreams(...) to IOUtils,Add closeStreams(...) in IOUtils. Originally suggested by Jason Lowe.,Resolved,Fixed,,Ajay Kumar,Ajay Kumar,Wed; 13 Dec 2017 01:22:20 +0000,Fri; 12 Jan 2018 17:10:16 +0000,Fri; 12 Jan 2018 00:51:33 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15114
HADOOP-15115,Bug,Major,,CLONE - Remove aspectj dependency,Per discussion on HDFS-2261 and Nicholas' suggestion; we should remove the aspectj dependencies for the moment till the AOP fault-injection tests are fixed to work with maven.,Open,Unresolved,,Karthik Kambatla,Fan,Wed; 13 Dec 2017 05:44:07 +0000,Wed; 13 Dec 2017 05:44:07 +0000,,,2.0.4-alpha,,,,https://issues.apache.org/jira/browse/HADOOP-15115
HADOOP-15116,Bug,Major,ha,NPE in ResourceManager when ZooKeeper goes down temporary (HA Mode),In an HA enabled cluster (3.0); we found that RM is failing to start with an NPE from ActiveStandbyElector. Zookeeper was down at this time; hence client retries were coming for a while,Open,Unresolved,,Unassigned,Sunil G,Wed; 13 Dec 2017 14:32:09 +0000,Wed; 13 Dec 2017 14:32:09 +0000,,,3.0.0-beta1,,,,https://issues.apache.org/jira/browse/HADOOP-15116
HADOOP-15117,Test,Major,,open(PathHandle) contract test should be exhaustive for default options,The current AbstractContractOpenTest covers many; but not all of the permutations of the default HandleOpt. It could also be refactored to be clearer as documentation,Resolved,Fixed,,Chris Douglas,Chris Douglas,Thu; 14 Dec 2017 19:20:43 +0000,Sun; 31 Dec 2017 02:21:35 +0000,Sun; 31 Dec 2017 01:59:50 +0000,,,,,HADOOP-15106,https://issues.apache.org/jira/browse/HADOOP-15117
HADOOP-15118,New Feature,Major,,Change default classpath to be only shaded jars,It would be desirable to change the default classpath to be just the shaded jars.,Open,Unresolved,,Unassigned,Bharat Viswanadham,Mon; 11 Dec 2017 22:42:49 +0000,Fri; 15 Dec 2017 22:02:42 +0000,,,,,,HADOOP-13952,https://issues.apache.org/jira/browse/HADOOP-15118
HADOOP-15119,Bug,Major,fs/azure,AzureNativeFileSystemStore's rename swallow InterruptedExceptions,AzureNativeFileSystemStore's rename calls waitForCopyToComplete; which swallow InterruptedExceptions and prevent the current thread being from interrupted. Once we catch the exception; it will be nice to call Thread.currentThread().interrupt(). So; if this thread is blocked at a later time; an InterruptedException will be properly thrown.,Open,Unresolved,,Unassigned,Yin Huai,Fri; 15 Dec 2017 01:43:21 +0000,Thu; 11 Jan 2018 01:41:39 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15119
HADOOP-15120,Improvement,Major,fs/azure,Reduce the overhead of AzureNativeFileSystemStore's rename,AzureNativeFileSystemStore's rename calls waitForCopyToComplete. If the copy executed inside waitForCopyToComplete cannot finish quickly; waitForCopyToComplete will wait for at least 1 second to check the status again. This behavior can introduce a significant overhead if the actual data operation finishes pretty fast.,Open,Unresolved,,Unassigned,Yin Huai,Fri; 15 Dec 2017 01:44:08 +0000,Fri; 15 Dec 2017 01:47:03 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15120
HADOOP-15121,Bug,Major,,Encounter NullPointerException when using DecayRpcScheduler,I set ipc.8020.scheduler.impl to org.apache.hadoop.ipc.DecayRpcScheduler; but got excetion in namenode: It seems that metricsProxy in DecayRpcScheduler should initiate its delegate field in its Initialization method,Patch Available,Unresolved,,Unassigned,Tao Jie,Fri; 15 Dec 2017 07:41:39 +0000,Sat; 13 Jan 2018 04:50:31 +0000,,,2.8.2,,,HADOOP-13159,https://issues.apache.org/jira/browse/HADOOP-15121
HADOOP-15122,Bug,Blocker,,Lock down version of doxia-module-markdown plugin,Since HADOOP-14364 we have a SNAPSHOT dependency in the main pom.xml: Most probably because some feature was missing from last released doxia markdown module.-I propose to lock down the version and use a fixed instance from the snapshot version. -UPDATE: in the meantime doxia 1.8 has been released. Snapshot artifacts could not been dowloaded any more:404: https://repository.apache.org/snapshots/org/apache/maven/doxia/doxia-module-markdown/1.8-SNAPSHOT/doxia-module-markdown-1.8-SNAPSHOT.jarIMHO Hadoop can be built without changing it to 1.8 fix version.,Resolved,Fixed,,Elek; Marton,Elek; Marton,Fri; 15 Dec 2017 11:42:11 +0000,Sat; 30 Dec 2017 02:08:41 +0000,Sat; 30 Dec 2017 02:05:40 +0000,,3.0.0;3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15122
HADOOP-15123,Bug,Minor,security,KDiag tries to load krb5.conf from KRB5CCNAME instead of KRB5_CONFIG,If Kerberos credential cache file location is overridden in environment; then KDiag tries to read its value for Kerberos configuration. For example; Expected Behavior:1. Kerberos configuration file location should be read from KRB5_CONFIG env. variable instead of KRB5CCNAME. Source: MIT KRB5 doc,Resolved,Fixed,,Vipin Rathor,Vipin Rathor,Sat; 16 Dec 2017 04:49:20 +0000,Mon; 18 Dec 2017 18:51:10 +0000,Mon; 18 Dec 2017 18:51:10 +0000,,2.8.0;2.9.0;3.0.0,,,HADOOP-12426,https://issues.apache.org/jira/browse/HADOOP-15123
HADOOP-15124,Sub-task,Major,common,Slow FileSystem.Statistics counters implementation,While profiling 1TB TeraGen job on Hadoop 2.8.2 cluster (Google Dataproc; 2 workers; GCS connector) I saw that FileSystem.Statistics code paths Wall time is 5.58% and CPU time is 26.5% of total execution time.After switching FileSystem.Statistics implementation to LongAdder; consumed Wall time decreased to 0.006% and CPU time to 0.104% of total execution time.Total job runtime decreased from 66 mins to 61 mins.These results are not conclusive; because I didn't benchmark multiple times to average results; but regardless of performance gains switching to LongAdder simplifies code and reduces its complexity.,Patch Available,Unresolved,,Igor Dvorzhak,Igor Dvorzhak,Sun; 17 Dec 2017 17:11:02 +0000,Fri; 22 Dec 2017 19:48:42 +0000,,,2.9.0;2.8.3;2.7.5;3.0.0,common;filesystem;statistics,,HADOOP-13065,https://issues.apache.org/jira/browse/HADOOP-15124
HADOOP-15125,Improvement,Major,fs,Complete integration of new StorageStatistics,HADOOP-13065 added the new StorageStatistics API. But there's a couple of subtasks remaining; and we are getting more experience of using it.This JIRA covers the task of pulling those patches in; evolving what we have; targeting; realistically; v 3.2,Open,Unresolved,,Unassigned,Steve Loughran,Mon; 18 Dec 2017 11:06:18 +0000,Mon; 18 Dec 2017 11:06:18 +0000,,,2.8.0;2.9.0;3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15125
HADOOP-15126,Sub-task,Major,fs,Hadoop FS contract spec & tests to cover StorageStatistics,Add docs on the storage stats to the FS contract specification; include the list of preferred names; then some tests on the operations itself.some of these could be implicit: if the FS supports the new API  standard op names; then you could just verify that the numbers matched those expected (bytes writte; read; seeks...); taking care not to make the assertions too brittle (sometimes S3A has suffered there).,Open,Unresolved,,Unassigned,Steve Loughran,Mon; 18 Dec 2017 11:27:46 +0000,Mon; 18 Dec 2017 11:27:46 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15126
HADOOP-15127,Bug,Major,,TestInMemoryLevelDBAliasMapClient has compilation issues with early versions of Java 8,I'm seeing this error message when using Java 8u25 on OS X: This seems to work with Java 8u65.,Open,Unresolved,,Unassigned,Ray Chiang,Mon; 18 Dec 2017 22:01:24 +0000,Mon; 18 Dec 2017 22:01:24 +0000,,,3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15127
HADOOP-15128,Bug,Major,viewfs,TestViewFileSystem tests are broken in trunk,The fix in Hadoop-10054 seems to have caused a test failure. Please take a look. Thanks Eric Yang for reporting this.,Resolved,Not A Problem,,Hanisha Koneru,Anu Engineer,Tue; 19 Dec 2017 00:03:17 +0000,Wed; 3 Jan 2018 01:36:40 +0000,Wed; 3 Jan 2018 01:36:35 +0000,,3.1.0,,,HADOOP-14600,https://issues.apache.org/jira/browse/HADOOP-15128
HADOOP-15129,Bug,Minor,ipc,Datanode caches namenode DNS lookup failure and cannot startup,"On startup; the Datanode creates an InetSocketAddress to register with each namenode. Though there are retries on connection failure throughout the stack; the same InetSocketAddress is reused.InetSocketAddress is an interesting class; because it resolves DNS names to IP addresses on construction; and it is never refreshed. Hadoop re-creates an InetSocketAddress in some cases just in case the remote IP has changed for a particular DNS name: https://issues.apache.org/jira/browse/HADOOP-7472.Anyway; on startup; you cna see the Datanode log: ""Namenode...remains unresolved""  referring to the fact that DNS lookup failed. The Datanode then proceeds to use this unresolved address; as it may work if the DN is configured to use a proxy. Since I'm not using a proxy; it forever prints out this message: Unfortunately; the log doesn't contain the exception that triggered it; but the culprit is actually in IPC Client: https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java#L444.This line was introduced in https://issues.apache.org/jira/browse/HADOOP-487 to give a clear error message when somebody mispells an address.However; the fix in HADOOP-7472 doesn't apply here; because that code happens in Client#getConnection after the Connection is constructed.My proposed fix (will attach a patch) is to move this exception out of the constructor and into a place that will trigger HADOOP-7472's logic to re-resolve addresses. If the DNS failure was temporary; this will allow the connection to succeed. If not; the connection will fail after ipc client retries (default 10 seconds worth of retries).I want to fix this in ipc client rather than just in Datanode startup; as this fixes temporary DNS issues for all of Hadoop.",Patch Available,Unresolved,,Karthik Palaniappan,Karthik Palaniappan,Tue; 19 Dec 2017 00:34:56 +0000,Wed; 10 Jan 2018 20:31:27 +0000,,,2.8.2,,,HADOOP-12125;HADOOP-487,https://issues.apache.org/jira/browse/HADOOP-15129
HADOOP-15130,Improvement,Minor,common,SocketIOWithTimeout async close idle selector,I start 1000 thread for NM and 1000 thread for AM in fairscheduler pressure test; using Scheduler Load Simulator SLS  tool.In jstack log;  I found SocketIOWithTimeout  remove  idle selectors after every select Operation.That will block 1000+ threads when doing iterator .,Open,Unresolved,,Unassigned,zhangshilong,Tue; 19 Dec 2017 06:32:11 +0000,Tue; 19 Dec 2017 06:39:07 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15130
HADOOP-15131,Sub-task,Minor,fs/adl;test,ADL TestAdlContractRenameLive.testRenameFileUnderFile failing,"TestAdlContractRenameLive.testRenameFileUnderFile raises an AccessControlException when you try to rename something under a file. Failure is a good outcome; though rename() normally likes to fail silently with an error code of ""false"" in such a situationOptions:	catch the specific exception; look for the text ""Forbidden. Parent path is not a folder."" and downgrade to a fail.	declare this is a valid outcome; override the test case to expect it.",Open,Unresolved,,Unassigned,Steve Loughran,Tue; 19 Dec 2017 16:14:18 +0000,Tue; 19 Dec 2017 16:14:40 +0000,,,3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15131
HADOOP-15132,Improvement,Major,fs/azure,√úber-jira: WASB client phase III: roll-up for Hadoop 3.1,Everything for the WASB connector for Hadoop 3.1,Open,Unresolved,,Unassigned,Steve Loughran,Tue; 19 Dec 2017 16:27:13 +0000,Tue; 19 Dec 2017 17:19:02 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15132
HADOOP-15133,Sub-task,Major,,[JDK9] Ignore com.sun.javadoc.* and com.sun.tools.* in animal-sniffer-maven-plugin to compile with Java 9,com.sun.javadoc and com.sun.tools are internal APIs and are not included in java18 profile; so signature check fails with JDK9.,Resolved,Fixed,,Akira Ajisaka,Akira Ajisaka,Wed; 20 Dec 2017 06:50:59 +0000,Thu; 21 Dec 2017 03:17:47 +0000,Thu; 21 Dec 2017 02:59:17 +0000,,,,,HADOOP-14986,https://issues.apache.org/jira/browse/HADOOP-15133
HADOOP-15134,Bug,Major,fs/adl,ADL problems parsing JSON responses to include error details,"Currently any failure of ADL's response JSON parsing results in the error text like ""Unexpected error happened reading response stream or parsing JSon from rename()"";"". This is not useful. Fix by: including the exception text; logging the Ex  info",Open,Unresolved,,Unassigned,Steve Loughran,Wed; 20 Dec 2017 13:59:19 +0000,Thu; 21 Dec 2017 06:13:16 +0000,,,3.0.0,supportability,,,https://issues.apache.org/jira/browse/HADOOP-15134
HADOOP-15135,Sub-task,Major,fs/s3,S3a to support get/set permissions through S3 object tags,Azure wasb supports get/set permissions (for persistence only) to help round trip distcp operations.Could aws tags be used similarly?,Open,Unresolved,,Unassigned,Steve Loughran,Wed; 20 Dec 2017 19:17:11 +0000,Wed; 20 Dec 2017 19:20:24 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15135
HADOOP-15136,Bug,Major,documentation,Typo in rename spec pseudocode,Location of issue: rename spec documentationThe text description for what rename does in the case when the destination exists and is a directory is correct. However; the pseudocode is not.What is written: What I expected:isDir(FS; src) should be isDir(FS; d).,Open,Unresolved,,Unassigned,Rae Marks,Thu; 21 Dec 2017 01:57:08 +0000,Thu; 21 Dec 2017 17:57:33 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15136
HADOOP-15137,Sub-task,Major,,ClassNotFoundException: org.apache.hadoop.yarn.server.api.DistributedSchedulingAMProtocol when using hadoop-client-minicluster,I'd like to use hadoop-client-minicluster for hadoop downstream project; but I encounter the following exception when starting hadoop minicluster.  And I check the hadoop-client-minicluster; it indeed does not have this class. Is this something that is missing when packaging the published jar ?,Patch Available,Unresolved,,Bharat Viswanadham,Jeff Zhang,Tue; 19 Dec 2017 07:40:38 +0000,Fri; 22 Dec 2017 19:50:11 +0000,,,3.0.0,,,HADOOP-11656,https://issues.apache.org/jira/browse/HADOOP-15137
HADOOP-15138,Improvement,Critical,fs/s3;hdfs-client;security,CLONE - Executing the command 'hdfs -Dhadoop.security.credential.provider.path=file1.jceks;file2.jceks' fails if permission is denied to some files,======= Request Use Case: UC1: The customer has the path to a directory and subdirectories full of keys. The customer knows that he does not have the access to all the keys; but ignoring this problem; the customer makes a list of the keys. UC1.2: The cust,Resolved,Duplicate,HADOOP-14821,Unassigned,Fan,Thu; 21 Dec 2017 08:26:50 +0000,Thu; 21 Dec 2017 11:43:59 +0000,Thu; 21 Dec 2017 11:43:59 +0000,,2.8.0,features,,,https://issues.apache.org/jira/browse/HADOOP-15138
HADOOP-15139,Bug,Critical,,[Umbrella] Improvements and fixes for Hadoop shaded client work ,"In HADOOP-11656; we have made great progress in splitting out third-party dependencies from shaded hadoop client jar (hadoop-client-api); put runtime dependencies in hadoop-client-runtime; and have shaded version of hadoop-client-minicluster for test. However; there are still some left work for this feature to be fully completed:	We don't have a comprehensive documentation to guide downstream projects/users to use shaded JARs instead of previous JARs	We should consider to wrap up hadoop tools (distcp; aws; azure) to have shaded version	More issues could be identified when shaded jars are adopted in more test and production environment; like HADOOP-15137.Let's have this umbrella JIRA to track all efforts that left to improve hadoop shaded client effort.CC Sean Busbey; Bharat Viswanadham and Vinod Kumar Vavilapalli.",Open,Unresolved,,Bharat Viswanadham,Junping Du,Thu; 21 Dec 2017 21:55:24 +0000,Sat; 30 Dec 2017 14:32:16 +0000,,,,,,HADOOP-11656,https://issues.apache.org/jira/browse/HADOOP-15139
HADOOP-15140,Sub-task,Major,fs/s3,S3guard mistakes root URI without / as non-absolute path,"If you call getFileStatus(""s3a://bucket"") then S3Guard will throw an exception in putMetadata; as it mistakes the empty path for ""non-absolute path""",Open,Unresolved,,Abraham Fine,Steve Loughran,Fri; 22 Dec 2017 20:03:18 +0000,Fri; 12 Jan 2018 11:45:52 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15140
HADOOP-15141,Sub-task,Major,fs/s3,Support IAM Assumed roles in S3A,"Add the ability to use assumed roles in S3A	Add a property fs.s3a.assumed.role.arn for the ARN of the assumed role	add a new provider which grabs that and other properties and then creates a STSAssumeRoleSessionCredentialsProvider from it.	This also needs to support building up its own list of aws credential  providers; from a different property; make the changes to S3AUtils for that	Tests	docs	and have the AwsProviderList forward closeable to it.	Get picked up automatically by DDB/s3guard",Patch Available,Unresolved,,Steve Loughran,Steve Loughran,Fri; 22 Dec 2017 20:58:10 +0000,Wed; 10 Jan 2018 17:57:06 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15141
HADOOP-15142,New Feature,Minor,common,Register FTP and SFTP as FS services,SFTPFileSystem and FTPFileSystem are not registered as a FS services.When calling the 'get' or 'newInstance' methods of the FileSystem class; the FS instance cannot be created due to the schema is not registered as a service FS.Also; the SFTPFileSystem class doesn't have the getScheme method implemented.,Patch Available,Unresolved,,Unassigned,Mario Molina,Sat; 23 Dec 2017 20:51:29 +0000,Wed; 10 Jan 2018 13:19:06 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15142
HADOOP-15143,Bug,Major,,NPE due to Invalid KerberosTicket in UGI,It could be related to jdk issuehttp://hg.openjdk.java.net/jdk8u/jdk8u-dev/jdk/rev/fd0e0898721c,Resolved,Fixed,,Mukul Kumar Singh,Jitendra Nath Pandey,Sun; 24 Dec 2017 20:07:39 +0000,Tue; 2 Jan 2018 05:54:02 +0000,Tue; 2 Jan 2018 05:53:21 +0000,,2.7.4,,,,https://issues.apache.org/jira/browse/HADOOP-15143
HADOOP-15144,Bug,Major,security,TestRaceWhenRelogin fails because of incorrect order of tickets,TestRaceWhenRelogin fails because of incorrect order of tickets. The test fails because of the following stack trace.,Open,Unresolved,,Unassigned,Mukul Kumar Singh,Wed; 27 Dec 2017 05:37:42 +0000,Wed; 27 Dec 2017 05:37:42 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15144
HADOOP-15145,Bug,Major,common,Remove the CORS related code in JMXJsonServlet,"JMXJsonServlet.java using hardcoded value for ""Access-Control-Allow-Origin"" and this is added in HADOOP-11385.But this change is not required after YARN-4009. YARN-4009 added one new filter for CORS support.Please refer CORS support in HttpAuthentication document",Patch Available,Unresolved,,Surendra Singh Lilhore,Surendra Singh Lilhore,Thu; 28 Dec 2017 13:18:55 +0000,Fri; 5 Jan 2018 12:10:13 +0000,,,2.7.0,,,,https://issues.apache.org/jira/browse/HADOOP-15145
HADOOP-15146,Improvement,Minor,common,Remove DataOutputByteBuffer,I can't seem to find any references to DataOutputByteBuffer maybe it should be deprecated or simply removed?,Patch Available,Unresolved,,BELUGA BEHR,BELUGA BEHR,Thu; 28 Dec 2017 16:32:17 +0000,Tue; 9 Jan 2018 18:31:05 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15146
HADOOP-15147,Improvement,Trivial,common,SnappyCompressor Typo,"Notice ""finish"" is checked twice",Resolved,Not A Problem,,Unassigned,BELUGA BEHR,Thu; 28 Dec 2017 16:42:56 +0000,Tue; 2 Jan 2018 16:58:48 +0000,Tue; 2 Jan 2018 16:58:48 +0000,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15147
HADOOP-15148,Improvement,Trivial,common,Improve DataOutputByteBuffer,"Use ArrayDeque instead of LinkedList	Replace an ArrayList that was being used as a queue with ArrayDeque	Improve write single byte method to hard-code sizes and save timeResizable-array implementation of the Deque interface. Array deques have no capacity restrictions; they grow as necessary to support usage. They are not thread-safe; in the absence of external synchronization; they do not support concurrent access by multiple threads. Null elements are prohibited. This class is likely to be ... faster than LinkedList when used as a queue.",Patch Available,Unresolved,,BELUGA BEHR,BELUGA BEHR,Thu; 28 Dec 2017 17:02:38 +0000,Tue; 9 Jan 2018 18:32:15 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15148
HADOOP-15149,Task,Major,fs,CryptoOutputStream should implement StreamCapabilities,Running some tests with HBase on HDFS with encryption; we noticed that CryptoOutputStream doesn't implement StreamCapabilities.Specifically; we have a HdfsDataOutputStream wrapping a CryptoOutputStream. The calls to hasCapability on the HDOS will always return false; even though the COS could be wrapping something that supports it; as evidenced by the implementation of hsync and hflush here.,Resolved,Fixed,,Xiao Chen,Mike Drob,Thu; 28 Dec 2017 21:12:13 +0000,Tue; 2 Jan 2018 17:59:25 +0000,Fri; 29 Dec 2017 21:57:58 +0000,,3.0.0,,,HDFS-11644,https://issues.apache.org/jira/browse/HADOOP-15149
HADOOP-15150,Bug,Major,,in FsShell; UGI params should be overidden through env vars(-D arg),org.apache.hadoop.security.UserGroupInformation#ensureInitialized;will always get the configure from the configuration files.So that; -D args will not take effect.,Patch Available,Unresolved,,Brahma Reddy Battula,Brahma Reddy Battula,Fri; 29 Dec 2017 04:52:11 +0000,Tue; 2 Jan 2018 04:46:11 +0000,,,2.7.0,,,,https://issues.apache.org/jira/browse/HADOOP-15150
HADOOP-15151,Bug,Major,common,MapFile.fix creates a wrong index file in case of block-compressed data file.,Index file created with MapFile.fix for an ordered block-compressed data file does not allow to find values for keys existing in the data file via the MapFile.get method.,Patch Available,Unresolved,,Unassigned,Grigori Rybkine,Sat; 30 Dec 2017 19:36:29 +0000,Sat; 13 Jan 2018 10:36:51 +0000,,,,patch,,,https://issues.apache.org/jira/browse/HADOOP-15151
HADOOP-15152,Bug,Trivial,common,Typo in javadoc of ReconfigurableBase#reconfigurePropertyImpl,There is a typo in javadoc of ReconfigurableBase#reconfigurePropertyImpl,Resolved,Fixed,,Nanda kumar,Nanda kumar,Tue; 2 Jan 2018 08:35:53 +0000,Tue; 2 Jan 2018 19:10:03 +0000,Tue; 2 Jan 2018 18:51:16 +0000,,,javadoc,,,https://issues.apache.org/jira/browse/HADOOP-15152
HADOOP-15153,Bug,Major,build,[branch-2.8] Increase heap memory to avoid the OOM in pre-commit,Refernce:https://builds.apache.org/job/PreCommit-HDFS-Build/22528/consoleFullhttps://builds.apache.org/job/PreCommit-HDFS-Build/22528/artifact/out/branch-mvninstall-root.txt,Patch Available,Unresolved,,Brahma Reddy Battula,Brahma Reddy Battula,Tue; 2 Jan 2018 15:02:05 +0000,Tue; 9 Jan 2018 18:34:10 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15153
HADOOP-15154,Improvement,Minor,test,Abstract new method assertCapability for StreamCapabilities testing,From Steve's comment:it'd have been cleaner for the asserts to have been one in a assertCapability(key; StreamCapabilities subject; bool outcome) and had it throw meaningful exceptions on a failureWe can consider abstract such a method to a test util class and use it for all StreamCapabilities tests as needed.,Open,Unresolved,,Unassigned,Xiao Chen,Tue; 2 Jan 2018 17:58:36 +0000,Wed; 3 Jan 2018 14:12:40 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15154
HADOOP-15155,Bug,Minor,,Error in javadoc of ReconfigurableBase#reconfigureProperty,There is an error in javadoc of reconfigureProperty#reconfigurePropertyImpl should change to,Resolved,Fixed,,Ajay Kumar,Ajay Kumar,Tue; 2 Jan 2018 23:20:36 +0000,Wed; 3 Jan 2018 22:34:00 +0000,Wed; 3 Jan 2018 01:23:46 +0000,,,newbie,,,https://issues.apache.org/jira/browse/HADOOP-15155
HADOOP-15156,Sub-task,Major,fs/azure,backport HADOOP-15086 rename fix to branch-2,backport HADOOP-15086 (rename fix) to branch-2,Resolved,Fixed,,Thomas Marquardt,Thomas Marquardt,Wed; 3 Jan 2018 23:30:13 +0000,Mon; 8 Jan 2018 15:14:42 +0000,Mon; 8 Jan 2018 15:14:42 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15156
HADOOP-15157,Improvement,Minor,security,Zookeeper authentication related properties to support CredentialProviders,The hadoop.zk.auth and ha.zookeeper.auth properties currently support either a plain-text authentication info (in scheme:value format); or a @/path/to/file notation which points to a plain-text file.This ticket proposes that the hadoop.zk.auth and ha.zookeeper.auth properties can be retrieved via the CredentialProviderAPI that's been configured using the credential.provider.path; with fallback provided to the clear-text value or @/path/to/file notation.,Patch Available,Unresolved,,Gergo Repas,Gergo Repas,Thu; 4 Jan 2018 13:36:19 +0000,Fri; 12 Jan 2018 00:50:55 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15157
HADOOP-15158,Improvement,Major,fs/oss,AliyunOSS: Supports role based credential,Currently; AliyunCredentialsProvider supports credential by configuration(core-site.xml). Sometimes; admin wants to create different temporary credential(key/secret/token) for different roles so that one role cannot read data that belongs to another role.So; our code should support pass in the URI when creates an XXXCredentialsProvider so that we can get user info(role) from the URI,Patch Available,Unresolved,,wujinhu,wujinhu,Fri; 5 Jan 2018 06:25:19 +0000,Wed; 10 Jan 2018 07:43:01 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15158
HADOOP-15159,Bug,Trivial,scripts,hadoop_connect_to_hosts shouldn't fail if both HADOOP_WORKERS and HADOOP_WORKERS_NAMES are defined,If a user provides a value for HADOOP_WORKERS in hadoop-env.sh; hadoop_connect_to_hosts shouldn't fail when the sbin/start and sbin/stop commands are used.  Instead; it should just use the HADOOP_WORKERS_NAMES value (probably with no warning; since it is a fairly common thing to do).,Open,Unresolved,,Allen Wittenauer,gehaijiang,Fri; 5 Jan 2018 14:20:42 +0000,Sat; 6 Jan 2018 05:10:38 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15159
HADOOP-15160,Bug,Minor,documentation,Confusing text in http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Compatibility.html,The text in wire formats; policy; is confusing.First; there are two subsections with the same heading:The following changes to a .proto file SHALL be considered incompatible:The following changes to a .proto file SHALL be considered incompatible:Second; one of the items listed under the first of those two headings seems like it is a compatible change; not an incompatible change:Delete an optional field as long as the optional field has reasonable defaults to allow deletions,Open,Unresolved,,Unassigned,Jim Showalter,Fri; 5 Jan 2018 19:16:35 +0000,Tue; 9 Jan 2018 19:22:28 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15160
HADOOP-15161,Sub-task,Major,,s3a: Stream and common statistics missing from metrics,"Input stream statistics aren't being passed through to metrics once merged. Also; the following ""common statistics"" are not being incremented or tracked by metrics: Most of those make sense; but we can easily add OP_CREATE (and it's non-recursive cousin); OP_DELETE; OP_OPEN.",Resolved,Fixed,,Sean Mackrory,Sean Mackrory,Fri; 5 Jan 2018 23:20:52 +0000,Tue; 9 Jan 2018 19:07:05 +0000,Tue; 9 Jan 2018 18:48:55 +0000,,3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15161
HADOOP-15162,Bug,Major,security,UserGroupInformation.createRemoteUser hardcode authentication method to SIMPLE,UserGroupInformation.createRemoteUser(String user) is hard coded Authentication method to SIMPLE by HADOOP-10683.  This by passed proxyuser ACL check; isSecurityEnabled check; and allow caller to impersonate as anyone.  This method could be abused in the main code base; which can cause part of Hadoop to become insecure without proxyuser check for both SIMPLE or Kerberos enabled environment.,Resolved,Not A Problem,,Unassigned,Eric Yang,Sat; 6 Jan 2018 00:12:12 +0000,Wed; 10 Jan 2018 18:51:00 +0000,Wed; 10 Jan 2018 18:51:00 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15162
HADOOP-15163,Bug,Minor,documentation;fs/s3,Fix S3ACommitter documentation,The current version of the documentation uses fs.s3a.committer.tmp.path instead of fs.s3a.committer.staging.tmp.path.,Resolved,Fixed,,Alessandro Andrioni,Alessandro Andrioni,Mon; 8 Jan 2018 19:27:01 +0000,Wed; 10 Jan 2018 16:30:50 +0000,Wed; 10 Jan 2018 15:39:58 +0000,,3.1.0,,,,https://issues.apache.org/jira/browse/HADOOP-15163
HADOOP-15164,New Feature,Major,,DataNode Replica Trash,DataNode Replica Trash will allow administrators to recover from a recent delete request that resulted in catastrophic loss of user data. This is achieved by placing all invalidated blocks in a replica trash on the datanode before completely purging them from the system. The design doc is attached here.,Resolved,Duplicate,NULL,Hanisha Koneru,Hanisha Koneru,Mon; 8 Jan 2018 22:31:52 +0000,Mon; 8 Jan 2018 22:35:16 +0000,Mon; 8 Jan 2018 22:35:16 +0000,,,,,,https://issues.apache.org/jira/browse/HADOOP-15164
YARN-7735,Bug,Minor,documentation,Fix typo in YARN documentation,"The link of ""YARN Federation"" is wrong.",Resolved,Fixed,,Takanobu Asanuma,Takanobu Asanuma,Thu; 11 Jan 2018 05:59:23 +0000,Thu; 11 Jan 2018 07:18:55 +0000,Thu; 11 Jan 2018 07:02:01 +0000,,,,,,https://issues.apache.org/jira/browse/YARN-7735
HADOOP-15166,Bug,Major,,CLI MiniCluster fails with ClassNotFoundException o.a.h.yarn.server.timelineservice.collector.TimelineCollectorManager,Following CLIMiniCluster.md.vm to start minicluster fails due to:,Patch Available,Unresolved,,Gera Shegalov,Gera Shegalov,Thu; 11 Jan 2018 08:37:11 +0000,Thu; 11 Jan 2018 23:55:41 +0000,,,3.0.0,,,,https://issues.apache.org/jira/browse/HADOOP-15166
HADOOP-15167,Bug,Major,viewfs,[viewfs] ls will fail when user doesn't exist,Have Secure federated cluster with atleast two nameservicesConfigure viewfs related configs  When we run the ls cmd in HDFS client ;we will call the method: org.apache.hadoop.fs.viewfs.ViewFileSystem.InternalDirOfViewFs#getFileStatus it will try to get the group of the kerberos user. If the node has not this user; it fails. Throws the following and exits.UserGroupInformation#getPrimaryGroupName,Patch Available,Unresolved,,Brahma Reddy Battula,Brahma Reddy Battula,Thu; 11 Jan 2018 15:10:47 +0000,Sat; 13 Jan 2018 00:26:19 +0000,,,2.7.0,,,,https://issues.apache.org/jira/browse/HADOOP-15167
HADOOP-15168,Bug,Major,,Add kdiag and HadoopKerberosName tools to hadoop command,,Open,Unresolved,,Bharat Viswanadham,Bharat Viswanadham,Thu; 11 Jan 2018 18:41:48 +0000,Thu; 11 Jan 2018 18:41:48 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15168
HADOOP-15169,Bug,Major,security,hadoop.ssl.enabled.protocols should be considered in httpserver2,"As of now hadoop.ssl.enabled.protocols"" will not take effect for all the http servers( only Datanodehttp server will use this config).",Patch Available,Unresolved,,Brahma Reddy Battula,Brahma Reddy Battula,Fri; 12 Jan 2018 09:36:54 +0000,Fri; 12 Jan 2018 20:11:59 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15169
HADOOP-15170,Improvement,Minor,util,Add symlink support to FileUtil#unTarUsingJava ,Now that JDK7 or later is required; we can leverage java.nio.Files.createSymbolicLink in FileUtil.unTarUsingJava to support archives that contain symbolic links.,Open,Unresolved,,Ajay Kumar,Jason Lowe,Fri; 12 Jan 2018 16:47:50 +0000,Fri; 12 Jan 2018 17:02:21 +0000,,,,,,,https://issues.apache.org/jira/browse/HADOOP-15170
HADOOP-15171,Bug,Critical,,Hadoop native ZLIB decompressor produces 0 bytes for some input,While reading some ORC file via direct buffers; Hive gets a 0-sized buffer for a particular compressed segment of the file. We narrowed it down to Hadoop native ZLIB codec; when the data is copied to heap-based buffer and the JDK Inflater is used; it produces correct output. Input is only 127 bytes so I can paste it here.All the other (many) blocks of the file are decompressed without problems by the same code. Hadoop version is based on 3.1 snapshot.The size of libhadoop.so is 824403 bytes; and libgplcompression is 78273 FWIW. Not sure how to extract versions from those.,Open,Unreso,,,,,,,,,,,,
